{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T06:32:47.406027Z",
     "start_time": "2018-03-01T06:32:42.929058Z"
    },
    "collapsed": true
   },
   "source": [
    "# 0. baseline re-preduce\n",
    "\n",
    "1. 延后第一次（越靠近后面的延缓带来的效果增益越小）的down sample确实会很有效，对于resnet18的效果甚至超过模型宽度(通道数)翻倍的效果; 同时使用实验证明这个性能提升是延缓down sample带来的，而不是卷积核的padding或kernel_size带来的。\n",
    "2. 一组合适的参数保证网络的充分训练和收敛（num_epoch, lr_period, lr_decay), 同时发现过大的weight_decay也会影响（降低）网络的收敛速度和结果\n",
    "```\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. import needed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T16:17:03.139079Z",
     "start_time": "2018-03-13T16:17:03.118774Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.insert(0, '../../utils')\n",
    "from netlib import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data loader, data argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T16:17:03.469645Z",
     "start_time": "2018-03-13T16:17:03.452895Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data loader\n",
    "\"\"\"\n",
    "def _transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def data_loader(batch_size, transform_train, transform_test=None, num_workers=0):\n",
    "    if transform_train is None:\n",
    "        transform_train = _transform_train\n",
    "    if transform_test is None:\n",
    "        transform_test = _transform_test\n",
    "        \n",
    "    # flag=1 mean 3 channel image\n",
    "    train_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=True, transform=transform_train)\n",
    "    test_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=False, transform=transform_test)\n",
    "\n",
    "    loader = gluon.data.DataLoader\n",
    "    train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep', num_workers=num_workers)\n",
    "    test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep', num_workers=num_workers)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T16:17:03.687488Z",
     "start_time": "2018-03-13T16:17:03.646075Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data argument\n",
    "\"\"\"\n",
    "def transform_train_DA1(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def transform_train_DA2(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "    \n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "    \n",
    "\n",
    "random_clip_rate = 0.3\n",
    "def transform_train_DA3(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "#                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "#                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "        \n",
    "    if random.random() > random_clip_rate:\n",
    "        im = im.clip(0, 1)\n",
    "    _aug = image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                   std=np.array([0.2023, 0.1994, 0.2010]),)\n",
    "    im = _aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 data aurgument: mixup\n",
    "1. mixup define\n",
    "2. mixup visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 mixup: define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T16:17:05.636603Z",
     "start_time": "2018-03-13T16:17:05.631599Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mixup(x1, y1, x2, y2, alpha, num_class):\n",
    "    y1 = nd.one_hot(y1, num_class)\n",
    "    y2 = nd.one_hot(y2, num_class)\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    x = lam * x1 + (1 - lam) * x2\n",
    "    y = lam * y1 + (1 - lam) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 mixup: visulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:44:07.847731Z",
     "start_time": "2018-03-09T14:44:07.001267Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from time import time\n",
    "batch_size = 32\n",
    "transform_train = _transform_test#transform_train_DA1\n",
    "train_data, test_data = data_loader(batch_size, transform_train)\n",
    "mixup_alpha = 1\n",
    "\n",
    "# for x1, y1 in train_data:\n",
    "#     for x2, y2 in mixup_train_data:\n",
    "#         data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "for x, y in train_data:\n",
    "    l = x.shape[0] / 2\n",
    "    data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:44:09.870996Z",
     "start_time": "2018-03-09T14:44:09.362270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAADYCAYAAACwTgnaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsvUmMbOl1JnbuGHNEzplvnopVpKqKokh1i+yW1HIDEizb\nsNG2YRhoGAa8MeyN3bZh2AsDRqPhRXtpwJv2oneGF73wzlITLaklihQHiVUka2C9qjcP+XKMOe7s\nxfnOPScqU1RlxgMNA/8HPGS8eyPu/e8/3f+c/zvf8aqqIgcHBwcHB4fLw///ugAODg4ODg7/f4d7\nmTo4ODg4OKwI9zJ1cHBwcHBYEe5l6uDg4ODgsCLcy9TBwcHBwWFFuJepg4ODg4PDinAvUwcHBwcH\nhxXhXqYODg4ODg4rwr1MHRwcHBwcVkR4kS8P1jaq3b1rS8fKsiQiIj/w6mOex+/oPCv4nK/nguDs\n+7s+7+GvEWWqqnLpFH+fr2EOme/zj0tzjTzja+RFcd5jnbl+fWzpwstFs8pR9T1RF3x/Pvb08f3D\nqqq2/9ob/wK0Or1qsLG99CyE6y7d/3OFq8wPStSfFbryvQp/+QmDUNukEUVEROSZCpFryPMtPWdZ\nLf3l73+uskx5paxLwlvniXDVXYI/NOKoPtXrtvic6VdyjU8+/ujy9d1qVoN+j85TBftFOmGNRqv+\nHMfxmR80Gg3+gOKmafqLCyL1i3Fkx4/UX1nmeqySfm3HYIBz0tbaxn4Q4Nv6/cK06ecR+Pz9KArq\nY+PxKRERPX7y4tL1TUQUxnEVt1rknzP+bDMUxfI8YPusHLN9sCr5oPZFPSdzkH0eHQs8JSamjUq5\ntylbhP6Y5doOWcqfpQ+EZq7T651T5yiab74v3yrtOMfzJdPJpet8fX2tunrtis4ZtgD2yOerzdSt\njDvbPjJOpb8FWrXanc+bsPGk3tJQlpt7n/sWLTVCPU7x4/PfB7aMcvDsN+Vannd2Tvngwy82p1zo\nZbq7d43+93/2f1Pp6cCbL+ZERNTqNOpjjYg/Hx6M+Fw7rs91e3wuMIWWySbERG5rIJkvcE7v2cVk\n6uuMW5+Tzp0meo2D/TGX53hUH5O+If3XC/T7oQws+0LBfJUXZ1+cScplXCyS+tg8yYiI6B/9F//W\nI7okBhvb9A//0T+hRaL3qrBAKYqsPpajLEXO59JEzy0WC3xfFxJxwN9vN/jhN9d79bk7N/aIaHmy\nkueaz7itp7NZfW6+SPFXJ5XZnI/JIoaIKC9ylJXLlpl1TVX/VNvAjzDhhTwq712/Wp/7e7/9VSIi\najTsiOW2+t3f/Obl67vfo3/4H/+DpbrSxZlZLMiaDx/efPNX6nPXr93k7xTad95440tERORhEfPo\n8YP6nCw8Pbv4S7iew7hJRERxs6n3rrj+JtNX9bE0mxARUeDrgiOOOkRElKT8LK1muz7X6/WJiMj3\ndcwORzxGAtR3VWjbDXprRES0t7NWH/ujP/4XRET0X/5X/+TS9U1EFLda9KVvfotaTW1LD/NLnmql\njIaoE/TLtQ2tEzk2m2u/TxOu/xn6ZVVqm64P+LdXdtbrY42Q625ja5OIiB48eVKfG4+nfB9jB1y/\nfoWIiJ7tH9bHXr04JiKim9fY4Bj0dJH14OkzLoenzzmZzFA2fs5uV8ehTD1pap4JL+uff+ePL13n\nV69dof/zX/xzKsxijLx86Z5ERMkCbYAxXJi2aDT4tZGacRKi75UY392BVtZszBeO47MvR9/nawWx\nXj/DPOZnWlcRKj8P9Z65fM/DNcxzynyemXVrGPJd/Rx93LRnlnEdNAIdQ4S5/p1f+ztfqL4v9DL1\nPI+CMKQUL1Aiogol6rR1sPr1zMAVmyy0otbXeJBvbnXrY7LKC2Q5YyaWZM4VkGY6gVcVXlp+jN+b\nF3mLjxUtbTgfDR3EWt0ZOklWv5x08pAVZmJm/IqWLfAo1EpPs7OrzurcddLFUJYVzWcpjWfa8cXa\nr0xPkKIXGHh5pu0ToS22ezr57O7woL15fYuIiDY3tC3abZ4ACrPilpW2vE8WibZFvXjJtNEWCR+b\njLQcRwdHREQ0Gh7j+uZFm/LnMNbu2OxwOUJM7jeubtTnGnjRZguzqDN1dFkURUmj8ZQKM/HWrWhM\nJzXyxPo7a9XZFa54UojOWjhybjEZ18eOX/BEHqEOuus6tuTHRakLt7KUSUX7t1irQSCeCbtER98v\n9J4+Jp/NjRtERHR08Lg+l6RDIiJKU73+Yq73XwVVWVE6L2k+1b7io3yDfr8+JhN4t8t14Zk6X8z5\nczLXZ5xM+Hpi1csinohoZ8D9f6eh/X6wwfeaYrFivTvirWk29BrzGY/58YmWO8U8d/zyhIiI7u7u\n1udOO7yQf/D0ZX0sDHlMJliQZuZFUZK84PQ5o2j1OYUvWhJ5po9753iLcNtGg8f+zLzUC0x3sbHs\nWy2umxJ90DPXJ58/52aOiNGePg6V5qXnFTJezBiSRaeZp2URJW3lLfV//ms9BxE8LAHqUV7aREQx\nibfG4Bwv6i+C2zN1cHBwcHBYEe5l6uDg4ODgsCIu5OYVZJma/GIqNyLdF2002VjegPtwPFIbvsL+\nnuGTUOCL6xKuVN+843Fs/+nz+tDaBrv8xL0dmL25Xo/dyLK3REQUNbg8g0GnPjadsZsqhOthOlG3\nVQKXRpqqGyAIZF+Dn7PMM3OOyyv7TbgIrQqPPIr8kIpMyyb7JnZPy4NLrxHysb01dV9d3eW6urGn\nbtJuhysuz3g/NZ9P6nN+m901fmhIY3AD1cQD424SV0u7rQ0aYc+82NFnWVzh+4+OD4iIqDRuI3Fz\nVYYkE8Dlm2I/uhnq9yv0uUcP9utjP/vgU1oZHpHn+xQZ9kQgJCBDsNPT/EHcgETqljaeYopAaqno\nPAIc12061TYgkFU8bLlN5+pKlL4mvAQiohhbDtaVmxXsio/gog9M+WUbxZKY4hBuOvyuMH63ssQz\nldaN+prcvBXXlWe2LeKmPI+WYXMTru6K63IfrlQis1fq2X09fp7t7QEREWXo6/VNiffIBR3sS392\nn7fHplN1gW+s897qbKLbG8NTJmAVhdZhC+Vu4i+Zc3ubvBf72aMX9TEP206buL4t46DLc1WjqVN0\nYea0S8NjolNV2S2ps9sVAbZSamKR5TNgbogNZ6HyMa/73GlL0rKWhfBqzOsGt4oqbMtlWleebK+Z\nd4S4wP1K71liPgrQd4yXl8S72/D1nn4pxFVs1fmGgAb+iN3uC33n5nVwcHBwcPil4kKWaVmWNJ/N\naDE3q1YSJp2uVLe32AoRItJiflCfq4rlsAw+CMYkVnKF2QhPwST9+IOf18eu37hDRES7N+8SEVEy\n0hWjkI00XIAoB9klM9aksJCLTAhOhh0LllpmVpZRxFUVYbWSmpWaMDKL/CwLdBWEgU9bgw5lxorL\nsBpsRnr9ASzN7Q1eaW+uq2WKfX7KUrVuhke8ql/AOm+1lHW4vgEr226+V8u099CwRkMw6axVKVx4\n31gKrSbOw7KnTNt/Lrwz0yXEOhlOeLU+m5/W5/yArY5Hj9Rb8emjh7QqfM+jditWLj+ZMCwbu+EL\nFZ8LHhpLNsTyuDQjS8hruTBrY/WQCNFkzaygA1i685q4YQvJf3JP+2botVEOJZmlBdebhMFExnNU\ngGls2bJJygSZ+ZwtMtP1KWwzOceGz0xmU3o9qKgoEopj45UIzhJtxJoYjWExmr7VG3B/nxtiZB/H\nhBhDpHPW8ZCf9aNCSVb3bjPxan2NrcSX42F9bjzjz/2+jish7fihttt0yg1VYm6YTHVeaiLaoWlI\nkGL99QfcNrnxhPX6PCaFeMXXUDb1pVERlUVFvjH7iprsZEJ5QOgrxZtSGjIQ2qLRNN+Hx0zY0zY6\nwi8lOsK0MazJGJZgbs6dwjMTVPrsfcxxlXZjKmBpihFcGs+PV3A9i7eAiCgDaa/CuaChZSzOCY2x\nhKkvAmeZOjg4ODg4rAj3MnVwcHBwcFgRF3LzVmVFSZosBRLnMNcPXykh4PpVdkesr7M76/TYEFzg\nIlvaSBaFnULciSZuD0yOyGwGCxFgAhfgfKYuZt9j91Onqy6vopQ4KksiCXF5xDiaiGwJIfWN663T\nZP9CE26dMtVrTRBnauMTX4OXl8LAo81+TK2GCS5H3Fe/rfUnngyvFNEGrW+pozy3ZCr+Qb/P7SQk\nFS64tIFV0/m84slZpZYlNR1RrSJDkkJxxZuT27BHid01cabynEdPeIvg1clxfe7LX/nb/NxGKGT3\npsYkXhae51EYeFQuSbWI6pMeEVEHX4aPVddBP/VNH37x7Bm+J2Qm09ewBdEzsdKdTWZuPazYtZ0m\n6r4sQcQKSr2GL1sahjQkhJASMXulb8qIx7N9QrZYPInF821b4LNxLRflayDDENd5HEcUmm2LHOPJ\nq7ROEl/iZbnMO9vqck2lqg2hhDwu32zGY6E0xJISLtqTqW4dTMZ8vSubPCaqmxojmqBu5nM7hrh/\n9jrqRpwveA58ech9NTZj6K03b/H1r6iQzhSkG0xFFHo6DoXg1DXx4Z2+bsdcGp5HvhcubcEIicq6\neSuMcZkaEtNXGm0IKJhYcRkf6YK/N1jT8bjR4+2n3Ii9LOYguskQMnNnD3P3zMSIeqjmLDF91pd5\nBltNhiAXgCQV2HlJtmCE/JRbW5IPBoElVTkCkoODg4ODwy8VFw6NqUqPMrPKm0PuL/IsmYFXhQOo\nHa1vKAV9MmQrcmEk74Qc4ePdXpkVcAwZwfWB0T+FylGF1XZhVunzGa9crPygKJdU9hjCd3zcyxKG\nSrBHLElKLFIJAUo8tRZykJfs6vc8VZyLIgg8Wu9FtDbQFWuATfbcWCvziUimyTPY0BVecVu1Egnz\nEGKMkGD4s5CHjJSikK0kRMZq7uKcDRGoNZDtF3GskmV4ZCwrLDsLQ6efQgbxkwcP+YAhqFRQV+mu\nqeUSmlCGVSA1o/9HPVjLDhZHWSuvGLIF6tbKCZZiyeIR0lw9KR/8xZ8TEVHnVL0J9+4xsa7RATmu\noe3fQXjBeHxUH3v/B3yNvb3r9bGbd/kaEchJviW+iHfILKVD9G8JBygN4SSKZVzaPr16/ybicVfk\nxZKUXZFJ+JweXIBcJP0tNKFb0wnXXbOlzyhKaxmkOK1F02pCTtOopHUgZ7fY/4wPzLR+owZf6+BY\nx9w44ToJI52X1jBHdURGcGbCv0DQ9JaIU3zddMZ9/fRQ+7Ao9wTmOV++UCLnZeERUeAF5Jkx7wmp\nMjHzbnPZtRaZ/iOerPHYzoEYk10ORdrdUPnPHG3XvnarPnZyyHKYI1jgoZW9gxUcmfaU6TkojUdT\ndMFhIUcm3jLH/Gu9qNLHvVq60nga4DWqDPEuu6D3xVmmDg4ODg4OK+JilqkHqv05qvubRgRbVlMi\nDt43vv5SNHGNP7zRYB/5IhWrVWn3DexlJkYEwY/4ez5WV3bFXGDlV5gylgjfKEsNivbgOJeFX8tQ\n0CMp/5KQM69mCqxmzMJdNSHN4iq6oL/9PJRlTvPZ0VI0ch2yYgyDCAH39T6wOSd7n9bSrLD8KmuR\nDHsOl7dNjLYSgY7ChBGJRToaaSjB+z/9kH9m9zCwBxeHvKq1QgfNFn/eMh4MKe/zl2whbO3pHlZV\nR3zryjiLjOjBqjjH8i7Nnk6J+qs1n404dvC5sBkire9a3NvolmbQvf3xT/+qPvbDH/+Af7fG46La\nNCEZsJBfHWpYkAS599q6T3XzKovrdyL+re3LGcbLxOgYiKUXw2OQGk6AL33f7MmGr0kntqqIkiRf\nEukXq35mQku6A4T/wLoYjrW9pY2aVjgG/SyDh6NhdHVvXOd9yyzRPdMHn7Hox16LrzUaq4Zuhr3E\ndku5C3OMnabpx2sQZpihPcS7QkTkwQNmxWoGEJRYTLidjw50DDVbfKzb1TaVfcZV4JFHoR/RYm4s\n0/CsPrnkVpDQwdiIsoiudmzCgtpIpLDV5zoY7j/T62MfcjpSXk2jw88u825k2v94Cp7HVOeZuIQO\ne6T3rDPbYM6PSdtfjM/MeAg9eSdgPE5T9RDFeEdERnjHhvJ8ETjL1MHBwcHBYUW4l6mDg4ODg8OK\nuHAKtigK62TBcoyIqNtXVRfRqBW34Hy2OHMuNuEYokz0BDn/Ol2TexGp2hKj3bgOt3AT11jE6n6K\nEV4RGnNdUqTZlYMQcvz4bBVI8mbrWg4DIX6we2liaN4SEmNp1WF0YW7XGRRFTifDA9pYUxdn3IIr\n1Mi8hqKtiXIsEUXq2BXjyhV+EGokMGFKdVox4+scn7L76cVL1sIdjZQoIW74uKEU/g9/xmpVY+vm\nguKK0P/7PUuq4XPrg0F97O5tzgt6B2pXqfFr1x5L62ovTXjPJVEUGZ2OXlKWWV1auHStMgrUhUqQ\nFoZjJf6cjOBqt0QGuMljEFn8UE8mGdflblf7yz5Ufp49YFfj+JX23BE0fK/sbNbHru6xes+t62/o\nsW3OqVkhXZ5pCvIl9styiGpXf3DmeaXvCNmQiKgqbZ+5PHzfp267Q5nRn64TTBuSWhhLai/+fyPS\nOWKBcqXGdfliwmSddoe/FzV1q6kmTZoJ4fAYyk8Rd66NDXWvNn1R6dFB1yQu70bfpH9s8ueTCbsz\nrd7sBtK+Nde1HCeYZ6YzVmSKrApUeDbVY9jSOfayqKikvJxTGeoWiYex3jAu6FrtDPf3TXvnUBLK\nTXxbBy7wxXSKc2bOh+JQbkiKZc7P3uxwvWQmTKkDffCJCUWq8woHljSEa4kCXWU14xEOadS0xN0s\nburQvMdibBF455EPvyCcZerg4ODg4LAiLmE+eUtZXdYGvIJrmJ11sQpFh3f/pWajTxFhffX6Xn1M\nrL2DA15NDgZ363MZ9Cp3dvT7V6/DEmjxqnNhNBRDCU0wpoEE2TcNyai2fvFTu2qS7B7JQldBYg3J\n92zy7Nr4M+ET1jK+LMIwpK3NLY1sJiK/hEalodjXRB9JoGtEASRcxuodC0FJVs6+yf4ge+6WcHN8\nxO334AGHDUjYBRFRty0aorri/vv/5u8REdHhiRIO/viP/pSvi3L/2q9/oz63s8tW1s8//KQ+1kCy\n+d/9Pb7WZK6ktByW9O7mnfpYu8MejH9O/wddFlmW0ouXj5faVqrNaqpW6FsJSDpHR5q9po9MJJVZ\ntUvi9LDBVmLgq0k4HXHf3zQr6DaC1lsTrtNJqpbyBhJmv3VHrdBOi7WwBz0lcE0nEHxA2EgQqAUV\nxVy3nbZad9JPRMs3NH2ohTaeLVQ4o91+DTqxevc6GxIRUYy67nSNhjG8XBMItlhCkXhaCuORkbln\nPOJ+Y4Vd8hnmoG19/k4bJKBX3JaBr56WHhKLByZ5dwe6ybNTrZNZhkxHSBy+2dPrlxlbgmVoQggX\n/CxZwmUb9PX7MbIC7W3vmO+vnqnHI5/CoEkpqeU4n8mEoH2wBVGYDAnXCxM2k8ECbDS1jryE54jp\nJz8hIqK81LaLd/lzYbxHCTxUPubksqF9d3jKlrpNSF7BJREay7TAuGrAu7gktgKyUWBIkIVkIYJH\nKfTt6w+/Nd+/qKnpLFMHBwcHB4cV4V6mDg4ODg4OK+KCKdgqWizSOn6USF2nR0eqGNIfwE2D9EdT\nE9A2nbKrYntXNSqfP+OYOdnAl3hJ/i3ShHVMDKJsysMn2TAkIkkP5Bt2RR0rWKjbwPNjOckwMVY5\nyBBzk7A5EjdmJWUwikK5JK612r+va53iUW7UW+Iu11FqNtsrSdUl8aCG+DAaMrFiPtU2mEEtqAll\nnTu3bmi5xdthPCYJFEwkNVbXEIXE5R+aNG5XbrJbp7ulJJngu3/B30P7XLt+rT537foWERENjaqP\nuGz6gx2UVV1c4rIetNTV2H8N5Iwg8GjQaVJpAnWlq9uw4bKUfgeN447eu99hN2xg40zxV5Ipz4av\n6nOex8/1UaJjav3Gl/n6ISeSLh8+rM/95jd/m4iIBiYB/ONPPiIiotyo9oyG7HZLQeKIDOmpja7f\nNSQweU7RZ/XMOrsDDeTRxJRxQ118q6GiigrqtI3urC8a00ZrGPsssQ9yyqkSaEKQCfsDnSMKbA8t\nRCnM5LErK67z032tr9mQx/oQbuGGiU/sQ5e7SLQPimt2Olc3b4FtoQZIgtcNSUx2Uk7MNY6O+LfN\niPuPTVZeq5nZ7RlaHWVVsbvY6AD7FdSlrKB4nTaN/xZma0Lm/DzX9pk+e0BERM//9bf5/ybF250v\nQ4Eu02OniI2fbV8hIqL17b9dn4saGENm6+MEakv+1Ki7gUBUQims07YMKtEyM9q80Knu4D1T2XEu\nn+04tyzPLwBnmTo4ODg4OKyIi2eNWSRL5JTJhK0cIyZBO7tsMYjOZ9sQHRZIHnt6qlRooapvwpJZ\nsgxwqyDWlWuGVVtQioanrkiESu6bMJVaN9YIL0q4gpCTbOLjAhZsMjfZbrxlQlFk7lkUZ1VyXgeq\nqqI0Tcir9NmfwYr3W3r/MSzNMUIq7Ipru8/klK01teKe4rmmoLHbBan8trLrYJwXSvl7779fn/Kg\nHPL2O+/Uxx4946TLaWGyy0i2bGz6n460w2xBp/neDQ0BOjmCRwLZVJK5WiI+CECeSR5cXHAVeR4C\n36e1Xms5Y46sbA3hS/SLJdxnbU0t0709XmlXqQ1z4O/NYHnPZtr3dwb8DB8/NLqvyLKUVTxu8kgt\nlqMRf29hMgOFFbdjOVai3/j4KRERxX32ACXGKghA9jGSv/VzSkiM7e+iCiMZW4iImq3VQ5HkvkEY\nLBEYZyCbTcbaR8QKkSTilXGdzKGBGxrru9YfRv/sto1Fg3Ck8aFalfMxW1ky95yMNPTtWo4xtK4e\nmXSf67oz0LafwJptNbiM3ZaSpCjm+yfGS1eiHXyQkmYm3G485jo4OtIwtEW2eh+vypKSZEGtSMtW\nwBtRGoKV6KSHsMqnhkx4hJAt2lfinbfP5NHjU7Yqn8y0L04QspSf6LP4TW6ztY0nRET0vKPf/8Z/\n8p8TEVFgxnyG8DxLpMwwd4/H0OE1eu8dzI+BIaVl8HSUCZ9rdc56s7LMei+dApKDg4ODg8MvFRez\nTKmiPM9ry43ICAWUeqknj3lPqHGXLY2d3a36nFiAx2aPVSKxZc/v6FhXQT6sm9Ox6lZG2B9rImrX\n7k9KaIfVm5U9WLuaTbGvKPsyNsekZCFYzDQco419ENHBjQdqbUcR6PfmGsVrslJ936fDoa6gf/Aj\npp7/2td+tT42OZVAadzTtOq4yavdvatq9b3b5dCWMZ5v3ay4hZK/yKymJf+dQyv1509VF1bq9I17\nX6qPyeruyWPVN71+jfdlJSj68aPH9bkSlP+GWS3nFa8eT6E2kJLuz9QiE5YKH66+o+T7PrWbXSIj\nYiF3yI0urVimBQLZrYBAC2EnVr5arJ1Pf/4BERH1PO3Lb9/lennyXFft3/kx6/SWXW6zXk/5BZ/c\nv09ERG9dU2/FV2+zNbxhFtr7zzjM6PYWj73cM5YptIRt5hCxSGV8eoENEeBjkhuUaDkL0Urw2Apu\nt7XtxcMzm9r8rAXKhT1dYzUU4BRMxmrZDWAxCg9jMjF7mwUEF3qqtRv43GeLkr83NRyDJy8OUVTd\np86xPxoGRg0DvWV9iwVHPjX9P43hTQmtfiyEAuBpS8weZJrysaEZ+0n6GpIke0RBQLQodG6TPeeq\n0jpNIFwyPuFn/4vvqjdq8YrL9K7pPxV0gyuEbhWbul/8DGE1R8mT+lgT2azeDLl/Ng1nYIF2t31C\nJnbrdZMwO/FUZmN9L02OeYzln/2sPhYc8Hslh6bw5q9/vT63eece38bYlxedwp1l6uDg4ODgsCLc\ny9TBwcHBwWFFXFibN47CWo+XryCb7upjakEHU4g5VCqRIAVTaTYziWXh3pVErou5CYMQ8994laZz\nSRTM7qrIpIMaIpyl3dbySNJYm2IoweZ/AHdRbAlOAVwWJm1VBxqfVcAhAaH5vqgd9YwCzWg8olVR\nliXN5/Ol5N13bnFIyc2eEoqaMT7DFVkYctQYcRCloZl3W+yKuXObXYy+SV3lN9j9MlmoK3Ix4fpu\nwj34pVu363NzqL30jDbvt77B7pNvvKtul1YDZBq4gEPTh3yP2+rBU3XTzRd83R+8x3rN3Y5J99Tm\nMgak/WR97TW4HauSynK0lB5QwsCWXD4gPlWyFjVhFxImFRjC0n24mg6fs4t286a61feg7PXv/M7f\nqo/tbjLh5c/fZ1d4aBI139vj9v+Nr6gb7eZVDh86PdU2e3Gf3by9bXZf3X1T3fBCeGk0dDwEeJZC\niCdGO3uBMZUYTeaWfwnxtHNQFCWNRlNKU6PlCldur6Puc0m6nmGOsGS/EOSeVsv0EYTzSFJxv9Ix\n8fIVP8fLF+rq7CHVWQrlqomZg0QaudVS2yPG7WeGINTcZrd8knB5Xg21PzeRODybarmn+OwjPKow\n6jsyPxWGceO/pjqvSo/IqqRh68Wm3ZuCAPXjn/DWxOZNTey9tsPPGX76kZatyS7zSYf7bmdtoz5X\nTPha75g+GPV4DMRQJcp/rupnzz/mFI677365PraYcz1nJrQoB5mrGPFcW+4blzhU27ynD+tj22iD\nFMy74UDn0N033uTvm3npohtHzjJ1cHBwcHBYERda6vieR61WTG3P0v7ZIhgMdHN+bY1XlCVWHclC\nV/qysX9iSEatJv92NuHvZ0aTUSJcWqEhHECTceHzKqXMDbFowb/1jPakMPpLs3ZIMwmrwcZ2bkUe\nRJxAV1ciIqCJtbWMtY6pqRerzXpZlGVJ89mc5sZSOkx4FRZfV1LKtU0moJQLtiDCVJ9FftvoWZIM\ngp1RbWluzC6skqcT9RwcQivz3l1eWb791XfrczGS+vaNiIBk+mmYzDktBNsHOBYbXU8hELw7UQLG\nn3yfg8D/4A9+TERLmho0Lx+i4FrGv/ctXfVeHiV5ZVqHdBEZEQOjqRr6IJOA5Ha8ryv0+/Ai9Ptq\nfT64/x4REWVzDolZzEwCYnz/K/eUIHbrNmtPv/0ur7RTXzOYDGAddUol8ElomF1Jlx7XzSef/iUR\nEd14462m+06aAAAgAElEQVT6XDPkvpAZz4voHZcI5o9MXz58yVbD7ECth6ih5J1VUJUVpUlOhi9I\nYhT3e4ZRVYoVJ54CfdqozhRl5iVcMAfRsDBjYjITMV9jCSY8Hy1m0rbaRrOEf/vihVr+X77Gc8Nt\nI3iSdrhOZvBelSbh9avDGe5jxibuJQINnp0/8NdaSr7/GghIFZFXFlQYz8kUAhiJGU+vQLpqwAP3\npTdVC5o+ZDLSwsyBUYOtvKcIA1vSmp5JRh715pUjzNMIZ2lXRszir7jPZicaLnP0kL00ldF5pxOe\nlwIIOpDxdhJCYgoj2vAS5K/Gr/CzxE3tX7kQY09n5tjF6ttZpg4ODg4ODivCvUwdHBwcHBxWxIV3\ntMuyrBPXEhGF8MOWJoXZHAomDWg42hhOIRrMTUqtAO6QOYhFmYlxXNtgUk9q9GZDxC0tcv5+wyYR\nBkkmNdcQkz9uKqlCODpCiJnmGkM3nfJ158bd7E+kvELQsLqh+Gu8AqkhpVwWvudTHLeomav7Yr3H\nLr/EqH0cQE2qKLhMLSNtI6nMoqa6eYMm2qUmiBmSA8hc7ba6Yd/5GpNjYrhwXr1Sd1cfZLOWIT2l\n0MgMexonlqAPtPDX6gdL7OnOmt6zEbDb5/2fMgEiK42LCDGBlYnLu3JNY5kvC488inyfbNZx0R62\nUsthwP1JPIFRaDRKJxxj/eCzD+tjz5+xxm4DbrrpRJ9T+lqzq+S1/gYTit78CrvyU0/dvMWE6z7Z\nV33fOeIrM9PnQigFHR5x2rz9Fz+vz129+SY+mThTjMGq5GtkhhA0m7DSzWKscYJhuHr/FnjkUWT0\nuMWVm8zV5dbA1oSPwdYxCkwtpGJMDDklmXH5K7iDw1Cvv7HB7thmpFsTU2hYn4K40jCZskvE2U5G\n2s7FFrsIe2s6rrIWXy/y+W/nVOeUF4d8fdF1JiIqZJ9FZHjPcSvatI7Va1Dn9X2PGu2ICkOwknr7\nwz/4dn2s1+Btihae87t/8i/rc7dBBhqMDYl0zvXrzVkJqUy17fwD3q5IzDwtKRPX+hjXEyVyDY8Q\nt2/idIdPWdGrZ+J0Q8QQ0zra8ZqmqwuQYjMzSnjBNd4+CW7w36nZ2ntxwC5jmwrQKSA5ODg4ODj8\nknEhyzTLMzo43l/SpQ1A147MqmN9HVlDerwCTxJdwRQVrxhbS+oWvEKQxNvTua6Kw6msLEzy8UhW\nD6DLGytHSrG0qsAqrGWSLMvpVJQ7jGV9fMirq5NjpVp7wQE+gIhkiAFxIGWzqkurh2pEUURXrlyh\nwuhFvoUQJK/SFVoB+n8YSTYTo7uJ1dfJWD0BUYpQIWTHaRuLfRf0/sysgtevcjjGGHq69z9TIsqX\nkSj85VBX4ekJk2Ou3dXk3RHIIRLF5Jt1nIQZhS1dhXtQZekMuGxZrt/vguxg2zhumowRl4TnEUW+\nZxN11MnmbXsKuaX0pc8YchLKnZq+Nhpz3fRg7VhC3tEhE1+mE22ftSH/ttnn8Jf5TPvhMTKNJCfP\n6mOSt/zUaJlWGFMJyB/Pnz6qz12/9RZKrcM/gBavkO9805eFjNPdUOt/+hpCv4iYYBP4HuVGVU2s\nSKtiVkH3VjKJLEyibEn8bUPfRHWqCX3cbket0C5CjfaffKoFwW/7Eo7jmUxN6LuDhlqhA1hDibHu\nE1hjFULNNvp6z0bM7ZaVxuKFJ6moJDH7ebaN8RoZUs9lUXkV5X5lSk3URNjQ3Z6GW43RZ+kl98/m\nVOs2xiAOF6bNWvy584z7ZWxCl16M+Bq9jnqXeiA9Jmi7/RP1dkn2l5NM3wMnCHVsfUOV3ypk5Wmv\nMxksamn7SJhRZCz7NjKP5aj31JQ/y8/KHV3QMHWWqYODg4ODw6q4kGVaFDmdjA7rzCxEah1EJr9n\nJeEVyJwRmrCCZpu/10p130go4sKPtxbHbC7hMrrqXsA/78OiWix0BZMhj2Hc0Ecrsc9i13VtrH5q\nS7bUVcpojBCGxASSwxqWbDS58bcT9nts1oLSX30VGfg+DVpt6q2ppTlAuE7ga/3leGZZjZdmNbvA\nfkhmLG8qJO8p/25uRDVGQ8nYMTfHuB4yaHcezbReUmgV20VzDAGHzsBon8JSCmABWO3aDJk9FmY/\nqbXGq86r1znsZ2rCZlqeiHDoNaxn5NLwkNTGeB0CWCWBCVuoowoKCXbXss1hYR6dqqW5j2wZOfpc\nThrWVGDlXJiMRk8fs+Uf+LzfOTPB9DNkf+k3zPPC+lrMdN9J+qKMrRcvXtTnJPQsis/q4dZZckwW\nngrPnhTa5/zX4HkhYuuz04opMXu06QLhL9Yb0eGxLnv5c2NVpLJ3bjSVRWyjgNfKesckLCuINDQi\nnbM3JUK95iZDSxPj6cZVDXeK+vzbk4Xec5xwm2cYTyMTHteCcExo+rhscZcF13nTeIikPWw5wmh1\n7wvlJdHxjBafPqgPLR7y/vu7hfazDPN0CM31cU+fMztGeJ7xXgmPZopwmdaV6/W5rW3eyzzu6vPN\n17guh8+53qcmTKnd5nseT82eLKzQpyZ0ZY556cpd7se9Nb3G0V9ySJ1nQoD6A+YezCEc1Lp5tT63\nuc1el+qcZ/qicJapg4ODg4PDinAvUwcHBwcHhxVxITdvWZU0T6bk+bqRLC7GzLp5h1ArGfPfrjHv\nhRxTGnPahytNqPBzQy4QtyAVeux0CNMd9P/SEHQIrsiooeXpdNmtkxrqfAkigJBvfJNEVshRsdH8\n7UHhKQTRZWZo8j5UTUalutmOpkrIuSwq4qTXo6m6DLFfT32jztQEQaTX4HKH1sVd8qZ8YVy/Bcgc\nkrKtMJvvPWj5No0XzwNdfAblqW7XuNOgzRwYNZHdLXbh9LvqlpKwGr/g3xa+Sdpbq9vodUdd7mMR\nlJOs4k0WynPY5OOvwQXmEXkRUWVcTnkpWrCG5CbJ0rF9kKXarxYlu6amE3VHCdFhhuTVeWxc7lA0\nWttW8oeHUKfnzznV3Wyuz95usdt7a0ddxZWE6oxVFUmIZzNoJ6cmzEva20SL1GpOJCEyZp3d6bIL\nbJIoiWl3+xq9Dvg+UbcTUNNoO+dwUUdGhaiJ8kwP2cXoG4JLC33LaiqLG7qDsdA2/bmBvrdzV9WL\n8hG0t6FvHdgwlQXf8/o1DV8qcM+jsXHxZyBvRcspJYmIenBd2nA7D9sVC4T69Xrah5sg5Q0Nsc+O\n68uiyDI6ff6cTp4p4Sc74s9erqSyCG7dBPNGZV3oCOMqjD74AltdEoKX7Gnd7u6wm7dlSF0Sznjn\nHfTF57oNcfCMtzfumGFyOOI58NQoIDXQZ2dzLnfS0RCy+REIo2b7pAJpzoMm70ZLCWLiVq/MPJml\nLjTGwcHBwcHhl4qLEZDKgkaTCSVmJS7WTRToqmoKS6ryJGmvWrJ9iA6UZuVXB1R7smI2SWpT0d9V\ngkIJi8TD5nhlKNTCbAlDvWcFKrRvrCGh3Tdhzc1N4mMJ44gNqaXbQyJfrEhnE60DAvGna4QRivD1\nEDSIvCURiwnq9sSsiCtYSF0INLRbSixpgPjQjLWpGyAyiDHSMmboGizwpiFsSBD9ouBrfC18uz6X\n+SCBVVrGf/Wn3+HrGjGDv/tbv0VERJubCKw2lHUfq0cbbtREvEcDQdplYcgISNkRxNo+kbd6fXsV\nWyRLsfPoTzZRkpB0NIjGPIvE4RurRDgQQrqygh5DkN06fbXsS/Q/IdiVifZND6FlVjs5qaD7urCe\nF4Rw4WEaRgvZP4fzX2vewkMTxlqerSvIGPKhClFcuasZPVZB4BN1WgV1W3q/BUhwXZMs/sYVZGRZ\nCKFO+9sMHqeJEYKR7/Va3Nc3unqt7U0eJ9bQ63oIr5CQIuNZGA7ZQ+CrIUNH0Fe2XolOly0eycLk\nFzovxW2ej0YjbbcZLNL1PrdNt6NlbCKMKg5tyJB6+C6LWZrS+48f0/d/pnrSf+sOt29jRzPDTPEM\nImlstbFjeItyk1z9BMS77h73u63d2/W5H733UyIi+osf/Xl9LIAH4Ftf/xoREf3mQD0zvRZ7tnZb\nSvjanbFX5GlDvX/S2ukBe2SqRC1fD56OZGGEP0DavPIrXyUios5tDd0rQMq0WsjFBUUynGXq4ODg\n4OCwItzL1MHBwcHBYUVczM2b53R8dLSkgCSfPRM4GMO1KKF5yUxdXuMhSBgmTrN2oYJIERqXlOhi\nzjN1GaYgwvjEvxu0TXwWrtszxxbYoPZMfGxRcOHmOFcZQoMnbjaz1FiAHFVvsBuXWjEFYcIwOmyi\n8MuiKivKZgnFsbkWXD2eiRGU9EEfPuTYsWasz7mH+KmWSfbcgqalkDQaTb2Wn6FeFtqep3O4eRHz\nWyz0WgmCPT+5r6pI3/6XrONpcjXT82P+7b/37/4+ERFNn6lWLKHNqq6SqqqYtwNu3+Xk2dtXlHCz\nvs310e+qW/3qlrqEVgF7n61PVwgYRiO1jl2TuF7tO8KZkdhmIqIg4O93QKqaGkLZ0+fcdwIzpkKQ\nOUS3NzPkp2kKNaRDPTbJ2fV1PDakEriSw0jSk2lj1GP2nDA6H3GCvonPK9C/yBxrGLWZVRCHAd3c\nXaPQuOnbm9y+X39LXcn3rnJM4ME+67XmRvVsgW2cmYlFT2Y8Xhtwwccm7vt0zIo8No3iNvqSh/48\nLXSbKAjYnTk0BB1x1ZvdDZrPuF1L6Eqvbeu4jTEfbaxrH5d5TPpTbOIapW26RinuHJGeC2M6ndNf\n/Og9+skH79fHbm0jTnP9Sn1sDP1tD9sJPauIhr7dNqp0Y5D2+oi5zloP63Pf/ssfEBHRzJCvTuEW\nfvGE9Z7vvqNpHd++9w4RERWGIDSTbR8zTw+hvifztTfS9ulunp33QqTqnLzHLu66XxPRuAkSpBkn\nVm/+i8BZpg4ODg4ODiviYqExZUWzxYJisypM6wwa+l5eYFNe6O65WQIPF7AwjaJMCgJRA/RxzxB/\nApIsEUZ1CStrSYq8u2cyohCvYKyVOIdi0kj3omk+5E3rv3rvz4iIaO+GJpd++1d/ne9jVq4lVqI5\n9E89u0KC1drtKIliEK9OiFksFvTxzz+hr7z7jfqYD6JGs6s08OMTXml/5zu8Arx6TVeYX/4Kb7bH\ndoUmbYWVWWGW16N5gWsq6eX0eIivQ1t2bkKRcK2HD1VRZQKtYDLhMj/6q58REdHX3r3HZXj0Q30m\n6Az33v7t+lgL7f3bv8NanFY7U8IWFobkE8erhw0QVZSXxZKClGjUWk6Sj/7swwwNTAiHJHDuGsWY\n6gD1CytxbogSJaydzvFJfawHiv8MmtJ5pl6QRhdqUYbccgJLdzzVYwtYZh6YI7F5ghzemE7fhOPA\nS6Ea1donCvR9z5IGXwPhi4goigK6urNG3VjH8DtfYov0K7fv6RdBiJsdcQhFYqyGEuFLRyY0qB2D\nbLLOpKB+V62crOLxcTxUzWMfc5aP67bbalWejvi5j/fV8mmCYNhqaD0dT3jMRNCR3RoYj08kpEbt\np9019rYcQwd3iSwDD1S7peXICtsLLwff96gZx3Tr1u362AITY/BXH9TH+tDhjlK+59WWzjfNlMvb\ntERHqJgN2lzfP5poxhcPnrIv3XmzPhagPz7/lOeFJy81C9KdAX9O5lrfzxMuY9MoX0UJe2QWlXgt\nTXL1fSaNlbF6GCKExIg61uyTz+pzon/t97W+vZb+9ovAWaYODg4ODg4rwr1MHRwcHBwcVsQFFZAq\nWiQFFeZXkrrJeBFJ+C851G4y4x6K4U5qhYa0AYJFvoAKRWGUc4TckaurS67XBKEoDgxZImN3UDIx\nG+YDNtePT9Wt0+vwQ7z1BgsyPztUZY2XcBH4lbrNukjf0xAh7FLdXLVKyEIJEOnhxTavz8NkMqM/\n/c6PqDNQQeYbd7kcE+Pu+PjTh0RE9BSukrkRx36xz+7DnR1NnyWuSA/uXavuEyBm73Sk8Vzf++GP\niIgorZMIGNcJkoM/f/m8Plbgeqlxxb06ZBfchz9n18qvv6FudUmc0NlUElGKNh6f8u9OoApERHSE\nFHnPjdvt1YHx4a8Cz1uKNZPPVjBb3Lzn5WiS7+/saQzePsq+gBrRwrj6crTF/itDbhlw35rj+7mJ\no/Y8xPXODakPbZYkRiUKZI+tPtdtN1K3/aOfc7zfYEOTKW/t8jjo9lm5pmHSjaVwt9k4xzh8PQSk\nwPdpvdOme9du18feuMFlkDFHRHQ847El/WJq4jsPRrwNcWCScd/cY7dkdw3qO03ts000W2ZIk8ND\nHjvifqxMSjjpn1Wu3+/CBVhasmTEZYohxt9uG1ImtqQ6xjW6gcTiUYVnM1tlCVTdUk/7ShxfzO14\nHgLPp16jRbtvaIzl6Sue+x4bN7MoB/ko98tKz4m63LohQYZNru8OXNefHavb9upV7lvXd1QxyUfc\neDPjue0vn6tb+Og+u37/7ld/rT62/sZtIiI6ePw9LaOkgkNcsvWCC5HM7MBQlvBceIAXWNZT13+w\nxnWbTLTd84lTQHJwcHBwcPil4mKWaVHSdDKhlknEHHn8Jm+aVbooGCVQCTJGFF3d4tXJzkCv8RS6\njFMkqS1DXZH2oVBUJWopnSZiLWzjIZToIuJJHz3UzeXdHaQtM2uH9QHfox3wNY6mutI9PODyhMZC\nEX1UD6nYktRqOEKBxYTvVMPVyQJFUdDpcEwvnqvVt3OVV+2FsfYPYKltbPFzWnr3831+lsFACQQB\nLCMRIcoNmSpM+BnmhmT24cccxnL/U06mHBllmibIFrOZSQAPYlDmq0VVYKX4ve8zyWE0/IqWJ+Rr\njP/sT/WZjvmZD/bZ6hgea3lu3WJrY2xWkU+M1ugq8DyiyjYdukBgCDdifVZQ4rJavgXSWFkVqltX\n2Svw6oAt1DTRfjKG7vFoqs93coqUd/Cy+CZkIj6V9GRangVCQkRXloioDz3qHYSZdIzOdDXnchw+\nUcLO8IAJZP0B96/N3Zv1udFsn597qtbG+FCTk68C3/OpFTVpd0Mt+Saet8i1Tx0cseVyPOF5IDPW\n+hxWnGfMkCtX2OLZ3OLxbdMjSmLxLDPhTiAuehg7nlF0k1CzqK2W1cMjLkdh5j1J/7i2I6F+Z+eA\nliG1FGi3AQz+xBDqZvJTMwelhbbvZeF5zIP71XeUDPTTn/I9/uy9x3oveLcktKrZUq9EAYt+YNLm\n9UFmbCNkamIIbHtrXG9ffUs9bPsvnhIR0ctnfJ8HZu78cMgW5PPvaAjZTRCDwhP1IHbhDhU9XUuk\nlHEV9LR91prcBzYx370yydtHV7ldMhMuc9Ekms4ydXBwcHBwWBEXskx9v6JenFNEJiSh5NVD39eV\neIRsCBJOEpjtw36bV6BvvaUr3wgpHT55xquO/aGGCfgpn2saXd0ZrMgk5XtubN7V8kT8/acvn9bH\n1rZ5ZdRMTXYUZLLpNNmau2kSERcEUYNS1yZPX/Aq/njCVmBhkvxGomtqVjV5ubplGscR3bp+pd7H\nISIanXA5fKNl6iM58q0bvPI7MfudB0dsTQz2dRU26PPepATv52ZvM0a9vP/+T+pjjx5zYLVo0uZW\nQGPOx1Kj5dvG/tSVK7on9wL70I8f89/9Z1p/jRa338RYG4v8ENfHvu5CV/T/0b//TSIi+vAjDcf5\n9EN95svDo8ALdU+UdH/Uhr+IlS26t7mnVk+CMJY01b4QQVykEbC1eGDCYA6HXG+ttu69vTrmehij\nn4eGkFBlbFXeubJXH5OwocxYctvr3D9kvy9NTPYMJK+24UQBrJ7x4cdERDQ71fFTVvx9z2RC+vkP\n/xW9Dvi+T81mk3om1KuBvcGZ6VMPHj8kIqLhCFrGRrBDPCutth4T0ZQSY9OKvswQnucHOmd1ESYU\nwzItjZ0RIqxlba71dfrsPb6W0fRuIpxLwnJagY65AuE7o5Hu/XcQfpOCa1EkVoeXy9YMrIDIa9Cf\n9ohasU/jY+PJgdDC1CTe7rbRHgH01Y0WtGzjjk60H/fbPP7mDx4SEdHmtiYHv3ud5/rIiObs7vBc\ndb/LHgeb+WuCerg/07r6BMntJyZMbACxl2vXeCwERmz5vVfcf9uZzhvfus3WeAu8g/ZtFdB4683b\nfP2pyQZ2QVPTWaYODg4ODg4rwr1MHRwcHBwcVsSF3LxR4NHuWoNooRvDp0/Y1VamShpqY9N/0GVX\nUxWom2oQIfmup26n3/g6azG+9Q6b7fcfHdTnhk95U9zLDDUbSkmbG1DO6eljSDqy23dUBWhrl92N\njz5Vt2BQgb4O+v29u0ou8OD+sW6XU4Q3fPaA3ZS51SKGP8ALtBzD6WsI1agqojyl2VjdHSNswEdG\nG7iDdE1ra+y+jQ1dv0JoUWBILCnCjDK4nmwS43bCbpHDQyWnTKDs0oC7MU3UHSteqNK40e68ye6U\n3/zt36qP/eG3/x8iInr4iK87MSpAJ0hLVVhtUh8uO5TN5BWmD37KqcCeGWJWVarL6bLwPI8CP6DK\nUA9qV64hZIlbXNIIVsalHxCepdB16gzkOd/nc92O9pO0auCvlqMAoamJEAQv0Iefgs339EDbZ3eL\n3Ylv3FLX75UNHnttaLs2mlYBC2pHhmlVGK1sIqLCPO9swe09GxtCWXZErwNVWVKWzmkyVjd9MeP7\nPH2pCaM/fcRjV1IK5obgImkJWyZ0RFzE1YjrbjRSd+xsxnUoWrpERAHIK1d2ONVbaMayEFtKo0Ur\nbtjrN9VV2IEYdRMsyGxuUlVCpaeyWyq4haSBpFDdwh4IUZVxf1b+RSkx56OsfBrPdX7aP0XCdV+f\nb32D3d7bu7wtN09taCI/y0fHP6uPnRxxfT8/4mt99WuqtUvE9zo51nl9awuhepiD5nN1I4ta29wQ\nHRcgnY7H2u4vRjwXTnOew9dNGrfPoL98tav9/tBDKFKDj90LdM7/yo23iIhoZNy8iwuKITvL1MHB\nwcHBYUVcUJu3oNl0SLHJzjB8xklbs2e6YhiC/t3qYCVgMmIct9hqGj9QHch3f5X1VzevcnLab1xV\nmvy0yyuSp599XB/rtpHwuskr69Bmi8BKbndNBQByrHQp0dVVhZV3BD1d32yO+1iVHg+Vhj1AAPbV\nTegNm4V8CGLJeK6rmleHGoS/CryqpFcvNaD51QkTBwITVC+Eintf5nATu0Laf8lhDTbjdRuJipv4\na5NFz+a8gn/5Qq0+IRxJFgWbTSFHoLlvaOm72+yZuHVNSWbXr1wjIqJPP3uCB1OrwPN5xeqZoPhK\nEmjnUzyTWhbf/S5bppUhwtmsKJdFVVWUFwWdxx0rbXqQz2k2WDGDAGQkSbJNRNRqo6/FfGwwUAuq\ndcL97uGhPp8Im1zbRSC80Ud4/Jz71cSINjQbvILe3VBSWkcSVCOzhuHG0RRentRYn5L1SfpCaJJS\nz6Hzm6ZG3CO+WOLkvw55ntLBwTP6zLSfD/LWB59oZqH7jx4SEdE6wixspqMglETyStAZDyGUgXo4\nPFHBliGs1MTMBx0IEFyRJOSJjt8hwr5OTzU06O5t/t5g02jnIol7EMKbUWodNjCnNBpG5EGEaFDn\nNhOV50dLf4lUp3wVFGVF42lGZaCegCFEL27evlEfe/fXWJ+804VIzULvPRlzOccm0fnjD3h+vnWP\nxSCu31YC0jHEZJZCKiewhjGGxxPt/wOIt3T62sYFnHNX95SodgjvxMOnHKZ1ZAhRW8jYEzbN3Abt\n6hjdODEhgQcjPlcVJjl4cTFPgLNMHRwcHBwcVoR7mTo4ODg4OKyIC/nG5vMF/eTDjymYqYtgAFdJ\nz7jBShAWpoiJ9GyiZLhixp9qMunn32O9xQbUdNrrGp842OPPRapuwUXEbrLOgN0BH87VvG/1+Fi3\np8SAIyjPJHNDQoAF74mrKzbuF6Ree2nKmCLx7C3oaRaGgHQIYsbzE3UD2QTdl0VR5HRyekqJiVmc\nQ3UnNySZGUgp3XXegJ+n6m7+AXR1Gw11RTZAeOiAINY0hKUmiCpPHj2qj4UgPtRxtMY9koCMJKQW\nIqL5lPvHk4d6jQ2kPyLUbVkpgSUM2F0UGvdqJSn6QNohX/tciWOeIbH5pC6hlVCVVFmXLomOsR6R\neNsA7rk41DWpcESShbZBiL4SQFWnLPT6A5CR7kXqvorg2us2+KYtk5Kwe4Nd6JWJP+x3+fu+icGb\nzRCLDVJXYPq3+KmtO1seuSTR2tbrZ3DxpYYME2Srq/EQEaV5Sk9fPSbfuDA7UEILjdt2gHEtybUr\n8/0sl+0HPbb/islL8hSnhuAkur6xIbg0EDf+DMo8NtH4M7gprVtw9xpvWyzMvBSiAzdB+pqZcRj4\niG00aRqTOZKJQ8u6rAzpC6SqMNbvN4xW8WURhhGtbe/S9FS3sG5fY5dsa02vvxhyfY2Rnq0wZJwZ\nNKM3t3UrbeNX3yYiojnG4eMnqpA1OkKi8VD74Azz7gzu3crog/d73P539nbrY9NNzBtGGL4Lla+X\n34eustFSj7D1Fhg3bw9u45013kasTH2fIA1emer3O007Zv5mOMvUwcHBwcFhRVyQgFTSbDqi3Kh+\nhFjSrvu6zPU8SZrMKwy7cR5iie8bXcwCNO0p1P8nr9RqefUJiEfmtV8KbR0r1/nbv1Kf62yBHm0s\nx/kBr4xs9o1kh1f4CVZc7b7SpAXDA6VynzxjQs4cdPexWXU+gSVWbeime/+GqjJdFmVZ0XyR0tQQ\nm6Y511tqLNMMls6HH3FdBZbWD43NWaEr7fmc6+HUZGIReGgraz15qHyvttiMF0IsGrNXX8FSWMyN\nNYl6DvDFytPy+LBWrTHvxfx9WRGnRr2KYEX4xhr2Vxec4vtSRUuSqnU/suEv0OSFaSfJsxmwEq3e\nsWj5os08EwLURhai7dAQajz+HEjmDnP1DuIpLLmlgILO3ITjiO5sCqvWJE0hH6Qxq0FcIguSh4r0\nTf+S5xOPDX//9VR4WVY0TxJ6vr9fH+s32EK6c1v1Y5t9tiZijPnFXMeyqCMdDVXVZ4zwPbFWUxP+\nVaZ3xI0AACAASURBVCsmRaZSpCOHovijFufxmMlLe9d1fPvoJI1SO61YOjnGXGZC63KMq9SQMXOM\nsWYbamyRMs0mIH3lRo+36a1e561mg9556w2qqlt6XZC0xhMlaWXwPHgBewKSmT6LhNUExmMSYo56\ndczvhtRkBTuAhu8HH6unT3hVB9BQn0y07UriObwzUEt5HT/49LGGS0kI2ZtfYuLUwhC4RD/86tVt\n8/Rc3gJ9O4zU8hQPm2cs36o0qdC+AJxl6uDg4ODgsCIuZJl6Hgs3kN1/w+quTDQIWPa7SuzvecaS\nKbAqDCtDhYeFUWKp7FlRxEpW/8ZSwipTFm2+SUuzOGVrqGX83V1c/2BiQmOw59RsYjVorIUSIQBN\nEyQ9OuCV8xhiDLpuVQGHNbOn0Qgv5m8/D77vU7fdodCIXgQo29zEOoh1ffSKyyjZTIg0ZMVmNtGs\nJ/z/aqm6YREay6q2QuQHnr0W/02NpZ5j/8OuXF/B8ihRDt8IEYj+rW/q2/ORCQR9KTNhMxKXVBlr\n0TgiVkBFVFX1niiRyfxhbhChvLU1bu+N+g6W8p/KeOA+bwU0JCLLI+2bBdpYMlhYyzfPIVpgLK0C\nbeaTWupZLuOGj4VGIDvCnrnULf8HYw+hCqW5vkd8f78ybWyC+FdBGES0PtijwmgHP3wCK9VXS63T\nYUGDUwgMRMb70oBOdWE23YuG1BP/f7qwGYz4eaZj9Y6cINdxswlxAKNv7UOXOU9VFGA2QVieb7kR\nsDQRtlMZTWXR5p3NNOSmiXJX6E+JKWMlmVnWlPvhB6vzMDyPKGoQpUYlJEXuW9FNJyIqMf4W2IfP\nTVjUOnSPQ8MV6A342TttnhkXZlA0It5bfXmsVuUU3rydTX6+qKl7w1chnHEy1LoqshL31Hb/EsJv\n1tfZg1EaZRdps6bJERvgtzHap2csXwlLMhFXNJ3bWf5vhrNMHRwcHBwcVoR7mTo4ODg4OKyIC7l5\nq6qioiyWXIAe3Lyp0euN4X5rSvJWK9yB3xbmoAhNiMswWPLZwaVnFGXUu+ct/Z5I1ZZKk15KFEZa\nntHfRR6hVgtpkAz1W1ydiXFPj0EgOEE6qoklOEXsotgyUjXeayALUMXEndC44zp4vsCEbzRxTBQ7\nMuMSF7KFySpWk4ESuGPzwpYVSdDN9StcQ9yq/pJ2Lf+1feI5NFWvHChh4ylCDgr45n1PXVYFXIuL\n1F4XF5Z+YtzaPlSGLIHG81b383rkUUjRknpRrXZksl/JM5+Tv7lWSiqMSpQQssR9XOQ2sTx/PzO0\nfhF2KuHazgzxR8KB0oV1kwvhzzt7DFscQRScObfEIfLEdY4ym/4d1m1liGef0/K9LIIwoo2NPUrM\nFkwI3exWU8eTpPibTNn11zVjTZKnj+fqFhTFHilnYPpbA8STJFM3b4Z+mc74d82uuh3XQE7MzRyR\nQyGpaRSNRAlrPuG27JlQE9lGmhtda9nWkO2tkSE9dXt8z3ZT9XrpNWjzVlVFWZ4tbTVU2ApYGL3s\nGHNKiO2wTtekFJS+ZELTIjxDG+TAZG4U0fb4HfGVd+7Vx0anU3wf/zdu/ijm62emvqfYRopNCNze\nJoeTyVZJGdmQPWwTmblKSIzId06R0atOQdiLreJUdbGUd84ydXBwcHBwWBEXtkyTZEGpCdg+GGJp\nYQgLEjrQCoVQZEI18ObPK12JCpFDiBqhsXJ8WT7b5b+n5SEimpgQlhaIEWNDTql5Iiah9whZNyQJ\ncG5W2nPc/8SsdA9BsJrA8pjZrDFYJRtOVU3kWAVlVVGS5nWoERERYUM9NOsgqaMKFnjTEH9qa9IE\n+XuotykEN+apWj7yvcwQihaSNeOcLCmy6W8Tar+Aru8Pf/D9+tjRkYQ7SSJ16wmo7Cl8hjWMv9ZS\nFn7asvH/GiylistlrVwJualMqFAhmWRK8ZoY1wu8A55xl3h12AmISGRCTOAVyBL9fo72yBBiUZkH\nDeoxpX0iQP/wDdlChCfq8tvuKJl+LIELj1eT08w5sZzIjJ8guNiq/a9DEAQ06G/QMFXhlQyZpaLQ\nhnglKIt4SbQ9GpIUWg1NGkNkpQ1iy/qm0fue8Fi2WVg8DF5p5syw8sRrFYVGCxqVmBc6j4kgzVQ0\nYBtqRbX7bGH6xsuQQQSjjaTmnbYR7qjDNiyRZ3VhkqpC2JRvw9tKlFfbNMX4lLCZstT5QOa5xcIQ\nvnBayKeW1NiGCINNDr7WZ1JSC/2/8LVe2sj0MjP6040OCE5N7RMteALCmG82L/QaLWjAJ2YeayEM\nLUJ/KY1rqw3t9cqE4DWCC70enWXq4ODg4OCwKtzL1MHBwcHBYUVcnIBUFMvpqKD5WUW6mbuA+1Ai\nTwOzqZv4UMJp6K1LiVeCuyEyhBg/FXefIcSIkgzcNK/2VRO3RDyj9WqJOyc1Wp+zmtACEoC5Zy6J\neU08bbTGMWZ7cNd017fqc4MtTsrc6SjByablWgW551No3eQob2UIOUEdN8p/bco74S55xq0jzBMP\nm/lWA1XIKXPjXgrhnpQkyZlx0RaFuICNVipcK8+eqT6nuIbrWrYcHymjca+K69ejs6QLeU7r+a+q\nM1+7BCqiKltyY4tKTW6P5eJ6PluO+qNl90jdlEI4MUSWUshUps/DhRqhLWw8aE3Esz5u2Qmx8bE4\nH6EfRr5VdoGr3TSCtLtc1jOua3EDemZro6LXUuFUlRUtZgs6MWpcp8e8JRDaeGbcT4hHgYlxlJjX\ntnGrNlus6R1gistt+kUQ76Km7ff8vem4qMslEGWd0qSsW0eMot3yShOupxbiR5fcsoU8h3X/4xRi\nODtGtzcEsTMyxLE0Wz22t8I/u23SQPo53xDvvJA/T0HICgNb31BzUmkBaoE0FKMfNazbFvq4mdED\nkG0hcasXZh9iiDawQyiOZStDj+X4jVzDpolrwPUbm3LLFpZoIdu4dnFTh2Yro7pgmkFnmTo4ODg4\nOKwIr7rAkt7zvAMievQ3ftHB4lZVVdt/89fOwtX3peDq+5eLS9c3kavzS8L18V8uvlB9X+hl6uDg\n4ODg4HAWzs3r4ODg4OCwItzL1MHBwcHBYUW4l6mDg4ODg8OKcC9TBwcHBweHFeFepg4ODg4ODivC\nvUwdHBwcHBxWhHuZOjg4ODg4rAj3MnVwcHBwcFgR7mXq4ODg4OCwItzL1MHBwcHBYUW4l6mDg4OD\ng8OKcC9TBwcHBweHFeFepg4ODg4ODivCvUwdHBwcHBxWhHuZOjg4ODg4rAj3MnVwcHBwcFgR7mXq\n4ODg4OCwItzL1MHBwcHBYUW4l6mDg4ODg8OKcC9TBwcHBweHFeFepg4ODg4ODivCvUwdHBwcHBxW\nhHuZOjg4ODg4rAj3MnVwcHBwcFgR7mXq4ODg4OCwItzL1MHBwcHBYUW4l6mDg4ODg8OKcC9TBwcH\nBweHFeFepg4ODg4ODivCvUwdHBwcHBxWhHuZOjg4ODg4rAj3MnVwcHBwcFgR7mXq4ODg4OCwItzL\n1MHBwcHBYUW4l6mDg4ODg8OKcC9TBwcHBweHFeFepg4ODg4ODivCvUwdHBwcHBxWhHuZOjg4ODg4\nrIjwIl/u9brV1vYmeZ4e8/CfsijqY1ky54v7fPk4iutzRZbw76KGFiJuERFREAR8rbLU63/uPvZg\nhXuWearXx+ciy+pjer3qzLEKFysqPSdli2Itozy0fK0s9XmLIsflTbl9Xqc8f7Z/WFXVNl0CcRxV\nrWbjc0erpXLYY55UjGfPLf1nGVKn5mK+f3Z9JXVV1+M597aHKlyvYcre7nQ/Vx77i7NllGtI0apz\nyujbn+Hz0yfPLl3fm5tb1fWbt/7Gsmm1VWfOfTHoNetr0XnX+gVtZ+/tnVPGS5asfqZzqqAy47JE\nX//ZT35y6fomIur2+tXG1jbZEmfZ2fHkn+2qttT8u1zHpJQvxNxTleaHOOdjviEiKkvpb399m9pT\n53/vr79GEHCftfOkzGlePWy1DqT89lry6eTw1aXrvNFuV61Bn0pz3TiMiIgoNPUh9e2hCZJkoecC\nzIW2k6D+4gaPeS+I6lPyfPbZZb6NI/6eZ8qT47qlub5cIzTzUyHzP/qlZ85J2ex7Q9o49Pwz5yKP\nnz2XuZyI0ozfJftPX3yh+r7Qy3Rre5P+8f/yP1AYaqEDnwuxmIzqYy8/eZ+IiLa7fP+rV27W54Yv\nPyMiosbOPb3unXeIiKjb6/FDTOdnCuiH2tAU8f2z6ZCIiOaHj+tTp/v8efjqeX1sPpsR0XKDLRZ8\njwTG+TjTho537xAR0c5NLaOHQZmjASdTfd7J6QERERXpTK/R7hMR0f/0P/7TR3RJtJoN+uavf408\n304qfP9c1wpUVfwfHyMgMHVVVT7+2mNcDwE6cmE6UBcvQNsxx5MpEWk90tIA5895rtcoMBHcuqf1\n943f+Dt8XXRaew3P8z9/iHIsVrKcDyaJPnCvy4uvZqRllIHx3/zX//2l6/v6zVv0h3/ynXqQEhF5\n6N/2xV1PDuUveJn+glOeuZh8zs2LQ34qk6t9V0od+Uv1hy+YdZDcIqjrW8/JRE3nTNSZtGNhro/r\nZmZClfHz1p1bl65vIqKNrW36b//nf6r9goj29w+5CPNpfawdYbLLUXbPlL3kvvHyZFIfmy34t9tX\nbnDZU11wlyk/R7e7Zr7P5zOZB8xiWSB9kYgoxWJ9abJGveaYhG0797ptIiKaD0/qYw2MvyjG4tC8\nTGcZl7Eotd/neGn8X//sf7v8nDLo0+/8Z/8pTZOkPnZjZ5eIiLYGg/pYG4ZQmPI9H9z/uD7X7GEu\nrLROKeXr3bj1JhERxetb9akI9TAdj+tjw4Kf6xbu7Zsxd1pxH5yQljHCi2Cj3a6PjUc8/0/nXFdx\nS88VJV/Di/Slns74nhsRzx+NSF9/2xHP1yejg/rY45dPiYjof/3v/vEXqu8LvUw9jyj0qrpyiIj8\nAg/iaaPv7WwSEVEHHXrxXBuiUXID9Jr6kJ1OB594wKxt6iIgF0uzMG8P4ooaDfmFefjwAz2DQW7L\nOEejV2a2aXW4Qiusgtu+XaXwsflMB2ejj4GHpVoHEzoRURxwQ+SpXr/ZWadVURFR4RF5ZtIOYlnJ\nmcFecNllZWlX4RG+H0ba0WSVKfNXFKrnIEn52TPztq5wrwoTTFWZ9ke92ZdvkfA1isy8YGXyCbA6\nDLX9pSDJQgfnHJOfvMiDULvqIOBnKcyE51/aFjPF8Ih8zyPf3EveN0uOER+rXjx7VdoJVb6kL0e9\nvrf0lwuOBRAF9ptLf62voH7/mZeJL69CewmpGim/6d9BdXZlLgus2mNj+leCiTJfXsHR60CWJfTq\nxX2iQusrQTcQC4KIKMv54eRr1ZJbgg82dUhSiqLOxzyGB2v64iwwfzQ8039C9G2f//pmASuLptFY\nJ3e/kpep3jPEIjZq4m9D+1G7zZ/99Z36WITvz7EIn8zViOgRj8nAV+9Onp3tUxeFT0RRWdGda3v1\nsSLjOjo41vs38DIN5twYQWSs/pDrb5rq+N4a8MuzRCecnp7W5+IOP8ui0v6z/4KNno1uk39X6Xww\nRH/LAtMGqMtRYMrY43ng4MURERFNHupCZbDB5zo3evWxTh9eimOeU6KqU59LGnyvw7kaRCdzXTx+\nEbg9UwcHBwcHhxXhXqYODg4ODg4r4oJuXo/iOKY4VrdgLi690Qv9Isz0Wc5msq/eEeq3mIjSG2zU\nxzogpyRwC9o9q2TG11iY/ZoG3Awe9lR8s9+UF3yz0uwD9rsdnDP7UnBLethMp9zsb+zxnmkRq2uU\n4NoUz4Yf6DpErpsZt4cfWbf05eB7PrWbrdr/T0RUCBnIuLnErVvC5xSYsjXb/Owts58QLZav1TD7\nCgHcO+IeJiJKU67nl8/h/l5oeaRGPUNeiJvsuqmMm64Uohd+OjT7Jwv49eZmX1T8Z7JH7ft6bjJl\nf16/09Trm/a+LKqqorIsl8gZny8PEZFf4rnETWjOCYfA7lGrr/WsK7qUvcnK7v/i7+eJZUTkh8LE\nOYeQt0SyERKH3Miek59ZVyY/SwI3V2r21Lxzyh01mmeOXQa+R9SJfSoNYTAouL+ZLkjNNs8Rucf9\n2G41LOb8QNf6m/Wx3XX+XBJfpLem+4FBhj0/M17LElMhXPiVGXPiAm/G2qYzcQH6hsgT8zWExDJJ\ntY8HqN/IEHNmqOO5bIeZfd0wiFF+naKzavU5JfCI1sKKdszeYxhy3R4dq5v0xXPeJmy32M2cGpcr\nYe688sYdLduQ+82r05dERHT3xt36XCn73cZ0e/tN5lOkGf8uKfSd4sPGW2uqG3aOfvn0SLkwe1d5\nO3BjnV34Tx99Wp/rguj61pu362MJ+sn3fszbgtMru/W5jXdxr03tJ4PoYrams0wdHBwcHBxWxMUs\nU2KiiaUPk2yQe7r7PzrkVU0BJliTjJWDDW3/WNmw5YBXRP0BrzCSudnoh8XjGULH5NVDIiJ68P73\niIhofKorKt/jR5rOlAk4AwvV8EqoCUtNWHwnI90w74x41Xnz6//GmecUkk+S6Ub18SH/Nk+Gev10\n9XWKR0RBVVBpmI4emNRNE1okRIZum62FTtue44cWNiER0WLBhIMZrP7QrJZzeAeaLbU8ZnNeVQvB\not3R6wv7c5FqmzVx/9CYFoen3N5TtH+aa3t6Gh9SQ8pUFEKM0ZNHp7zit2y86BzCz6XgLXF7TOyK\nWkIF2IZPn90nIqLZXPvy1avMHo1DYzXDspG2iIwnwIMnwDdWvCeeBQnDMiEf89FZlqe0S6OhK3m1\nbBA2YMp/TkQUZbAMC9zLWqO11+Gca6yKKIpoe2+XkpmOv+mI+1Iz0nkmbHC5JNwqbvbrc7M5SITm\n+WVM5LAXcjMHtStYkMZyTIS4GH8+hIvo+JgZngtfv99o5Weum6NfzHKee7JCx8QQ7P/AjDUJvdCQ\nntCck2c3jRSsTvoqipyGoyOaPdS6nRdcjkZLnzkl7mcZvEa+ISnubV7ha2VqSfsYn2KBB3Mlb45n\nfN1FqGM0wD33D/aJiKizqYSoVoPnqjjT+huP+Hq7G0rs7GOOuroHRvCJts8GvEa/9fabeo0jfqb3\n/vT7/GxNHXNzEF2HhoDUtIy2LwBnmTo4ODg4OKwI9zJ1cHBwcHBYERdy81bEPIaGiRGcZ2xaTxJ1\nq6aIRxPTf2ridYYem+5bhiRTYOO9QHCYuBaIiAJsAofm+ycn7HYp4eqKGkbJBG5BPzRB5zHIL8Y1\nViGW1GsgYLqpboys5DL219Rt5EeIh4L7Z5GaqssRY5Wq+7NlyBCXRVUVlGcTSkt99gDliIyLYnsb\nG/Eb7CbvtoxqEtwpgSnuYMDXGMG1PRlrvcym/HlhCCiHRxxEn6O7bG+pq6WN+LPMfF/igPNSXT2P\nPvkJF8dD/FxDN/rXt7j8vvEdpgncjSJSYIRCSrgbxzPtV+s9QxZbAVVVLanO1CUyRJPjQw7m/uiT\n7xIR0aujJ/W5tYdMrGvX7kKi2ZhdiP0BuyYHJjg+CuG2bOj3a3IbynFsiCEHr5jg0eupC2qx4HrY\nWL9WH3vzra/zvdYk/s+41eVRziFaSfxvZaYGiSHOUu0nVgRhFfh+QK12j1omJrMJIk821+eOEWgf\n+IgxLrQ/RNjyaLe1TkKM/xSxma1Ax3eUsysvK/QZWh1u6QbmktIzrviCPzeNe3604LpIDPkxBYkp\nAMmyU2l5xKVrtytCiDXIVkZhrlVCICJaUvVZnYCUVhU9z3MKx1q3IbZ9cjJiJSCYiZDD+kDjNasp\nj/VuX49t3bhKREQP7n9IRESvTo7qc9OArzXJNEY0g85AChJkx8zNTWzfjE5126y/zeXo7WmdRokQ\nIvm3N99QcaC317mf2O2KtS2eH//t/+D3iIjo258pYWkCTYFuU98lV3YuJjLlLFMHBwcHB4cVccHQ\nGJ+iRkxxQ1d5iwWvcCpPrQQPNOo2VhilZf60eaXc7SuBoNXCShSknoWR5euEvBIpcj2WYCUVRRIK\nYggxogZm1E1KWLfpQldGYs0WUOxoF2rZpFhB5al+P4Z+cAaiTWKuJWpBlZHUm0+tpbYCqoqCSq/b\nwqp9wyi6hAjHaGI1GRpiTvE59SIitVIasAamE9N2sMAKY2lub7G1tdbj6/eN+lMIM2c2Uc/EaMrt\nkxriTIOWpQ4TQ8442mdLr9HWZ+pgdf95jV57LDEkpqR4TQSkatl68GoCiLbB0YjL67dAWmnpuU/v\n/4yIiBq+jpF19PXjY1Z9WR9o388KUdIxoQGwDkWyrmksrgDtOT5R2bMhVH5ePHtQH3v+lO/193/3\nHxARUa+r3oSzIoLaLufpO9fap0uyhq9HAcn3feq2O+SReoFEy3thrLK1Ppe/8tkaykz4nPR334SL\n5ZAM7LX4WmmmVs5scYBj6jkJESIn5EfP0znFgzpSq2vIRlJfJrymBa/RDAbvIjV65bA+Q+PhCGGd\neZiDLIcuEC1wo66V5TrnXBae71HQ8shbGHnOkOe+wa5KALa32bM2GrLn0TNkwnaL+2NpFMt8EI7e\nuslEomKo80EAT9arkT5L4vFvY5AVp4WZ30fc7p/89H597NqvsNXZMeGKb20zEerg9JiIiK6bMvbg\nnctMvxJy3fY1fs5dQxh9Oeby9rvmXeVfrL6dZerg4ODg4LAiLqzNG4f+kpnQQTD1LNI9nzLglUsI\ny2E4VstnPmOK+HykNOZehy0SWcf5JmBagrkT8/3TA97Du/8Rr8Rr4QUiarV55TqZ6KrzcMKrHptd\nZrvPq8jU59XM8UhXUqXsM/7wX9fH9r7yTSJSoW0R0iYimmLvMTR7GkFug/Yvh6qqKMvyJRHwAtbv\neKYruRasDh/7b5nJ7rEGwek80zYoYRUGsIA837YPr8Y8EyA+6PH3dtchxhBp2MfBAe+NDEcaHiJC\n36EJ3+l12Rprd7mfpGb/bfaEBT+efqoJC7Z2bhER0eb2LurCrGoTsZTMPnqp/WMVsHCD0b1FOMLR\n0bP62Aef/JiIiMYJl3s+0r42OuZ26bf0GtOAj02RwCFLjScAHpTKtLGET4iueqOlYQMjrMJt71ob\ncPvPDDfh8VPeu/rjP+I2+P3f/w/1mWA1L+k+4P6yX1vZ/TsRDTdL78gIt6yCoixpOJ0s7V9W+Jwb\noZYW5gG/KfWlc4SUKzR1WD9Hwm1TpmYPDwIz1roeLvjzy6fH+J16ThK4u5pravmsbWNv2ViaJfp9\n4PP4aJhwEh97sFVhxiE0w0VjuixM6BFCqySjFhFRFauVdVlEPtFeNyDPcETWd9nqu3n7an1MLP+4\nz5ZgYOaUPeyfbqV67N1Nnmc2MFccPdHxsr3Nnq1hX+eDWgSiya+gx0c6fl/MuE53v/X1+lixweU9\nNB6GZ0NuU8li02+pCMPNa/xML2daxn6f62+aIExvcVyfG0B7vWm0SMriYnvUzjJ1cHBwcHBYEe5l\n6uDg4ODgsCIu5ObNs4z2X75aSreVQv3l+aef1ccqKIb42JwfTw0xJ2Y3x96R0fKF5mUbepE2KbdI\ndmbGrfriGVKvTUDRbhl3CpRS2muq/VtBdWdmXDei6NHFZnrimVAAEHTKiZaxytkl0N1ghRt/YXMv\ngZQ0M6pO8etQQPIo8gIqQ62PAs7wxNRHCvJNBfedVSgarLMLMDAppZIERC9oG3cn6mIbwSUfGS3O\n/oDbpdXnOn3w9GV97vlzbgv7tCGILanR2vXgUu6AOPX/tvdlTXJc2XknMyuX2qv3BtBYCBDgMiSH\nQw01Glm2RmGF3hx+9s/xq3+Iws+eCFuhsKUIS7KlGWmGGg4XECTQQKP37tqX3NMP57t1DsyJEBrV\noad7XghWVudy781b93z3O99XU1BqE9e6uyMlI0UFaBResevbAkGlppRAEZzm5eplA2VZ0ny+eIV8\nM7hghZa/+b8/X3523t8nIqLN7XXco4yrIIC/qyoz+Ow3bEG4DYJHfyzwUhvQU6DgvOEQ8BXG5PnJ\nxfJYA/BcQxmvu/jbsC7n2Nzih/jbv/1LIiK6dVPKBt794Pf5HpWHr7FoM+VJ2iO+MtrT2lLvmghf\nZVnSLI5f2YLp4X2uGkLUSnGvi5xhPk91UqvktnAjRdZJABtijM9n0uYltjJitdXw+PEZ/6Picw2O\npdzJDLNMQd9373PZxI3bijiF8poEJLFUQddGI5gUMXKx4O2hdMHP5Krp2Avx3gbaCu/7HqtXDc/z\nqN3pUrcp8+PGI4wNBeWOTvjeQqgABXN5lg7+/V5HCIMbudmO4zlwqEpjUpB7ek0ppWmY+izoMIfq\n/d0GYbSvYPj6Os8NTxMZmEc4bwPfLxK5/2cv+J35q3/67fKzf/dHPO7bu9wX9YbA9l1A13kh82pB\nVxvjNjO1YcOGDRs2VoyriTY4DlW+9wrhp5jxL3mRKm3bhimJgWm1K8SAu2+x08Db74rjQAsrHEP8\nqKtSgATu6Afn+8vPLpD5rsMAth7K/YQoum13ZCe5ijir+Wr/pXyGRYkPU9hQsTFqoF9XpVCj55d8\n/d4er+IKV5ouzviaWaELrK8hHIfcmk+poscnWBVuqDKStQ1exa5vMp29rXbRQ5TLGOcXIqIFKO0O\niC6Ntpzr/jv8b1+ZvU8GnIkenHAZ1HgsmVieoaC9Jn28SLjd1rblvIuM2+sCBJ2uKo4OUe5z+9aD\n5We1iPvg8eNv+P4XQjy4cYcdKcZD7TxzDZlpUdBsNqZUlSD88lf/i4iIzs6lwNuUZA1POIsvF9Lv\nxjmk1ZGM5b5/j4iItrY4My1KVVKAZghVZjoB2a4RGiKLrKBruHaotJMzZHXTS7nvBYymi5L7/b/9\n/L8ujyUQMrh3753lZ4ZwZLSwfdWfy1KkRJNnriczDXyfbm/fovFAsu8CYzUdS583UJYyR2laK5As\nJ8W9TGYyxkPXCMcgC83l3mc5n2OqNMCPz/laDx6+T0REY0VgNPBYqcrhhn1u896W9Fu9CaIWm8ul\nJgAAIABJREFUSvW6dRn/DlCj2VzG7LwEogYXG1MORkQUtdbxbDK2NDHzTSOviPpZSb2etN8FSISB\nKufqm9IW6Be/rbSmt4CKZJlk+wdAYrIGBHJCOf/jA0Z3NjuK1IiyvCb+O1Hvd3/CY2F2Lm0VHLGp\n+p/82R8tP5vc5Pmu7oNYdCx9/Nd/949ERHQ6UyU3IIg1oc2sSwjNNTPlXnSpxuTrhM1MbdiwYcOG\njRXD/pjasGHDhg0bK8aVYN4g8OnW7W1SqBMNK4YAU1VD1AQEenbKMECgCEI7awyLLPpCYml0eHPZ\nqO9opaIM0NL8QiBaI1JRoXbr+EJ0Jht1hiccZVfkAbZdKBWgc5hT32lg81rV/pVk7IQEVhm9ZILV\nxn2GgRxVQ9ZqggDi68K91etMi7Kk8WxBQ0XgiqDJundXYPLdXVYCMW3lNwUuIkBgC9WmBls0ajct\nBfMag/MsFtjFqMKMseF/fKbsstCOgRpJ/RFfa65IKgEgyxcvmeixo5RGtjb4+qmydGq2Gbq5t8fP\n9uTb/eWxmztcT3ZjR0gUo5GQv940KiqpLOd0dipkuvGI7QSbDYF/xgNuh68//5qIpH6NiGhjh8dF\nLZGxEAGyjmO+x3VlQJwAjp0OhbBxY4th+xnaO1DCygZq3d4W7Wejff3yQOp0h9A1LQABvxx8szz2\n85//ORER/RnUkYiIHj58j79v6osVvLhUoVIqUJ5zPevwNCvo8HxIpYL5PKjhzMfSJhfo3znIPQ3U\nIRMR1WESH5RaxYzbJDWQuq759Pl7GnLd2GEY8e593kKYTBRkbrRrVU1pBC1oU4dJRFRg+6uBelD9\nXvmoww/U9lC7wd9bjHj+KhXBK0MdbazeId+7htp1x6My6NDzg7PlZyXGmakxJyKqtfl+t7FltBYo\nS8YB3+9LtX1XGt3imP/rzmWuPT3m7/u+vCd1tN/hIb9fgwtR9Gq2+bckUQSx8zMmOj46F3Pwu5v8\nHoVgiI2V9WQMC85vvhQi2Yt3+W9373xIRERTRVg6OOPrf/vs8fKzyVTG3+uEzUxt2LBhw4aNFePK\nCki+5xHlsnE/g6NIGivVGKyqRjCVnqoyAf+Qy00u+7IqvJvx3+7dfcjXUSueUZ+zoESVtXhQNxkh\ni1o4smpqgsgxmCkFF+hGeipbnOJ8GejmWq0khmPJQinKBCgnGV9CR3ZLHDqKCnqeyqmmrFansRdF\nSZfj2VKBhYjo4ftsdmuIXERENZ+fyxAIzP8TEc1m3M7xQlad/7/prdY0HQ6wYlXqH0YpaTTg7ME4\nLBARNbCKPDxWilMT7hevLRDG/XVe6cZH+3xfhQy9Wzc5+7y8kJVgr8vfb8HU/MaWZKEHTzkjfOfD\nj5efbawL4eFNw3GY4LOIldm8x327ti4ZSB1Z9XzG4yNXbdXeAPlOOWQYJ6PzwRjXkayyAQWYU6zQ\niYgaQBZmU2T4ylA6Qobw7ImsoM/QbtrAeb0DZbIlIiFEjNMzJlP99f+Wcp/dXTgPrXGGVikKnRkf\njlp7O9eVmSYxvXj6eKk5TER0d4fv3fXlGhmIiGZkh0rvewJlqVSNqRnQnBGINOO+jK3Tk/NX/ktE\n9OABZ+ZbmzwWWy3J8k9PmUCj9afXUS7mlmoKzbhvnCacbRSylQOViJUSWVzxvycwE48n8g4F0L8u\ntUNXuro2r1/zaGezS3U19ccRX3f/qaAXWy0my3149x4REc2/218ee/ENj5+2cj+KoMKWwcmloRSW\nBkBT6jNFAnvByOTwnPvAd+Ud8jMoGvUEYWuB9DTry/emIz7H8veiJXPEwwc8Px+N5ZrHT3luu9zj\n9/uzX36+PPb4JavpRXXlXhRdTXHKZqY2bNiwYcPGinFF0YaCzk+GlKtV98khZysvnwmN2Eex6+WY\nV8NnufxmP9nn7223pCD7r3/BK6I/+MlPiYjo4w9/sDw27POK7nQo+2mPX/Be7FOUWTh1WUEMG/xZ\nXW3imX2gXGWmAVw6INNImVoZV1j/VrmcI8M5EogaBOuqPAilBjVf6ZUW15GZFjSaTKlyJEvoQvSg\noQqgc+wFuyhnGE5k/ynDajaJZYVW4N4aTW63ULkA1dBuQSAuI5vYT0riAn8vupspvAoXmWQ+gdk/\nUXT6zOX77mHV/sF7klkHbV7hPnspIhk7u3xN44TT25BV5+Vwn/+r9ll2bghSsEpUZUWR8oNNsZfe\nUCoGjTb3/Z23uQ/maj/aq/OxRV+QlJu7e0REFC8YJciVK1IGYYY1Vco1gNdkZ52fOVQr5MWQr/Xs\nqezrtrrctuOh7GWnECqJQfXPK5UVzPh7T7+TcfI//oLfx//4H/4TERHVI+UPaxxMSIVzXaUxNbp9\nY40eP95ffpZsckbS7knm02zhfV3w8/RV+55dcnulpbzfJlsxTk6zkdzv57/la40Gss+eJDxmf/D+\nR0REtIEMlYjo17/iDCZXe3gFdHWnY+l7Y4GaO5zpOR3JTEPsozbUHJFh3HSAFCUTGRc5dHqdmswz\nXrD6nqnnELVqtSVyQURU3+Vx9uimuMa8POR3/PCEkbgtNZ+18X4nap4ZXPCY2tjiMsRmV8ZPY8Lv\nyXPlH+qD02KMXiLFN0n7cJ1Sns0lSm40ommmlwhzykLpq5us8tMfPlp+Nn7BCMMl3JV2m3KPB/CM\n9nzlQau0kl8nbGZqw4YNGzZsrBj2x9SGDRs2bNhYMa4E8xZFQaPhhHKlozkcMHw0upTN86iG32io\n0uh98wns0Ibnop5xPOPvd6AY8oMHkprf2mMtXEfpXBqIqzhlmGESK9gMcM3DnsASLqzRRhOBhkqQ\nmB5fwj5NwSmdjinVUN8HxT54DiiyJbZYBgHMawIDLeLVzcHLiijNSmp3BeYzm/6OIg0VIGIlKGE4\nPRIYto0Sk0Ff2ttoK98ALFzTZA6QLDprAvOOQYSpQzs59IUUMZh9X4e3g1Ko2YVA/2f4mxs3uT99\nZVZ9dsn9OFIw1xCmxL0uzMdnahABcpopW7504xrM2Csmjm2uiw5wFPJYODnYX34WojtMH4RNBe9j\nHJWuQE6HR0wucrCVECi4LgU5wxA3iIhCkIwmaPfhSCDa6Tm/Z0Um/d+E7WCWymcTaKQW0Dzd3JEy\njR5KeRotBWcnTLh58fzXRET06NGny2OlUeBS2w2vOIWvEL7v0Y3dHk2mAul+u8/tlS+UwbQhMaLU\na6wg2pqB5lQJkeHgtRt8XqO0Q0TUqoM8pPR9myivMe21sS3j8/f/gNuiyOSa7XWeD9Y35HsJCDYz\nlPm8vJQyjrUdEJtUuUwH71MThueJUharQODJlaa2U1s99/Fcj9bDDkVK8c0FfH2rJ3NajtKg3OV2\n/+m7ou1Mt/h7/ZG830MQvSYoCZuqd7MCEeviQrZx6pgjathiaoUyp9TxDnmJjLcexmqlSovcAM+A\n+azIpa36KA0bjWROWYugdtfkvuuo7cEtPPuRIh8OZlcjfNnM1IYNGzZs2FgxrpSZcu2AS64rqwiv\nDoNpJeTgYlm4hhXgXBuuYqWz3lDlLCiliLBKqTcV5RplAvfefn/52ac/5hXf2el/5+vNZEXXwUb2\njc3t5WcVnBq8uqyWMpSKDOa8mnQLWQU5MBTW5ThmIb4BanuuBAYIGo+lys7K6jrUeSsqipw6HWkP\nF6uwRJFeDkEW8LEyX1OU9SmMy8/OpDTAMMkjrH5rasV7eMR080yt8hwYGp+85A38RBE9llquShDj\nJlxuJrGgFS/394mIKO5zfx6/kNKDJsZCpfrg7IwzaWNM7SpHiyjka+aqREuvSt80KqqoonJJFiEi\n+vB9NoV/8q0M8LM+F4IPhrwyd1VJVIB7c9TK34hj3IDYhOtJW8U4NlDIyxzjyAEZLIm17jD3Wbsl\n2bDRfQ2Vtmu3xRnQJkqGOiqDMmOz2xKyxRQlUd8+ZpeNvdvvLo9FOG+lSHoOrU6GISIqqoLG2ZjC\nDRmDt3y+98uX0r+XIBkdQee1GUh/bO/weLu4FCGCEOV5Ocq40kK+32oa4209jvm/n/0zG78/uH97\neeyjD94molfdmI7Ov+T7JyX24oHslXM2lCZKBAEZ5iSWOahEpu83uH09VQo0gz7xdCHfj67Biaos\niOaTghJSDlq4j2ZXxo8xJ/dALFzrSUa90+Vx8+I7GZd1nKO45O8/PZWs/BmQhkLpI2cgbVZo91QR\nkFxkpHVPCYdAMKNUP1nGVcnMG6Vy1Xn+Lc+JQyX8sfMjLn96esH98uRECHs1vB/BobyHb+1yCdtX\n9HphM1MbNmzYsGFjxbA/pjZs2LBhw8aKcXUFpKC2JO8QEQUgX6SuEChc/EYbyHVLmch60LF9+6Zo\nayYL/tvWGm/S58oKaBab88o1t+9xun5r95+IiKiCegWRWIdNxpLC37h1C+eVc0yMgguIMYGCjWrQ\nIJ0sBKY0WpLrGwyFRMr2jZbkBWUGXF4HzEtEjkOeUsDxoYii60brsOPygFVpePocmpOlgkmNuo1R\nttG6vTlMfgfK3PfeHW6/kxOGP/efCsHp/II3+jOlDjOAufsilzFxfsbkg2LG7ffv//SPl8eMMe9T\nZTA/hZ1Wr8fjJVJKMIZApQ2qr8Os2iEip6yoVPVlW2sM97V/KGP4os/P/+LFt0REdPRS7nt0ylBj\nqeow6xgzNVg/pYpsNIJCS0Gy7WEISg1soSRq7CeAtHIF/b6EQs/uzb3lZw/u3CMiov4pt/vZWIgY\nx8cM5UdqK6K65P5u7/DzvkKIgk1hXmpt3usZ31mR0WH/mNqB0u/eYcitWZP3qdXj68UgZRUL1b4R\nt2+9LjDfEKbuc9Qie8oSbAhiid4aiAEtuh6/559+Km156zbDvJ4i3p2PuZ+nQ3kXspT7twCkXBRK\nUxtbMKF6zsGCCU2XI37O/lTVvYK1OVPWiX1FtHzTSJOU9p8eUrMhEHcbcP/jhRCEDOJ8t8N9EdWl\n/RYg9Wgi4s46vx9nl/wMB88EHDXf2tyUdyiGDnEJYlmS6vkM5t0tuWYG3eNLZctXlnzfKbTDD89l\nzjfkPWM3SEQ0K/j7I2i1n6nxfHjEz/6B2h7cbMr9vk7YzNSGDRs2bNhYMa5IQCLyfCJfrUjqa0ZH\nU1Y6BbIgDxnKxoaoHQW7UGtR3rt39tgUumlo4yoTy6BLW6pVXr3Nih0f/ORnREQ0UJqWyYRXJ7ki\ngIygZKTESugUZTURsoDOhjL5xSpooRwhjBpLA7fWayiFJWTbqRY9uoaFu+M4FAY+rW3ICmk4wn03\nhCRzCxnJ0SETYw6Ue0iOshm9Sk7gDmF0e2tafQTlQInS33Xv8PkrEM9661tyP0NepQ4UIesIZUGl\novV3QGD4N/+WCT1//Kd/sjw2hxHziwNZ5buGNIb/11qxc5ibh20hTKSFJum8ebjkUlVJWxXQjXZL\nyRxvbLF60/Yat8P4tqg5naAM5rMvfr387PKMiUoJFFrcUM4FUaklIY6IaMO4EBkVnEyefTrg8T1V\nhK/N24wceMpVpA9nj/19Rm0ydU2TkKYqM7uzxc/SRAYVKxcb4+BT5qrc7BocTIiIaq5H2/U2NZUy\nUGVQBoU2rKOcZ3ub+/zp1+I6VaJUamtbFHwmyGBG0H5NR4LWpCgrqhRS1YNG8g8//iEREX360z9c\nHguC7xt750AIPM28RImZMTKfziTTXOBvO4p4WcecORgy2a5YSObZbpjyHbnH06Fc/02jVvNoe7NH\nqSOZYAoEYBzL/TozbtMo5+ccqVKXJnSOdZbt1uCMhGfwXRkrIQiaa215dg9lKZdDfie0Q4zpqTOV\niBdwJUuU0trxYxAvMaDHSpnvbMjPt3NDyiw3H9wjIqJvLlCqlsi51lEm2CWZ852F6tvXCJuZ2rBh\nw4YNGyvGlTJT13UpqgfkqIxj7yFT6E+fi+vFy8e8Gs6xIumpovZoh1fRZ0rLN8P+YtRlfL5UxdSE\nfbcsl5VLhf3OW++wL93Hc9lXOHnC1P7NW5ItfHfK1zpQ2q8eyiqKFIW8alkRbTBWv6nKCTr4LETp\nQ0u5CxBW7jPlj1eWOk19s/A8l3qdiPZuie5sDCeboyOhnlfIwo/gyFOovUpTjO6oNh0M+Nlf7PNe\n39b2zvJYDh/TDUWTP8e+W6/DbfDoPSlT+sOfcd999YXskXz1JZcNRJGsRD/4kPe5f/KTT4iIKAyk\nzGD/hO+jfy5jYhOOEQ4ygEQpfxhXn5qSj82SaxBtICLHzalSOrZlyefN1Z6O2a+uSv5vzZfM8dZN\n3nOZzqTI/fyI9U33dlkMwqnJs9fQRscvpT8J+6Ft7AWmF5KFnR1wX9z5ofTB/Ue8+v6r//k3y88K\ntMejPb6f93/y4+UxI+ARz2Rl/tF95jB89fecUR99/dvlsZ09RiYyhfaUxfXsmXqOQ70oIi2D6iFj\nDHXSh8vtYT/14DsZK8/2uX3fuiv7nEZ7djbnZ8xVKU8YMlKWzNUecMVzSORyuzWVBzOSFopVCV4I\nYYGaFr7AHHXZh5esmgMOIKTSVIhPE9qwpuQlbcoktNtj9G0ylXE3ODulVcNxXfJbEaUkc1Xm8rvl\nKv3dVpefy+vC1ehS0K6dEY+f+UyexYPgzsJlVGWo3XRSlK4sBL368bs8d0ctRlqeH8ncPAUS49Vl\n/hhNue33v1P8mIyv3wMnZ6KrFQPu/3d+8M7yswt4Lz9Dmd6m0uYNS0ZmkrkqaetcLde0makNGzZs\n2LCxYtgfUxs2bNiwYWPFuBoBiYi4mkLBPSjH6Cmo8OnXDNuNQRTpKiWQNRCW0qlSU4FFW7Us7RBo\nowIc6ygqfoE1gFHRuPPow+Wxm3eYxj5WRrSjp9D6TJV+sOF+45rzhUA4J32+t826wLwZiDnjKUM4\n60q3dwn0qPIT7xqkSz3XpU6zTq2mQK45iDiTiTK9PT7G5fn63ZaQk27CysxTxtEdwO4vnrMl0tFL\nIf74gDh3N0Wnc3OXS5Ycn8+blrIG8/HvH33yo+Vne3v8/ZoqZ9ncZtgqArybZwLbjwYMG+VqnFDF\n34uhzzpVkOR0YfRvhQqfqxKCN42KKsrznNJMxkkKj75ElUmVqSkN4Nen5il4H1Z97+2Jgk4Xz/rO\nHf4sagghLwEJyJsJwePXv2SIdQ5q/tlAjt1+xOO7qyCq599wic5OT8771h2GmaspE5FaqgzADAW9\nEdGqwQS9zc/kLqQEIQRhzffVOLweaV7Ki5LOx5OlVisRLYuEZooxOOgDDjwARK3nCECLeaVLiPjp\nzkHYcxXB6f4OE5W2FTGyf8LXKgvu2+NjMV8fjvn9ckuBdLvYknoBCJ+IqA4yYw1zg94Oa4JwU6WC\nRZ5h3Ht4p0dzaXNzrnZDrAe32qJs9qaRZRkdHp+QEyltb5fb0lfbQxdQL3NRajhw5NotlDJOYpmT\nByAG/Qaa6+6eEH/aKIm5PJftirHDvRxBE9efK4Ih9NtHsdzP2TmTK0OSMdGFutcpSnXOJ0pFqcH9\nfXAhqliXp/w7MES799T2WQobPF1yFrpX+3m0makNGzZs2LCxYlxNtIHYOMJTaVcFunw1FrV94+na\nQqFvqRxUXKz89m5L5hOf83EXArgLtUovsamvSzsqmHwHoXGLkBVjAFPaIhZCR4JVVvlK5ohHR7nH\nRGlgZj6f73ZDVmMhaOwuMrFXvJFB4XdfWZtcA0GjIqK8olw9exvZR7xQeQWo4QGIG722ZC2viEsg\nTCbVgJOGJiwZJ5SGMocOoN0bYbW3UGbv/SG3bayK/ANkATVl0B6hnCozGYVatbdQLrC5KUiAWdXP\nQC5L1YrerBi9VMZcmij2wZtGVVJZLKjIlGYndIndSs5vTL4rJMtB4/um8DVF1rkJreRpn1fJXdW2\nIch025Fk8RsoWv/yK844O1sby2M//TGXbgyHyt0CDhnbqlQoG3H5wuSM34OJusWdLW7nWJH64ksm\n9Pi4n8GJkF0yOCu1NuS+tXDLKpHlKZ2cvaQ1hb74phxjKPPG46ecTZxAJCRX5XBhwN9fqPKKCUzE\npyCLFar0yJhUG2SBiGizxdc37lB//w9/sTyWYJzt7b63/Gxji7OauRZaqF6dx+qqdGQN7+14IvOM\nQeA8n8dM4cj9n56iXKan9YNXhwM8x6WOG776zsMGqaqpSS0CEcvlY/O+zDcvgfp9dyKI1j8PuASp\n94CRk71NcV4yTkp1VboUg2A1POW2PVSVbYYoNB+K0xXh/d9uC+q26XGb/vgTRmF+8eR8eezzF5yF\n1i+EGJugnLEJrfPOutKAX+c5v96U83c9xXB8jbCZqQ0bNmzYsLFi2B9TGzZs2LBhY8W4OgGp5r2i\nmBOPGOaI+5JiG0jPQ3qfKXWcEhZmG8qsmLAZPoMh7W9/I5v/4wnDfBrmbUMh5gFqXKO2wGA+iEqG\nsEBElKBuKVMEhTm0IZu4V6O5S0S0tcPna/UEdgxRC+ZBLaR0BH4RiVNpl0zd75tGVZWUZukrEOcC\ncJVWBKpQp9tCzVujHqpz8DFH4dIl1IKyNMF/FRkBkLWnoHMfjBWjC1xTOsYO2uFEmb0vALFViuJi\nBHPMNVNlMN+BNu/OtigrjWA8PJ2Yc6mtBUMQUqpH6bVo81bkU/EqQo860yIWcsgctk5zqL2UCqJs\nASYqFcwb1Xns5NgSOXy5vzzWbTP01VQEuyb6rwX47ScfSK1cF3WQB6cCX93ZYUhtrBRyvjth0sz9\nW0wGK5X+8v5jvv9C1Rrevc/w3C++eEJERG4mNX6/BzWhek/6x6muSwHJoY1GQG4pBDNTN+0FMn7a\nqHs8POf3qlJbNjkIP0UlJLSozt+7+xY/10i1TbnguaFwdUfjmtjmONQqYiBB7qzL/aTQyfUdZaeH\n7awFthy0PrfZrhgpIt0Ec2GAbZdA1R+XUG1LlTVfo3c12PF3hes61GhEtFDqP2CVUqkIg9vYcnkL\nanTNC9lWOBzxv//PE5mnOx89JCKi2z/m+udE1f43sC3zzeXB8rPJOa6PY8ma2vYBBFzPBYY1JbDr\naq7v4BV77949PkdTCGWjNZ4bFi2Z88Mtfk/u3ODfnkIpYKHUliIFf3vR1baObGZqw4YNGzZsrBhX\n1uZ1PIcqtYrOkZlUdVk1FcQrlxk22IOWHPNCXn3NlKLNepNX567Pq+HPv/5ieezJUy6z8RVN+Se/\n93tERPRxmzNUR5stQyFIm2ebpCZRiiRtKBjdusurIEetUhugoG/dub/8zAcJZzGH+sdcVi2Fz5lS\nFcrKsrgGBSRyXaIgWhqCExEVEHOtlFqJizKfCBmjr3SSwwAKSMrQ3eiqmpX0ZKRWv00cWwixIk14\nJVcDMctxJSN0kUU4KjN0QMAIlIZzDeMkx0pbG4EHaLeNTVmJ5glnEmnEx8YT6U/R65V2ya6BgFRV\nzHMYL2RV7QyZNOSORXHHgUl2EAB5yeX7hohVqvaIUUpjpHRGsfRdUQdqMpdSj6++YAWpWo2zg42O\nQglAiKoV0h4zZPFNNf4+fsgKYFXC1/z622+WxzLc/6P3JeM9mfB9PzmGKbsSlDqCTu/2Ww+Xn5XV\n9SggUVWRm2U0TyVTMg4isWqTbovbenuN55LMl8zZx7u7synjLU74352Y/zvrSdtcnPJY2eqJHvet\nW5zB3oES1LwQAuP+PmfmxyeCvmUlv/PTuSAyNYz3OeYgXZ6XAEVxVVlZhHczMs+SKh1elPQM54Kw\nra8JOeZNw3FdilrNJSmIiKgCAWqitG2nE37WesCoUV0RBr9CRloE4urS3GYS0Mklxo/KqC/RDAcn\nUqYynfDz3bzN2WK9peYnl/vYV041vS3+jQgX8jvQSvgcjsf9v7Em/bmZsRpW6ku/t3d5fhlifKVj\nmceinPug25NxVaQ2M7Vhw4YNGzb+VcP+mNqwYcOGDRsrxpVg3qKoaDxZkKcUXwimuzvvf7z86PgC\nllowIZ4rss4mDJKLiYLSAEFuwNLq3t17y2NffM0C6mvbkvL/7Gd/RERin6bJOCnBHHwqkIzJ9Ls7\noibilDCPRco/HErd2laD76MMFOyIdUcJxe2TlwJZ+BC2Xtt7e/lZ6a9O0PBqAbV39ihQgvExTLMr\n1QWmrrMFON2YfhMRFYCDi1zaKIPqzhhKI6nadA9zPtdkKoSNqMGwvYGHfUVUKAuIdKffF5p3tJUe\nYM8FyGA+aTN5/qxRF5jG2PjVUAtYqj6uA05zFYGmqlaH1avKoSxzKFEwrCFKaXuzBmosfUD5cwUL\nz1DTnCrSXQyYfgDSzmQsz+LVQbaIZHz7IDS1GhAb9+T7vsf9s70j6i0zY5idyveOoTazv8//dVXN\n40c/+oiIiE4GQir5hy+4prWElfPOTSEbnV7yOZ49F8LJjXuicLNKFGVJw9mUStV/TUD7a8quLAch\nZ3DO72v7hhBRdjf42aJQzjGawDqv4Ge8sakgSZ+JKqlS8FnfZKLW9g7DvE9f/GZ5bBP2dLlSxhpA\nlSpVCv1TzCU5CH0LpaZT8/jfnUhgx02ogU0X1fe+XxicPZUxPpmurvJVVAX14yE5yiItASxdc+Tn\nwANMG9T4ey+UktAC7/LOrtSSroOcloCsdTmT8w/OGKquqdrknTXeOmrB8s/J5dp1qEt1tqQutdeG\nZoEytJjl/D4dQZUuD5Vl5jrf2/aOkJJ8iPY/fs5Q9I1QjoVN7rNUGdInVzTPsJmpDRs2bNiwsWJc\nKTOtqoriOHtFc9KUvaQ12Rzf++RTIiKqH/Im/vBSVsAxMtN4JpvdE6iZ5FiFvX3/wfLY2/d5Bdxo\nqKwFmdQCK6qqkse46DMZYzAUBRcjyxlEkplWKF15AmukzTUpg1nD6jRWFk0uCFCTMa+4Lk+H6hh/\nL+rcWH4WdVfX0fSDgG7dvE2VSkONpZTrSqZh2qaBDLYsZYWbLUtQpI1qyBhNWQupVaqxt9P8kjnI\nSKHJHEtFhHDMfShSGlaufvB90pPxo9ZiKzmID7qEKsIK3ljvZZn0RYbnC5RvnudducqXKUlqAAAW\nD0lEQVTrd4RDrueT58hYKwmanaHKJtGmDbRVpMgwccXPkqpSnlHC95vg+x2lVFRBM/eLp1LqMoXm\ndA8kuaNDUZoB8ELPTiVTyEG+clXplwvrvbcf8li+sSeZrIdx8ovffLb87PCIz3f3Hr97a0od5tlL\ntr2KVanTziOxgFspHKLSd5ZENiKiVovvrx7IZxPowTo+//f+I9E+fniPs5z+qZReFBg30ymUbVTm\nv7fN5z84lEzz8IizlY8/4XPd2/toeazuMgmyUi9FVnCWOEnkMzNHOCASNR3p57DirClVGskNQxjE\n+5erebWB0rSgLohInK2OvhRU0YxyisdCnHKAsKwHcr97Xc7a6hgrn72U8bmAclpXqYIRzM8LfJaO\nxFx+vQGLzbZkmnX4J5YlyIp1mVPCbZSuqLKjAqU2lTJQf3nMY3Ydykrrb8kc7gf8m3M0FD3gtYqv\nEcJKb1eVeiUh98vZTH43djqi0vc6YTNTGzZs2LBhY8W4WmZaVlQm+Ssbdi5KHFxhgVPk86rjDgrB\n926rMo6MVyK+cjGZEmc+L472iYiozGRF+vEPP8S15ZqHoFgHyAhmM9lLODrh1el8KqtOQvG0Lq9p\novyliz3f3pqsxANTAqKyM7MqNaIDnlqlFnNeBY+OJFvotK+2qvld4ToOhfWQFrGsAGcw5F3qEhNR\niJWiwfgrJWZQoWygpqjwKUqWXGRPdbUqbMC9ox7J+WtoiGTC/eR1lGsz9EIrRYU3SaqvVtqm1iLy\nsAdZyQp9gb/V2qPrm4wilLh2lkmpAhK8ZQkOEZFKbN44XM+hqBlSniq3oITHVqJsUnyUgZkMtVBr\nUmNirbbvyDNjF/vFN2+Jw1KAZ/jmu+/kmiifWNvmfcGDE8kicjhp5Cpr29zm/aEbO7KH5WC/ldD/\nB0eyQp+cMnrTn0qf3bvHpQ3vvcvlYKMLuWY74jHRViVuY6WfvUoUZUmTxZQ8VzKwtG/2quT+DC9g\n723OQhqqQL/Z5iz59ETGVJLze9JtMlqUqe3GGJm8H8k1k4zb5+XB10REVA9EVGaKEhdXtXkMl6lY\n7Wma98j1IAjTFsTCA3qWK9TIxUDe7fKzxZHc/1EJnoJKdxZTVe73huFWREFBVCkUw2gU12v6YuCU\ngB8xUvNHAjER15X938UFc1SOZtyO9a6MlVbEc6tbypzfW+N3wK0B8ZtJX0/HPLb0nuUi5nuLz6VE\nbTbka05a7Iz1qCP76GsQxBiM5bxzID4O+CNf7ovR+MYdHlcttXf77Ft5J18nbGZqw4YNGzZsrBj2\nx9SGDRs2bNhYMa7G2qgqqrJyqQVLROR7pkxB0ukK6XQF0knNUZqTCR8rfaWAg03upIQyhYIzYhjm\nTmcC215eMJTgkNH+FTjAlLrUFCGlAiEmT1TJg8v32wRkV48ENmoH/O/NNYFqzSPPzhnKTZS5dQli\nQKkUT5x4dW3esqooSZNlOQyR2Da9AtuiHGMU82e5JvKY8gJHPru4hDYroOpGU+Cobo9hb1fphBoi\nUQFYczyUsiNj35QqSMY1ur7a8g60+Ca0a3OFWEUVQ8o1pZjk4d8l4EYDiRER1aF1G4RKH9lZXR2G\nyCGv5r1i3t3qMEmhNpdrBVDcqgOCDnxlq+VyXxQkn3WgPV2BdFdTz2J2HlxVbtSCRVoAUtxOW4gS\nm+ifWlvGdwjI/0LpI//dL/+RiIhSEGSePROtWVPO1FClBJ98yupGt24w3DW9EDgtQN/Nlc3id998\nSdcRZV7SpJ9Qqdokh06vX5M5ohGg7AtlecX82+WxpstqN2MR7SKPGKZF9QmdXoqJ9wzqWlkhbZiC\nOPOLX/0lERFFkZBxhoC0u+vSXhlKQGYLGfeBa2zcKnMTy6gBxg60dyOmiBikpFIRbtZhd1cou8Py\nimbVvyvKoqR4PKVmQ55lNuM59lzZ7m29xQSvHFt6iSqbOYEilqNU5m53eQ4xWx4LVXa01uN3U5PA\nUpR7hSBPJmMhFkVookLt1B2cwVJwInNPuw4FKegIt1tCcMpfcH/PhzLvmSE2RLnYVJEmbz3iuT6b\nSntPpjLeXydsZmrDhg0bNmysGFda6pRlRdNF/ApF3GiQznThemJWlliNqSxnKYGrCpQNj8TDbntT\nER0WyEhralVmSi9MBhapEowMeoq50sY1TiiV+swQCFKULZBaefmgiDu+ZDvm3vonTIRZqIL3Eg/g\nDJRzykhKZ940HIdJMYV2R0FGHKmN8ilcPQglQ9oEvah4NWhKaoiIBlhZGrGLQJkkt2CSXOmhAVGN\nAAbWmTJhjmG+7NXk+w6yLN2PrRbcVFCSlFQ6c0fZgyIxjUDdv0BZlfaE8YwghiIsBZFyv37DcByH\nak5ITkPGU1XhvKpcxkV9SgnkxY2kPfwWvwf1mfR/BXSlqlC8rsbrk6f7RETUHwqqcec2EyouL3nl\n323Kiv5iwuP2/LkQsk5BKDo7kXKEOca3ITt9+pGUenhozXogiMQ6SmFO0d4XSgs5anG/DMZSetJq\nK9enFcLzXOq268txxGFcilRqZ7SoCyNkIceePPkVERE5al4KQRYy7ihaFKKNseiQPH+Gd6dCmYrj\nSZ+GKJtIY3nnOxDbqGaqnMXhfs2RUcdKRKMWoeSsUEQrzEcx4R1SmZJ5hyuVnYe+ctB+wyjLghbT\nCeUk70sCZG+cyBxew7MMQUTqzwVpaXW572cXMt4WU5C0OvzOjyYy39SbfI65uv0ARMcS5y8T5QoG\nU+7x8fHysxenPN437glaeOMhk+YqZMVD9ZuSXnJ7h4n63QCSNBzx827cFqLhACInZ0Npg7pyEnud\nsJmpDRs2bNiwsWLYH1MbNmzYsGFjxbi6OXhZCS5LRMbfrKbqxEw95/eUdoho6UCkijiX8IZRx4lU\nHSMulSkzaeNuFQHaC5XuaAyYV2u5Gjs019Gm1iCF4P8TZQlWQV1oMhZoYwZFpWT6KkRKRJSBcBAr\n4s/0GurwyrKkNJlTkQsEZiDrPFdQrjEiX+rwCqR1gbqs0UQgKh+QbOzEuI5SegLcrRWHCkCdRmHH\nVRDbUidXQYYe6ozrofSLgTZNN2oDrwz3ra85R22tqfGrFHHD8YyBuYIBndULTR1yKag1lm1MRFRh\nLCZK6caQTwzPxFFKMBnG2kRZ9DlG0Qiao6dnAo99+RWr63QVyWgHOrGbqLWdKmbNl1/z958+F0Wa\n8YihKe1H/4P3PyAiorfeYnJOWQjstpjy+YYK+r94AkIPyEx7t6VmdQM6ql6pDN0bsnWzSrieQ62e\nR8FcpqKoxgQwX9kGLkC8M57WrtLEDaCK5Dky3hzD7kF/+CpvCMEMclVdpdnqyByGCmcKdm6jxrXX\nUspYBcaFukcf1/dBwEsTYdBkqDGOFSycgRSYgGmz1ZJadxfzo+PLcy7ia6gzdR1qNn2a+0p3FrfU\nVBrg3RpDnAPYnLW2BdYPE36/L86VVvGCn6XW4b/r1pWV2YLbOVFQbg0a0EnOHTpXBKT+jEmeI6Wi\ntL0LO8JtIRmZ/qtgQdnoyLbSZMhbKjPFdFzD3378B58QEdHzF0+Wx/7xV6zFnChibLMrMPDrhM1M\nbdiwYcOGjRXjagpIDlFec6istPIM/1uviiuspjIseQqVVRgijza8NuQABzTsXOUtlSGZKPNuk4m5\nyIY9RUQxLiO5VmkyZBf1mSHMGD1N7fJh9H1vbIvWrtGPTWFqHijCjTH8LZQea5xqQsUbRpFSNXxB\npEp6CCSdRJ1+Bt3KFspOtK5ujrZqqHKPHLqlDZQF5Yrg5KBNQ6WAZFxm3BA6mmoFa1RQVPJMPsp2\ndMZr0AHTA5nKnlOgCVqb13Vh/I7+CYLv6/B6agxdhwJSWZY0mS4oVQQrQ1qrHF2igAwIx2oqw7lA\necHL78SMuwciltGS/uJrWRFPkcGGNWnTFwdMtkhTbr9v4JxERHSJsqQHj0S/2iBAx8dSNnAKwgZV\nnIF014TENEWJzmgkZIsSbf/Oezzm339ftHfnc2SyfSmdePZc1GNWiSIvaXQZL9ESIiIHzjjxTGlM\nY0wXeIcdNaeEyOLKUtAgD23iYWAUinxYGRRNjx+8NB7QK52l1YHWdDzJxjO865OaDPwcer2hx387\nGQohMWzC2UYZagfIZQaX/HfjUtpgD8QxXyF+kzNVK/KGUVBJg2pGqUJfytxkzfL+/eoLHr95wMfG\nQ8lkGygbqj+QMZhDhc08XzWVdpmiZCtXE5MDVyKvAZRMISeTFEprqvzrrXdZYzpXaNTFKWeuT864\n/T65LU5GDZjBhyNFJJuizy6Y2DQ4FVUwD+VrpBDKbv1qJDubmdqwYcOGDRsrxtX2TB3G8NNYVjU1\n4NWlysqM36nJcvxQVnQmiXSUA4XJPlALv9TvJSLqdjijylIl5AAqdw3XDFWpQbPJWYDea6iFJlOS\na5qszDh+FKrC2mSri8Vo+ZkpqEa9O3nKl7ARQnRCZYSLePU9U6eqyCsyctSzJyNe5S1K5a/p8MrM\nWeO2ypU/aRobHVKl5dvg1aNZvZtMlYjIr3NbNZSzybI0x/hqqlKnDJ6Cs4U8b4CMtL6Q4dXAflMO\nbWN9jxWyB1PWREQ0HnHbt5t8Dt+TTMGU6uiloB5PbxplWdEijpeuRDq0c48LfdUS6fhz5fP59W9/\nSURE/b7oNM+QCU7G/N/dG+J4srUJL1q139Pv84r7xSHvi9Zb0t4/usd7odtbokNaId+PlJDH0QFn\nkTsYEzqT/eYpZ5WaQ9DGfp0Z++fncv+nZ+xak+aSnUShZFirhOvWqFlfX2bhRETTKbxvlSa1Qa/a\nDUZf5uqdmGNvUpeQeRBkMGIhC8W5qPC1hnp+s08eAd0JfbUnjDGbJHIOIypSU16zA3goe3BE0RyK\nPObMqyavIXXX+B275XEpVJ6pMhhkeoHyRV7bWL0cKc8LGgxGryA5fsXP6qryr2OURmXgA3jKFcwh\n/r6vytF8zLsJ7rcWStuGEQQolLDLBO1xfMJtNlsIL2Bzk/c2d2/dk5us+P33lGDQzjpn78OnLEjy\n688/Xx4ze7ytmoyrGd6J/c/5fc27Sm97m/dH7yl3pe1N4TG8TtjM1IYNGzZs2Fgx7I+pDRs2bNiw\nsWJcCeZ1HCZbOKqsZamANFMalTBt9kEUcRSEamCRMNSwKmzC8NuejeVc9TWGFzwS2nicgAwESQ1N\nTmm0GdaaJwIzlFCsqXQJCK4VgWjgegJx9GAaWynN3zFIAl7B0IyrYKBGl2GJeaw26cPVtXkrKqnI\n01cs5lJcQxMTEpQ6pD5KTBQcV6KEQBu6h4Bg3MpY08k1AyiuFKXAaMbuzcDBWgHLmPqWA4HEYxBt\nLi9F37UBqrzR8J1NhJyRxPx8o758P57xM91GicbwUsg1GbSI9X1cA/+IT+KQiHgSkQMynFbXcal4\n5bPFVMgTPiDwzrrSdQZJ5cH7rNjywfs/WB4LfUPqkv48P2c4qn/GUGtdbSnch1War0lPl/y9tbXt\n5WcPH7zHn8Gk/vRCyBYtQPj3Hrwj9w1SSQL41BDuiIhuP3yXiIju3Htr+Vm3w7DYf/4vf06rhOu6\n1Gg2aDqT8VCAEOMo9Z88hoE2CHV5KQSXAFsYSSqjoASM6EALvFLj/3IIvW/lG2lIZAXenamaK4xq\nl6tK/Mw7lCrVoMWCx2UJm0GjO0tE1Ol0zcnkPqDyFQX8vUiV2Zzg3akpKDVsrq7NS0VF1SinTD2f\nZ/h0ypZtinm91eR7W1elbxlKXHqkSoXQNkNA4g01p2zeYFJbbSJQ7ijlUr0Uc2yrKRD2Wofn0yKW\nk8xS7s9WR+6jrIFICYj5QM0RVcb3djoUJbL+OcPBccB9FtWlzKZCH9SUTvnF5GqEL5uZ2rBhw4YN\nGyvG1TJTcqnmBeT7sroy2Uqaq2JXkCmWdHNlBO44vBLRpCHDdjaEosqTzKoGqnJdCTlUKB5PoWlZ\nqrocUzDdbakVDMg6ldKDlcJ8fpZuW1ZZ29jozxJZnddKXlW16kwe8VtS0OugrGHYl5VRQKvraFKF\n8hKlteuAORCqbGWOTDCMTMmI9E8IcpTW600WvMqrg4ikWPJUoAxmPhECStAD2QWEDXM9IqK6A3KB\nopSnyGo0gcRHyZKHUqpUEQ6yxRDXlvbeucmrWUMg0SbJ0ymPp1zpQZeKYLJKOI5LniLyGHEPY+5M\nROTCBcl866NP/+3y2Ps/+gR/J+eskHE4GJttlbEEOK0u89naukdERMkDkFbUsXYPGY4qDalB8GHr\ntvS775vxz9/bmAtyYMgzpgSFiMhD5hFGBrXQjj949/R7fE3r8KqsKF5UNJnKuxng/m5vCwHEzDMt\nZA66nMvMMyNF+qvgLOTlpmxNzt/b4PFcV0S6FGoQDSM0ohg6pRFD0e+h0RNXZXkNuKNMFkZoRJ6z\nDuShXZcStTEQjQUED9y6/MEYSJ+SDKfQWX1Occght6qRozJkQyxUw55C9IEHAk+eStvWgQT4yuVm\nMuL3eQLSpqvG1qXP7VEUylUL/95Bxr62vr48lsGFazIWNGUbmav7CqLJ/66DNJZ60j8XKHsZK+Gd\nC8wzVEJoSDXnHhCcUs1ti+nVSI02M7Vhw4YNGzZWDPtjasOGDRs2bKwYV9zRroiKnEpdbwUykqt0\nOxdThigCQIxVKXCKIRAM+wLlbsD+yVgBkdroH6CuMlAYRElGBQUEkLlsFDuAXZpqI9mIJ82VXqoP\nWMto3HqKJBWBcNNoSK2lMRH3jQ6wgrxmgAbOj8UWq6pWV0ByXYfqjTo1W3IfYcQb94XSJjVKQw60\nQVsKsvY8QyjSWsUG8gKRRpGZhoBFQgW1z3CpOohWWmFpAvs3dXoKQWaJY4EWR0O+7wg1xzVdZwzA\ndGNzb/lZE9dKoJE6nQikazRbHUWKSFcvMyXPdanTaoi5M4miljYnd3G/BurTOsOO0XhSMGy11IEG\nZK0M5YwGq8YEm13ux3rRxd8ptSjAr7oPGhsgzGmfOtyvQSHbG4psYZTJSqVktjwfTqJt/PC8pWpk\n19VP/eYxXyT068+e0lzpBG+g/vL+npC4GnVYr7mmtlymLg/3lypSUgJCUQQN2Lny/1rDtkWkCE5F\nxmPJdFuojOoXGIOZVlEytb1qu8VsCZDDkOc0ljFr7lHrZteA4YYgUM5SeecKMwcqver0Gga543kU\ndlqUKm5NDAW3UFkxbkQMq46nDI3WfU3oxP2OZKvGBdk0xHzjNmV+zFB7OlEKSwZmDupQOoukjjXw\nuW132wL9Pmhyn/3zQKDfmx3+bANbhkcDITCenrJdYKXG8e4ajydjh7exJ4S9Rpe3vHL17qe5jMnX\nCZuZ2rBhw4YNGyuGo8sL/sUvO845ET3/F79oQ8fdqqquJqWBsO39RmHb+1833ri9iWybv2HYMf6v\nG6/V3lf6MbVhw4YNGzZsfD8szGvDhg0bNmysGPbH1IYNGzZs2Fgx7I+pDRs2bNiwsWLYH1MbNmzY\nsGFjxbA/pjZs2LBhw8aKYX9MbdiwYcOGjRXD/pjasGHDhg0bK4b9MbVhw4YNGzZWDPtjasOGDRs2\nbKwY/w+khsxhHWyQtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd520040990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'airplane'), (1, 'automobile'), (2, 'bird'), (3, 'cat'), (4, 'deer'), (5, 'dog'), (6, 'frog'), (7, 'horse'), (8, 'ship'), (9, 'truck')]\n",
      "\n",
      "[[[ 0.26917797  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.73082203  0.        ]]\n",
      "\n",
      " [[ 0.26917797  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.73082203]]\n",
      "\n",
      " [[ 0.          0.          0.          0.73082203  0.          0.          0.\n",
      "    0.26917797  0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.26917797  0.          0.73082203  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.26917797\n",
      "    0.          0.          0.          0.73082203]]\n",
      "\n",
      " [[ 0.          0.          0.26917797  0.          0.          0.73082203\n",
      "    0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.73082203\n",
      "    0.          0.26917797  0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.26917797  0.          0.          0.          0.\n",
      "    0.73082203  0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.73082203\n",
      "    0.          0.          0.          0.26917797]]\n",
      "\n",
      " [[ 0.          0.          0.26917797  0.          0.73082203  0.          0.\n",
      "    0.          0.          0.        ]]]\n",
      "<NDArray 10x1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from cifar10_utils import show_images\n",
    "%matplotlib inline\n",
    "mean=np.array([0.4914, 0.4822, 0.4465])\n",
    "std=np.array([0.2023, 0.1994, 0.2010])\n",
    "images = data[:10].transpose((0, 2, 3, 1)).asnumpy()\n",
    "images = images * std + mean\n",
    "images = images.transpose((0, 3, 1, 2)) * 255\n",
    "show_images(images)\n",
    "#show_images(data[:9], rgb_mean=mean*255, std=std*255)\n",
    "\n",
    "print [(i, l) for i, l in enumerate(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])]\n",
    "print label[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 mixup: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:44:12.380361Z",
     "start_time": "2018-03-09T14:44:12.339450Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "mixup_test = False\n",
    "\n",
    "if mixup_test:\n",
    "    net = ResNet164_v2(10)\n",
    "    net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    loss_f = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "    num_epochs = 1\n",
    "    learning_rate = 0.1\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    cur_time = time()\n",
    "    iters = 0\n",
    "    \"\"\"\n",
    "    data loader first time run will cost about 3x time than after run.\n",
    "    \"\"\"\n",
    "    for x1, y1 in train_data:\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time\n",
    "        iters += 1\n",
    "    print \"cost time:\", time() - cur_time\n",
    "    print\n",
    "    cur_time = time()\n",
    "\n",
    "    iters = 0\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'momentum': 0.9, 'wd': 1e-4})\n",
    "    for x, y in train_data:\n",
    "        l = x.shape[0] / 2\n",
    "        data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label.as_in_context(ctx))\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time, nd.mean(loss).asscalar()\n",
    "        iters += 1\n",
    "    print iters, time() - cur_time\n",
    "\n",
    "    # load data one by one batch\n",
    "    # iters = 0\n",
    "    # for x1, y1 in train_data:\n",
    "    #     for x2, y2 in mixup_train_data:\n",
    "    #         data, label = mixup(x1, y1, x2[:x1.shape[0]], y2[:y1.shape[0]], mixup_alpha, 10)\n",
    "    #         break\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time\n",
    "    #     iters += 1\n",
    "    # print \"cost time:\", time() - cur_time\n",
    "    # print\n",
    "    # cur_time = time()\n",
    "\n",
    "    # zip will load all datas and then iterate them, too cost memory, will drop speed when memory over.\n",
    "    # iters = 0\n",
    "    # for (x1, y1), (x2, y2) in zip(train_data, mixup_train_data):\n",
    "    #     data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time#, nd.mean(loss).asscalar()\n",
    "    #     iters += 1\n",
    "    # print time() - cur_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T16:17:13.256448Z",
     "start_time": "2018-03-13T16:17:13.170808Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def abs_mean(W):\n",
    "    return nd.mean(nd.abs(W)).asscalar()\n",
    "\n",
    "def in_list(e, l):\n",
    "    for i in l:\n",
    "        if i == e:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, \n",
    "          lr_decay, wd, ctx, w_key, output_file=None, verbose=False, loss_f=gluon.loss.SoftmaxCrossEntropyLoss(), \n",
    "          use_mixup=False, mixup_alpha=0.2):\n",
    "    def train_batch(data, label, i):\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        _loss = nd.mean(loss).asscalar()\n",
    "        if not use_mixup:\n",
    "            _acc = utils.accuracy(output, label)\n",
    "        else:\n",
    "            _acc = None\n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            print \" # iter\", i,\n",
    "            print \"loss %.5f\" % _loss, \n",
    "            if not use_mixup: print \"acc %.5f\" % _acc,\n",
    "            print \"w (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.data()),\n",
    "            print \") g (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.grad()),\n",
    "            print \")\"\n",
    "        return _loss, _acc\n",
    "            \n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        stdout = sys.stdout\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = output_file\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    if verbose:\n",
    "        print \" #\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "    \n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if in_list(epoch, lr_period):\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            \n",
    "        if not use_mixup:\n",
    "            for data, label in train_data:\n",
    "                _loss, _acc = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                train_acc += _acc\n",
    "                i += 1\n",
    "        else:\n",
    "            for x, y in train_data:\n",
    "                l = x.shape[0] / 2\n",
    "                data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "                _loss, _ = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                i += 1\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        if train_acc < 1e-6:\n",
    "            train_acc = utils.evaluate_accuracy(train_data, net, ctx)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "    if output_file != stdout:\n",
    "        sys.stdout = stdout\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. get net and do EXP\n",
    "```\n",
    "I want to find out the reason that resnet18_v2 get fail baseline in CIFAR10_train2_* script, design follow exp.\n",
    "\n",
    "1. 5.1 train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs.\n",
    "2. 5.2 train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18.\n",
    " there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 re-exp resnet18_v1\n",
    "train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T16:30:21.462308Z",
     "start_time": "2018-03-03T16:30:13.299911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932408146965\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(10)\n",
    "net.load_params('../../models/res18_9', ctx=ctx)\n",
    "valid_acc = utils.evaluate_accuracy(test_data, net, ctx)\n",
    "print valid_acc\n",
    "def get_net(ctx):\n",
    "    num_outputs = 10\n",
    "    net = ResNet(num_outputs)\n",
    "    net.collect_params().initialize(init=init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    return net  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T10:46:50.521728Z",
     "start_time": "2018-03-03T10:05:27.156785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.80346, train_acc 0.3266, valid_acc 0.3598, Time 00:00:28,lr 0.1\n",
      "epoch 1, loss 1.28281, train_acc 0.5411, valid_acc 0.5778, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.01865, train_acc 0.6431, valid_acc 0.6533, Time 00:00:30,lr 0.1\n",
      "epoch 3, loss 0.90869, train_acc 0.6870, valid_acc 0.5823, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 0.85972, train_acc 0.7049, valid_acc 0.6255, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 0.82922, train_acc 0.7158, valid_acc 0.6503, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 0.81416, train_acc 0.7211, valid_acc 0.6621, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 0.79129, train_acc 0.7286, valid_acc 0.6731, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.79221, train_acc 0.7293, valid_acc 0.6960, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 0.77941, train_acc 0.7357, valid_acc 0.7085, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.77304, train_acc 0.7350, valid_acc 0.6773, Time 00:00:33,lr 0.1\n",
      "epoch 11, loss 0.77300, train_acc 0.7362, valid_acc 0.6708, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 0.77379, train_acc 0.7359, valid_acc 0.7297, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.76346, train_acc 0.7400, valid_acc 0.6792, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.75981, train_acc 0.7400, valid_acc 0.7052, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 0.76189, train_acc 0.7409, valid_acc 0.7065, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 0.75300, train_acc 0.7431, valid_acc 0.7030, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 0.74843, train_acc 0.7451, valid_acc 0.7334, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 0.75400, train_acc 0.7439, valid_acc 0.6873, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 0.74822, train_acc 0.7453, valid_acc 0.6994, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 0.74226, train_acc 0.7492, valid_acc 0.6627, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 0.74857, train_acc 0.7449, valid_acc 0.6691, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 0.73817, train_acc 0.7513, valid_acc 0.6690, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.74331, train_acc 0.7469, valid_acc 0.7008, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 0.74029, train_acc 0.7473, valid_acc 0.6862, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 0.73600, train_acc 0.7491, valid_acc 0.7335, Time 00:00:30,lr 0.1\n",
      "epoch 26, loss 0.73561, train_acc 0.7486, valid_acc 0.7344, Time 00:00:30,lr 0.1\n",
      "epoch 27, loss 0.73582, train_acc 0.7491, valid_acc 0.5638, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.73652, train_acc 0.7464, valid_acc 0.5980, Time 00:00:30,lr 0.1\n",
      "epoch 29, loss 0.73622, train_acc 0.7506, valid_acc 0.7386, Time 00:00:30,lr 0.1\n",
      "epoch 30, loss 0.73189, train_acc 0.7487, valid_acc 0.6339, Time 00:00:30,lr 0.1\n",
      "epoch 31, loss 0.73360, train_acc 0.7494, valid_acc 0.6679, Time 00:00:30,lr 0.1\n",
      "epoch 32, loss 0.73711, train_acc 0.7486, valid_acc 0.6363, Time 00:00:30,lr 0.1\n",
      "epoch 33, loss 0.73315, train_acc 0.7495, valid_acc 0.6735, Time 00:00:30,lr 0.1\n",
      "epoch 34, loss 0.73185, train_acc 0.7496, valid_acc 0.7033, Time 00:00:30,lr 0.1\n",
      "epoch 35, loss 0.73066, train_acc 0.7499, valid_acc 0.7409, Time 00:00:30,lr 0.1\n",
      "epoch 36, loss 0.73286, train_acc 0.7492, valid_acc 0.6494, Time 00:00:30,lr 0.1\n",
      "epoch 37, loss 0.72791, train_acc 0.7502, valid_acc 0.6779, Time 00:00:30,lr 0.1\n",
      "epoch 38, loss 0.73055, train_acc 0.7519, valid_acc 0.6367, Time 00:00:30,lr 0.1\n",
      "epoch 39, loss 0.73307, train_acc 0.7487, valid_acc 0.7078, Time 00:00:30,lr 0.1\n",
      "epoch 40, loss 0.72583, train_acc 0.7522, valid_acc 0.6997, Time 00:00:30,lr 0.1\n",
      "epoch 41, loss 0.73065, train_acc 0.7513, valid_acc 0.6822, Time 00:00:30,lr 0.1\n",
      "epoch 42, loss 0.72384, train_acc 0.7533, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 43, loss 0.73071, train_acc 0.7507, valid_acc 0.6380, Time 00:00:30,lr 0.1\n",
      "epoch 44, loss 0.73206, train_acc 0.7511, valid_acc 0.6253, Time 00:00:30,lr 0.1\n",
      "epoch 45, loss 0.72313, train_acc 0.7533, valid_acc 0.6761, Time 00:00:30,lr 0.1\n",
      "epoch 46, loss 0.72663, train_acc 0.7531, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.72931, train_acc 0.7500, valid_acc 0.6451, Time 00:00:30,lr 0.1\n",
      "epoch 48, loss 0.72035, train_acc 0.7536, valid_acc 0.6857, Time 00:00:30,lr 0.1\n",
      "epoch 49, loss 0.73334, train_acc 0.7483, valid_acc 0.6765, Time 00:00:30,lr 0.1\n",
      "epoch 50, loss 0.72158, train_acc 0.7515, valid_acc 0.7363, Time 00:00:30,lr 0.1\n",
      "epoch 51, loss 0.72392, train_acc 0.7522, valid_acc 0.6832, Time 00:00:30,lr 0.1\n",
      "epoch 52, loss 0.72609, train_acc 0.7512, valid_acc 0.7577, Time 00:00:30,lr 0.1\n",
      "epoch 53, loss 0.72894, train_acc 0.7522, valid_acc 0.7357, Time 00:00:30,lr 0.1\n",
      "epoch 54, loss 0.72575, train_acc 0.7526, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 55, loss 0.72233, train_acc 0.7526, valid_acc 0.6178, Time 00:00:30,lr 0.1\n",
      "epoch 56, loss 0.72252, train_acc 0.7532, valid_acc 0.7112, Time 00:00:30,lr 0.1\n",
      "epoch 57, loss 0.72007, train_acc 0.7541, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 58, loss 0.72001, train_acc 0.7533, valid_acc 0.6893, Time 00:00:30,lr 0.1\n",
      "epoch 59, loss 0.72442, train_acc 0.7541, valid_acc 0.7461, Time 00:00:30,lr 0.1\n",
      "epoch 60, loss 0.72184, train_acc 0.7506, valid_acc 0.6913, Time 00:00:30,lr 0.1\n",
      "epoch 61, loss 0.72317, train_acc 0.7547, valid_acc 0.6189, Time 00:00:30,lr 0.1\n",
      "epoch 62, loss 0.72607, train_acc 0.7537, valid_acc 0.7175, Time 00:00:30,lr 0.1\n",
      "epoch 63, loss 0.71946, train_acc 0.7564, valid_acc 0.5596, Time 00:00:30,lr 0.1\n",
      "epoch 64, loss 0.72751, train_acc 0.7509, valid_acc 0.7269, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.72550, train_acc 0.7516, valid_acc 0.6568, Time 00:00:30,lr 0.1\n",
      "epoch 66, loss 0.72351, train_acc 0.7542, valid_acc 0.6292, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.72099, train_acc 0.7540, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 68, loss 0.72052, train_acc 0.7541, valid_acc 0.6509, Time 00:00:30,lr 0.1\n",
      "epoch 69, loss 0.71841, train_acc 0.7530, valid_acc 0.6510, Time 00:00:30,lr 0.1\n",
      "epoch 70, loss 0.72424, train_acc 0.7510, valid_acc 0.6668, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.71802, train_acc 0.7542, valid_acc 0.6872, Time 00:00:30,lr 0.1\n",
      "epoch 72, loss 0.72042, train_acc 0.7525, valid_acc 0.7391, Time 00:00:30,lr 0.1\n",
      "epoch 73, loss 0.72243, train_acc 0.7534, valid_acc 0.6784, Time 00:00:30,lr 0.1\n",
      "epoch 74, loss 0.72777, train_acc 0.7511, valid_acc 0.6956, Time 00:00:30,lr 0.1\n",
      "epoch 75, loss 0.72433, train_acc 0.7536, valid_acc 0.7030, Time 00:00:30,lr 0.1\n",
      "epoch 76, loss 0.72253, train_acc 0.7537, valid_acc 0.6999, Time 00:00:30,lr 0.1\n",
      "epoch 77, loss 0.72566, train_acc 0.7514, valid_acc 0.6808, Time 00:00:30,lr 0.1\n",
      "epoch 78, loss 0.72288, train_acc 0.7538, valid_acc 0.7191, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 0.72042, train_acc 0.7558, valid_acc 0.6245, Time 00:00:30,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.748897Z",
     "start_time": "2018-03-03T16:30:22.024115Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.61285, train_acc 0.7915, valid_acc 0.7278, Time 00:00:31,lr 0.05\n",
      "epoch 1, loss 0.68711, train_acc 0.7663, valid_acc 0.6986, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.70354, train_acc 0.7601, valid_acc 0.7469, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.70402, train_acc 0.7604, valid_acc 0.6044, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.70819, train_acc 0.7573, valid_acc 0.7099, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.71752, train_acc 0.7569, valid_acc 0.7387, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.71241, train_acc 0.7581, valid_acc 0.7282, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.71346, train_acc 0.7582, valid_acc 0.7717, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.71035, train_acc 0.7555, valid_acc 0.7430, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.71255, train_acc 0.7570, valid_acc 0.7069, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.71683, train_acc 0.7539, valid_acc 0.5970, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.71341, train_acc 0.7572, valid_acc 0.7481, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.71346, train_acc 0.7568, valid_acc 0.7049, Time 00:00:38,lr 0.05\n",
      "epoch 13, loss 0.70515, train_acc 0.7603, valid_acc 0.7366, Time 00:00:40,lr 0.05\n",
      "epoch 14, loss 0.71341, train_acc 0.7570, valid_acc 0.6632, Time 00:00:35,lr 0.05\n",
      "epoch 15, loss 0.70901, train_acc 0.7593, valid_acc 0.6943, Time 00:00:39,lr 0.05\n",
      "epoch 16, loss 0.71456, train_acc 0.7563, valid_acc 0.6658, Time 00:00:41,lr 0.05\n",
      "epoch 17, loss 0.71572, train_acc 0.7575, valid_acc 0.6613, Time 00:00:34,lr 0.05\n",
      "epoch 18, loss 0.71535, train_acc 0.7565, valid_acc 0.6999, Time 00:00:37,lr 0.05\n",
      "epoch 19, loss 0.70597, train_acc 0.7611, valid_acc 0.7358, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.71559, train_acc 0.7546, valid_acc 0.6635, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.71545, train_acc 0.7573, valid_acc 0.6657, Time 00:00:32,lr 0.05\n",
      "epoch 22, loss 0.71076, train_acc 0.7595, valid_acc 0.7046, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.71619, train_acc 0.7562, valid_acc 0.6314, Time 00:00:33,lr 0.05\n",
      "epoch 24, loss 0.70736, train_acc 0.7596, valid_acc 0.7478, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.70959, train_acc 0.7582, valid_acc 0.6643, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.71528, train_acc 0.7543, valid_acc 0.6441, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.72031, train_acc 0.7560, valid_acc 0.7425, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.71745, train_acc 0.7578, valid_acc 0.7464, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.70926, train_acc 0.7598, valid_acc 0.6095, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.46802, train_acc 0.8419, valid_acc 0.8559, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.42038, train_acc 0.8549, valid_acc 0.8552, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.41659, train_acc 0.8567, valid_acc 0.8418, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.41601, train_acc 0.8581, valid_acc 0.8321, Time 00:00:33,lr 0.01\n",
      "epoch 34, loss 0.41853, train_acc 0.8580, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.42103, train_acc 0.8563, valid_acc 0.8627, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.41386, train_acc 0.8588, valid_acc 0.8500, Time 00:00:31,lr 0.01\n",
      "epoch 37, loss 0.41497, train_acc 0.8594, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.40632, train_acc 0.8621, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.40439, train_acc 0.8627, valid_acc 0.8521, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.40516, train_acc 0.8633, valid_acc 0.8540, Time 00:00:31,lr 0.01\n",
      "epoch 41, loss 0.40254, train_acc 0.8627, valid_acc 0.8626, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.39611, train_acc 0.8632, valid_acc 0.8397, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.39748, train_acc 0.8639, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.39310, train_acc 0.8676, valid_acc 0.8517, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.38917, train_acc 0.8680, valid_acc 0.8554, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.38661, train_acc 0.8681, valid_acc 0.8591, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.38413, train_acc 0.8689, valid_acc 0.8555, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.37997, train_acc 0.8711, valid_acc 0.8538, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.38270, train_acc 0.8692, valid_acc 0.8484, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.38268, train_acc 0.8702, valid_acc 0.8478, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.38004, train_acc 0.8712, valid_acc 0.8528, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.37946, train_acc 0.8712, valid_acc 0.8603, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.37720, train_acc 0.8728, valid_acc 0.8533, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.37159, train_acc 0.8733, valid_acc 0.8092, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.37223, train_acc 0.8734, valid_acc 0.8453, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.37370, train_acc 0.8742, valid_acc 0.8661, Time 00:00:31,lr 0.01\n",
      "epoch 57, loss 0.37211, train_acc 0.8736, valid_acc 0.8377, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.37039, train_acc 0.8741, valid_acc 0.8310, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.37287, train_acc 0.8743, valid_acc 0.8420, Time 00:00:31,lr 0.01\n",
      "epoch 60, loss 0.23431, train_acc 0.9229, valid_acc 0.9057, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.19372, train_acc 0.9352, valid_acc 0.9090, Time 00:00:31,lr 0.002\n",
      "epoch 62, loss 0.17911, train_acc 0.9395, valid_acc 0.9086, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.17003, train_acc 0.9431, valid_acc 0.9157, Time 00:00:31,lr 0.002\n",
      "epoch 64, loss 0.16376, train_acc 0.9459, valid_acc 0.9081, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.15820, train_acc 0.9475, valid_acc 0.9153, Time 00:00:32,lr 0.002\n",
      "epoch 66, loss 0.15374, train_acc 0.9485, valid_acc 0.9127, Time 00:00:32,lr 0.002\n",
      "epoch 67, loss 0.15401, train_acc 0.9480, valid_acc 0.9136, Time 00:00:31,lr 0.002\n",
      "epoch 68, loss 0.15006, train_acc 0.9496, valid_acc 0.9137, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.15038, train_acc 0.9497, valid_acc 0.9058, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.15246, train_acc 0.9476, valid_acc 0.9106, Time 00:00:31,lr 0.002\n",
      "epoch 71, loss 0.15303, train_acc 0.9474, valid_acc 0.9042, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.15150, train_acc 0.9477, valid_acc 0.9030, Time 00:00:31,lr 0.002\n",
      "epoch 73, loss 0.15295, train_acc 0.9472, valid_acc 0.9058, Time 00:00:32,lr 0.002\n",
      "epoch 74, loss 0.15067, train_acc 0.9496, valid_acc 0.9097, Time 00:00:31,lr 0.002\n",
      "epoch 75, loss 0.15562, train_acc 0.9473, valid_acc 0.9034, Time 00:00:31,lr 0.002\n",
      "epoch 76, loss 0.15555, train_acc 0.9475, valid_acc 0.9029, Time 00:00:31,lr 0.002\n",
      "epoch 77, loss 0.15436, train_acc 0.9484, valid_acc 0.9031, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.15387, train_acc 0.9485, valid_acc 0.9032, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.15457, train_acc 0.9481, valid_acc 0.9065, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.15198, train_acc 0.9484, valid_acc 0.9091, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.15804, train_acc 0.9467, valid_acc 0.9018, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.15374, train_acc 0.9477, valid_acc 0.9049, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.15341, train_acc 0.9491, valid_acc 0.9067, Time 00:00:32,lr 0.002\n",
      "epoch 84, loss 0.15361, train_acc 0.9499, valid_acc 0.9019, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.15058, train_acc 0.9491, valid_acc 0.8981, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.15706, train_acc 0.9464, valid_acc 0.9073, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.15415, train_acc 0.9483, valid_acc 0.9021, Time 00:00:31,lr 0.002\n",
      "epoch 88, loss 0.14994, train_acc 0.9494, valid_acc 0.8976, Time 00:00:32,lr 0.002\n",
      "epoch 89, loss 0.15133, train_acc 0.9490, valid_acc 0.8961, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.08904, train_acc 0.9723, valid_acc 0.9240, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.06755, train_acc 0.9796, valid_acc 0.9256, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.05880, train_acc 0.9830, valid_acc 0.9272, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.05345, train_acc 0.9850, valid_acc 0.9282, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.05022, train_acc 0.9862, valid_acc 0.9306, Time 00:00:31,lr 0.0004\n",
      "epoch 95, loss 0.04724, train_acc 0.9864, valid_acc 0.9297, Time 00:00:31,lr 0.0004\n",
      "epoch 96, loss 0.04417, train_acc 0.9878, valid_acc 0.9303, Time 00:00:31,lr 0.0004\n",
      "epoch 97, loss 0.04230, train_acc 0.9886, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.04097, train_acc 0.9887, valid_acc 0.9272, Time 00:00:32,lr 0.0004\n",
      "epoch 99, loss 0.03953, train_acc 0.9899, valid_acc 0.9291, Time 00:00:31,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.03685, train_acc 0.9899, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.03604, train_acc 0.9903, valid_acc 0.9277, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.03277, train_acc 0.9917, valid_acc 0.9261, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.03316, train_acc 0.9913, valid_acc 0.9253, Time 00:00:32,lr 0.0004\n",
      "epoch 104, loss 0.03211, train_acc 0.9920, valid_acc 0.9288, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.03214, train_acc 0.9913, valid_acc 0.9277, Time 00:00:32,lr 0.0004\n",
      "epoch 106, loss 0.03095, train_acc 0.9922, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 107, loss 0.03007, train_acc 0.9922, valid_acc 0.9275, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.02834, train_acc 0.9926, valid_acc 0.9271, Time 00:00:31,lr 0.0004\n",
      "epoch 109, loss 0.03008, train_acc 0.9920, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.02855, train_acc 0.9928, valid_acc 0.9279, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.02935, train_acc 0.9926, valid_acc 0.9269, Time 00:00:32,lr 0.0004\n",
      "epoch 112, loss 0.02706, train_acc 0.9931, valid_acc 0.9291, Time 00:00:32,lr 0.0004\n",
      "epoch 113, loss 0.02690, train_acc 0.9933, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.02585, train_acc 0.9932, valid_acc 0.9263, Time 00:00:31,lr 0.0004\n",
      "epoch 115, loss 0.02478, train_acc 0.9938, valid_acc 0.9280, Time 00:00:31,lr 0.0004\n",
      "epoch 116, loss 0.02669, train_acc 0.9930, valid_acc 0.9264, Time 00:00:31,lr 0.0004\n",
      "epoch 117, loss 0.02483, train_acc 0.9937, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 118, loss 0.02543, train_acc 0.9936, valid_acc 0.9281, Time 00:00:31,lr 0.0004\n",
      "epoch 119, loss 0.02483, train_acc 0.9939, valid_acc 0.9300, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_me_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 use gluon renset18_v1\n",
    "train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:09.871178Z",
     "start_time": "2018-03-04T08:31:09.864612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and add must in name_scope, or may got name error\n",
    "def get_resnet18_v1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features)\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T01:41:50.461216Z",
     "start_time": "2018-03-04T01:08:08.533086Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.51019, train_acc 0.2129, valid_acc 0.3261, Time 00:00:27,lr 0.1\n",
      "epoch 1, loss 1.75673, train_acc 0.3417, valid_acc 0.4151, Time 00:00:25,lr 0.1\n",
      "epoch 2, loss 1.60616, train_acc 0.4121, valid_acc 0.4824, Time 00:00:24,lr 0.1\n",
      "epoch 3, loss 1.49376, train_acc 0.4615, valid_acc 0.4921, Time 00:00:24,lr 0.1\n",
      "epoch 4, loss 1.42743, train_acc 0.4928, valid_acc 0.4964, Time 00:00:25,lr 0.1\n",
      "epoch 5, loss 1.37983, train_acc 0.5144, valid_acc 0.5435, Time 00:00:24,lr 0.1\n",
      "epoch 6, loss 1.36243, train_acc 0.5234, valid_acc 0.5876, Time 00:00:25,lr 0.1\n",
      "epoch 7, loss 1.33671, train_acc 0.5313, valid_acc 0.5255, Time 00:00:25,lr 0.1\n",
      "epoch 8, loss 1.31173, train_acc 0.5461, valid_acc 0.5368, Time 00:00:24,lr 0.1\n",
      "epoch 9, loss 1.27490, train_acc 0.5587, valid_acc 0.5911, Time 00:00:24,lr 0.1\n",
      "epoch 10, loss 1.27032, train_acc 0.5597, valid_acc 0.6087, Time 00:00:28,lr 0.1\n",
      "epoch 11, loss 1.25436, train_acc 0.5670, valid_acc 0.5886, Time 00:00:24,lr 0.1\n",
      "epoch 12, loss 1.24745, train_acc 0.5690, valid_acc 0.5414, Time 00:00:27,lr 0.1\n",
      "epoch 13, loss 1.23173, train_acc 0.5756, valid_acc 0.5944, Time 00:00:25,lr 0.1\n",
      "epoch 14, loss 1.22901, train_acc 0.5782, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 15, loss 1.22526, train_acc 0.5812, valid_acc 0.5576, Time 00:00:24,lr 0.1\n",
      "epoch 16, loss 1.22470, train_acc 0.5828, valid_acc 0.5957, Time 00:00:24,lr 0.1\n",
      "epoch 17, loss 1.21181, train_acc 0.5865, valid_acc 0.6237, Time 00:00:25,lr 0.1\n",
      "epoch 18, loss 1.21252, train_acc 0.5851, valid_acc 0.5971, Time 00:00:25,lr 0.1\n",
      "epoch 19, loss 1.20350, train_acc 0.5899, valid_acc 0.6076, Time 00:00:25,lr 0.1\n",
      "epoch 20, loss 1.20077, train_acc 0.5904, valid_acc 0.6148, Time 00:00:25,lr 0.1\n",
      "epoch 21, loss 1.19665, train_acc 0.5925, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 22, loss 1.19718, train_acc 0.5942, valid_acc 0.5745, Time 00:00:25,lr 0.1\n",
      "epoch 23, loss 1.19424, train_acc 0.5912, valid_acc 0.5711, Time 00:00:25,lr 0.1\n",
      "epoch 24, loss 1.19114, train_acc 0.5953, valid_acc 0.6069, Time 00:00:25,lr 0.1\n",
      "epoch 25, loss 1.19080, train_acc 0.5965, valid_acc 0.6262, Time 00:00:25,lr 0.1\n",
      "epoch 26, loss 1.19219, train_acc 0.5964, valid_acc 0.6112, Time 00:00:25,lr 0.1\n",
      "epoch 27, loss 1.18217, train_acc 0.5991, valid_acc 0.5783, Time 00:00:25,lr 0.1\n",
      "epoch 28, loss 1.19244, train_acc 0.5922, valid_acc 0.6090, Time 00:00:25,lr 0.1\n",
      "epoch 29, loss 1.18442, train_acc 0.5964, valid_acc 0.6002, Time 00:00:27,lr 0.1\n",
      "epoch 30, loss 1.18418, train_acc 0.5984, valid_acc 0.5866, Time 00:00:25,lr 0.1\n",
      "epoch 31, loss 1.18398, train_acc 0.5969, valid_acc 0.5779, Time 00:00:25,lr 0.1\n",
      "epoch 32, loss 1.18966, train_acc 0.5938, valid_acc 0.6059, Time 00:00:25,lr 0.1\n",
      "epoch 33, loss 1.19080, train_acc 0.5951, valid_acc 0.6048, Time 00:00:25,lr 0.1\n",
      "epoch 34, loss 1.18026, train_acc 0.5991, valid_acc 0.6034, Time 00:00:25,lr 0.1\n",
      "epoch 35, loss 1.18109, train_acc 0.5999, valid_acc 0.6194, Time 00:00:25,lr 0.1\n",
      "epoch 36, loss 1.18156, train_acc 0.6008, valid_acc 0.5972, Time 00:00:25,lr 0.1\n",
      "epoch 37, loss 1.19249, train_acc 0.5947, valid_acc 0.5651, Time 00:00:25,lr 0.1\n",
      "epoch 38, loss 1.18308, train_acc 0.5992, valid_acc 0.6097, Time 00:00:25,lr 0.1\n",
      "epoch 39, loss 1.18060, train_acc 0.6001, valid_acc 0.6363, Time 00:00:25,lr 0.1\n",
      "epoch 40, loss 1.17950, train_acc 0.6004, valid_acc 0.6394, Time 00:00:25,lr 0.1\n",
      "epoch 41, loss 1.18485, train_acc 0.5964, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 42, loss 1.18305, train_acc 0.5999, valid_acc 0.6338, Time 00:00:25,lr 0.1\n",
      "epoch 43, loss 1.18409, train_acc 0.5985, valid_acc 0.6278, Time 00:00:24,lr 0.1\n",
      "epoch 44, loss 1.17604, train_acc 0.5990, valid_acc 0.6171, Time 00:00:25,lr 0.1\n",
      "epoch 45, loss 1.18259, train_acc 0.5979, valid_acc 0.6266, Time 00:00:25,lr 0.1\n",
      "epoch 46, loss 1.17980, train_acc 0.6023, valid_acc 0.5782, Time 00:00:25,lr 0.1\n",
      "epoch 47, loss 1.18290, train_acc 0.5980, valid_acc 0.6226, Time 00:00:25,lr 0.1\n",
      "epoch 48, loss 1.17870, train_acc 0.5994, valid_acc 0.5915, Time 00:00:25,lr 0.1\n",
      "epoch 49, loss 1.17969, train_acc 0.5983, valid_acc 0.5877, Time 00:00:24,lr 0.1\n",
      "epoch 50, loss 1.18743, train_acc 0.5986, valid_acc 0.6399, Time 00:00:25,lr 0.1\n",
      "epoch 51, loss 1.17115, train_acc 0.6031, valid_acc 0.5693, Time 00:00:25,lr 0.1\n",
      "epoch 52, loss 1.16321, train_acc 0.6042, valid_acc 0.6509, Time 00:00:24,lr 0.1\n",
      "epoch 53, loss 1.17745, train_acc 0.6004, valid_acc 0.6119, Time 00:00:25,lr 0.1\n",
      "epoch 54, loss 1.18013, train_acc 0.5981, valid_acc 0.5628, Time 00:00:25,lr 0.1\n",
      "epoch 55, loss 1.17807, train_acc 0.5997, valid_acc 0.5863, Time 00:00:26,lr 0.1\n",
      "epoch 56, loss 1.18230, train_acc 0.6013, valid_acc 0.6077, Time 00:00:26,lr 0.1\n",
      "epoch 57, loss 1.17381, train_acc 0.6016, valid_acc 0.6339, Time 00:00:24,lr 0.1\n",
      "epoch 58, loss 1.17931, train_acc 0.5979, valid_acc 0.6215, Time 00:00:24,lr 0.1\n",
      "epoch 59, loss 1.17566, train_acc 0.6000, valid_acc 0.5958, Time 00:00:24,lr 0.1\n",
      "epoch 60, loss 1.16982, train_acc 0.6028, valid_acc 0.6310, Time 00:00:25,lr 0.1\n",
      "epoch 61, loss 1.17248, train_acc 0.6016, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 62, loss 1.17132, train_acc 0.6028, valid_acc 0.6392, Time 00:00:24,lr 0.1\n",
      "epoch 63, loss 1.16930, train_acc 0.6059, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 64, loss 1.17535, train_acc 0.6018, valid_acc 0.6257, Time 00:00:24,lr 0.1\n",
      "epoch 65, loss 1.16613, train_acc 0.6031, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 66, loss 1.17103, train_acc 0.6037, valid_acc 0.5942, Time 00:00:24,lr 0.1\n",
      "epoch 67, loss 1.17368, train_acc 0.6013, valid_acc 0.6163, Time 00:00:25,lr 0.1\n",
      "epoch 68, loss 1.17817, train_acc 0.5984, valid_acc 0.5712, Time 00:00:27,lr 0.1\n",
      "epoch 69, loss 1.18750, train_acc 0.5986, valid_acc 0.6246, Time 00:00:24,lr 0.1\n",
      "epoch 70, loss 1.17631, train_acc 0.6010, valid_acc 0.6244, Time 00:00:24,lr 0.1\n",
      "epoch 71, loss 1.18138, train_acc 0.5989, valid_acc 0.5719, Time 00:00:24,lr 0.1\n",
      "epoch 72, loss 1.17125, train_acc 0.6019, valid_acc 0.6373, Time 00:00:24,lr 0.1\n",
      "epoch 73, loss 1.17880, train_acc 0.6007, valid_acc 0.6064, Time 00:00:24,lr 0.1\n",
      "epoch 74, loss 1.17069, train_acc 0.6043, valid_acc 0.6049, Time 00:00:24,lr 0.1\n",
      "epoch 75, loss 1.17566, train_acc 0.6009, valid_acc 0.5390, Time 00:00:24,lr 0.1\n",
      "epoch 76, loss 1.17658, train_acc 0.5984, valid_acc 0.6027, Time 00:00:24,lr 0.1\n",
      "epoch 77, loss 1.17818, train_acc 0.5994, valid_acc 0.6432, Time 00:00:24,lr 0.1\n",
      "epoch 78, loss 1.18056, train_acc 0.6011, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 79, loss 1.18418, train_acc 0.5995, valid_acc 0.6297, Time 00:00:24,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T02:39:40.682812Z",
     "start_time": "2018-03-04T01:49:21.279182Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.03273, train_acc 0.6475, valid_acc 0.6599, Time 00:00:24,lr 0.05\n",
      "epoch 1, loss 1.10147, train_acc 0.6258, valid_acc 0.6101, Time 00:00:24,lr 0.05\n",
      "epoch 2, loss 1.10971, train_acc 0.6221, valid_acc 0.6303, Time 00:00:24,lr 0.05\n",
      "epoch 3, loss 1.11335, train_acc 0.6201, valid_acc 0.6158, Time 00:00:24,lr 0.05\n",
      "epoch 4, loss 1.11669, train_acc 0.6186, valid_acc 0.6162, Time 00:00:24,lr 0.05\n",
      "epoch 5, loss 1.10626, train_acc 0.6214, valid_acc 0.6486, Time 00:00:24,lr 0.05\n",
      "epoch 6, loss 1.11353, train_acc 0.6192, valid_acc 0.6361, Time 00:00:24,lr 0.05\n",
      "epoch 7, loss 1.10059, train_acc 0.6249, valid_acc 0.6364, Time 00:00:24,lr 0.05\n",
      "epoch 8, loss 1.11230, train_acc 0.6210, valid_acc 0.6317, Time 00:00:24,lr 0.05\n",
      "epoch 9, loss 1.10704, train_acc 0.6200, valid_acc 0.5400, Time 00:00:24,lr 0.05\n",
      "epoch 10, loss 1.10685, train_acc 0.6223, valid_acc 0.6251, Time 00:00:24,lr 0.05\n",
      "epoch 11, loss 1.11229, train_acc 0.6209, valid_acc 0.6055, Time 00:00:24,lr 0.05\n",
      "epoch 12, loss 1.11007, train_acc 0.6232, valid_acc 0.6257, Time 00:00:25,lr 0.05\n",
      "epoch 13, loss 1.10097, train_acc 0.6244, valid_acc 0.6220, Time 00:00:25,lr 0.05\n",
      "epoch 14, loss 1.10608, train_acc 0.6263, valid_acc 0.5887, Time 00:00:24,lr 0.05\n",
      "epoch 15, loss 1.10504, train_acc 0.6224, valid_acc 0.6416, Time 00:00:24,lr 0.05\n",
      "epoch 16, loss 1.10434, train_acc 0.6223, valid_acc 0.6064, Time 00:00:24,lr 0.05\n",
      "epoch 17, loss 1.10511, train_acc 0.6221, valid_acc 0.6365, Time 00:00:24,lr 0.05\n",
      "epoch 18, loss 1.10933, train_acc 0.6206, valid_acc 0.6311, Time 00:00:24,lr 0.05\n",
      "epoch 19, loss 1.10533, train_acc 0.6211, valid_acc 0.6559, Time 00:00:25,lr 0.05\n",
      "epoch 20, loss 1.10336, train_acc 0.6231, valid_acc 0.6529, Time 00:00:24,lr 0.05\n",
      "epoch 21, loss 1.11074, train_acc 0.6202, valid_acc 0.6060, Time 00:00:25,lr 0.05\n",
      "epoch 22, loss 1.10799, train_acc 0.6211, valid_acc 0.6211, Time 00:00:24,lr 0.05\n",
      "epoch 23, loss 1.11143, train_acc 0.6213, valid_acc 0.6449, Time 00:00:24,lr 0.05\n",
      "epoch 24, loss 1.10708, train_acc 0.6227, valid_acc 0.6420, Time 00:00:24,lr 0.05\n",
      "epoch 25, loss 1.11583, train_acc 0.6182, valid_acc 0.6106, Time 00:00:24,lr 0.05\n",
      "epoch 26, loss 1.10713, train_acc 0.6237, valid_acc 0.6260, Time 00:00:25,lr 0.05\n",
      "epoch 27, loss 1.10516, train_acc 0.6226, valid_acc 0.6302, Time 00:00:24,lr 0.05\n",
      "epoch 28, loss 1.10940, train_acc 0.6245, valid_acc 0.6110, Time 00:00:24,lr 0.05\n",
      "epoch 29, loss 1.10750, train_acc 0.6217, valid_acc 0.5529, Time 00:00:24,lr 0.05\n",
      "epoch 30, loss 0.82673, train_acc 0.7164, valid_acc 0.7511, Time 00:00:25,lr 0.01\n",
      "epoch 31, loss 0.76458, train_acc 0.7374, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 32, loss 0.75552, train_acc 0.7419, valid_acc 0.7292, Time 00:00:25,lr 0.01\n",
      "epoch 33, loss 0.75799, train_acc 0.7418, valid_acc 0.7546, Time 00:00:25,lr 0.01\n",
      "epoch 34, loss 0.75636, train_acc 0.7434, valid_acc 0.7697, Time 00:00:24,lr 0.01\n",
      "epoch 35, loss 0.74936, train_acc 0.7466, valid_acc 0.7569, Time 00:00:24,lr 0.01\n",
      "epoch 36, loss 0.74819, train_acc 0.7459, valid_acc 0.7532, Time 00:00:25,lr 0.01\n",
      "epoch 37, loss 0.74411, train_acc 0.7466, valid_acc 0.7663, Time 00:00:24,lr 0.01\n",
      "epoch 38, loss 0.74887, train_acc 0.7453, valid_acc 0.7722, Time 00:00:24,lr 0.01\n",
      "epoch 39, loss 0.73962, train_acc 0.7479, valid_acc 0.7629, Time 00:00:24,lr 0.01\n",
      "epoch 40, loss 0.73451, train_acc 0.7482, valid_acc 0.7582, Time 00:00:24,lr 0.01\n",
      "epoch 41, loss 0.72975, train_acc 0.7514, valid_acc 0.7482, Time 00:00:24,lr 0.01\n",
      "epoch 42, loss 0.72403, train_acc 0.7536, valid_acc 0.7622, Time 00:00:24,lr 0.01\n",
      "epoch 43, loss 0.72834, train_acc 0.7540, valid_acc 0.7566, Time 00:00:24,lr 0.01\n",
      "epoch 44, loss 0.72512, train_acc 0.7522, valid_acc 0.7666, Time 00:00:24,lr 0.01\n",
      "epoch 45, loss 0.71810, train_acc 0.7576, valid_acc 0.7462, Time 00:00:25,lr 0.01\n",
      "epoch 46, loss 0.72037, train_acc 0.7547, valid_acc 0.7660, Time 00:00:25,lr 0.01\n",
      "epoch 47, loss 0.71576, train_acc 0.7564, valid_acc 0.7687, Time 00:00:24,lr 0.01\n",
      "epoch 48, loss 0.71955, train_acc 0.7534, valid_acc 0.7430, Time 00:00:24,lr 0.01\n",
      "epoch 49, loss 0.71184, train_acc 0.7587, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 50, loss 0.71714, train_acc 0.7561, valid_acc 0.7544, Time 00:00:24,lr 0.01\n",
      "epoch 51, loss 0.71158, train_acc 0.7576, valid_acc 0.7566, Time 00:00:25,lr 0.01\n",
      "epoch 52, loss 0.70936, train_acc 0.7582, valid_acc 0.7613, Time 00:00:24,lr 0.01\n",
      "epoch 53, loss 0.70673, train_acc 0.7591, valid_acc 0.7554, Time 00:00:24,lr 0.01\n",
      "epoch 54, loss 0.70670, train_acc 0.7597, valid_acc 0.7607, Time 00:00:24,lr 0.01\n",
      "epoch 55, loss 0.70951, train_acc 0.7566, valid_acc 0.7609, Time 00:00:24,lr 0.01\n",
      "epoch 56, loss 0.70382, train_acc 0.7595, valid_acc 0.7573, Time 00:00:24,lr 0.01\n",
      "epoch 57, loss 0.70531, train_acc 0.7601, valid_acc 0.7356, Time 00:00:24,lr 0.01\n",
      "epoch 58, loss 0.70683, train_acc 0.7595, valid_acc 0.7572, Time 00:00:25,lr 0.01\n",
      "epoch 59, loss 0.70069, train_acc 0.7610, valid_acc 0.7556, Time 00:00:24,lr 0.01\n",
      "epoch 60, loss 0.54787, train_acc 0.8137, valid_acc 0.8178, Time 00:00:24,lr 0.002\n",
      "epoch 61, loss 0.49978, train_acc 0.8285, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 62, loss 0.48376, train_acc 0.8345, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 63, loss 0.47104, train_acc 0.8396, valid_acc 0.8270, Time 00:00:24,lr 0.002\n",
      "epoch 64, loss 0.46549, train_acc 0.8415, valid_acc 0.8376, Time 00:00:25,lr 0.002\n",
      "epoch 65, loss 0.45923, train_acc 0.8430, valid_acc 0.8264, Time 00:00:25,lr 0.002\n",
      "epoch 66, loss 0.45508, train_acc 0.8428, valid_acc 0.8332, Time 00:00:24,lr 0.002\n",
      "epoch 67, loss 0.45490, train_acc 0.8441, valid_acc 0.8351, Time 00:00:24,lr 0.002\n",
      "epoch 68, loss 0.45410, train_acc 0.8435, valid_acc 0.8346, Time 00:00:24,lr 0.002\n",
      "epoch 69, loss 0.44802, train_acc 0.8458, valid_acc 0.8378, Time 00:00:24,lr 0.002\n",
      "epoch 70, loss 0.44962, train_acc 0.8464, valid_acc 0.8339, Time 00:00:24,lr 0.002\n",
      "epoch 71, loss 0.45422, train_acc 0.8447, valid_acc 0.8261, Time 00:00:24,lr 0.002\n",
      "epoch 72, loss 0.44599, train_acc 0.8474, valid_acc 0.8287, Time 00:00:25,lr 0.002\n",
      "epoch 73, loss 0.44833, train_acc 0.8458, valid_acc 0.8317, Time 00:00:24,lr 0.002\n",
      "epoch 74, loss 0.44645, train_acc 0.8466, valid_acc 0.8322, Time 00:00:24,lr 0.002\n",
      "epoch 75, loss 0.45153, train_acc 0.8453, valid_acc 0.8240, Time 00:00:24,lr 0.002\n",
      "epoch 76, loss 0.44996, train_acc 0.8456, valid_acc 0.8233, Time 00:00:24,lr 0.002\n",
      "epoch 77, loss 0.44440, train_acc 0.8473, valid_acc 0.8235, Time 00:00:24,lr 0.002\n",
      "epoch 78, loss 0.44680, train_acc 0.8452, valid_acc 0.8229, Time 00:00:24,lr 0.002\n",
      "epoch 79, loss 0.44781, train_acc 0.8466, valid_acc 0.8210, Time 00:00:24,lr 0.002\n",
      "epoch 80, loss 0.44728, train_acc 0.8452, valid_acc 0.8299, Time 00:00:24,lr 0.002\n",
      "epoch 81, loss 0.44830, train_acc 0.8462, valid_acc 0.8278, Time 00:00:24,lr 0.002\n",
      "epoch 82, loss 0.44525, train_acc 0.8456, valid_acc 0.8325, Time 00:00:24,lr 0.002\n",
      "epoch 83, loss 0.44259, train_acc 0.8477, valid_acc 0.8257, Time 00:00:24,lr 0.002\n",
      "epoch 84, loss 0.44439, train_acc 0.8464, valid_acc 0.8292, Time 00:00:24,lr 0.002\n",
      "epoch 85, loss 0.44217, train_acc 0.8476, valid_acc 0.8280, Time 00:00:24,lr 0.002\n",
      "epoch 86, loss 0.44183, train_acc 0.8487, valid_acc 0.8294, Time 00:00:24,lr 0.002\n",
      "epoch 87, loss 0.43943, train_acc 0.8472, valid_acc 0.8248, Time 00:00:24,lr 0.002\n",
      "epoch 88, loss 0.43877, train_acc 0.8485, valid_acc 0.8164, Time 00:00:26,lr 0.002\n",
      "epoch 89, loss 0.43657, train_acc 0.8498, valid_acc 0.8276, Time 00:00:25,lr 0.002\n",
      "epoch 90, loss 0.35472, train_acc 0.8789, valid_acc 0.8585, Time 00:00:25,lr 0.0004\n",
      "epoch 91, loss 0.32633, train_acc 0.8876, valid_acc 0.8528, Time 00:00:25,lr 0.0004\n",
      "epoch 92, loss 0.31385, train_acc 0.8927, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 93, loss 0.30717, train_acc 0.8945, valid_acc 0.8557, Time 00:00:25,lr 0.0004\n",
      "epoch 94, loss 0.30039, train_acc 0.8969, valid_acc 0.8598, Time 00:00:27,lr 0.0004\n",
      "epoch 95, loss 0.29545, train_acc 0.8995, valid_acc 0.8572, Time 00:00:25,lr 0.0004\n",
      "epoch 96, loss 0.29173, train_acc 0.8990, valid_acc 0.8566, Time 00:00:26,lr 0.0004\n",
      "epoch 97, loss 0.28834, train_acc 0.8994, valid_acc 0.8571, Time 00:00:28,lr 0.0004\n",
      "epoch 98, loss 0.28529, train_acc 0.9006, valid_acc 0.8560, Time 00:00:27,lr 0.0004\n",
      "epoch 99, loss 0.28147, train_acc 0.9024, valid_acc 0.8564, Time 00:00:25,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.27405, train_acc 0.9057, valid_acc 0.8599, Time 00:00:26,lr 0.0004\n",
      "epoch 101, loss 0.27207, train_acc 0.9066, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 102, loss 0.27203, train_acc 0.9067, valid_acc 0.8584, Time 00:00:25,lr 0.0004\n",
      "epoch 103, loss 0.26894, train_acc 0.9070, valid_acc 0.8580, Time 00:00:27,lr 0.0004\n",
      "epoch 104, loss 0.26925, train_acc 0.9077, valid_acc 0.8566, Time 00:00:25,lr 0.0004\n",
      "epoch 105, loss 0.26636, train_acc 0.9088, valid_acc 0.8558, Time 00:00:24,lr 0.0004\n",
      "epoch 106, loss 0.26520, train_acc 0.9085, valid_acc 0.8559, Time 00:00:26,lr 0.0004\n",
      "epoch 107, loss 0.26192, train_acc 0.9094, valid_acc 0.8562, Time 00:00:27,lr 0.0004\n",
      "epoch 108, loss 0.26339, train_acc 0.9091, valid_acc 0.8547, Time 00:00:25,lr 0.0004\n",
      "epoch 109, loss 0.26113, train_acc 0.9096, valid_acc 0.8568, Time 00:00:24,lr 0.0004\n",
      "epoch 110, loss 0.25569, train_acc 0.9102, valid_acc 0.8575, Time 00:00:24,lr 0.0004\n",
      "epoch 111, loss 0.25539, train_acc 0.9117, valid_acc 0.8565, Time 00:00:25,lr 0.0004\n",
      "epoch 112, loss 0.25576, train_acc 0.9120, valid_acc 0.8555, Time 00:00:24,lr 0.0004\n",
      "epoch 113, loss 0.25426, train_acc 0.9129, valid_acc 0.8522, Time 00:00:24,lr 0.0004\n",
      "epoch 114, loss 0.25399, train_acc 0.9125, valid_acc 0.8544, Time 00:00:24,lr 0.0004\n",
      "epoch 115, loss 0.25090, train_acc 0.9127, valid_acc 0.8521, Time 00:00:24,lr 0.0004\n",
      "epoch 116, loss 0.24849, train_acc 0.9130, valid_acc 0.8567, Time 00:00:24,lr 0.0004\n",
      "epoch 117, loss 0.25278, train_acc 0.9111, valid_acc 0.8523, Time 00:00:24,lr 0.0004\n",
      "epoch 118, loss 0.25027, train_acc 0.9130, valid_acc 0.8512, Time 00:00:24,lr 0.0004\n",
      "epoch 119, loss 0.24702, train_acc 0.9142, valid_acc 0.8548, Time 00:00:24,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 gluon resnet18 vs my resnet18\n",
    "```\n",
    "there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:40:01.865661Z",
     "start_time": "2018-03-04T08:40:01.754397Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (net): HybridSequential(\n",
      "    (0): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "    (2): Activation(relu)\n",
      "    (3): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (6): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (7): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (8): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (9): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (10): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (11): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (12): AvgPool2D(size=(8, 8), stride=(8, 8), padding=(0, 0), ceil_mode=False)\n",
      "    (13): Flatten\n",
      "    (14): Dense(None -> 10, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net1 = ResNet(10)\n",
    "'''\n",
    "block1: conv(32, 3, 1, 1) + BN + relu\n",
    "block2: Residual_block(32) * 3\n",
    "block3: Resisual_block(64) * 3\n",
    "block4: Residual_block(128) * 3\n",
    "block5: AvgPool(8) + Flatten + Dense(10)\n",
    "\n",
    "actually it has 19 conv + 1 fc, some code name it as resnet20 (each Residual_block has 2 conv or 2 + 1 conv(two line))\n",
    "for cifar10 dataset, most code use this arch (https://github.com/yinglang/pytorch-cifar-models).\n",
    "'''\n",
    "net2 = get_resnet18_v1()\n",
    "'''\n",
    "block1: conv(64, 7, 3, 2) + BN + relu + MaxPool(3, 1, 2)\n",
    "block2: Residual_block(64) * 2\n",
    "block3: Resisual_block(128) * 2\n",
    "block4: Residual_block(256) * 2\n",
    "block5: Residual_block(256) * 2\n",
    "block6: GlobalAvgPool() + Dense(10) \n",
    "\n",
    "has 1 + (2 + 2 + 2 + 2) * 2 = 15 conv + 1 fc, name it as resnet18\n",
    "'''\n",
    "print net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:21.510639Z",
     "start_time": "2018-03-04T08:31:21.405189Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): HybridSequential(\n",
      "    (0): Conv2D(3 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n",
      "    (4): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  )\n",
      "  (1): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 gluon resnet18 vs my resnet 18: first conv is conv(k=7,p=3,s=2) vs conv(k=3,p=1,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T03:26:13.432236Z",
     "start_time": "2018-03-04T03:26:13.422231Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T04:08:38.921689Z",
     "start_time": "2018-03-04T03:26:16.924298Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.20239, train_acc 0.2669, valid_acc 0.3902, Time 00:00:32,lr 0.1\n",
      "epoch 1, loss 1.65517, train_acc 0.3912, valid_acc 0.4528, Time 00:00:33,lr 0.1\n",
      "epoch 2, loss 1.47576, train_acc 0.4642, valid_acc 0.4337, Time 00:00:33,lr 0.1\n",
      "epoch 3, loss 1.34809, train_acc 0.5176, valid_acc 0.5469, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 1.25687, train_acc 0.5574, valid_acc 0.5480, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 1.20031, train_acc 0.5783, valid_acc 0.5777, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 1.15576, train_acc 0.5958, valid_acc 0.6329, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 1.11209, train_acc 0.6100, valid_acc 0.6637, Time 00:00:30,lr 0.1\n",
      "epoch 8, loss 1.06140, train_acc 0.6298, valid_acc 0.5927, Time 00:00:30,lr 0.1\n",
      "epoch 9, loss 1.02432, train_acc 0.6445, valid_acc 0.6795, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 1.00051, train_acc 0.6551, valid_acc 0.6433, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 0.98188, train_acc 0.6637, valid_acc 0.6974, Time 00:00:30,lr 0.1\n",
      "epoch 12, loss 0.96989, train_acc 0.6664, valid_acc 0.6623, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.95365, train_acc 0.6740, valid_acc 0.6860, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.95321, train_acc 0.6733, valid_acc 0.6337, Time 00:00:30,lr 0.1\n",
      "epoch 15, loss 0.94458, train_acc 0.6777, valid_acc 0.6930, Time 00:00:30,lr 0.1\n",
      "epoch 16, loss 0.93070, train_acc 0.6824, valid_acc 0.7069, Time 00:00:30,lr 0.1\n",
      "epoch 17, loss 0.93221, train_acc 0.6827, valid_acc 0.6382, Time 00:00:30,lr 0.1\n",
      "epoch 18, loss 0.92776, train_acc 0.6840, valid_acc 0.7080, Time 00:00:30,lr 0.1\n",
      "epoch 19, loss 0.92143, train_acc 0.6869, valid_acc 0.6862, Time 00:00:30,lr 0.1\n",
      "epoch 20, loss 0.92236, train_acc 0.6877, valid_acc 0.6391, Time 00:00:30,lr 0.1\n",
      "epoch 21, loss 0.92049, train_acc 0.6863, valid_acc 0.6934, Time 00:00:30,lr 0.1\n",
      "epoch 22, loss 0.91650, train_acc 0.6887, valid_acc 0.6944, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.91073, train_acc 0.6888, valid_acc 0.6668, Time 00:00:30,lr 0.1\n",
      "epoch 24, loss 0.91163, train_acc 0.6908, valid_acc 0.6598, Time 00:00:30,lr 0.1\n",
      "epoch 25, loss 0.90582, train_acc 0.6902, valid_acc 0.6914, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 0.90684, train_acc 0.6901, valid_acc 0.6054, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 0.90768, train_acc 0.6901, valid_acc 0.6733, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.90371, train_acc 0.6923, valid_acc 0.6340, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 0.90607, train_acc 0.6906, valid_acc 0.6989, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 0.90220, train_acc 0.6922, valid_acc 0.5081, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.90443, train_acc 0.6917, valid_acc 0.6913, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 0.89974, train_acc 0.6936, valid_acc 0.6244, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 0.89704, train_acc 0.6960, valid_acc 0.7255, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 0.90115, train_acc 0.6939, valid_acc 0.6980, Time 00:00:32,lr 0.1\n",
      "epoch 35, loss 0.89469, train_acc 0.6961, valid_acc 0.6947, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 0.89557, train_acc 0.6974, valid_acc 0.6494, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 0.89949, train_acc 0.6951, valid_acc 0.7107, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 0.89622, train_acc 0.6963, valid_acc 0.5968, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 0.89347, train_acc 0.6955, valid_acc 0.6612, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 0.89192, train_acc 0.6984, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 0.89004, train_acc 0.6967, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 0.88865, train_acc 0.6992, valid_acc 0.6024, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 0.90129, train_acc 0.6922, valid_acc 0.6781, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 0.89063, train_acc 0.6980, valid_acc 0.6661, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 0.89031, train_acc 0.6983, valid_acc 0.6445, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 0.88648, train_acc 0.6996, valid_acc 0.7091, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.88954, train_acc 0.6960, valid_acc 0.7163, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 0.89009, train_acc 0.6982, valid_acc 0.7027, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 0.88359, train_acc 0.7001, valid_acc 0.6971, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 0.89414, train_acc 0.6958, valid_acc 0.6732, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 0.88717, train_acc 0.6997, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 0.89172, train_acc 0.6956, valid_acc 0.6487, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.89213, train_acc 0.6993, valid_acc 0.6922, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 0.88663, train_acc 0.6969, valid_acc 0.7236, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 0.88679, train_acc 0.6988, valid_acc 0.6824, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 0.88335, train_acc 0.6991, valid_acc 0.6743, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 0.88368, train_acc 0.7007, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 0.88596, train_acc 0.6983, valid_acc 0.7060, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 0.88443, train_acc 0.6993, valid_acc 0.6722, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 0.88799, train_acc 0.6990, valid_acc 0.6508, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 0.88132, train_acc 0.7016, valid_acc 0.6703, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 0.87931, train_acc 0.7012, valid_acc 0.6996, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 0.88957, train_acc 0.6987, valid_acc 0.7010, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 0.88078, train_acc 0.7020, valid_acc 0.6454, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.88306, train_acc 0.7007, valid_acc 0.6412, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 0.88245, train_acc 0.7001, valid_acc 0.6534, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 0.87989, train_acc 0.7020, valid_acc 0.6466, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 0.89072, train_acc 0.6977, valid_acc 0.7111, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 0.88862, train_acc 0.6984, valid_acc 0.6285, Time 00:00:35,lr 0.1\n",
      "epoch 70, loss 0.87356, train_acc 0.7045, valid_acc 0.5976, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.88543, train_acc 0.6981, valid_acc 0.7188, Time 00:00:38,lr 0.1\n",
      "epoch 72, loss 0.88926, train_acc 0.6963, valid_acc 0.7050, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 0.87890, train_acc 0.7006, valid_acc 0.6988, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 0.88213, train_acc 0.6988, valid_acc 0.6710, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 0.89143, train_acc 0.6980, valid_acc 0.6485, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 0.88201, train_acc 0.6990, valid_acc 0.6410, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.87847, train_acc 0.7010, valid_acc 0.6491, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.88841, train_acc 0.6974, valid_acc 0.6903, Time 00:00:35,lr 0.1\n",
      "epoch 79, loss 0.88179, train_acc 0.7004, valid_acc 0.6880, Time 00:00:34,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T05:14:50.043433Z",
     "start_time": "2018-03-04T04:10:53.460734Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.77403, train_acc 0.7361, valid_acc 0.6271, Time 00:00:28,lr 0.05\n",
      "epoch 1, loss 0.84400, train_acc 0.7124, valid_acc 0.6732, Time 00:00:30,lr 0.05\n",
      "epoch 2, loss 0.86126, train_acc 0.7060, valid_acc 0.7009, Time 00:00:30,lr 0.05\n",
      "epoch 3, loss 0.86573, train_acc 0.7063, valid_acc 0.6787, Time 00:00:30,lr 0.05\n",
      "epoch 4, loss 0.85972, train_acc 0.7096, valid_acc 0.6623, Time 00:00:30,lr 0.05\n",
      "epoch 5, loss 0.86412, train_acc 0.7051, valid_acc 0.6821, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.86789, train_acc 0.7018, valid_acc 0.6822, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.86329, train_acc 0.7054, valid_acc 0.6411, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.87379, train_acc 0.7028, valid_acc 0.7026, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.86685, train_acc 0.7050, valid_acc 0.6352, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.86982, train_acc 0.7020, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.86871, train_acc 0.7042, valid_acc 0.5869, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.86923, train_acc 0.7037, valid_acc 0.6458, Time 00:00:31,lr 0.05\n",
      "epoch 13, loss 0.86655, train_acc 0.7048, valid_acc 0.6600, Time 00:00:31,lr 0.05\n",
      "epoch 14, loss 0.86976, train_acc 0.7038, valid_acc 0.7020, Time 00:00:31,lr 0.05\n",
      "epoch 15, loss 0.85832, train_acc 0.7083, valid_acc 0.6998, Time 00:00:31,lr 0.05\n",
      "epoch 16, loss 0.86706, train_acc 0.7046, valid_acc 0.6969, Time 00:00:31,lr 0.05\n",
      "epoch 17, loss 0.86458, train_acc 0.7044, valid_acc 0.6914, Time 00:00:31,lr 0.05\n",
      "epoch 18, loss 0.86462, train_acc 0.7072, valid_acc 0.7017, Time 00:00:31,lr 0.05\n",
      "epoch 19, loss 0.86589, train_acc 0.7067, valid_acc 0.7007, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.87160, train_acc 0.7042, valid_acc 0.7153, Time 00:00:31,lr 0.05\n",
      "epoch 21, loss 0.86976, train_acc 0.7027, valid_acc 0.6829, Time 00:00:31,lr 0.05\n",
      "epoch 22, loss 0.85921, train_acc 0.7075, valid_acc 0.7036, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.86768, train_acc 0.7058, valid_acc 0.7057, Time 00:00:31,lr 0.05\n",
      "epoch 24, loss 0.86215, train_acc 0.7067, valid_acc 0.7118, Time 00:00:31,lr 0.05\n",
      "epoch 25, loss 0.86632, train_acc 0.7029, valid_acc 0.7084, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.87295, train_acc 0.7031, valid_acc 0.6990, Time 00:00:31,lr 0.05\n",
      "epoch 27, loss 0.87456, train_acc 0.7033, valid_acc 0.6865, Time 00:00:31,lr 0.05\n",
      "epoch 28, loss 0.86871, train_acc 0.7044, valid_acc 0.6970, Time 00:00:31,lr 0.05\n",
      "epoch 29, loss 0.86998, train_acc 0.7056, valid_acc 0.6560, Time 00:00:31,lr 0.05\n",
      "epoch 30, loss 0.59346, train_acc 0.7979, valid_acc 0.8265, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.54326, train_acc 0.8155, valid_acc 0.8227, Time 00:00:31,lr 0.01\n",
      "epoch 32, loss 0.53949, train_acc 0.8170, valid_acc 0.8028, Time 00:00:31,lr 0.01\n",
      "epoch 33, loss 0.54204, train_acc 0.8157, valid_acc 0.8154, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.53898, train_acc 0.8156, valid_acc 0.8274, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.54012, train_acc 0.8164, valid_acc 0.8242, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.54208, train_acc 0.8150, valid_acc 0.8189, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.53263, train_acc 0.8171, valid_acc 0.8306, Time 00:00:35,lr 0.01\n",
      "epoch 38, loss 0.53156, train_acc 0.8190, valid_acc 0.8076, Time 00:00:33,lr 0.01\n",
      "epoch 39, loss 0.52680, train_acc 0.8204, valid_acc 0.8144, Time 00:00:33,lr 0.01\n",
      "epoch 40, loss 0.52358, train_acc 0.8228, valid_acc 0.7965, Time 00:00:33,lr 0.01\n",
      "epoch 41, loss 0.52257, train_acc 0.8211, valid_acc 0.8152, Time 00:00:34,lr 0.01\n",
      "epoch 42, loss 0.52220, train_acc 0.8232, valid_acc 0.8375, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.51315, train_acc 0.8264, valid_acc 0.8182, Time 00:00:31,lr 0.01\n",
      "epoch 44, loss 0.51323, train_acc 0.8268, valid_acc 0.8144, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.51435, train_acc 0.8240, valid_acc 0.8080, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.51004, train_acc 0.8262, valid_acc 0.8189, Time 00:00:31,lr 0.01\n",
      "epoch 47, loss 0.50912, train_acc 0.8285, valid_acc 0.8285, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.51028, train_acc 0.8259, valid_acc 0.8171, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.50687, train_acc 0.8291, valid_acc 0.8301, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.50086, train_acc 0.8297, valid_acc 0.8241, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.49983, train_acc 0.8290, valid_acc 0.8161, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.50052, train_acc 0.8278, valid_acc 0.8267, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.49692, train_acc 0.8309, valid_acc 0.8120, Time 00:00:31,lr 0.01\n",
      "epoch 54, loss 0.50006, train_acc 0.8279, valid_acc 0.8300, Time 00:00:31,lr 0.01\n",
      "epoch 55, loss 0.49913, train_acc 0.8295, valid_acc 0.7949, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.49605, train_acc 0.8320, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.49692, train_acc 0.8300, valid_acc 0.8328, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.49206, train_acc 0.8337, valid_acc 0.8387, Time 00:00:31,lr 0.01\n",
      "epoch 59, loss 0.49343, train_acc 0.8311, valid_acc 0.8150, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.34705, train_acc 0.8820, valid_acc 0.8809, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.30127, train_acc 0.8983, valid_acc 0.8846, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.28316, train_acc 0.9037, valid_acc 0.8870, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.27436, train_acc 0.9062, valid_acc 0.8866, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.26887, train_acc 0.9072, valid_acc 0.8916, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.26500, train_acc 0.9096, valid_acc 0.8894, Time 00:00:31,lr 0.002\n",
      "epoch 66, loss 0.26247, train_acc 0.9101, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 67, loss 0.26025, train_acc 0.9112, valid_acc 0.8920, Time 00:00:32,lr 0.002\n",
      "epoch 68, loss 0.25301, train_acc 0.9135, valid_acc 0.8888, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.25118, train_acc 0.9139, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.25089, train_acc 0.9137, valid_acc 0.8884, Time 00:00:32,lr 0.002\n",
      "epoch 71, loss 0.25031, train_acc 0.9147, valid_acc 0.8795, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.25265, train_acc 0.9130, valid_acc 0.8856, Time 00:00:33,lr 0.002\n",
      "epoch 73, loss 0.25271, train_acc 0.9129, valid_acc 0.8883, Time 00:00:31,lr 0.002\n",
      "epoch 74, loss 0.25271, train_acc 0.9128, valid_acc 0.8770, Time 00:00:32,lr 0.002\n",
      "epoch 75, loss 0.25109, train_acc 0.9137, valid_acc 0.8796, Time 00:00:32,lr 0.002\n",
      "epoch 76, loss 0.25106, train_acc 0.9132, valid_acc 0.8824, Time 00:00:32,lr 0.002\n",
      "epoch 77, loss 0.25249, train_acc 0.9136, valid_acc 0.8863, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.25310, train_acc 0.9126, valid_acc 0.8882, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.25188, train_acc 0.9132, valid_acc 0.8809, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.25087, train_acc 0.9143, valid_acc 0.8808, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.25043, train_acc 0.9133, valid_acc 0.8659, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.25028, train_acc 0.9126, valid_acc 0.8830, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.24958, train_acc 0.9140, valid_acc 0.8806, Time 00:00:31,lr 0.002\n",
      "epoch 84, loss 0.25462, train_acc 0.9128, valid_acc 0.8817, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.25416, train_acc 0.9109, valid_acc 0.8797, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.24963, train_acc 0.9134, valid_acc 0.8856, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.25088, train_acc 0.9143, valid_acc 0.8799, Time 00:00:32,lr 0.002\n",
      "epoch 88, loss 0.24692, train_acc 0.9147, valid_acc 0.8874, Time 00:00:31,lr 0.002\n",
      "epoch 89, loss 0.24720, train_acc 0.9146, valid_acc 0.8826, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.17336, train_acc 0.9407, valid_acc 0.9051, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.14650, train_acc 0.9515, valid_acc 0.9075, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.13297, train_acc 0.9564, valid_acc 0.9078, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.12877, train_acc 0.9568, valid_acc 0.9069, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.12118, train_acc 0.9599, valid_acc 0.9086, Time 00:00:32,lr 0.0004\n",
      "epoch 95, loss 0.11531, train_acc 0.9619, valid_acc 0.9090, Time 00:00:32,lr 0.0004\n",
      "epoch 96, loss 0.11811, train_acc 0.9601, valid_acc 0.9075, Time 00:00:35,lr 0.0004\n",
      "epoch 97, loss 0.11135, train_acc 0.9634, valid_acc 0.9066, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.10657, train_acc 0.9653, valid_acc 0.9044, Time 00:00:31,lr 0.0004\n",
      "epoch 99, loss 0.10713, train_acc 0.9637, valid_acc 0.9063, Time 00:00:31,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.10466, train_acc 0.9656, valid_acc 0.9050, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.09925, train_acc 0.9674, valid_acc 0.9043, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.09841, train_acc 0.9671, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.09499, train_acc 0.9692, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 104, loss 0.09518, train_acc 0.9685, valid_acc 0.9089, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.09390, train_acc 0.9696, valid_acc 0.9105, Time 00:00:31,lr 0.0004\n",
      "epoch 106, loss 0.09006, train_acc 0.9702, valid_acc 0.9070, Time 00:00:32,lr 0.0004\n",
      "epoch 107, loss 0.09035, train_acc 0.9709, valid_acc 0.9081, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.08887, train_acc 0.9702, valid_acc 0.9073, Time 00:00:33,lr 0.0004\n",
      "epoch 109, loss 0.08896, train_acc 0.9715, valid_acc 0.9060, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.08627, train_acc 0.9710, valid_acc 0.9103, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.08818, train_acc 0.9706, valid_acc 0.9070, Time 00:00:31,lr 0.0004\n",
      "epoch 112, loss 0.08570, train_acc 0.9718, valid_acc 0.9063, Time 00:00:33,lr 0.0004\n",
      "epoch 113, loss 0.08473, train_acc 0.9719, valid_acc 0.9086, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.08530, train_acc 0.9715, valid_acc 0.9090, Time 00:00:33,lr 0.0004\n",
      "epoch 115, loss 0.08070, train_acc 0.9733, valid_acc 0.9075, Time 00:00:33,lr 0.0004\n",
      "epoch 116, loss 0.07957, train_acc 0.9737, valid_acc 0.9041, Time 00:00:32,lr 0.0004\n",
      "epoch 117, loss 0.07966, train_acc 0.9733, valid_acc 0.9027, Time 00:00:32,lr 0.0004\n",
      "epoch 118, loss 0.08058, train_acc 0.9734, valid_acc 0.9058, Time 00:00:32,lr 0.0004\n",
      "epoch 119, loss 0.08208, train_acc 0.9728, valid_acc 0.9074, Time 00:00:35,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 I wanto find out acc impove may from cancel downsample, so use k=7 but not downsmaple and k=3 but downsample to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:41.147882Z",
     "start_time": "2018-03-04T09:17:41.140250Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k7p3s1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=7, strides=1, padding=3, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T11:07:06.585909Z",
     "start_time": "2018-03-04T09:17:41.978248Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.30376, train_acc 0.2260, valid_acc 0.3361, Time 00:00:31,lr 0.1\n",
      "epoch 1, loss 1.68781, train_acc 0.3729, valid_acc 0.4189, Time 00:00:35,lr 0.1\n",
      "epoch 2, loss 1.52445, train_acc 0.4432, valid_acc 0.5236, Time 00:00:32,lr 0.1\n",
      "epoch 3, loss 1.37861, train_acc 0.5034, valid_acc 0.5116, Time 00:00:32,lr 0.1\n",
      "epoch 4, loss 1.29522, train_acc 0.5379, valid_acc 0.5644, Time 00:00:32,lr 0.1\n",
      "epoch 5, loss 1.23774, train_acc 0.5634, valid_acc 0.5271, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.18491, train_acc 0.5845, valid_acc 0.6008, Time 00:00:32,lr 0.1\n",
      "epoch 7, loss 1.13172, train_acc 0.6043, valid_acc 0.6084, Time 00:00:32,lr 0.1\n",
      "epoch 8, loss 1.10778, train_acc 0.6163, valid_acc 0.6573, Time 00:00:33,lr 0.1\n",
      "epoch 9, loss 1.06816, train_acc 0.6311, valid_acc 0.6118, Time 00:00:32,lr 0.1\n",
      "epoch 10, loss 1.03542, train_acc 0.6430, valid_acc 0.6466, Time 00:00:32,lr 0.1\n",
      "epoch 11, loss 1.02805, train_acc 0.6471, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 1.01376, train_acc 0.6525, valid_acc 0.6565, Time 00:00:33,lr 0.1\n",
      "epoch 13, loss 1.00736, train_acc 0.6535, valid_acc 0.6435, Time 00:00:32,lr 0.1\n",
      "epoch 14, loss 0.99958, train_acc 0.6599, valid_acc 0.6482, Time 00:00:32,lr 0.1\n",
      "epoch 15, loss 0.98714, train_acc 0.6617, valid_acc 0.6799, Time 00:00:32,lr 0.1\n",
      "epoch 16, loss 0.97678, train_acc 0.6662, valid_acc 0.6960, Time 00:00:32,lr 0.1\n",
      "epoch 17, loss 0.97524, train_acc 0.6663, valid_acc 0.6830, Time 00:00:32,lr 0.1\n",
      "epoch 18, loss 0.97347, train_acc 0.6669, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 19, loss 0.97192, train_acc 0.6669, valid_acc 0.6242, Time 00:00:32,lr 0.1\n",
      "epoch 20, loss 0.97201, train_acc 0.6679, valid_acc 0.6656, Time 00:00:32,lr 0.1\n",
      "epoch 21, loss 0.96170, train_acc 0.6740, valid_acc 0.6367, Time 00:00:33,lr 0.1\n",
      "epoch 22, loss 0.96810, train_acc 0.6708, valid_acc 0.6185, Time 00:00:32,lr 0.1\n",
      "epoch 23, loss 0.95803, train_acc 0.6731, valid_acc 0.6805, Time 00:00:33,lr 0.1\n",
      "epoch 24, loss 0.95777, train_acc 0.6729, valid_acc 0.6490, Time 00:00:32,lr 0.1\n",
      "epoch 25, loss 0.96024, train_acc 0.6753, valid_acc 0.6834, Time 00:00:32,lr 0.1\n",
      "epoch 26, loss 0.95364, train_acc 0.6740, valid_acc 0.6864, Time 00:00:32,lr 0.1\n",
      "epoch 27, loss 0.95158, train_acc 0.6774, valid_acc 0.6646, Time 00:00:32,lr 0.1\n",
      "epoch 28, loss 0.94868, train_acc 0.6751, valid_acc 0.6987, Time 00:00:32,lr 0.1\n",
      "epoch 29, loss 0.95404, train_acc 0.6749, valid_acc 0.6318, Time 00:00:32,lr 0.1\n",
      "epoch 30, loss 0.95379, train_acc 0.6741, valid_acc 0.6917, Time 00:00:32,lr 0.1\n",
      "epoch 31, loss 0.94801, train_acc 0.6744, valid_acc 0.6283, Time 00:00:32,lr 0.1\n",
      "epoch 32, loss 0.95098, train_acc 0.6751, valid_acc 0.6732, Time 00:00:33,lr 0.1\n",
      "epoch 33, loss 0.94359, train_acc 0.6791, valid_acc 0.6295, Time 00:00:34,lr 0.1\n",
      "epoch 34, loss 0.94709, train_acc 0.6774, valid_acc 0.6200, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.94835, train_acc 0.6774, valid_acc 0.6354, Time 00:00:32,lr 0.1\n",
      "epoch 36, loss 0.94460, train_acc 0.6784, valid_acc 0.6478, Time 00:00:32,lr 0.1\n",
      "epoch 37, loss 0.94238, train_acc 0.6794, valid_acc 0.6423, Time 00:00:32,lr 0.1\n",
      "epoch 38, loss 0.94366, train_acc 0.6784, valid_acc 0.5573, Time 00:00:32,lr 0.1\n",
      "epoch 39, loss 0.93595, train_acc 0.6804, valid_acc 0.6669, Time 00:00:32,lr 0.1\n",
      "epoch 40, loss 0.94138, train_acc 0.6788, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 41, loss 0.94039, train_acc 0.6797, valid_acc 0.6339, Time 00:00:34,lr 0.1\n",
      "epoch 42, loss 0.93921, train_acc 0.6797, valid_acc 0.6167, Time 00:00:32,lr 0.1\n",
      "epoch 43, loss 0.93964, train_acc 0.6790, valid_acc 0.6928, Time 00:00:32,lr 0.1\n",
      "epoch 44, loss 0.93891, train_acc 0.6801, valid_acc 0.6485, Time 00:00:32,lr 0.1\n",
      "epoch 45, loss 0.93846, train_acc 0.6814, valid_acc 0.5983, Time 00:00:32,lr 0.1\n",
      "epoch 46, loss 0.93314, train_acc 0.6822, valid_acc 0.6216, Time 00:00:32,lr 0.1\n",
      "epoch 47, loss 0.93256, train_acc 0.6823, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 48, loss 0.94286, train_acc 0.6781, valid_acc 0.6123, Time 00:00:32,lr 0.1\n",
      "epoch 49, loss 0.93333, train_acc 0.6848, valid_acc 0.6390, Time 00:00:32,lr 0.1\n",
      "epoch 50, loss 0.93153, train_acc 0.6829, valid_acc 0.6682, Time 00:00:32,lr 0.1\n",
      "epoch 51, loss 0.93787, train_acc 0.6817, valid_acc 0.7057, Time 00:00:32,lr 0.1\n",
      "epoch 52, loss 0.93450, train_acc 0.6823, valid_acc 0.6080, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.93795, train_acc 0.6822, valid_acc 0.6733, Time 00:00:32,lr 0.1\n",
      "epoch 54, loss 0.92976, train_acc 0.6836, valid_acc 0.6498, Time 00:00:32,lr 0.1\n",
      "epoch 55, loss 0.93671, train_acc 0.6794, valid_acc 0.6334, Time 00:00:32,lr 0.1\n",
      "epoch 56, loss 0.93371, train_acc 0.6807, valid_acc 0.6849, Time 00:00:32,lr 0.1\n",
      "epoch 57, loss 0.93283, train_acc 0.6830, valid_acc 0.6737, Time 00:00:32,lr 0.1\n",
      "epoch 58, loss 0.94008, train_acc 0.6813, valid_acc 0.6681, Time 00:00:32,lr 0.1\n",
      "epoch 59, loss 0.92572, train_acc 0.6860, valid_acc 0.6837, Time 00:00:32,lr 0.1\n",
      "epoch 60, loss 0.93572, train_acc 0.6810, valid_acc 0.6855, Time 00:00:32,lr 0.1\n",
      "epoch 61, loss 0.92939, train_acc 0.6856, valid_acc 0.6800, Time 00:00:32,lr 0.1\n",
      "epoch 62, loss 0.92851, train_acc 0.6848, valid_acc 0.6933, Time 00:00:32,lr 0.1\n",
      "epoch 63, loss 0.93826, train_acc 0.6798, valid_acc 0.5912, Time 00:00:32,lr 0.1\n",
      "epoch 64, loss 0.92919, train_acc 0.6859, valid_acc 0.6371, Time 00:00:32,lr 0.1\n",
      "epoch 65, loss 0.92681, train_acc 0.6829, valid_acc 0.6720, Time 00:00:32,lr 0.1\n",
      "epoch 66, loss 0.92689, train_acc 0.6861, valid_acc 0.6765, Time 00:00:32,lr 0.1\n",
      "epoch 67, loss 0.92941, train_acc 0.6836, valid_acc 0.6244, Time 00:00:32,lr 0.1\n",
      "epoch 68, loss 0.92887, train_acc 0.6831, valid_acc 0.6602, Time 00:00:32,lr 0.1\n",
      "epoch 69, loss 0.93088, train_acc 0.6826, valid_acc 0.6989, Time 00:00:32,lr 0.1\n",
      "epoch 70, loss 0.92295, train_acc 0.6847, valid_acc 0.7020, Time 00:00:32,lr 0.1\n",
      "epoch 71, loss 0.92583, train_acc 0.6863, valid_acc 0.6810, Time 00:00:32,lr 0.1\n",
      "epoch 72, loss 0.92959, train_acc 0.6839, valid_acc 0.6843, Time 00:00:32,lr 0.1\n",
      "epoch 73, loss 0.93152, train_acc 0.6827, valid_acc 0.6619, Time 00:00:32,lr 0.1\n",
      "epoch 74, loss 0.92909, train_acc 0.6839, valid_acc 0.6328, Time 00:00:32,lr 0.1\n",
      "epoch 75, loss 0.92891, train_acc 0.6839, valid_acc 0.6598, Time 00:00:32,lr 0.1\n",
      "epoch 76, loss 0.93157, train_acc 0.6843, valid_acc 0.6716, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.93235, train_acc 0.6823, valid_acc 0.6870, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.92986, train_acc 0.6865, valid_acc 0.6388, Time 00:00:32,lr 0.1\n",
      "epoch 79, loss 0.92761, train_acc 0.6849, valid_acc 0.6662, Time 00:00:32,lr 0.1\n",
      "epoch 0, loss 0.80896, train_acc 0.7271, valid_acc 0.6203, Time 00:00:30,lr 0.05\n",
      "epoch 1, loss 0.88513, train_acc 0.6974, valid_acc 0.6184, Time 00:00:32,lr 0.05\n",
      "epoch 2, loss 0.90256, train_acc 0.6931, valid_acc 0.7099, Time 00:00:32,lr 0.05\n",
      "epoch 3, loss 0.90922, train_acc 0.6893, valid_acc 0.6401, Time 00:00:32,lr 0.05\n",
      "epoch 4, loss 0.91353, train_acc 0.6878, valid_acc 0.6681, Time 00:00:32,lr 0.05\n",
      "epoch 5, loss 0.90272, train_acc 0.6925, valid_acc 0.6439, Time 00:00:32,lr 0.05\n",
      "epoch 6, loss 0.91182, train_acc 0.6895, valid_acc 0.7033, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.91077, train_acc 0.6885, valid_acc 0.6915, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.91324, train_acc 0.6878, valid_acc 0.6665, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.91369, train_acc 0.6868, valid_acc 0.6593, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.91246, train_acc 0.6879, valid_acc 0.6637, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.91111, train_acc 0.6916, valid_acc 0.7026, Time 00:00:32,lr 0.05\n",
      "epoch 12, loss 0.91954, train_acc 0.6874, valid_acc 0.6634, Time 00:00:32,lr 0.05\n",
      "epoch 13, loss 0.91728, train_acc 0.6863, valid_acc 0.6654, Time 00:00:32,lr 0.05\n",
      "epoch 14, loss 0.91430, train_acc 0.6878, valid_acc 0.6514, Time 00:00:32,lr 0.05\n",
      "epoch 15, loss 0.91699, train_acc 0.6876, valid_acc 0.6709, Time 00:00:32,lr 0.05\n",
      "epoch 16, loss 0.91586, train_acc 0.6865, valid_acc 0.6684, Time 00:00:32,lr 0.05\n",
      "epoch 17, loss 0.90909, train_acc 0.6914, valid_acc 0.6667, Time 00:00:32,lr 0.05\n",
      "epoch 18, loss 0.90777, train_acc 0.6873, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 19, loss 0.91364, train_acc 0.6880, valid_acc 0.6613, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.91116, train_acc 0.6876, valid_acc 0.6495, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.90711, train_acc 0.6921, valid_acc 0.6621, Time 00:00:32,lr 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss 0.91377, train_acc 0.6874, valid_acc 0.7030, Time 00:00:32,lr 0.05\n",
      "epoch 23, loss 0.91145, train_acc 0.6884, valid_acc 0.6387, Time 00:00:32,lr 0.05\n",
      "epoch 24, loss 0.91180, train_acc 0.6891, valid_acc 0.7214, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.91215, train_acc 0.6880, valid_acc 0.6815, Time 00:00:32,lr 0.05\n",
      "epoch 26, loss 0.90945, train_acc 0.6892, valid_acc 0.6991, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.91134, train_acc 0.6903, valid_acc 0.6897, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.91023, train_acc 0.6889, valid_acc 0.6460, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.90890, train_acc 0.6892, valid_acc 0.6105, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.62314, train_acc 0.7879, valid_acc 0.8119, Time 00:00:32,lr 0.01\n",
      "epoch 31, loss 0.57048, train_acc 0.8056, valid_acc 0.8057, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.56539, train_acc 0.8086, valid_acc 0.8094, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.56050, train_acc 0.8097, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.56481, train_acc 0.8090, valid_acc 0.7986, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.56655, train_acc 0.8061, valid_acc 0.8173, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.55780, train_acc 0.8100, valid_acc 0.7797, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.55701, train_acc 0.8100, valid_acc 0.8271, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.55273, train_acc 0.8132, valid_acc 0.8075, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.55390, train_acc 0.8106, valid_acc 0.8073, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.54441, train_acc 0.8154, valid_acc 0.8010, Time 00:00:32,lr 0.01\n",
      "epoch 41, loss 0.54516, train_acc 0.8139, valid_acc 0.8111, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.53969, train_acc 0.8148, valid_acc 0.8155, Time 00:00:32,lr 0.01\n",
      "epoch 43, loss 0.53975, train_acc 0.8157, valid_acc 0.8236, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.53627, train_acc 0.8176, valid_acc 0.8247, Time 00:00:32,lr 0.01\n",
      "epoch 45, loss 0.53035, train_acc 0.8196, valid_acc 0.8118, Time 00:00:32,lr 0.01\n",
      "epoch 46, loss 0.53219, train_acc 0.8190, valid_acc 0.8203, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.53024, train_acc 0.8185, valid_acc 0.8109, Time 00:00:32,lr 0.01\n",
      "epoch 48, loss 0.52706, train_acc 0.8210, valid_acc 0.8323, Time 00:00:32,lr 0.01\n",
      "epoch 49, loss 0.52530, train_acc 0.8220, valid_acc 0.7924, Time 00:00:32,lr 0.01\n",
      "epoch 50, loss 0.52793, train_acc 0.8215, valid_acc 0.8195, Time 00:00:32,lr 0.01\n",
      "epoch 51, loss 0.52560, train_acc 0.8216, valid_acc 0.8287, Time 00:00:32,lr 0.01\n",
      "epoch 52, loss 0.51781, train_acc 0.8240, valid_acc 0.8058, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.51830, train_acc 0.8228, valid_acc 0.8306, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.51420, train_acc 0.8254, valid_acc 0.8258, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.51919, train_acc 0.8242, valid_acc 0.8067, Time 00:00:32,lr 0.01\n",
      "epoch 56, loss 0.51772, train_acc 0.8231, valid_acc 0.8133, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.50818, train_acc 0.8279, valid_acc 0.8285, Time 00:00:32,lr 0.01\n",
      "epoch 58, loss 0.51014, train_acc 0.8252, valid_acc 0.8101, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.51187, train_acc 0.8270, valid_acc 0.8182, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.35304, train_acc 0.8818, valid_acc 0.8777, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.31041, train_acc 0.8950, valid_acc 0.8848, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.29029, train_acc 0.9032, valid_acc 0.8857, Time 00:00:32,lr 0.002\n",
      "epoch 63, loss 0.28051, train_acc 0.9045, valid_acc 0.8852, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.27056, train_acc 0.9079, valid_acc 0.8854, Time 00:00:32,lr 0.002\n",
      "epoch 65, loss 0.26793, train_acc 0.9099, valid_acc 0.8821, Time 00:00:33,lr 0.002\n",
      "epoch 66, loss 0.26292, train_acc 0.9110, valid_acc 0.8788, Time 00:00:35,lr 0.002\n",
      "epoch 67, loss 0.25597, train_acc 0.9123, valid_acc 0.8886, Time 00:00:33,lr 0.002\n",
      "epoch 68, loss 0.25621, train_acc 0.9120, valid_acc 0.8809, Time 00:00:34,lr 0.002\n",
      "epoch 69, loss 0.25495, train_acc 0.9139, valid_acc 0.8817, Time 00:00:34,lr 0.002\n",
      "epoch 70, loss 0.25461, train_acc 0.9128, valid_acc 0.8820, Time 00:00:35,lr 0.002\n",
      "epoch 71, loss 0.25337, train_acc 0.9143, valid_acc 0.8872, Time 00:00:33,lr 0.002\n",
      "epoch 72, loss 0.25769, train_acc 0.9113, valid_acc 0.8756, Time 00:00:32,lr 0.002\n",
      "epoch 73, loss 0.25496, train_acc 0.9130, valid_acc 0.8843, Time 00:00:33,lr 0.002\n",
      "epoch 74, loss 0.25236, train_acc 0.9132, valid_acc 0.8803, Time 00:00:32,lr 0.002\n",
      "epoch 75, loss 0.25699, train_acc 0.9118, valid_acc 0.8812, Time 00:00:33,lr 0.002\n",
      "epoch 76, loss 0.25216, train_acc 0.9150, valid_acc 0.8855, Time 00:00:34,lr 0.002\n",
      "epoch 77, loss 0.25850, train_acc 0.9112, valid_acc 0.8700, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.24882, train_acc 0.9136, valid_acc 0.8773, Time 00:00:33,lr 0.002\n",
      "epoch 79, loss 0.25507, train_acc 0.9135, valid_acc 0.8743, Time 00:00:33,lr 0.002\n",
      "epoch 80, loss 0.25569, train_acc 0.9138, valid_acc 0.8738, Time 00:00:32,lr 0.002\n",
      "epoch 81, loss 0.25226, train_acc 0.9132, valid_acc 0.8836, Time 00:00:32,lr 0.002\n",
      "epoch 82, loss 0.24877, train_acc 0.9143, valid_acc 0.8696, Time 00:00:32,lr 0.002\n",
      "epoch 83, loss 0.24857, train_acc 0.9146, valid_acc 0.8788, Time 00:00:33,lr 0.002\n",
      "epoch 84, loss 0.25128, train_acc 0.9141, valid_acc 0.8837, Time 00:00:33,lr 0.002\n",
      "epoch 85, loss 0.24834, train_acc 0.9156, valid_acc 0.8790, Time 00:00:32,lr 0.002\n",
      "epoch 86, loss 0.24991, train_acc 0.9156, valid_acc 0.8798, Time 00:00:33,lr 0.002\n",
      "epoch 87, loss 0.24662, train_acc 0.9153, valid_acc 0.8713, Time 00:00:36,lr 0.002\n",
      "epoch 88, loss 0.25288, train_acc 0.9130, valid_acc 0.8833, Time 00:00:34,lr 0.002\n",
      "epoch 89, loss 0.24991, train_acc 0.9138, valid_acc 0.8753, Time 00:00:32,lr 0.002\n",
      "epoch 90, loss 0.16786, train_acc 0.9432, valid_acc 0.9054, Time 00:00:33,lr 0.0004\n",
      "epoch 91, loss 0.14234, train_acc 0.9528, valid_acc 0.9077, Time 00:00:33,lr 0.0004\n",
      "epoch 92, loss 0.13349, train_acc 0.9558, valid_acc 0.9064, Time 00:00:33,lr 0.0004\n",
      "epoch 93, loss 0.12081, train_acc 0.9609, valid_acc 0.9081, Time 00:00:32,lr 0.0004\n",
      "epoch 94, loss 0.11819, train_acc 0.9613, valid_acc 0.9066, Time 00:00:33,lr 0.0004\n",
      "epoch 95, loss 0.11401, train_acc 0.9624, valid_acc 0.9075, Time 00:00:33,lr 0.0004\n",
      "epoch 96, loss 0.10900, train_acc 0.9646, valid_acc 0.9096, Time 00:00:33,lr 0.0004\n",
      "epoch 97, loss 0.10655, train_acc 0.9661, valid_acc 0.9090, Time 00:00:32,lr 0.0004\n",
      "epoch 98, loss 0.10042, train_acc 0.9670, valid_acc 0.9089, Time 00:00:33,lr 0.0004\n",
      "epoch 99, loss 0.09990, train_acc 0.9671, valid_acc 0.9062, Time 00:00:33,lr 0.0004\n",
      "epoch 100, loss 0.09451, train_acc 0.9689, valid_acc 0.9094, Time 00:00:33,lr 0.0004\n",
      "epoch 101, loss 0.09150, train_acc 0.9703, valid_acc 0.9071, Time 00:00:33,lr 0.0004\n",
      "epoch 102, loss 0.09121, train_acc 0.9704, valid_acc 0.9092, Time 00:00:33,lr 0.0004\n",
      "epoch 103, loss 0.08833, train_acc 0.9716, valid_acc 0.9052, Time 00:00:32,lr 0.0004\n",
      "epoch 104, loss 0.08499, train_acc 0.9719, valid_acc 0.9048, Time 00:00:35,lr 0.0004\n",
      "epoch 105, loss 0.08679, train_acc 0.9713, valid_acc 0.9080, Time 00:00:33,lr 0.0004\n",
      "epoch 106, loss 0.08194, train_acc 0.9739, valid_acc 0.9084, Time 00:00:32,lr 0.0004\n",
      "epoch 107, loss 0.08463, train_acc 0.9727, valid_acc 0.9057, Time 00:00:32,lr 0.0004\n",
      "epoch 108, loss 0.08137, train_acc 0.9731, valid_acc 0.9066, Time 00:00:32,lr 0.0004\n",
      "epoch 109, loss 0.07945, train_acc 0.9746, valid_acc 0.9056, Time 00:00:33,lr 0.0004\n",
      "epoch 110, loss 0.07836, train_acc 0.9753, valid_acc 0.9054, Time 00:00:33,lr 0.0004\n",
      "epoch 111, loss 0.07749, train_acc 0.9753, valid_acc 0.9080, Time 00:00:34,lr 0.0004\n",
      "epoch 112, loss 0.07876, train_acc 0.9742, valid_acc 0.9073, Time 00:00:34,lr 0.0004\n",
      "epoch 113, loss 0.07657, train_acc 0.9749, valid_acc 0.9067, Time 00:00:33,lr 0.0004\n",
      "epoch 114, loss 0.07602, train_acc 0.9756, valid_acc 0.9083, Time 00:00:32,lr 0.0004\n",
      "epoch 115, loss 0.07467, train_acc 0.9759, valid_acc 0.9054, Time 00:00:33,lr 0.0004\n",
      "epoch 116, loss 0.07413, train_acc 0.9758, valid_acc 0.9057, Time 00:00:34,lr 0.0004\n",
      "epoch 117, loss 0.07005, train_acc 0.9773, valid_acc 0.9093, Time 00:00:32,lr 0.0004\n",
      "epoch 118, loss 0.07078, train_acc 0.9771, valid_acc 0.9078, Time 00:00:32,lr 0.0004\n",
      "epoch 119, loss 0.07212, train_acc 0.9771, valid_acc 0.9074, Time 00:00:32,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k7p3s1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k7p3s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T11:07:06.593926Z",
     "start_time": "2018-03-04T11:07:06.587399Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k3p1s2(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=2, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T12:30:35.597723Z",
     "start_time": "2018-03-04T11:07:06.595085Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.50262, train_acc 0.2350, valid_acc 0.3242, Time 00:00:23,lr 0.1\n",
      "epoch 1, loss 1.78462, train_acc 0.3364, valid_acc 0.4290, Time 00:00:24,lr 0.1\n",
      "epoch 2, loss 1.62260, train_acc 0.4035, valid_acc 0.4589, Time 00:00:24,lr 0.1\n",
      "epoch 3, loss 1.54967, train_acc 0.4401, valid_acc 0.4553, Time 00:00:25,lr 0.1\n",
      "epoch 4, loss 1.47895, train_acc 0.4708, valid_acc 0.4980, Time 00:00:28,lr 0.1\n",
      "epoch 5, loss 1.42545, train_acc 0.4961, valid_acc 0.5139, Time 00:00:25,lr 0.1\n",
      "epoch 6, loss 1.38317, train_acc 0.5145, valid_acc 0.5491, Time 00:00:25,lr 0.1\n",
      "epoch 7, loss 1.33647, train_acc 0.5331, valid_acc 0.5664, Time 00:00:26,lr 0.1\n",
      "epoch 8, loss 1.30906, train_acc 0.5418, valid_acc 0.5851, Time 00:00:26,lr 0.1\n",
      "epoch 9, loss 1.29921, train_acc 0.5507, valid_acc 0.5837, Time 00:00:25,lr 0.1\n",
      "epoch 10, loss 1.28389, train_acc 0.5545, valid_acc 0.5244, Time 00:00:26,lr 0.1\n",
      "epoch 11, loss 1.25926, train_acc 0.5661, valid_acc 0.5465, Time 00:00:25,lr 0.1\n",
      "epoch 12, loss 1.25738, train_acc 0.5654, valid_acc 0.5883, Time 00:00:27,lr 0.1\n",
      "epoch 13, loss 1.24461, train_acc 0.5716, valid_acc 0.5379, Time 00:00:28,lr 0.1\n",
      "epoch 14, loss 1.24978, train_acc 0.5737, valid_acc 0.6021, Time 00:00:26,lr 0.1\n",
      "epoch 15, loss 1.22359, train_acc 0.5794, valid_acc 0.5540, Time 00:00:28,lr 0.1\n",
      "epoch 16, loss 1.23121, train_acc 0.5796, valid_acc 0.5539, Time 00:00:25,lr 0.1\n",
      "epoch 17, loss 1.21983, train_acc 0.5814, valid_acc 0.5995, Time 00:00:26,lr 0.1\n",
      "epoch 18, loss 1.21139, train_acc 0.5877, valid_acc 0.5875, Time 00:00:24,lr 0.1\n",
      "epoch 19, loss 1.22130, train_acc 0.5828, valid_acc 0.5786, Time 00:00:24,lr 0.1\n",
      "epoch 20, loss 1.21126, train_acc 0.5862, valid_acc 0.6172, Time 00:00:24,lr 0.1\n",
      "epoch 21, loss 1.20354, train_acc 0.5890, valid_acc 0.6017, Time 00:00:24,lr 0.1\n",
      "epoch 22, loss 1.20505, train_acc 0.5921, valid_acc 0.5667, Time 00:00:24,lr 0.1\n",
      "epoch 23, loss 1.20091, train_acc 0.5939, valid_acc 0.6174, Time 00:00:24,lr 0.1\n",
      "epoch 24, loss 1.20286, train_acc 0.5919, valid_acc 0.5933, Time 00:00:24,lr 0.1\n",
      "epoch 25, loss 1.19615, train_acc 0.5933, valid_acc 0.6055, Time 00:00:24,lr 0.1\n",
      "epoch 26, loss 1.18975, train_acc 0.5981, valid_acc 0.5990, Time 00:00:24,lr 0.1\n",
      "epoch 27, loss 1.19463, train_acc 0.5954, valid_acc 0.6119, Time 00:00:24,lr 0.1\n",
      "epoch 28, loss 1.19693, train_acc 0.5941, valid_acc 0.5994, Time 00:00:24,lr 0.1\n",
      "epoch 29, loss 1.19191, train_acc 0.5949, valid_acc 0.5786, Time 00:00:25,lr 0.1\n",
      "epoch 30, loss 1.19812, train_acc 0.5941, valid_acc 0.6081, Time 00:00:26,lr 0.1\n",
      "epoch 31, loss 1.19063, train_acc 0.5957, valid_acc 0.6407, Time 00:00:24,lr 0.1\n",
      "epoch 32, loss 1.19470, train_acc 0.5954, valid_acc 0.5912, Time 00:00:24,lr 0.1\n",
      "epoch 33, loss 1.18433, train_acc 0.5977, valid_acc 0.5361, Time 00:00:25,lr 0.1\n",
      "epoch 34, loss 1.18371, train_acc 0.5981, valid_acc 0.6132, Time 00:00:26,lr 0.1\n",
      "epoch 35, loss 1.19304, train_acc 0.5930, valid_acc 0.5312, Time 00:00:24,lr 0.1\n",
      "epoch 36, loss 1.18548, train_acc 0.5992, valid_acc 0.6146, Time 00:00:24,lr 0.1\n",
      "epoch 37, loss 1.19769, train_acc 0.5952, valid_acc 0.5817, Time 00:00:24,lr 0.1\n",
      "epoch 38, loss 1.17540, train_acc 0.6027, valid_acc 0.5742, Time 00:00:24,lr 0.1\n",
      "epoch 39, loss 1.18462, train_acc 0.5991, valid_acc 0.5887, Time 00:00:24,lr 0.1\n",
      "epoch 40, loss 1.18646, train_acc 0.5946, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 41, loss 1.19338, train_acc 0.5940, valid_acc 0.5486, Time 00:00:25,lr 0.1\n",
      "epoch 42, loss 1.18848, train_acc 0.5971, valid_acc 0.6175, Time 00:00:26,lr 0.1\n",
      "epoch 43, loss 1.19524, train_acc 0.5988, valid_acc 0.6187, Time 00:00:29,lr 0.1\n",
      "epoch 44, loss 1.18320, train_acc 0.5983, valid_acc 0.6223, Time 00:00:26,lr 0.1\n",
      "epoch 45, loss 1.17820, train_acc 0.6003, valid_acc 0.5944, Time 00:00:24,lr 0.1\n",
      "epoch 46, loss 1.17758, train_acc 0.6003, valid_acc 0.5844, Time 00:00:24,lr 0.1\n",
      "epoch 47, loss 1.18451, train_acc 0.5996, valid_acc 0.5785, Time 00:00:24,lr 0.1\n",
      "epoch 48, loss 1.18422, train_acc 0.6030, valid_acc 0.6022, Time 00:00:24,lr 0.1\n",
      "epoch 49, loss 1.18552, train_acc 0.5979, valid_acc 0.5716, Time 00:00:24,lr 0.1\n",
      "epoch 50, loss 1.18247, train_acc 0.5989, valid_acc 0.6341, Time 00:00:25,lr 0.1\n",
      "epoch 51, loss 1.18669, train_acc 0.5968, valid_acc 0.5807, Time 00:00:26,lr 0.1\n",
      "epoch 52, loss 1.18293, train_acc 0.5982, valid_acc 0.5857, Time 00:00:25,lr 0.1\n",
      "epoch 53, loss 1.18378, train_acc 0.6022, valid_acc 0.6234, Time 00:00:25,lr 0.1\n",
      "epoch 54, loss 1.17740, train_acc 0.6008, valid_acc 0.5988, Time 00:00:24,lr 0.1\n",
      "epoch 55, loss 1.17670, train_acc 0.5992, valid_acc 0.6202, Time 00:00:24,lr 0.1\n",
      "epoch 56, loss 1.18444, train_acc 0.6006, valid_acc 0.6178, Time 00:00:24,lr 0.1\n",
      "epoch 57, loss 1.18786, train_acc 0.5994, valid_acc 0.5740, Time 00:00:25,lr 0.1\n",
      "epoch 58, loss 1.18102, train_acc 0.6033, valid_acc 0.5980, Time 00:00:27,lr 0.1\n",
      "epoch 59, loss 1.17959, train_acc 0.6037, valid_acc 0.6219, Time 00:00:25,lr 0.1\n",
      "epoch 60, loss 1.18183, train_acc 0.5997, valid_acc 0.5931, Time 00:00:24,lr 0.1\n",
      "epoch 61, loss 1.18130, train_acc 0.5995, valid_acc 0.6252, Time 00:00:24,lr 0.1\n",
      "epoch 62, loss 1.18737, train_acc 0.5990, valid_acc 0.5961, Time 00:00:24,lr 0.1\n",
      "epoch 63, loss 1.18772, train_acc 0.5994, valid_acc 0.6248, Time 00:00:24,lr 0.1\n",
      "epoch 64, loss 1.18769, train_acc 0.5959, valid_acc 0.5748, Time 00:00:25,lr 0.1\n",
      "epoch 65, loss 1.18745, train_acc 0.5981, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 66, loss 1.17686, train_acc 0.6023, valid_acc 0.6277, Time 00:00:25,lr 0.1\n",
      "epoch 67, loss 1.18466, train_acc 0.5998, valid_acc 0.6262, Time 00:00:26,lr 0.1\n",
      "epoch 68, loss 1.18047, train_acc 0.6003, valid_acc 0.6232, Time 00:00:25,lr 0.1\n",
      "epoch 69, loss 1.18025, train_acc 0.5982, valid_acc 0.5672, Time 00:00:25,lr 0.1\n",
      "epoch 70, loss 1.18145, train_acc 0.5999, valid_acc 0.5342, Time 00:00:25,lr 0.1\n",
      "epoch 71, loss 1.18055, train_acc 0.6016, valid_acc 0.6221, Time 00:00:25,lr 0.1\n",
      "epoch 72, loss 1.18384, train_acc 0.5982, valid_acc 0.6094, Time 00:00:24,lr 0.1\n",
      "epoch 73, loss 1.17971, train_acc 0.6009, valid_acc 0.6354, Time 00:00:24,lr 0.1\n",
      "epoch 74, loss 1.18438, train_acc 0.6002, valid_acc 0.6417, Time 00:00:24,lr 0.1\n",
      "epoch 75, loss 1.17807, train_acc 0.5993, valid_acc 0.6025, Time 00:00:24,lr 0.1\n",
      "epoch 76, loss 1.18557, train_acc 0.5996, valid_acc 0.6152, Time 00:00:24,lr 0.1\n",
      "epoch 77, loss 1.18545, train_acc 0.5993, valid_acc 0.5532, Time 00:00:24,lr 0.1\n",
      "epoch 78, loss 1.17480, train_acc 0.5997, valid_acc 0.5950, Time 00:00:24,lr 0.1\n",
      "epoch 79, loss 1.18309, train_acc 0.5995, valid_acc 0.6056, Time 00:00:24,lr 0.1\n",
      "epoch 0, loss 1.03273, train_acc 0.6483, valid_acc 0.6740, Time 00:00:22,lr 0.05\n",
      "epoch 1, loss 1.09674, train_acc 0.6251, valid_acc 0.6415, Time 00:00:24,lr 0.05\n",
      "epoch 2, loss 1.10671, train_acc 0.6231, valid_acc 0.6117, Time 00:00:24,lr 0.05\n",
      "epoch 3, loss 1.11485, train_acc 0.6204, valid_acc 0.6554, Time 00:00:24,lr 0.05\n",
      "epoch 4, loss 1.11149, train_acc 0.6221, valid_acc 0.6662, Time 00:00:26,lr 0.05\n",
      "epoch 5, loss 1.11029, train_acc 0.6228, valid_acc 0.6449, Time 00:00:25,lr 0.05\n",
      "epoch 6, loss 1.10534, train_acc 0.6226, valid_acc 0.6285, Time 00:00:24,lr 0.05\n",
      "epoch 7, loss 1.11431, train_acc 0.6230, valid_acc 0.5960, Time 00:00:24,lr 0.05\n",
      "epoch 8, loss 1.11183, train_acc 0.6227, valid_acc 0.6311, Time 00:00:24,lr 0.05\n",
      "epoch 9, loss 1.11037, train_acc 0.6222, valid_acc 0.6452, Time 00:00:24,lr 0.05\n",
      "epoch 10, loss 1.11295, train_acc 0.6200, valid_acc 0.5944, Time 00:00:24,lr 0.05\n",
      "epoch 11, loss 1.12221, train_acc 0.6177, valid_acc 0.6159, Time 00:00:25,lr 0.05\n",
      "epoch 12, loss 1.11013, train_acc 0.6216, valid_acc 0.6719, Time 00:00:25,lr 0.05\n",
      "epoch 13, loss 1.10863, train_acc 0.6211, valid_acc 0.6305, Time 00:00:29,lr 0.05\n",
      "epoch 14, loss 1.10922, train_acc 0.6231, valid_acc 0.6379, Time 00:00:25,lr 0.05\n",
      "epoch 15, loss 1.11353, train_acc 0.6212, valid_acc 0.6409, Time 00:00:28,lr 0.05\n",
      "epoch 16, loss 1.11767, train_acc 0.6190, valid_acc 0.6056, Time 00:00:24,lr 0.05\n",
      "epoch 17, loss 1.10862, train_acc 0.6219, valid_acc 0.6377, Time 00:00:26,lr 0.05\n",
      "epoch 18, loss 1.10578, train_acc 0.6234, valid_acc 0.6432, Time 00:00:27,lr 0.05\n",
      "epoch 19, loss 1.11750, train_acc 0.6168, valid_acc 0.5950, Time 00:00:25,lr 0.05\n",
      "epoch 20, loss 1.11371, train_acc 0.6195, valid_acc 0.6746, Time 00:00:25,lr 0.05\n",
      "epoch 21, loss 1.10867, train_acc 0.6208, valid_acc 0.5958, Time 00:00:25,lr 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss 1.11496, train_acc 0.6212, valid_acc 0.6180, Time 00:00:24,lr 0.05\n",
      "epoch 23, loss 1.11273, train_acc 0.6213, valid_acc 0.6098, Time 00:00:24,lr 0.05\n",
      "epoch 24, loss 1.11474, train_acc 0.6186, valid_acc 0.6466, Time 00:00:24,lr 0.05\n",
      "epoch 25, loss 1.11465, train_acc 0.6209, valid_acc 0.6061, Time 00:00:24,lr 0.05\n",
      "epoch 26, loss 1.11365, train_acc 0.6206, valid_acc 0.6113, Time 00:00:25,lr 0.05\n",
      "epoch 27, loss 1.11495, train_acc 0.6179, valid_acc 0.5848, Time 00:00:26,lr 0.05\n",
      "epoch 28, loss 1.10766, train_acc 0.6230, valid_acc 0.6258, Time 00:00:25,lr 0.05\n",
      "epoch 29, loss 1.11466, train_acc 0.6197, valid_acc 0.6257, Time 00:00:24,lr 0.05\n",
      "epoch 30, loss 0.83832, train_acc 0.7133, valid_acc 0.7441, Time 00:00:24,lr 0.01\n",
      "epoch 31, loss 0.77775, train_acc 0.7343, valid_acc 0.7425, Time 00:00:24,lr 0.01\n",
      "epoch 32, loss 0.78149, train_acc 0.7322, valid_acc 0.7436, Time 00:00:25,lr 0.01\n",
      "epoch 33, loss 0.77605, train_acc 0.7342, valid_acc 0.7492, Time 00:00:24,lr 0.01\n",
      "epoch 34, loss 0.77268, train_acc 0.7366, valid_acc 0.7493, Time 00:00:24,lr 0.01\n",
      "epoch 35, loss 0.77276, train_acc 0.7349, valid_acc 0.7450, Time 00:00:24,lr 0.01\n",
      "epoch 36, loss 0.77670, train_acc 0.7335, valid_acc 0.7512, Time 00:00:24,lr 0.01\n",
      "epoch 37, loss 0.76556, train_acc 0.7388, valid_acc 0.7374, Time 00:00:24,lr 0.01\n",
      "epoch 38, loss 0.76203, train_acc 0.7419, valid_acc 0.7737, Time 00:00:24,lr 0.01\n",
      "epoch 39, loss 0.76107, train_acc 0.7383, valid_acc 0.7531, Time 00:00:24,lr 0.01\n",
      "epoch 40, loss 0.75353, train_acc 0.7417, valid_acc 0.7496, Time 00:00:24,lr 0.01\n",
      "epoch 41, loss 0.75731, train_acc 0.7428, valid_acc 0.7497, Time 00:00:24,lr 0.01\n",
      "epoch 42, loss 0.75287, train_acc 0.7437, valid_acc 0.7447, Time 00:00:24,lr 0.01\n",
      "epoch 43, loss 0.74855, train_acc 0.7449, valid_acc 0.7481, Time 00:00:24,lr 0.01\n",
      "epoch 44, loss 0.74202, train_acc 0.7472, valid_acc 0.7389, Time 00:00:24,lr 0.01\n",
      "epoch 45, loss 0.74359, train_acc 0.7445, valid_acc 0.7470, Time 00:00:24,lr 0.01\n",
      "epoch 46, loss 0.73820, train_acc 0.7492, valid_acc 0.7462, Time 00:00:24,lr 0.01\n",
      "epoch 47, loss 0.73883, train_acc 0.7470, valid_acc 0.7622, Time 00:00:24,lr 0.01\n",
      "epoch 48, loss 0.73880, train_acc 0.7491, valid_acc 0.7629, Time 00:00:24,lr 0.01\n",
      "epoch 49, loss 0.73663, train_acc 0.7489, valid_acc 0.7739, Time 00:00:25,lr 0.01\n",
      "epoch 50, loss 0.73340, train_acc 0.7486, valid_acc 0.7617, Time 00:00:24,lr 0.01\n",
      "epoch 51, loss 0.73506, train_acc 0.7482, valid_acc 0.7459, Time 00:00:26,lr 0.01\n",
      "epoch 52, loss 0.73561, train_acc 0.7481, valid_acc 0.7577, Time 00:00:25,lr 0.01\n",
      "epoch 53, loss 0.73704, train_acc 0.7482, valid_acc 0.7637, Time 00:00:24,lr 0.01\n",
      "epoch 54, loss 0.72151, train_acc 0.7545, valid_acc 0.7602, Time 00:00:24,lr 0.01\n",
      "epoch 55, loss 0.72973, train_acc 0.7516, valid_acc 0.7717, Time 00:00:24,lr 0.01\n",
      "epoch 56, loss 0.72597, train_acc 0.7522, valid_acc 0.7561, Time 00:00:24,lr 0.01\n",
      "epoch 57, loss 0.72498, train_acc 0.7512, valid_acc 0.7513, Time 00:00:25,lr 0.01\n",
      "epoch 58, loss 0.72307, train_acc 0.7534, valid_acc 0.7575, Time 00:00:24,lr 0.01\n",
      "epoch 59, loss 0.72512, train_acc 0.7530, valid_acc 0.7345, Time 00:00:24,lr 0.01\n",
      "epoch 60, loss 0.57424, train_acc 0.8031, valid_acc 0.8110, Time 00:00:25,lr 0.002\n",
      "epoch 61, loss 0.53185, train_acc 0.8193, valid_acc 0.8204, Time 00:00:24,lr 0.002\n",
      "epoch 62, loss 0.52125, train_acc 0.8227, valid_acc 0.8243, Time 00:00:24,lr 0.002\n",
      "epoch 63, loss 0.50715, train_acc 0.8265, valid_acc 0.8171, Time 00:00:26,lr 0.002\n",
      "epoch 64, loss 0.50566, train_acc 0.8266, valid_acc 0.8231, Time 00:00:24,lr 0.002\n",
      "epoch 65, loss 0.49758, train_acc 0.8306, valid_acc 0.8185, Time 00:00:24,lr 0.002\n",
      "epoch 66, loss 0.49151, train_acc 0.8314, valid_acc 0.8244, Time 00:00:24,lr 0.002\n",
      "epoch 67, loss 0.49002, train_acc 0.8309, valid_acc 0.8193, Time 00:00:24,lr 0.002\n",
      "epoch 68, loss 0.49131, train_acc 0.8323, valid_acc 0.8247, Time 00:00:24,lr 0.002\n",
      "epoch 69, loss 0.48864, train_acc 0.8319, valid_acc 0.8245, Time 00:00:26,lr 0.002\n",
      "epoch 70, loss 0.48861, train_acc 0.8329, valid_acc 0.8152, Time 00:00:26,lr 0.002\n",
      "epoch 71, loss 0.49203, train_acc 0.8310, valid_acc 0.8241, Time 00:00:24,lr 0.002\n",
      "epoch 72, loss 0.48694, train_acc 0.8326, valid_acc 0.8207, Time 00:00:25,lr 0.002\n",
      "epoch 73, loss 0.48480, train_acc 0.8329, valid_acc 0.8162, Time 00:00:26,lr 0.002\n",
      "epoch 74, loss 0.49028, train_acc 0.8328, valid_acc 0.8217, Time 00:00:24,lr 0.002\n",
      "epoch 75, loss 0.48933, train_acc 0.8325, valid_acc 0.8256, Time 00:00:24,lr 0.002\n",
      "epoch 76, loss 0.48938, train_acc 0.8311, valid_acc 0.8240, Time 00:00:24,lr 0.002\n",
      "epoch 77, loss 0.48646, train_acc 0.8330, valid_acc 0.8162, Time 00:00:24,lr 0.002\n",
      "epoch 78, loss 0.48905, train_acc 0.8328, valid_acc 0.8183, Time 00:00:24,lr 0.002\n",
      "epoch 79, loss 0.48690, train_acc 0.8318, valid_acc 0.8164, Time 00:00:24,lr 0.002\n",
      "epoch 80, loss 0.48914, train_acc 0.8336, valid_acc 0.8162, Time 00:00:24,lr 0.002\n",
      "epoch 81, loss 0.49061, train_acc 0.8331, valid_acc 0.8200, Time 00:00:24,lr 0.002\n",
      "epoch 82, loss 0.48518, train_acc 0.8342, valid_acc 0.8108, Time 00:00:24,lr 0.002\n",
      "epoch 83, loss 0.48351, train_acc 0.8340, valid_acc 0.8190, Time 00:00:24,lr 0.002\n",
      "epoch 84, loss 0.48935, train_acc 0.8316, valid_acc 0.8246, Time 00:00:24,lr 0.002\n",
      "epoch 85, loss 0.48496, train_acc 0.8341, valid_acc 0.8153, Time 00:00:24,lr 0.002\n",
      "epoch 86, loss 0.48636, train_acc 0.8330, valid_acc 0.8215, Time 00:00:24,lr 0.002\n",
      "epoch 87, loss 0.48367, train_acc 0.8343, valid_acc 0.8203, Time 00:00:24,lr 0.002\n",
      "epoch 88, loss 0.48439, train_acc 0.8336, valid_acc 0.8209, Time 00:00:24,lr 0.002\n",
      "epoch 89, loss 0.48604, train_acc 0.8307, valid_acc 0.8233, Time 00:00:24,lr 0.002\n",
      "epoch 90, loss 0.40677, train_acc 0.8612, valid_acc 0.8445, Time 00:00:24,lr 0.0004\n",
      "epoch 91, loss 0.37624, train_acc 0.8704, valid_acc 0.8463, Time 00:00:24,lr 0.0004\n",
      "epoch 92, loss 0.36739, train_acc 0.8739, valid_acc 0.8504, Time 00:00:24,lr 0.0004\n",
      "epoch 93, loss 0.35553, train_acc 0.8765, valid_acc 0.8514, Time 00:00:27,lr 0.0004\n",
      "epoch 94, loss 0.35360, train_acc 0.8777, valid_acc 0.8497, Time 00:00:24,lr 0.0004\n",
      "epoch 95, loss 0.34861, train_acc 0.8797, valid_acc 0.8496, Time 00:00:24,lr 0.0004\n",
      "epoch 96, loss 0.34631, train_acc 0.8805, valid_acc 0.8536, Time 00:00:24,lr 0.0004\n",
      "epoch 97, loss 0.34272, train_acc 0.8825, valid_acc 0.8508, Time 00:00:24,lr 0.0004\n",
      "epoch 98, loss 0.33808, train_acc 0.8834, valid_acc 0.8506, Time 00:00:24,lr 0.0004\n",
      "epoch 99, loss 0.33606, train_acc 0.8850, valid_acc 0.8530, Time 00:00:24,lr 0.0004\n",
      "epoch 100, loss 0.33146, train_acc 0.8853, valid_acc 0.8521, Time 00:00:25,lr 0.0004\n",
      "epoch 101, loss 0.32776, train_acc 0.8870, valid_acc 0.8508, Time 00:00:26,lr 0.0004\n",
      "epoch 102, loss 0.32878, train_acc 0.8876, valid_acc 0.8494, Time 00:00:24,lr 0.0004\n",
      "epoch 103, loss 0.32975, train_acc 0.8859, valid_acc 0.8497, Time 00:00:25,lr 0.0004\n",
      "epoch 104, loss 0.32299, train_acc 0.8885, valid_acc 0.8508, Time 00:00:24,lr 0.0004\n",
      "epoch 105, loss 0.32167, train_acc 0.8903, valid_acc 0.8495, Time 00:00:24,lr 0.0004\n",
      "epoch 106, loss 0.32123, train_acc 0.8899, valid_acc 0.8451, Time 00:00:24,lr 0.0004\n",
      "epoch 107, loss 0.31717, train_acc 0.8901, valid_acc 0.8499, Time 00:00:24,lr 0.0004\n",
      "epoch 108, loss 0.32094, train_acc 0.8888, valid_acc 0.8480, Time 00:00:24,lr 0.0004\n",
      "epoch 109, loss 0.31830, train_acc 0.8905, valid_acc 0.8485, Time 00:00:24,lr 0.0004\n",
      "epoch 110, loss 0.31645, train_acc 0.8901, valid_acc 0.8507, Time 00:00:24,lr 0.0004\n",
      "epoch 111, loss 0.31573, train_acc 0.8911, valid_acc 0.8499, Time 00:00:24,lr 0.0004\n",
      "epoch 112, loss 0.31620, train_acc 0.8903, valid_acc 0.8511, Time 00:00:24,lr 0.0004\n",
      "epoch 113, loss 0.31348, train_acc 0.8922, valid_acc 0.8513, Time 00:00:24,lr 0.0004\n",
      "epoch 114, loss 0.31418, train_acc 0.8923, valid_acc 0.8472, Time 00:00:24,lr 0.0004\n",
      "epoch 115, loss 0.31038, train_acc 0.8922, valid_acc 0.8499, Time 00:00:24,lr 0.0004\n",
      "epoch 116, loss 0.31282, train_acc 0.8913, valid_acc 0.8484, Time 00:00:24,lr 0.0004\n",
      "epoch 117, loss 0.30687, train_acc 0.8954, valid_acc 0.8487, Time 00:00:24,lr 0.0004\n",
      "epoch 118, loss 0.31341, train_acc 0.8917, valid_acc 0.8544, Time 00:00:25,lr 0.0004\n",
      "epoch 119, loss 0.30818, train_acc 0.8922, valid_acc 0.8467, Time 00:00:24,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k3p1s2()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k3p1s2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 gluon resnet18 vs my resnet 18: use max pooling vs no max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T15:04:54.477541Z",
     "start_time": "2018-03-04T15:04:54.449458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:3])\n",
    "        net.add(*resnet.features[4:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:31:03.467325Z",
     "start_time": "2018-03-04T05:14:50.124830Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.38056, train_acc 0.1657, valid_acc 0.2737, Time 00:00:56,lr 0.1\n",
      "epoch 1, loss 1.72326, train_acc 0.3529, valid_acc 0.3568, Time 00:00:57,lr 0.1\n",
      "epoch 2, loss 1.41782, train_acc 0.4814, valid_acc 0.5539, Time 00:00:56,lr 0.1\n",
      "epoch 3, loss 1.18895, train_acc 0.5769, valid_acc 0.4499, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.06284, train_acc 0.6269, valid_acc 0.6079, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 0.94214, train_acc 0.6724, valid_acc 0.6066, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.88145, train_acc 0.6946, valid_acc 0.5705, Time 00:00:57,lr 0.1\n",
      "epoch 7, loss 0.84403, train_acc 0.7099, valid_acc 0.5912, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.81196, train_acc 0.7204, valid_acc 0.7287, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.79473, train_acc 0.7280, valid_acc 0.6951, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.78191, train_acc 0.7316, valid_acc 0.6971, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.77675, train_acc 0.7329, valid_acc 0.7301, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.75665, train_acc 0.7406, valid_acc 0.6933, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.75395, train_acc 0.7421, valid_acc 0.6948, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.74696, train_acc 0.7435, valid_acc 0.7115, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.73717, train_acc 0.7483, valid_acc 0.6977, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.73225, train_acc 0.7511, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 17, loss 0.73271, train_acc 0.7495, valid_acc 0.7008, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.72847, train_acc 0.7509, valid_acc 0.6897, Time 00:00:56,lr 0.1\n",
      "epoch 19, loss 0.72694, train_acc 0.7517, valid_acc 0.5876, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.72430, train_acc 0.7523, valid_acc 0.7296, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.71836, train_acc 0.7536, valid_acc 0.6899, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.71967, train_acc 0.7531, valid_acc 0.6885, Time 00:00:56,lr 0.1\n",
      "epoch 23, loss 0.71953, train_acc 0.7555, valid_acc 0.7130, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.70939, train_acc 0.7559, valid_acc 0.6943, Time 00:00:56,lr 0.1\n",
      "epoch 25, loss 0.71292, train_acc 0.7573, valid_acc 0.7349, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.71025, train_acc 0.7568, valid_acc 0.7351, Time 00:00:56,lr 0.1\n",
      "epoch 27, loss 0.71876, train_acc 0.7551, valid_acc 0.7187, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.70902, train_acc 0.7583, valid_acc 0.7208, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.70804, train_acc 0.7554, valid_acc 0.7308, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.69986, train_acc 0.7599, valid_acc 0.7021, Time 00:00:56,lr 0.1\n",
      "epoch 31, loss 0.70725, train_acc 0.7590, valid_acc 0.7294, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.70756, train_acc 0.7607, valid_acc 0.7474, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.70504, train_acc 0.7586, valid_acc 0.7125, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.70649, train_acc 0.7594, valid_acc 0.6955, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.70067, train_acc 0.7614, valid_acc 0.7556, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.70107, train_acc 0.7616, valid_acc 0.5762, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.69916, train_acc 0.7612, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.70580, train_acc 0.7588, valid_acc 0.6723, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.69736, train_acc 0.7607, valid_acc 0.7138, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.70295, train_acc 0.7603, valid_acc 0.7250, Time 00:00:56,lr 0.1\n",
      "epoch 41, loss 0.69599, train_acc 0.7622, valid_acc 0.6537, Time 00:00:57,lr 0.1\n",
      "epoch 42, loss 0.70267, train_acc 0.7622, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 43, loss 0.70145, train_acc 0.7609, valid_acc 0.6835, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.69758, train_acc 0.7642, valid_acc 0.6921, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.69759, train_acc 0.7621, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.69861, train_acc 0.7599, valid_acc 0.6791, Time 00:00:56,lr 0.1\n",
      "epoch 47, loss 0.69245, train_acc 0.7646, valid_acc 0.7376, Time 00:00:58,lr 0.1\n",
      "epoch 48, loss 0.70034, train_acc 0.7613, valid_acc 0.6330, Time 00:01:02,lr 0.1\n",
      "epoch 49, loss 0.69684, train_acc 0.7638, valid_acc 0.7106, Time 00:00:59,lr 0.1\n",
      "epoch 50, loss 0.69907, train_acc 0.7642, valid_acc 0.7437, Time 00:01:02,lr 0.1\n",
      "epoch 51, loss 0.69313, train_acc 0.7632, valid_acc 0.7042, Time 00:01:03,lr 0.1\n",
      "epoch 52, loss 0.69796, train_acc 0.7624, valid_acc 0.6378, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.69387, train_acc 0.7634, valid_acc 0.7631, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.69179, train_acc 0.7639, valid_acc 0.6815, Time 00:00:56,lr 0.1\n",
      "epoch 55, loss 0.70082, train_acc 0.7620, valid_acc 0.7479, Time 00:00:56,lr 0.1\n",
      "epoch 56, loss 0.69588, train_acc 0.7631, valid_acc 0.7191, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.69733, train_acc 0.7639, valid_acc 0.7269, Time 00:00:57,lr 0.1\n",
      "epoch 58, loss 0.69333, train_acc 0.7659, valid_acc 0.6627, Time 00:00:56,lr 0.1\n",
      "epoch 59, loss 0.69576, train_acc 0.7629, valid_acc 0.7450, Time 00:00:56,lr 0.1\n",
      "epoch 60, loss 0.69494, train_acc 0.7634, valid_acc 0.7199, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.69445, train_acc 0.7622, valid_acc 0.7363, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.69379, train_acc 0.7620, valid_acc 0.7078, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.69461, train_acc 0.7638, valid_acc 0.7163, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.69229, train_acc 0.7644, valid_acc 0.7119, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.69231, train_acc 0.7630, valid_acc 0.7010, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.69897, train_acc 0.7628, valid_acc 0.6015, Time 00:01:00,lr 0.1\n",
      "epoch 67, loss 0.69971, train_acc 0.7618, valid_acc 0.7291, Time 00:01:00,lr 0.1\n",
      "epoch 68, loss 0.69992, train_acc 0.7627, valid_acc 0.6869, Time 00:00:59,lr 0.1\n",
      "epoch 69, loss 0.69482, train_acc 0.7612, valid_acc 0.7649, Time 00:01:03,lr 0.1\n",
      "epoch 70, loss 0.69304, train_acc 0.7629, valid_acc 0.6619, Time 00:00:57,lr 0.1\n",
      "epoch 71, loss 0.69240, train_acc 0.7654, valid_acc 0.7104, Time 00:00:56,lr 0.1\n",
      "epoch 72, loss 0.69880, train_acc 0.7608, valid_acc 0.7013, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.69195, train_acc 0.7630, valid_acc 0.6779, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.69141, train_acc 0.7646, valid_acc 0.6973, Time 00:00:59,lr 0.1\n",
      "epoch 75, loss 0.68878, train_acc 0.7666, valid_acc 0.6534, Time 00:00:59,lr 0.1\n",
      "epoch 76, loss 0.69063, train_acc 0.7644, valid_acc 0.6998, Time 00:01:00,lr 0.1\n",
      "epoch 77, loss 0.69243, train_acc 0.7637, valid_acc 0.7247, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.70374, train_acc 0.7610, valid_acc 0.6934, Time 00:00:57,lr 0.1\n",
      "epoch 79, loss 0.69154, train_acc 0.7635, valid_acc 0.7347, Time 00:01:01,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:28:52.313297Z",
     "start_time": "2018-03-04T06:33:29.684980Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.58195, train_acc 0.8029, valid_acc 0.7700, Time 00:00:52,lr 0.05\n",
      "epoch 1, loss 0.65821, train_acc 0.7764, valid_acc 0.7590, Time 00:00:55,lr 0.05\n",
      "epoch 2, loss 0.68045, train_acc 0.7693, valid_acc 0.4827, Time 00:00:56,lr 0.05\n",
      "epoch 3, loss 0.68091, train_acc 0.7667, valid_acc 0.7464, Time 00:00:56,lr 0.05\n",
      "epoch 4, loss 0.68455, train_acc 0.7671, valid_acc 0.7237, Time 00:00:56,lr 0.05\n",
      "epoch 5, loss 0.68415, train_acc 0.7694, valid_acc 0.7485, Time 00:00:57,lr 0.05\n",
      "epoch 6, loss 0.68346, train_acc 0.7678, valid_acc 0.5743, Time 00:00:58,lr 0.05\n",
      "epoch 7, loss 0.68119, train_acc 0.7682, valid_acc 0.6967, Time 00:00:56,lr 0.05\n",
      "epoch 8, loss 0.68399, train_acc 0.7670, valid_acc 0.6504, Time 00:00:58,lr 0.05\n",
      "epoch 9, loss 0.68451, train_acc 0.7676, valid_acc 0.6430, Time 00:00:57,lr 0.05\n",
      "epoch 10, loss 0.68556, train_acc 0.7673, valid_acc 0.6783, Time 00:00:59,lr 0.05\n",
      "epoch 11, loss 0.68058, train_acc 0.7669, valid_acc 0.6660, Time 00:00:57,lr 0.05\n",
      "epoch 12, loss 0.69183, train_acc 0.7643, valid_acc 0.6486, Time 00:00:57,lr 0.05\n",
      "epoch 13, loss 0.68374, train_acc 0.7672, valid_acc 0.7040, Time 00:00:57,lr 0.05\n",
      "epoch 14, loss 0.68587, train_acc 0.7647, valid_acc 0.7557, Time 00:00:58,lr 0.05\n",
      "epoch 15, loss 0.68559, train_acc 0.7704, valid_acc 0.6823, Time 00:00:57,lr 0.05\n",
      "epoch 16, loss 0.68928, train_acc 0.7640, valid_acc 0.6444, Time 00:00:59,lr 0.05\n",
      "epoch 17, loss 0.68680, train_acc 0.7662, valid_acc 0.5039, Time 00:00:58,lr 0.05\n",
      "epoch 18, loss 0.68851, train_acc 0.7660, valid_acc 0.6354, Time 00:01:01,lr 0.05\n",
      "epoch 19, loss 0.68628, train_acc 0.7667, valid_acc 0.6202, Time 00:00:59,lr 0.05\n",
      "epoch 20, loss 0.69103, train_acc 0.7665, valid_acc 0.7290, Time 00:01:01,lr 0.05\n",
      "epoch 21, loss 0.68566, train_acc 0.7666, valid_acc 0.6471, Time 00:00:59,lr 0.05\n",
      "epoch 22, loss 0.69282, train_acc 0.7654, valid_acc 0.7272, Time 00:01:01,lr 0.05\n",
      "epoch 23, loss 0.68336, train_acc 0.7706, valid_acc 0.6115, Time 00:01:02,lr 0.05\n",
      "epoch 24, loss 0.68217, train_acc 0.7695, valid_acc 0.7322, Time 00:01:00,lr 0.05\n",
      "epoch 25, loss 0.68338, train_acc 0.7686, valid_acc 0.7349, Time 00:01:01,lr 0.05\n",
      "epoch 26, loss 0.68748, train_acc 0.7667, valid_acc 0.7034, Time 00:01:04,lr 0.05\n",
      "epoch 27, loss 0.69171, train_acc 0.7651, valid_acc 0.7070, Time 00:00:59,lr 0.05\n",
      "epoch 28, loss 0.68284, train_acc 0.7680, valid_acc 0.7419, Time 00:00:57,lr 0.05\n",
      "epoch 29, loss 0.68182, train_acc 0.7672, valid_acc 0.6750, Time 00:00:57,lr 0.05\n",
      "epoch 30, loss 0.43710, train_acc 0.8514, valid_acc 0.8458, Time 00:00:56,lr 0.01\n",
      "epoch 31, loss 0.38423, train_acc 0.8686, valid_acc 0.8636, Time 00:00:56,lr 0.01\n",
      "epoch 32, loss 0.38351, train_acc 0.8686, valid_acc 0.8521, Time 00:01:01,lr 0.01\n",
      "epoch 33, loss 0.38735, train_acc 0.8679, valid_acc 0.8384, Time 00:01:00,lr 0.01\n",
      "epoch 34, loss 0.38082, train_acc 0.8711, valid_acc 0.8641, Time 00:00:56,lr 0.01\n",
      "epoch 35, loss 0.38738, train_acc 0.8672, valid_acc 0.8248, Time 00:00:55,lr 0.01\n",
      "epoch 36, loss 0.38062, train_acc 0.8709, valid_acc 0.8618, Time 00:00:55,lr 0.01\n",
      "epoch 37, loss 0.38049, train_acc 0.8718, valid_acc 0.8291, Time 00:00:57,lr 0.01\n",
      "epoch 38, loss 0.37551, train_acc 0.8724, valid_acc 0.8629, Time 00:00:58,lr 0.01\n",
      "epoch 39, loss 0.37463, train_acc 0.8717, valid_acc 0.8566, Time 00:01:00,lr 0.01\n",
      "epoch 40, loss 0.37176, train_acc 0.8730, valid_acc 0.8559, Time 00:01:00,lr 0.01\n",
      "epoch 41, loss 0.36386, train_acc 0.8775, valid_acc 0.8362, Time 00:00:56,lr 0.01\n",
      "epoch 42, loss 0.36582, train_acc 0.8751, valid_acc 0.8740, Time 00:00:56,lr 0.01\n",
      "epoch 43, loss 0.36425, train_acc 0.8759, valid_acc 0.8561, Time 00:00:56,lr 0.01\n",
      "epoch 44, loss 0.35750, train_acc 0.8785, valid_acc 0.8442, Time 00:00:56,lr 0.01\n",
      "epoch 45, loss 0.35708, train_acc 0.8781, valid_acc 0.8600, Time 00:00:57,lr 0.01\n",
      "epoch 46, loss 0.35498, train_acc 0.8784, valid_acc 0.8639, Time 00:00:56,lr 0.01\n",
      "epoch 47, loss 0.35540, train_acc 0.8789, valid_acc 0.8602, Time 00:00:56,lr 0.01\n",
      "epoch 48, loss 0.35173, train_acc 0.8803, valid_acc 0.8466, Time 00:00:56,lr 0.01\n",
      "epoch 49, loss 0.35126, train_acc 0.8813, valid_acc 0.8660, Time 00:00:56,lr 0.01\n",
      "epoch 50, loss 0.34843, train_acc 0.8821, valid_acc 0.8473, Time 00:00:56,lr 0.01\n",
      "epoch 51, loss 0.34383, train_acc 0.8816, valid_acc 0.8726, Time 00:00:57,lr 0.01\n",
      "epoch 52, loss 0.34999, train_acc 0.8809, valid_acc 0.8602, Time 00:01:01,lr 0.01\n",
      "epoch 53, loss 0.34417, train_acc 0.8835, valid_acc 0.8419, Time 00:00:56,lr 0.01\n",
      "epoch 54, loss 0.34483, train_acc 0.8828, valid_acc 0.8654, Time 00:00:56,lr 0.01\n",
      "epoch 55, loss 0.34106, train_acc 0.8856, valid_acc 0.8673, Time 00:00:56,lr 0.01\n",
      "epoch 56, loss 0.34146, train_acc 0.8839, valid_acc 0.8758, Time 00:01:01,lr 0.01\n",
      "epoch 57, loss 0.34200, train_acc 0.8854, valid_acc 0.8456, Time 00:00:58,lr 0.01\n",
      "epoch 58, loss 0.34399, train_acc 0.8828, valid_acc 0.8639, Time 00:00:58,lr 0.01\n",
      "epoch 59, loss 0.34053, train_acc 0.8847, valid_acc 0.8597, Time 00:00:58,lr 0.01\n",
      "epoch 60, loss 0.20173, train_acc 0.9333, valid_acc 0.9198, Time 00:00:56,lr 0.002\n",
      "epoch 61, loss 0.15869, train_acc 0.9455, valid_acc 0.9215, Time 00:00:56,lr 0.002\n",
      "epoch 62, loss 0.14265, train_acc 0.9519, valid_acc 0.9222, Time 00:00:56,lr 0.002\n",
      "epoch 63, loss 0.13336, train_acc 0.9559, valid_acc 0.9245, Time 00:00:58,lr 0.002\n",
      "epoch 64, loss 0.12715, train_acc 0.9568, valid_acc 0.9178, Time 00:00:56,lr 0.002\n",
      "epoch 65, loss 0.12213, train_acc 0.9591, valid_acc 0.9210, Time 00:00:58,lr 0.002\n",
      "epoch 66, loss 0.11907, train_acc 0.9592, valid_acc 0.9224, Time 00:00:57,lr 0.002\n",
      "epoch 67, loss 0.11596, train_acc 0.9613, valid_acc 0.9201, Time 00:00:56,lr 0.002\n",
      "epoch 68, loss 0.11954, train_acc 0.9596, valid_acc 0.9202, Time 00:00:56,lr 0.002\n",
      "epoch 69, loss 0.11487, train_acc 0.9614, valid_acc 0.9212, Time 00:00:56,lr 0.002\n",
      "epoch 70, loss 0.11803, train_acc 0.9598, valid_acc 0.9105, Time 00:00:56,lr 0.002\n",
      "epoch 71, loss 0.11419, train_acc 0.9616, valid_acc 0.9124, Time 00:00:56,lr 0.002\n",
      "epoch 72, loss 0.11577, train_acc 0.9608, valid_acc 0.9108, Time 00:00:56,lr 0.002\n",
      "epoch 73, loss 0.11624, train_acc 0.9604, valid_acc 0.9160, Time 00:00:56,lr 0.002\n",
      "epoch 74, loss 0.11776, train_acc 0.9603, valid_acc 0.9185, Time 00:00:56,lr 0.002\n",
      "epoch 75, loss 0.11450, train_acc 0.9622, valid_acc 0.9117, Time 00:00:56,lr 0.002\n",
      "epoch 76, loss 0.11900, train_acc 0.9613, valid_acc 0.9097, Time 00:01:00,lr 0.002\n",
      "epoch 77, loss 0.12027, train_acc 0.9592, valid_acc 0.9142, Time 00:00:57,lr 0.002\n",
      "epoch 78, loss 0.12142, train_acc 0.9593, valid_acc 0.9121, Time 00:00:56,lr 0.002\n",
      "epoch 79, loss 0.12121, train_acc 0.9592, valid_acc 0.9147, Time 00:00:57,lr 0.002\n",
      "epoch 80, loss 0.12328, train_acc 0.9580, valid_acc 0.9057, Time 00:00:57,lr 0.002\n",
      "epoch 81, loss 0.12675, train_acc 0.9566, valid_acc 0.9083, Time 00:00:56,lr 0.002\n",
      "epoch 82, loss 0.11973, train_acc 0.9583, valid_acc 0.9099, Time 00:00:58,lr 0.002\n",
      "epoch 83, loss 0.12241, train_acc 0.9586, valid_acc 0.9161, Time 00:00:55,lr 0.002\n",
      "epoch 84, loss 0.11883, train_acc 0.9593, valid_acc 0.9097, Time 00:00:56,lr 0.002\n",
      "epoch 85, loss 0.12814, train_acc 0.9568, valid_acc 0.9161, Time 00:00:56,lr 0.002\n",
      "epoch 86, loss 0.11971, train_acc 0.9590, valid_acc 0.9075, Time 00:00:55,lr 0.002\n",
      "epoch 87, loss 0.12493, train_acc 0.9573, valid_acc 0.9143, Time 00:00:56,lr 0.002\n",
      "epoch 88, loss 0.11748, train_acc 0.9599, valid_acc 0.9079, Time 00:01:00,lr 0.002\n",
      "epoch 89, loss 0.12178, train_acc 0.9585, valid_acc 0.9043, Time 00:00:55,lr 0.002\n",
      "epoch 90, loss 0.06359, train_acc 0.9809, valid_acc 0.9297, Time 00:00:56,lr 0.0004\n",
      "epoch 91, loss 0.04226, train_acc 0.9878, valid_acc 0.9347, Time 00:00:55,lr 0.0004\n",
      "epoch 92, loss 0.03643, train_acc 0.9901, valid_acc 0.9339, Time 00:00:56,lr 0.0004\n",
      "epoch 93, loss 0.03193, train_acc 0.9917, valid_acc 0.9344, Time 00:00:55,lr 0.0004\n",
      "epoch 94, loss 0.03020, train_acc 0.9919, valid_acc 0.9354, Time 00:00:56,lr 0.0004\n",
      "epoch 95, loss 0.02928, train_acc 0.9922, valid_acc 0.9342, Time 00:00:55,lr 0.0004\n",
      "epoch 96, loss 0.02602, train_acc 0.9928, valid_acc 0.9355, Time 00:00:55,lr 0.0004\n",
      "epoch 97, loss 0.02409, train_acc 0.9935, valid_acc 0.9350, Time 00:00:55,lr 0.0004\n",
      "epoch 98, loss 0.02386, train_acc 0.9939, valid_acc 0.9329, Time 00:00:56,lr 0.0004\n",
      "epoch 99, loss 0.01987, train_acc 0.9951, valid_acc 0.9358, Time 00:00:56,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.02084, train_acc 0.9946, valid_acc 0.9337, Time 00:00:57,lr 0.0004\n",
      "epoch 101, loss 0.01851, train_acc 0.9954, valid_acc 0.9360, Time 00:00:58,lr 0.0004\n",
      "epoch 102, loss 0.01870, train_acc 0.9954, valid_acc 0.9357, Time 00:00:56,lr 0.0004\n",
      "epoch 103, loss 0.01807, train_acc 0.9956, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 104, loss 0.01849, train_acc 0.9954, valid_acc 0.9336, Time 00:00:56,lr 0.0004\n",
      "epoch 105, loss 0.01738, train_acc 0.9961, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 106, loss 0.01575, train_acc 0.9961, valid_acc 0.9359, Time 00:01:01,lr 0.0004\n",
      "epoch 107, loss 0.01618, train_acc 0.9963, valid_acc 0.9350, Time 00:00:56,lr 0.0004\n",
      "epoch 108, loss 0.01708, train_acc 0.9954, valid_acc 0.9334, Time 00:00:56,lr 0.0004\n",
      "epoch 109, loss 0.01544, train_acc 0.9964, valid_acc 0.9364, Time 00:00:56,lr 0.0004\n",
      "epoch 110, loss 0.01496, train_acc 0.9965, valid_acc 0.9346, Time 00:00:57,lr 0.0004\n",
      "epoch 111, loss 0.01420, train_acc 0.9966, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 112, loss 0.01472, train_acc 0.9962, valid_acc 0.9342, Time 00:00:56,lr 0.0004\n",
      "epoch 113, loss 0.01399, train_acc 0.9968, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 114, loss 0.01392, train_acc 0.9969, valid_acc 0.9344, Time 00:00:57,lr 0.0004\n",
      "epoch 115, loss 0.01281, train_acc 0.9975, valid_acc 0.9342, Time 00:00:58,lr 0.0004\n",
      "epoch 116, loss 0.01356, train_acc 0.9968, valid_acc 0.9359, Time 00:00:56,lr 0.0004\n",
      "epoch 117, loss 0.01356, train_acc 0.9971, valid_acc 0.9362, Time 00:00:58,lr 0.0004\n",
      "epoch 118, loss 0.01320, train_acc 0.9970, valid_acc 0.9353, Time 00:00:59,lr 0.0004\n",
      "epoch 119, loss 0.01289, train_acc 0.9971, valid_acc 0.9349, Time 00:01:00,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\", ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3_no_max_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 general lr policy train resnet18_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T18:12:46.252720Z",
     "start_time": "2018-03-04T15:04:58.881087Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.15846, train_acc 0.2636, valid_acc 0.3974, Time 00:00:50,lr 0.1\n",
      "epoch 1, loss 1.62803, train_acc 0.3977, valid_acc 0.4551, Time 00:00:54,lr 0.1\n",
      "epoch 2, loss 1.41918, train_acc 0.4863, valid_acc 0.5159, Time 00:00:55,lr 0.1\n",
      "epoch 3, loss 1.19825, train_acc 0.5719, valid_acc 0.6059, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.01131, train_acc 0.6444, valid_acc 0.6491, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 0.88996, train_acc 0.6878, valid_acc 0.7103, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.78305, train_acc 0.7267, valid_acc 0.7235, Time 00:00:57,lr 0.1\n",
      "epoch 7, loss 0.69410, train_acc 0.7577, valid_acc 0.7858, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.63683, train_acc 0.7805, valid_acc 0.7856, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.60257, train_acc 0.7929, valid_acc 0.7577, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.57003, train_acc 0.8039, valid_acc 0.7868, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.54821, train_acc 0.8109, valid_acc 0.8210, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.52209, train_acc 0.8195, valid_acc 0.8023, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.50669, train_acc 0.8247, valid_acc 0.8284, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.49168, train_acc 0.8307, valid_acc 0.7976, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.48094, train_acc 0.8356, valid_acc 0.8335, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.46743, train_acc 0.8393, valid_acc 0.8340, Time 00:00:55,lr 0.1\n",
      "epoch 17, loss 0.45708, train_acc 0.8422, valid_acc 0.8399, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.44915, train_acc 0.8447, valid_acc 0.8338, Time 00:00:55,lr 0.1\n",
      "epoch 19, loss 0.44107, train_acc 0.8492, valid_acc 0.8440, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.42885, train_acc 0.8522, valid_acc 0.8271, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.41722, train_acc 0.8576, valid_acc 0.8403, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.41695, train_acc 0.8535, valid_acc 0.8352, Time 00:00:55,lr 0.1\n",
      "epoch 23, loss 0.40497, train_acc 0.8610, valid_acc 0.8293, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.40128, train_acc 0.8627, valid_acc 0.8544, Time 00:00:55,lr 0.1\n",
      "epoch 25, loss 0.39778, train_acc 0.8637, valid_acc 0.8255, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.38924, train_acc 0.8654, valid_acc 0.8475, Time 00:00:55,lr 0.1\n",
      "epoch 27, loss 0.38375, train_acc 0.8672, valid_acc 0.8315, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.38154, train_acc 0.8695, valid_acc 0.8524, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.37514, train_acc 0.8702, valid_acc 0.8548, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.37547, train_acc 0.8701, valid_acc 0.8264, Time 00:00:55,lr 0.1\n",
      "epoch 31, loss 0.36880, train_acc 0.8730, valid_acc 0.8621, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.36823, train_acc 0.8750, valid_acc 0.8452, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.36444, train_acc 0.8735, valid_acc 0.8324, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.35993, train_acc 0.8753, valid_acc 0.8566, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.35871, train_acc 0.8765, valid_acc 0.8386, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.35150, train_acc 0.8794, valid_acc 0.8577, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.35466, train_acc 0.8782, valid_acc 0.8438, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.34956, train_acc 0.8793, valid_acc 0.8389, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.34803, train_acc 0.8814, valid_acc 0.8594, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.34794, train_acc 0.8803, valid_acc 0.8586, Time 00:00:55,lr 0.1\n",
      "epoch 41, loss 0.33894, train_acc 0.8836, valid_acc 0.8612, Time 00:00:55,lr 0.1\n",
      "epoch 42, loss 0.33981, train_acc 0.8850, valid_acc 0.8690, Time 00:00:55,lr 0.1\n",
      "epoch 43, loss 0.34106, train_acc 0.8833, valid_acc 0.8577, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.33637, train_acc 0.8857, valid_acc 0.8618, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.33335, train_acc 0.8851, valid_acc 0.8739, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.33010, train_acc 0.8878, valid_acc 0.8711, Time 00:00:55,lr 0.1\n",
      "epoch 47, loss 0.33107, train_acc 0.8860, valid_acc 0.8427, Time 00:00:56,lr 0.1\n",
      "epoch 48, loss 0.33052, train_acc 0.8873, valid_acc 0.8533, Time 00:00:56,lr 0.1\n",
      "epoch 49, loss 0.32679, train_acc 0.8881, valid_acc 0.8696, Time 00:00:56,lr 0.1\n",
      "epoch 50, loss 0.33123, train_acc 0.8852, valid_acc 0.8655, Time 00:00:56,lr 0.1\n",
      "epoch 51, loss 0.33064, train_acc 0.8860, valid_acc 0.8566, Time 00:00:56,lr 0.1\n",
      "epoch 52, loss 0.32772, train_acc 0.8872, valid_acc 0.8538, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.32102, train_acc 0.8909, valid_acc 0.8782, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.32483, train_acc 0.8884, valid_acc 0.8752, Time 00:00:55,lr 0.1\n",
      "epoch 55, loss 0.32031, train_acc 0.8922, valid_acc 0.8608, Time 00:00:58,lr 0.1\n",
      "epoch 56, loss 0.32091, train_acc 0.8904, valid_acc 0.8685, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.31825, train_acc 0.8908, valid_acc 0.8731, Time 00:00:56,lr 0.1\n",
      "epoch 58, loss 0.32040, train_acc 0.8905, valid_acc 0.8439, Time 00:00:59,lr 0.1\n",
      "epoch 59, loss 0.31838, train_acc 0.8906, valid_acc 0.8543, Time 00:01:01,lr 0.1\n",
      "epoch 60, loss 0.31636, train_acc 0.8924, valid_acc 0.8622, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.31622, train_acc 0.8912, valid_acc 0.8482, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.31287, train_acc 0.8932, valid_acc 0.8822, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.30961, train_acc 0.8930, valid_acc 0.8839, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.31417, train_acc 0.8945, valid_acc 0.8728, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.31620, train_acc 0.8919, valid_acc 0.8675, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.30825, train_acc 0.8954, valid_acc 0.8811, Time 00:00:56,lr 0.1\n",
      "epoch 67, loss 0.30862, train_acc 0.8947, valid_acc 0.8865, Time 00:00:56,lr 0.1\n",
      "epoch 68, loss 0.30983, train_acc 0.8933, valid_acc 0.8414, Time 00:00:56,lr 0.1\n",
      "epoch 69, loss 0.30705, train_acc 0.8954, valid_acc 0.8583, Time 00:00:56,lr 0.1\n",
      "epoch 70, loss 0.30614, train_acc 0.8944, valid_acc 0.8689, Time 00:00:59,lr 0.1\n",
      "epoch 71, loss 0.30487, train_acc 0.8951, valid_acc 0.8686, Time 00:00:57,lr 0.1\n",
      "epoch 72, loss 0.30635, train_acc 0.8959, valid_acc 0.8605, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.30478, train_acc 0.8955, valid_acc 0.8763, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.30377, train_acc 0.8953, valid_acc 0.8621, Time 00:01:01,lr 0.1\n",
      "epoch 75, loss 0.30006, train_acc 0.8972, valid_acc 0.8836, Time 00:00:58,lr 0.1\n",
      "epoch 76, loss 0.30860, train_acc 0.8948, valid_acc 0.8890, Time 00:00:56,lr 0.1\n",
      "epoch 77, loss 0.30585, train_acc 0.8953, valid_acc 0.8825, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.30411, train_acc 0.8961, valid_acc 0.8783, Time 00:00:56,lr 0.1\n",
      "epoch 79, loss 0.30075, train_acc 0.8967, valid_acc 0.8733, Time 00:00:55,lr 0.1\n",
      "epoch 80, loss 0.16171, train_acc 0.9455, valid_acc 0.9280, Time 00:00:58,lr 0.01\n",
      "epoch 81, loss 0.12098, train_acc 0.9590, valid_acc 0.9312, Time 00:00:57,lr 0.01\n",
      "epoch 82, loss 0.10316, train_acc 0.9647, valid_acc 0.9346, Time 00:00:56,lr 0.01\n",
      "epoch 83, loss 0.08788, train_acc 0.9699, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 84, loss 0.08168, train_acc 0.9721, valid_acc 0.9355, Time 00:00:56,lr 0.01\n",
      "epoch 85, loss 0.07426, train_acc 0.9743, valid_acc 0.9345, Time 00:00:56,lr 0.01\n",
      "epoch 86, loss 0.06671, train_acc 0.9770, valid_acc 0.9364, Time 00:00:57,lr 0.01\n",
      "epoch 87, loss 0.06325, train_acc 0.9785, valid_acc 0.9351, Time 00:00:56,lr 0.01\n",
      "epoch 88, loss 0.05709, train_acc 0.9806, valid_acc 0.9365, Time 00:00:56,lr 0.01\n",
      "epoch 89, loss 0.05294, train_acc 0.9820, valid_acc 0.9345, Time 00:00:56,lr 0.01\n",
      "epoch 90, loss 0.05128, train_acc 0.9830, valid_acc 0.9374, Time 00:00:56,lr 0.01\n",
      "epoch 91, loss 0.04591, train_acc 0.9843, valid_acc 0.9338, Time 00:00:56,lr 0.01\n",
      "epoch 92, loss 0.04429, train_acc 0.9850, valid_acc 0.9394, Time 00:00:56,lr 0.01\n",
      "epoch 93, loss 0.04115, train_acc 0.9867, valid_acc 0.9374, Time 00:00:56,lr 0.01\n",
      "epoch 94, loss 0.04127, train_acc 0.9853, valid_acc 0.9354, Time 00:00:56,lr 0.01\n",
      "epoch 95, loss 0.03846, train_acc 0.9870, valid_acc 0.9360, Time 00:00:56,lr 0.01\n",
      "epoch 96, loss 0.03880, train_acc 0.9865, valid_acc 0.9355, Time 00:00:56,lr 0.01\n",
      "epoch 97, loss 0.03761, train_acc 0.9873, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 98, loss 0.03572, train_acc 0.9885, valid_acc 0.9355, Time 00:00:56,lr 0.01\n",
      "epoch 99, loss 0.03503, train_acc 0.9886, valid_acc 0.9352, Time 00:00:56,lr 0.01\n",
      "epoch 100, loss 0.03311, train_acc 0.9891, valid_acc 0.9379, Time 00:00:56,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.03337, train_acc 0.9887, valid_acc 0.9390, Time 00:00:56,lr 0.01\n",
      "epoch 102, loss 0.03445, train_acc 0.9890, valid_acc 0.9373, Time 00:00:56,lr 0.01\n",
      "epoch 103, loss 0.03368, train_acc 0.9888, valid_acc 0.9336, Time 00:00:56,lr 0.01\n",
      "epoch 104, loss 0.03256, train_acc 0.9898, valid_acc 0.9330, Time 00:00:56,lr 0.01\n",
      "epoch 105, loss 0.03219, train_acc 0.9896, valid_acc 0.9352, Time 00:00:56,lr 0.01\n",
      "epoch 106, loss 0.03368, train_acc 0.9877, valid_acc 0.9321, Time 00:00:56,lr 0.01\n",
      "epoch 107, loss 0.03187, train_acc 0.9894, valid_acc 0.9285, Time 00:00:56,lr 0.01\n",
      "epoch 108, loss 0.03688, train_acc 0.9875, valid_acc 0.9326, Time 00:00:56,lr 0.01\n",
      "epoch 109, loss 0.03780, train_acc 0.9873, valid_acc 0.9341, Time 00:00:56,lr 0.01\n",
      "epoch 110, loss 0.03586, train_acc 0.9875, valid_acc 0.9356, Time 00:00:56,lr 0.01\n",
      "epoch 111, loss 0.03375, train_acc 0.9887, valid_acc 0.9339, Time 00:00:56,lr 0.01\n",
      "epoch 112, loss 0.03716, train_acc 0.9871, valid_acc 0.9310, Time 00:00:56,lr 0.01\n",
      "epoch 113, loss 0.03485, train_acc 0.9883, valid_acc 0.9332, Time 00:00:56,lr 0.01\n",
      "epoch 114, loss 0.03535, train_acc 0.9884, valid_acc 0.9346, Time 00:00:56,lr 0.01\n",
      "epoch 115, loss 0.03670, train_acc 0.9874, valid_acc 0.9343, Time 00:00:56,lr 0.01\n",
      "epoch 116, loss 0.03895, train_acc 0.9865, valid_acc 0.9309, Time 00:00:56,lr 0.01\n",
      "epoch 117, loss 0.03772, train_acc 0.9875, valid_acc 0.9293, Time 00:00:56,lr 0.01\n",
      "epoch 118, loss 0.03680, train_acc 0.9874, valid_acc 0.9319, Time 00:00:56,lr 0.01\n",
      "epoch 119, loss 0.03773, train_acc 0.9870, valid_acc 0.9334, Time 00:00:56,lr 0.01\n",
      "epoch 120, loss 0.04006, train_acc 0.9861, valid_acc 0.9331, Time 00:00:56,lr 0.01\n",
      "epoch 121, loss 0.04120, train_acc 0.9861, valid_acc 0.9315, Time 00:00:56,lr 0.01\n",
      "epoch 122, loss 0.04192, train_acc 0.9860, valid_acc 0.9294, Time 00:00:56,lr 0.01\n",
      "epoch 123, loss 0.04074, train_acc 0.9864, valid_acc 0.9280, Time 00:00:56,lr 0.01\n",
      "epoch 124, loss 0.04274, train_acc 0.9852, valid_acc 0.9279, Time 00:00:56,lr 0.01\n",
      "epoch 125, loss 0.04207, train_acc 0.9860, valid_acc 0.9332, Time 00:00:56,lr 0.01\n",
      "epoch 126, loss 0.04055, train_acc 0.9862, valid_acc 0.9292, Time 00:00:56,lr 0.01\n",
      "epoch 127, loss 0.04287, train_acc 0.9857, valid_acc 0.9241, Time 00:00:56,lr 0.01\n",
      "epoch 128, loss 0.04564, train_acc 0.9842, valid_acc 0.9270, Time 00:00:56,lr 0.01\n",
      "epoch 129, loss 0.04434, train_acc 0.9847, valid_acc 0.9281, Time 00:00:56,lr 0.01\n",
      "epoch 130, loss 0.04602, train_acc 0.9845, valid_acc 0.9340, Time 00:00:56,lr 0.01\n",
      "epoch 131, loss 0.04299, train_acc 0.9857, valid_acc 0.9290, Time 00:00:56,lr 0.01\n",
      "epoch 132, loss 0.04678, train_acc 0.9846, valid_acc 0.9289, Time 00:00:56,lr 0.01\n",
      "epoch 133, loss 0.04226, train_acc 0.9860, valid_acc 0.9291, Time 00:00:55,lr 0.01\n",
      "epoch 134, loss 0.04568, train_acc 0.9842, valid_acc 0.9266, Time 00:00:56,lr 0.01\n",
      "epoch 135, loss 0.04841, train_acc 0.9839, valid_acc 0.9294, Time 00:00:55,lr 0.01\n",
      "epoch 136, loss 0.04353, train_acc 0.9854, valid_acc 0.9302, Time 00:00:55,lr 0.01\n",
      "epoch 137, loss 0.04479, train_acc 0.9853, valid_acc 0.9284, Time 00:00:55,lr 0.01\n",
      "epoch 138, loss 0.04460, train_acc 0.9849, valid_acc 0.9290, Time 00:00:56,lr 0.01\n",
      "epoch 139, loss 0.05203, train_acc 0.9821, valid_acc 0.9315, Time 00:00:55,lr 0.01\n",
      "epoch 140, loss 0.04833, train_acc 0.9832, valid_acc 0.9216, Time 00:00:55,lr 0.01\n",
      "epoch 141, loss 0.04433, train_acc 0.9851, valid_acc 0.9331, Time 00:00:55,lr 0.01\n",
      "epoch 142, loss 0.04645, train_acc 0.9843, valid_acc 0.9234, Time 00:00:56,lr 0.01\n",
      "epoch 143, loss 0.04340, train_acc 0.9854, valid_acc 0.9286, Time 00:00:55,lr 0.01\n",
      "epoch 144, loss 0.04866, train_acc 0.9828, valid_acc 0.9302, Time 00:00:56,lr 0.01\n",
      "epoch 145, loss 0.05015, train_acc 0.9834, valid_acc 0.9280, Time 00:00:55,lr 0.01\n",
      "epoch 146, loss 0.04580, train_acc 0.9843, valid_acc 0.9310, Time 00:00:56,lr 0.01\n",
      "epoch 147, loss 0.04804, train_acc 0.9835, valid_acc 0.9258, Time 00:00:56,lr 0.01\n",
      "epoch 148, loss 0.04823, train_acc 0.9837, valid_acc 0.9259, Time 00:00:56,lr 0.01\n",
      "epoch 149, loss 0.04857, train_acc 0.9837, valid_acc 0.9285, Time 00:00:56,lr 0.01\n",
      "epoch 150, loss 0.02528, train_acc 0.9924, valid_acc 0.9435, Time 00:00:55,lr 0.001\n",
      "epoch 151, loss 0.01440, train_acc 0.9963, valid_acc 0.9431, Time 00:00:56,lr 0.001\n",
      "epoch 152, loss 0.01215, train_acc 0.9965, valid_acc 0.9442, Time 00:00:55,lr 0.001\n",
      "epoch 153, loss 0.00935, train_acc 0.9976, valid_acc 0.9453, Time 00:00:56,lr 0.001\n",
      "epoch 154, loss 0.00826, train_acc 0.9983, valid_acc 0.9451, Time 00:00:56,lr 0.001\n",
      "epoch 155, loss 0.00747, train_acc 0.9985, valid_acc 0.9453, Time 00:00:55,lr 0.001\n",
      "epoch 156, loss 0.00651, train_acc 0.9986, valid_acc 0.9444, Time 00:00:55,lr 0.001\n",
      "epoch 157, loss 0.00666, train_acc 0.9987, valid_acc 0.9455, Time 00:00:55,lr 0.001\n",
      "epoch 158, loss 0.00589, train_acc 0.9987, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 159, loss 0.00533, train_acc 0.9989, valid_acc 0.9463, Time 00:00:56,lr 0.001\n",
      "epoch 160, loss 0.00520, train_acc 0.9987, valid_acc 0.9466, Time 00:00:56,lr 0.001\n",
      "epoch 161, loss 0.00459, train_acc 0.9991, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 162, loss 0.00516, train_acc 0.9989, valid_acc 0.9456, Time 00:00:55,lr 0.001\n",
      "epoch 163, loss 0.00442, train_acc 0.9991, valid_acc 0.9462, Time 00:00:56,lr 0.001\n",
      "epoch 164, loss 0.00472, train_acc 0.9990, valid_acc 0.9467, Time 00:00:55,lr 0.001\n",
      "epoch 165, loss 0.00432, train_acc 0.9992, valid_acc 0.9462, Time 00:00:56,lr 0.001\n",
      "epoch 166, loss 0.00408, train_acc 0.9993, valid_acc 0.9473, Time 00:00:56,lr 0.001\n",
      "epoch 167, loss 0.00390, train_acc 0.9991, valid_acc 0.9471, Time 00:00:55,lr 0.001\n",
      "epoch 168, loss 0.00405, train_acc 0.9991, valid_acc 0.9457, Time 00:00:56,lr 0.001\n",
      "epoch 169, loss 0.00342, train_acc 0.9994, valid_acc 0.9453, Time 00:00:55,lr 0.001\n",
      "epoch 170, loss 0.00392, train_acc 0.9991, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 171, loss 0.00307, train_acc 0.9994, valid_acc 0.9472, Time 00:00:56,lr 0.001\n",
      "epoch 172, loss 0.00323, train_acc 0.9995, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 173, loss 0.00349, train_acc 0.9992, valid_acc 0.9480, Time 00:00:55,lr 0.001\n",
      "epoch 174, loss 0.00285, train_acc 0.9996, valid_acc 0.9486, Time 00:00:56,lr 0.001\n",
      "epoch 175, loss 0.00360, train_acc 0.9992, valid_acc 0.9475, Time 00:00:56,lr 0.001\n",
      "epoch 176, loss 0.00316, train_acc 0.9996, valid_acc 0.9482, Time 00:00:56,lr 0.001\n",
      "epoch 177, loss 0.00330, train_acc 0.9993, valid_acc 0.9486, Time 00:00:56,lr 0.001\n",
      "epoch 178, loss 0.00291, train_acc 0.9994, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 179, loss 0.00328, train_acc 0.9993, valid_acc 0.9477, Time 00:00:55,lr 0.001\n",
      "epoch 180, loss 0.00271, train_acc 0.9995, valid_acc 0.9472, Time 00:00:56,lr 0.001\n",
      "epoch 181, loss 0.00288, train_acc 0.9994, valid_acc 0.9485, Time 00:00:56,lr 0.001\n",
      "epoch 182, loss 0.00273, train_acc 0.9995, valid_acc 0.9481, Time 00:00:56,lr 0.001\n",
      "epoch 183, loss 0.00283, train_acc 0.9993, valid_acc 0.9480, Time 00:00:55,lr 0.001\n",
      "epoch 184, loss 0.00248, train_acc 0.9996, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 185, loss 0.00256, train_acc 0.9995, valid_acc 0.9480, Time 00:00:56,lr 0.001\n",
      "epoch 186, loss 0.00240, train_acc 0.9995, valid_acc 0.9469, Time 00:00:55,lr 0.001\n",
      "epoch 187, loss 0.00271, train_acc 0.9996, valid_acc 0.9491, Time 00:00:55,lr 0.001\n",
      "epoch 188, loss 0.00239, train_acc 0.9996, valid_acc 0.9476, Time 00:00:56,lr 0.001\n",
      "epoch 189, loss 0.00219, train_acc 0.9997, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 190, loss 0.00211, train_acc 0.9997, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 191, loss 0.00261, train_acc 0.9995, valid_acc 0.9468, Time 00:00:56,lr 0.001\n",
      "epoch 192, loss 0.00210, train_acc 0.9997, valid_acc 0.9472, Time 00:00:55,lr 0.001\n",
      "epoch 193, loss 0.00229, train_acc 0.9996, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 194, loss 0.00222, train_acc 0.9996, valid_acc 0.9486, Time 00:00:56,lr 0.001\n",
      "epoch 195, loss 0.00208, train_acc 0.9996, valid_acc 0.9490, Time 00:00:55,lr 0.001\n",
      "epoch 196, loss 0.00211, train_acc 0.9997, valid_acc 0.9479, Time 00:00:56,lr 0.001\n",
      "epoch 197, loss 0.00225, train_acc 0.9996, valid_acc 0.9476, Time 00:00:56,lr 0.001\n",
      "epoch 198, loss 0.00184, train_acc 0.9997, valid_acc 0.9473, Time 00:00:56,lr 0.001\n",
      "epoch 199, loss 0.00202, train_acc 0.9997, valid_acc 0.9479, Time 00:00:56,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 general lr policy train resnet18_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T18:12:46.262239Z",
     "start_time": "2018-03-04T18:12:46.254255Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v2_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v2(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features[0])\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[2:4])\n",
    "        net.add(*resnet.features[5:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T21:20:09.948413Z",
     "start_time": "2018-03-04T18:12:46.263723Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.44951, train_acc 0.1700, valid_acc 0.2141, Time 00:00:52,lr 0.1\n",
      "epoch 1, loss 1.86926, train_acc 0.2854, valid_acc 0.3249, Time 00:00:56,lr 0.1\n",
      "epoch 2, loss 1.68936, train_acc 0.3693, valid_acc 0.4108, Time 00:00:56,lr 0.1\n",
      "epoch 3, loss 1.49349, train_acc 0.4522, valid_acc 0.4833, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.28756, train_acc 0.5355, valid_acc 0.4992, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 1.08968, train_acc 0.6136, valid_acc 0.6383, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.91630, train_acc 0.6771, valid_acc 0.6946, Time 00:00:56,lr 0.1\n",
      "epoch 7, loss 0.79485, train_acc 0.7227, valid_acc 0.7397, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.71892, train_acc 0.7512, valid_acc 0.7302, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.66048, train_acc 0.7693, valid_acc 0.7255, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.62220, train_acc 0.7848, valid_acc 0.7608, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.59447, train_acc 0.7961, valid_acc 0.7898, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.56015, train_acc 0.8065, valid_acc 0.8082, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.53974, train_acc 0.8164, valid_acc 0.8009, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.51667, train_acc 0.8223, valid_acc 0.8149, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.50768, train_acc 0.8256, valid_acc 0.7942, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.49144, train_acc 0.8322, valid_acc 0.8099, Time 00:00:56,lr 0.1\n",
      "epoch 17, loss 0.47786, train_acc 0.8355, valid_acc 0.8245, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.49227, train_acc 0.8324, valid_acc 0.8198, Time 00:00:56,lr 0.1\n",
      "epoch 19, loss 0.45751, train_acc 0.8445, valid_acc 0.8412, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.45357, train_acc 0.8442, valid_acc 0.8123, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.44426, train_acc 0.8465, valid_acc 0.8173, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.42997, train_acc 0.8532, valid_acc 0.8631, Time 00:00:56,lr 0.1\n",
      "epoch 23, loss 0.42412, train_acc 0.8549, valid_acc 0.8350, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.42291, train_acc 0.8548, valid_acc 0.8455, Time 00:00:56,lr 0.1\n",
      "epoch 25, loss 0.41066, train_acc 0.8580, valid_acc 0.8565, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.39995, train_acc 0.8625, valid_acc 0.8302, Time 00:00:56,lr 0.1\n",
      "epoch 27, loss 0.39947, train_acc 0.8624, valid_acc 0.8342, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.39504, train_acc 0.8641, valid_acc 0.8619, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.38650, train_acc 0.8688, valid_acc 0.8529, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.38075, train_acc 0.8688, valid_acc 0.8649, Time 00:00:56,lr 0.1\n",
      "epoch 31, loss 0.38200, train_acc 0.8699, valid_acc 0.8489, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.37619, train_acc 0.8716, valid_acc 0.8620, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.36805, train_acc 0.8738, valid_acc 0.8418, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.36680, train_acc 0.8751, valid_acc 0.8586, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.36398, train_acc 0.8745, valid_acc 0.8426, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.36144, train_acc 0.8759, valid_acc 0.8554, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.36382, train_acc 0.8748, valid_acc 0.8499, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.35524, train_acc 0.8783, valid_acc 0.8435, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.35648, train_acc 0.8791, valid_acc 0.8483, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.35342, train_acc 0.8771, valid_acc 0.8595, Time 00:00:56,lr 0.1\n",
      "epoch 41, loss 0.34319, train_acc 0.8819, valid_acc 0.8546, Time 00:00:56,lr 0.1\n",
      "epoch 42, loss 0.34849, train_acc 0.8813, valid_acc 0.8763, Time 00:00:56,lr 0.1\n",
      "epoch 43, loss 0.34387, train_acc 0.8830, valid_acc 0.8559, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.34304, train_acc 0.8825, valid_acc 0.8425, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.34640, train_acc 0.8812, valid_acc 0.8577, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.34386, train_acc 0.8830, valid_acc 0.8669, Time 00:00:56,lr 0.1\n",
      "epoch 47, loss 0.34541, train_acc 0.8797, valid_acc 0.8602, Time 00:00:56,lr 0.1\n",
      "epoch 48, loss 0.33837, train_acc 0.8826, valid_acc 0.8406, Time 00:00:56,lr 0.1\n",
      "epoch 49, loss 0.33817, train_acc 0.8843, valid_acc 0.8596, Time 00:00:56,lr 0.1\n",
      "epoch 50, loss 0.33068, train_acc 0.8861, valid_acc 0.8688, Time 00:00:56,lr 0.1\n",
      "epoch 51, loss 0.33042, train_acc 0.8865, valid_acc 0.8732, Time 00:00:56,lr 0.1\n",
      "epoch 52, loss 0.32935, train_acc 0.8885, valid_acc 0.8560, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.33289, train_acc 0.8855, valid_acc 0.8453, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.32762, train_acc 0.8890, valid_acc 0.8629, Time 00:00:56,lr 0.1\n",
      "epoch 55, loss 0.32562, train_acc 0.8886, valid_acc 0.8745, Time 00:00:56,lr 0.1\n",
      "epoch 56, loss 0.32622, train_acc 0.8895, valid_acc 0.8809, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.32513, train_acc 0.8890, valid_acc 0.8741, Time 00:00:56,lr 0.1\n",
      "epoch 58, loss 0.32801, train_acc 0.8882, valid_acc 0.8769, Time 00:00:56,lr 0.1\n",
      "epoch 59, loss 0.32038, train_acc 0.8905, valid_acc 0.8665, Time 00:00:56,lr 0.1\n",
      "epoch 60, loss 0.32088, train_acc 0.8897, valid_acc 0.8747, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.32201, train_acc 0.8899, valid_acc 0.8699, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.32008, train_acc 0.8901, valid_acc 0.8592, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.31851, train_acc 0.8914, valid_acc 0.8809, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.31505, train_acc 0.8915, valid_acc 0.8750, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.31955, train_acc 0.8915, valid_acc 0.8677, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.31765, train_acc 0.8909, valid_acc 0.8683, Time 00:00:56,lr 0.1\n",
      "epoch 67, loss 0.32005, train_acc 0.8906, valid_acc 0.8666, Time 00:00:56,lr 0.1\n",
      "epoch 68, loss 0.31254, train_acc 0.8927, valid_acc 0.8861, Time 00:00:56,lr 0.1\n",
      "epoch 69, loss 0.30988, train_acc 0.8934, valid_acc 0.8846, Time 00:00:56,lr 0.1\n",
      "epoch 70, loss 0.31350, train_acc 0.8937, valid_acc 0.8795, Time 00:00:56,lr 0.1\n",
      "epoch 71, loss 0.31071, train_acc 0.8928, valid_acc 0.8512, Time 00:00:56,lr 0.1\n",
      "epoch 72, loss 0.31702, train_acc 0.8909, valid_acc 0.8794, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.30821, train_acc 0.8951, valid_acc 0.8532, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.30979, train_acc 0.8936, valid_acc 0.7775, Time 00:00:56,lr 0.1\n",
      "epoch 75, loss 0.30905, train_acc 0.8950, valid_acc 0.8663, Time 00:00:56,lr 0.1\n",
      "epoch 76, loss 0.30563, train_acc 0.8947, valid_acc 0.8680, Time 00:00:56,lr 0.1\n",
      "epoch 77, loss 0.30650, train_acc 0.8947, valid_acc 0.8465, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.30130, train_acc 0.8974, valid_acc 0.8824, Time 00:00:56,lr 0.1\n",
      "epoch 79, loss 0.30596, train_acc 0.8963, valid_acc 0.8652, Time 00:00:56,lr 0.1\n",
      "epoch 80, loss 0.17533, train_acc 0.9401, valid_acc 0.9288, Time 00:00:56,lr 0.01\n",
      "epoch 81, loss 0.12621, train_acc 0.9566, valid_acc 0.9311, Time 00:00:56,lr 0.01\n",
      "epoch 82, loss 0.10747, train_acc 0.9627, valid_acc 0.9352, Time 00:00:56,lr 0.01\n",
      "epoch 83, loss 0.09768, train_acc 0.9671, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 84, loss 0.08784, train_acc 0.9699, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 85, loss 0.07791, train_acc 0.9734, valid_acc 0.9379, Time 00:00:56,lr 0.01\n",
      "epoch 86, loss 0.07193, train_acc 0.9752, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 87, loss 0.06827, train_acc 0.9765, valid_acc 0.9379, Time 00:00:56,lr 0.01\n",
      "epoch 88, loss 0.06251, train_acc 0.9786, valid_acc 0.9407, Time 00:00:56,lr 0.01\n",
      "epoch 89, loss 0.05615, train_acc 0.9811, valid_acc 0.9362, Time 00:00:56,lr 0.01\n",
      "epoch 90, loss 0.05426, train_acc 0.9811, valid_acc 0.9380, Time 00:00:56,lr 0.01\n",
      "epoch 91, loss 0.05197, train_acc 0.9825, valid_acc 0.9376, Time 00:00:56,lr 0.01\n",
      "epoch 92, loss 0.04575, train_acc 0.9843, valid_acc 0.9390, Time 00:00:56,lr 0.01\n",
      "epoch 93, loss 0.04560, train_acc 0.9843, valid_acc 0.9365, Time 00:00:56,lr 0.01\n",
      "epoch 94, loss 0.04354, train_acc 0.9850, valid_acc 0.9388, Time 00:00:56,lr 0.01\n",
      "epoch 95, loss 0.04189, train_acc 0.9855, valid_acc 0.9404, Time 00:00:56,lr 0.01\n",
      "epoch 96, loss 0.04202, train_acc 0.9863, valid_acc 0.9385, Time 00:00:56,lr 0.01\n",
      "epoch 97, loss 0.03752, train_acc 0.9877, valid_acc 0.9356, Time 00:00:56,lr 0.01\n",
      "epoch 98, loss 0.03980, train_acc 0.9863, valid_acc 0.9395, Time 00:00:56,lr 0.01\n",
      "epoch 99, loss 0.03526, train_acc 0.9883, valid_acc 0.9376, Time 00:00:56,lr 0.01\n",
      "epoch 100, loss 0.03696, train_acc 0.9875, valid_acc 0.9352, Time 00:00:56,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.03855, train_acc 0.9868, valid_acc 0.9370, Time 00:00:56,lr 0.01\n",
      "epoch 102, loss 0.03788, train_acc 0.9867, valid_acc 0.9364, Time 00:00:56,lr 0.01\n",
      "epoch 103, loss 0.03558, train_acc 0.9879, valid_acc 0.9353, Time 00:00:56,lr 0.01\n",
      "epoch 104, loss 0.03517, train_acc 0.9886, valid_acc 0.9349, Time 00:00:56,lr 0.01\n",
      "epoch 105, loss 0.03818, train_acc 0.9870, valid_acc 0.9386, Time 00:00:56,lr 0.01\n",
      "epoch 106, loss 0.03449, train_acc 0.9884, valid_acc 0.9323, Time 00:00:56,lr 0.01\n",
      "epoch 107, loss 0.03584, train_acc 0.9881, valid_acc 0.9361, Time 00:00:56,lr 0.01\n",
      "epoch 108, loss 0.03502, train_acc 0.9877, valid_acc 0.9363, Time 00:00:56,lr 0.01\n",
      "epoch 109, loss 0.03759, train_acc 0.9875, valid_acc 0.9258, Time 00:00:56,lr 0.01\n",
      "epoch 110, loss 0.03876, train_acc 0.9870, valid_acc 0.9316, Time 00:00:56,lr 0.01\n",
      "epoch 111, loss 0.03911, train_acc 0.9871, valid_acc 0.9317, Time 00:00:56,lr 0.01\n",
      "epoch 112, loss 0.03818, train_acc 0.9868, valid_acc 0.9322, Time 00:00:56,lr 0.01\n",
      "epoch 113, loss 0.03854, train_acc 0.9872, valid_acc 0.9324, Time 00:00:56,lr 0.01\n",
      "epoch 114, loss 0.03957, train_acc 0.9870, valid_acc 0.9312, Time 00:00:56,lr 0.01\n",
      "epoch 115, loss 0.03727, train_acc 0.9877, valid_acc 0.9328, Time 00:00:56,lr 0.01\n",
      "epoch 116, loss 0.03950, train_acc 0.9867, valid_acc 0.9341, Time 00:00:56,lr 0.01\n",
      "epoch 117, loss 0.03904, train_acc 0.9873, valid_acc 0.9369, Time 00:00:56,lr 0.01\n",
      "epoch 118, loss 0.04037, train_acc 0.9864, valid_acc 0.9323, Time 00:00:56,lr 0.01\n",
      "epoch 119, loss 0.04153, train_acc 0.9859, valid_acc 0.9333, Time 00:00:56,lr 0.01\n",
      "epoch 120, loss 0.04340, train_acc 0.9856, valid_acc 0.9303, Time 00:00:56,lr 0.01\n",
      "epoch 121, loss 0.03868, train_acc 0.9870, valid_acc 0.9308, Time 00:00:56,lr 0.01\n",
      "epoch 122, loss 0.04523, train_acc 0.9845, valid_acc 0.9321, Time 00:00:56,lr 0.01\n",
      "epoch 123, loss 0.04460, train_acc 0.9852, valid_acc 0.9260, Time 00:00:56,lr 0.01\n",
      "epoch 124, loss 0.04559, train_acc 0.9850, valid_acc 0.9319, Time 00:00:56,lr 0.01\n",
      "epoch 125, loss 0.04768, train_acc 0.9842, valid_acc 0.9292, Time 00:00:56,lr 0.01\n",
      "epoch 126, loss 0.04399, train_acc 0.9851, valid_acc 0.9295, Time 00:00:56,lr 0.01\n",
      "epoch 127, loss 0.04340, train_acc 0.9852, valid_acc 0.9296, Time 00:00:56,lr 0.01\n",
      "epoch 128, loss 0.04687, train_acc 0.9844, valid_acc 0.9247, Time 00:00:56,lr 0.01\n",
      "epoch 129, loss 0.04552, train_acc 0.9849, valid_acc 0.9326, Time 00:00:56,lr 0.01\n",
      "epoch 130, loss 0.04505, train_acc 0.9845, valid_acc 0.9259, Time 00:00:56,lr 0.01\n",
      "epoch 131, loss 0.04696, train_acc 0.9838, valid_acc 0.9226, Time 00:00:56,lr 0.01\n",
      "epoch 132, loss 0.04853, train_acc 0.9839, valid_acc 0.9269, Time 00:00:56,lr 0.01\n",
      "epoch 133, loss 0.05106, train_acc 0.9828, valid_acc 0.9255, Time 00:00:56,lr 0.01\n",
      "epoch 134, loss 0.04962, train_acc 0.9829, valid_acc 0.9282, Time 00:00:56,lr 0.01\n",
      "epoch 135, loss 0.04603, train_acc 0.9844, valid_acc 0.9287, Time 00:00:56,lr 0.01\n",
      "epoch 136, loss 0.05080, train_acc 0.9827, valid_acc 0.9321, Time 00:00:56,lr 0.01\n",
      "epoch 137, loss 0.04662, train_acc 0.9839, valid_acc 0.9287, Time 00:00:56,lr 0.01\n",
      "epoch 138, loss 0.04839, train_acc 0.9832, valid_acc 0.9236, Time 00:00:56,lr 0.01\n",
      "epoch 139, loss 0.05286, train_acc 0.9814, valid_acc 0.9283, Time 00:00:56,lr 0.01\n",
      "epoch 140, loss 0.04797, train_acc 0.9834, valid_acc 0.9238, Time 00:00:56,lr 0.01\n",
      "epoch 141, loss 0.04590, train_acc 0.9841, valid_acc 0.9296, Time 00:00:56,lr 0.01\n",
      "epoch 142, loss 0.05083, train_acc 0.9826, valid_acc 0.9292, Time 00:00:56,lr 0.01\n",
      "epoch 143, loss 0.04744, train_acc 0.9837, valid_acc 0.9221, Time 00:00:56,lr 0.01\n",
      "epoch 144, loss 0.04808, train_acc 0.9835, valid_acc 0.9270, Time 00:00:56,lr 0.01\n",
      "epoch 145, loss 0.05711, train_acc 0.9806, valid_acc 0.9244, Time 00:00:56,lr 0.01\n",
      "epoch 146, loss 0.05352, train_acc 0.9817, valid_acc 0.9268, Time 00:00:56,lr 0.01\n",
      "epoch 147, loss 0.05069, train_acc 0.9826, valid_acc 0.9252, Time 00:00:56,lr 0.01\n",
      "epoch 148, loss 0.05457, train_acc 0.9812, valid_acc 0.9238, Time 00:00:56,lr 0.01\n",
      "epoch 149, loss 0.04820, train_acc 0.9835, valid_acc 0.9207, Time 00:00:56,lr 0.01\n",
      "epoch 150, loss 0.02426, train_acc 0.9927, valid_acc 0.9386, Time 00:00:56,lr 0.001\n",
      "epoch 151, loss 0.01617, train_acc 0.9954, valid_acc 0.9411, Time 00:00:56,lr 0.001\n",
      "epoch 152, loss 0.01236, train_acc 0.9969, valid_acc 0.9421, Time 00:00:56,lr 0.001\n",
      "epoch 153, loss 0.01000, train_acc 0.9978, valid_acc 0.9432, Time 00:00:56,lr 0.001\n",
      "epoch 154, loss 0.00899, train_acc 0.9979, valid_acc 0.9432, Time 00:00:56,lr 0.001\n",
      "epoch 155, loss 0.00864, train_acc 0.9978, valid_acc 0.9441, Time 00:00:56,lr 0.001\n",
      "epoch 156, loss 0.00746, train_acc 0.9982, valid_acc 0.9453, Time 00:00:56,lr 0.001\n",
      "epoch 157, loss 0.00767, train_acc 0.9983, valid_acc 0.9448, Time 00:00:56,lr 0.001\n",
      "epoch 158, loss 0.00699, train_acc 0.9985, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 159, loss 0.00615, train_acc 0.9988, valid_acc 0.9464, Time 00:00:56,lr 0.001\n",
      "epoch 160, loss 0.00609, train_acc 0.9987, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 161, loss 0.00539, train_acc 0.9989, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 162, loss 0.00604, train_acc 0.9984, valid_acc 0.9447, Time 00:00:56,lr 0.001\n",
      "epoch 163, loss 0.00496, train_acc 0.9990, valid_acc 0.9453, Time 00:00:56,lr 0.001\n",
      "epoch 164, loss 0.00543, train_acc 0.9988, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 165, loss 0.00462, train_acc 0.9989, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 166, loss 0.00484, train_acc 0.9989, valid_acc 0.9454, Time 00:00:56,lr 0.001\n",
      "epoch 167, loss 0.00405, train_acc 0.9992, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 168, loss 0.00447, train_acc 0.9990, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 169, loss 0.00409, train_acc 0.9991, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 170, loss 0.00435, train_acc 0.9990, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 171, loss 0.00335, train_acc 0.9993, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 172, loss 0.00361, train_acc 0.9993, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 173, loss 0.00369, train_acc 0.9994, valid_acc 0.9470, Time 00:00:56,lr 0.001\n",
      "epoch 174, loss 0.00371, train_acc 0.9992, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 175, loss 0.00349, train_acc 0.9993, valid_acc 0.9463, Time 00:00:56,lr 0.001\n",
      "epoch 176, loss 0.00300, train_acc 0.9995, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 177, loss 0.00323, train_acc 0.9994, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 178, loss 0.00298, train_acc 0.9994, valid_acc 0.9462, Time 00:00:55,lr 0.001\n",
      "epoch 179, loss 0.00293, train_acc 0.9995, valid_acc 0.9467, Time 00:00:56,lr 0.001\n",
      "epoch 180, loss 0.00275, train_acc 0.9996, valid_acc 0.9463, Time 00:00:56,lr 0.001\n",
      "epoch 181, loss 0.00334, train_acc 0.9992, valid_acc 0.9469, Time 00:00:56,lr 0.001\n",
      "epoch 182, loss 0.00327, train_acc 0.9994, valid_acc 0.9464, Time 00:00:56,lr 0.001\n",
      "epoch 183, loss 0.00269, train_acc 0.9995, valid_acc 0.9459, Time 00:00:55,lr 0.001\n",
      "epoch 184, loss 0.00283, train_acc 0.9995, valid_acc 0.9472, Time 00:00:56,lr 0.001\n",
      "epoch 185, loss 0.00267, train_acc 0.9996, valid_acc 0.9475, Time 00:00:56,lr 0.001\n",
      "epoch 186, loss 0.00273, train_acc 0.9996, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 187, loss 0.00273, train_acc 0.9995, valid_acc 0.9479, Time 00:00:56,lr 0.001\n",
      "epoch 188, loss 0.00232, train_acc 0.9997, valid_acc 0.9450, Time 00:00:56,lr 0.001\n",
      "epoch 189, loss 0.00264, train_acc 0.9995, valid_acc 0.9464, Time 00:00:56,lr 0.001\n",
      "epoch 190, loss 0.00256, train_acc 0.9994, valid_acc 0.9470, Time 00:00:56,lr 0.001\n",
      "epoch 191, loss 0.00225, train_acc 0.9997, valid_acc 0.9462, Time 00:00:56,lr 0.001\n",
      "epoch 192, loss 0.00242, train_acc 0.9997, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 193, loss 0.00261, train_acc 0.9996, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 194, loss 0.00224, train_acc 0.9997, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 195, loss 0.00238, train_acc 0.9995, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 196, loss 0.00232, train_acc 0.9996, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 197, loss 0.00220, train_acc 0.9996, valid_acc 0.9468, Time 00:00:56,lr 0.001\n",
      "epoch 198, loss 0.00265, train_acc 0.9995, valid_acc 0.9454, Time 00:00:56,lr 0.001\n",
      "epoch 199, loss 0.00222, train_acc 0.9996, valid_acc 0.9473, Time 00:00:56,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v2_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 general lr policy train my resnet18v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T03:45:19.543171Z",
     "start_time": "2018-03-05T02:01:38.930989Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.75249, train_acc 0.3416, valid_acc 0.4678, Time 00:00:31,lr 0.1\n",
      "epoch 1, loss 1.23604, train_acc 0.5568, valid_acc 0.5724, Time 00:00:32,lr 0.1\n",
      "epoch 2, loss 0.93259, train_acc 0.6715, valid_acc 0.7011, Time 00:00:32,lr 0.1\n",
      "epoch 3, loss 0.78639, train_acc 0.7277, valid_acc 0.6987, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 0.68671, train_acc 0.7642, valid_acc 0.7434, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 0.62936, train_acc 0.7856, valid_acc 0.7396, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 0.59543, train_acc 0.7951, valid_acc 0.7949, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 0.56563, train_acc 0.8073, valid_acc 0.7511, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.54190, train_acc 0.8125, valid_acc 0.7714, Time 00:00:30,lr 0.1\n",
      "epoch 9, loss 0.51894, train_acc 0.8220, valid_acc 0.8141, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.50352, train_acc 0.8259, valid_acc 0.7803, Time 00:00:30,lr 0.1\n",
      "epoch 11, loss 0.49073, train_acc 0.8327, valid_acc 0.8127, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 0.47917, train_acc 0.8354, valid_acc 0.8078, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.47001, train_acc 0.8364, valid_acc 0.8317, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.45984, train_acc 0.8415, valid_acc 0.8142, Time 00:00:30,lr 0.1\n",
      "epoch 15, loss 0.45590, train_acc 0.8441, valid_acc 0.7748, Time 00:00:30,lr 0.1\n",
      "epoch 16, loss 0.44058, train_acc 0.8479, valid_acc 0.8230, Time 00:00:30,lr 0.1\n",
      "epoch 17, loss 0.43728, train_acc 0.8481, valid_acc 0.8120, Time 00:00:30,lr 0.1\n",
      "epoch 18, loss 0.43228, train_acc 0.8511, valid_acc 0.7777, Time 00:00:30,lr 0.1\n",
      "epoch 19, loss 0.42770, train_acc 0.8548, valid_acc 0.8265, Time 00:00:30,lr 0.1\n",
      "epoch 20, loss 0.42149, train_acc 0.8558, valid_acc 0.8383, Time 00:00:30,lr 0.1\n",
      "epoch 21, loss 0.41593, train_acc 0.8559, valid_acc 0.8471, Time 00:00:30,lr 0.1\n",
      "epoch 22, loss 0.41718, train_acc 0.8565, valid_acc 0.8438, Time 00:00:30,lr 0.1\n",
      "epoch 23, loss 0.41097, train_acc 0.8591, valid_acc 0.8104, Time 00:00:30,lr 0.1\n",
      "epoch 24, loss 0.40569, train_acc 0.8603, valid_acc 0.8265, Time 00:00:30,lr 0.1\n",
      "epoch 25, loss 0.39741, train_acc 0.8625, valid_acc 0.8375, Time 00:00:30,lr 0.1\n",
      "epoch 26, loss 0.39868, train_acc 0.8648, valid_acc 0.8310, Time 00:00:30,lr 0.1\n",
      "epoch 27, loss 0.39217, train_acc 0.8654, valid_acc 0.8222, Time 00:00:30,lr 0.1\n",
      "epoch 28, loss 0.39285, train_acc 0.8661, valid_acc 0.8436, Time 00:00:30,lr 0.1\n",
      "epoch 29, loss 0.38964, train_acc 0.8661, valid_acc 0.8419, Time 00:00:30,lr 0.1\n",
      "epoch 30, loss 0.38233, train_acc 0.8675, valid_acc 0.8302, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.38502, train_acc 0.8687, valid_acc 0.8512, Time 00:00:30,lr 0.1\n",
      "epoch 32, loss 0.37983, train_acc 0.8693, valid_acc 0.8574, Time 00:00:30,lr 0.1\n",
      "epoch 33, loss 0.37806, train_acc 0.8686, valid_acc 0.8553, Time 00:00:30,lr 0.1\n",
      "epoch 34, loss 0.37425, train_acc 0.8709, valid_acc 0.8625, Time 00:00:30,lr 0.1\n",
      "epoch 35, loss 0.37156, train_acc 0.8724, valid_acc 0.8599, Time 00:00:30,lr 0.1\n",
      "epoch 36, loss 0.37042, train_acc 0.8747, valid_acc 0.8524, Time 00:00:30,lr 0.1\n",
      "epoch 37, loss 0.37193, train_acc 0.8734, valid_acc 0.8277, Time 00:00:30,lr 0.1\n",
      "epoch 38, loss 0.37142, train_acc 0.8734, valid_acc 0.8223, Time 00:00:30,lr 0.1\n",
      "epoch 39, loss 0.36700, train_acc 0.8739, valid_acc 0.8126, Time 00:00:30,lr 0.1\n",
      "epoch 40, loss 0.36533, train_acc 0.8738, valid_acc 0.8582, Time 00:00:30,lr 0.1\n",
      "epoch 41, loss 0.36709, train_acc 0.8759, valid_acc 0.8450, Time 00:00:30,lr 0.1\n",
      "epoch 42, loss 0.36452, train_acc 0.8751, valid_acc 0.8551, Time 00:00:30,lr 0.1\n",
      "epoch 43, loss 0.36363, train_acc 0.8752, valid_acc 0.8559, Time 00:00:30,lr 0.1\n",
      "epoch 44, loss 0.36318, train_acc 0.8768, valid_acc 0.8331, Time 00:00:30,lr 0.1\n",
      "epoch 45, loss 0.36180, train_acc 0.8765, valid_acc 0.8542, Time 00:00:30,lr 0.1\n",
      "epoch 46, loss 0.35892, train_acc 0.8770, valid_acc 0.8296, Time 00:00:30,lr 0.1\n",
      "epoch 47, loss 0.36113, train_acc 0.8765, valid_acc 0.8493, Time 00:00:30,lr 0.1\n",
      "epoch 48, loss 0.35691, train_acc 0.8780, valid_acc 0.8250, Time 00:00:30,lr 0.1\n",
      "epoch 49, loss 0.35613, train_acc 0.8781, valid_acc 0.8409, Time 00:00:30,lr 0.1\n",
      "epoch 50, loss 0.35971, train_acc 0.8755, valid_acc 0.8462, Time 00:00:30,lr 0.1\n",
      "epoch 51, loss 0.35123, train_acc 0.8807, valid_acc 0.8555, Time 00:00:30,lr 0.1\n",
      "epoch 52, loss 0.35600, train_acc 0.8809, valid_acc 0.8634, Time 00:00:30,lr 0.1\n",
      "epoch 53, loss 0.34899, train_acc 0.8804, valid_acc 0.8415, Time 00:00:30,lr 0.1\n",
      "epoch 54, loss 0.35340, train_acc 0.8780, valid_acc 0.8486, Time 00:00:30,lr 0.1\n",
      "epoch 55, loss 0.35170, train_acc 0.8798, valid_acc 0.8252, Time 00:00:30,lr 0.1\n",
      "epoch 56, loss 0.35164, train_acc 0.8809, valid_acc 0.8500, Time 00:00:30,lr 0.1\n",
      "epoch 57, loss 0.34906, train_acc 0.8793, valid_acc 0.8432, Time 00:00:30,lr 0.1\n",
      "epoch 58, loss 0.35115, train_acc 0.8790, valid_acc 0.8634, Time 00:00:30,lr 0.1\n",
      "epoch 59, loss 0.35323, train_acc 0.8779, valid_acc 0.8571, Time 00:00:30,lr 0.1\n",
      "epoch 60, loss 0.34844, train_acc 0.8792, valid_acc 0.8580, Time 00:00:30,lr 0.1\n",
      "epoch 61, loss 0.34465, train_acc 0.8826, valid_acc 0.8559, Time 00:00:30,lr 0.1\n",
      "epoch 62, loss 0.34941, train_acc 0.8816, valid_acc 0.8703, Time 00:00:30,lr 0.1\n",
      "epoch 63, loss 0.35071, train_acc 0.8794, valid_acc 0.8159, Time 00:00:30,lr 0.1\n",
      "epoch 64, loss 0.34600, train_acc 0.8824, valid_acc 0.8438, Time 00:00:30,lr 0.1\n",
      "epoch 65, loss 0.34546, train_acc 0.8836, valid_acc 0.8596, Time 00:00:30,lr 0.1\n",
      "epoch 66, loss 0.34340, train_acc 0.8827, valid_acc 0.8552, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.34738, train_acc 0.8812, valid_acc 0.8569, Time 00:00:30,lr 0.1\n",
      "epoch 68, loss 0.34896, train_acc 0.8806, valid_acc 0.8501, Time 00:00:30,lr 0.1\n",
      "epoch 69, loss 0.34608, train_acc 0.8815, valid_acc 0.8504, Time 00:00:30,lr 0.1\n",
      "epoch 70, loss 0.34501, train_acc 0.8821, valid_acc 0.8615, Time 00:00:30,lr 0.1\n",
      "epoch 71, loss 0.34624, train_acc 0.8812, valid_acc 0.8541, Time 00:00:30,lr 0.1\n",
      "epoch 72, loss 0.34812, train_acc 0.8805, valid_acc 0.8727, Time 00:00:30,lr 0.1\n",
      "epoch 73, loss 0.34509, train_acc 0.8817, valid_acc 0.8514, Time 00:00:30,lr 0.1\n",
      "epoch 74, loss 0.34422, train_acc 0.8802, valid_acc 0.8745, Time 00:00:30,lr 0.1\n",
      "epoch 75, loss 0.34185, train_acc 0.8834, valid_acc 0.8565, Time 00:00:30,lr 0.1\n",
      "epoch 76, loss 0.34527, train_acc 0.8807, valid_acc 0.8577, Time 00:00:30,lr 0.1\n",
      "epoch 77, loss 0.33983, train_acc 0.8838, valid_acc 0.8720, Time 00:00:30,lr 0.1\n",
      "epoch 78, loss 0.33928, train_acc 0.8850, valid_acc 0.8603, Time 00:00:30,lr 0.1\n",
      "epoch 79, loss 0.34337, train_acc 0.8827, valid_acc 0.8280, Time 00:00:30,lr 0.1\n",
      "epoch 80, loss 0.19619, train_acc 0.9336, valid_acc 0.9207, Time 00:00:30,lr 0.01\n",
      "epoch 81, loss 0.14760, train_acc 0.9501, valid_acc 0.9249, Time 00:00:30,lr 0.01\n",
      "epoch 82, loss 0.12989, train_acc 0.9549, valid_acc 0.9265, Time 00:00:30,lr 0.01\n",
      "epoch 83, loss 0.11951, train_acc 0.9593, valid_acc 0.9292, Time 00:00:30,lr 0.01\n",
      "epoch 84, loss 0.10738, train_acc 0.9632, valid_acc 0.9288, Time 00:00:30,lr 0.01\n",
      "epoch 85, loss 0.10071, train_acc 0.9657, valid_acc 0.9295, Time 00:00:30,lr 0.01\n",
      "epoch 86, loss 0.09417, train_acc 0.9681, valid_acc 0.9312, Time 00:00:30,lr 0.01\n",
      "epoch 87, loss 0.08605, train_acc 0.9708, valid_acc 0.9300, Time 00:00:30,lr 0.01\n",
      "epoch 88, loss 0.08259, train_acc 0.9722, valid_acc 0.9313, Time 00:00:30,lr 0.01\n",
      "epoch 89, loss 0.07684, train_acc 0.9739, valid_acc 0.9288, Time 00:00:31,lr 0.01\n",
      "epoch 90, loss 0.07300, train_acc 0.9756, valid_acc 0.9294, Time 00:00:30,lr 0.01\n",
      "epoch 91, loss 0.07099, train_acc 0.9757, valid_acc 0.9294, Time 00:00:30,lr 0.01\n",
      "epoch 92, loss 0.06592, train_acc 0.9776, valid_acc 0.9317, Time 00:00:30,lr 0.01\n",
      "epoch 93, loss 0.06440, train_acc 0.9783, valid_acc 0.9258, Time 00:00:30,lr 0.01\n",
      "epoch 94, loss 0.06085, train_acc 0.9799, valid_acc 0.9287, Time 00:00:30,lr 0.01\n",
      "epoch 95, loss 0.06026, train_acc 0.9803, valid_acc 0.9313, Time 00:00:30,lr 0.01\n",
      "epoch 96, loss 0.05759, train_acc 0.9805, valid_acc 0.9257, Time 00:00:30,lr 0.01\n",
      "epoch 97, loss 0.05517, train_acc 0.9812, valid_acc 0.9270, Time 00:00:30,lr 0.01\n",
      "epoch 98, loss 0.05672, train_acc 0.9807, valid_acc 0.9301, Time 00:00:30,lr 0.01\n",
      "epoch 99, loss 0.05231, train_acc 0.9819, valid_acc 0.9292, Time 00:00:30,lr 0.01\n",
      "epoch 100, loss 0.05057, train_acc 0.9828, valid_acc 0.9245, Time 00:00:30,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.05123, train_acc 0.9828, valid_acc 0.9259, Time 00:00:30,lr 0.01\n",
      "epoch 102, loss 0.04977, train_acc 0.9830, valid_acc 0.9254, Time 00:00:30,lr 0.01\n",
      "epoch 103, loss 0.05176, train_acc 0.9827, valid_acc 0.9254, Time 00:00:30,lr 0.01\n",
      "epoch 104, loss 0.05073, train_acc 0.9826, valid_acc 0.9237, Time 00:00:30,lr 0.01\n",
      "epoch 105, loss 0.05045, train_acc 0.9829, valid_acc 0.9311, Time 00:00:30,lr 0.01\n",
      "epoch 106, loss 0.04769, train_acc 0.9841, valid_acc 0.9267, Time 00:00:35,lr 0.01\n",
      "epoch 107, loss 0.05069, train_acc 0.9823, valid_acc 0.9261, Time 00:00:34,lr 0.01\n",
      "epoch 108, loss 0.04908, train_acc 0.9833, valid_acc 0.9308, Time 00:00:31,lr 0.01\n",
      "epoch 109, loss 0.05262, train_acc 0.9825, valid_acc 0.9267, Time 00:00:30,lr 0.01\n",
      "epoch 110, loss 0.05038, train_acc 0.9829, valid_acc 0.9237, Time 00:00:30,lr 0.01\n",
      "epoch 111, loss 0.04573, train_acc 0.9848, valid_acc 0.9262, Time 00:00:30,lr 0.01\n",
      "epoch 112, loss 0.05370, train_acc 0.9811, valid_acc 0.9257, Time 00:00:30,lr 0.01\n",
      "epoch 113, loss 0.05240, train_acc 0.9824, valid_acc 0.9216, Time 00:00:30,lr 0.01\n",
      "epoch 114, loss 0.05037, train_acc 0.9833, valid_acc 0.9265, Time 00:00:30,lr 0.01\n",
      "epoch 115, loss 0.04898, train_acc 0.9839, valid_acc 0.9200, Time 00:00:30,lr 0.01\n",
      "epoch 116, loss 0.05494, train_acc 0.9815, valid_acc 0.9220, Time 00:00:30,lr 0.01\n",
      "epoch 117, loss 0.05175, train_acc 0.9825, valid_acc 0.9261, Time 00:00:30,lr 0.01\n",
      "epoch 118, loss 0.05293, train_acc 0.9816, valid_acc 0.9246, Time 00:00:31,lr 0.01\n",
      "epoch 119, loss 0.05280, train_acc 0.9816, valid_acc 0.9216, Time 00:00:30,lr 0.01\n",
      "epoch 120, loss 0.05557, train_acc 0.9807, valid_acc 0.9241, Time 00:00:30,lr 0.01\n",
      "epoch 121, loss 0.05531, train_acc 0.9812, valid_acc 0.9272, Time 00:00:30,lr 0.01\n",
      "epoch 122, loss 0.05505, train_acc 0.9816, valid_acc 0.9233, Time 00:00:30,lr 0.01\n",
      "epoch 123, loss 0.05293, train_acc 0.9821, valid_acc 0.9178, Time 00:00:30,lr 0.01\n",
      "epoch 124, loss 0.05691, train_acc 0.9807, valid_acc 0.9250, Time 00:00:30,lr 0.01\n",
      "epoch 125, loss 0.05509, train_acc 0.9815, valid_acc 0.9178, Time 00:00:30,lr 0.01\n",
      "epoch 126, loss 0.05544, train_acc 0.9806, valid_acc 0.9197, Time 00:00:30,lr 0.01\n",
      "epoch 127, loss 0.05728, train_acc 0.9800, valid_acc 0.9217, Time 00:00:30,lr 0.01\n",
      "epoch 128, loss 0.05760, train_acc 0.9804, valid_acc 0.9254, Time 00:00:30,lr 0.01\n",
      "epoch 129, loss 0.05628, train_acc 0.9812, valid_acc 0.9191, Time 00:00:30,lr 0.01\n",
      "epoch 130, loss 0.05874, train_acc 0.9795, valid_acc 0.9190, Time 00:00:30,lr 0.01\n",
      "epoch 131, loss 0.05753, train_acc 0.9803, valid_acc 0.9250, Time 00:00:30,lr 0.01\n",
      "epoch 132, loss 0.05882, train_acc 0.9794, valid_acc 0.9200, Time 00:00:30,lr 0.01\n",
      "epoch 133, loss 0.06110, train_acc 0.9796, valid_acc 0.9173, Time 00:00:30,lr 0.01\n",
      "epoch 134, loss 0.06017, train_acc 0.9798, valid_acc 0.9207, Time 00:00:30,lr 0.01\n",
      "epoch 135, loss 0.05784, train_acc 0.9803, valid_acc 0.9088, Time 00:00:30,lr 0.01\n",
      "epoch 136, loss 0.06184, train_acc 0.9790, valid_acc 0.9193, Time 00:00:30,lr 0.01\n",
      "epoch 137, loss 0.05722, train_acc 0.9803, valid_acc 0.9279, Time 00:00:30,lr 0.01\n",
      "epoch 138, loss 0.05880, train_acc 0.9800, valid_acc 0.9203, Time 00:00:30,lr 0.01\n",
      "epoch 139, loss 0.05727, train_acc 0.9806, valid_acc 0.9189, Time 00:00:30,lr 0.01\n",
      "epoch 140, loss 0.06348, train_acc 0.9784, valid_acc 0.9242, Time 00:00:30,lr 0.01\n",
      "epoch 141, loss 0.05603, train_acc 0.9808, valid_acc 0.9251, Time 00:00:30,lr 0.01\n",
      "epoch 142, loss 0.05919, train_acc 0.9797, valid_acc 0.9173, Time 00:00:30,lr 0.01\n",
      "epoch 143, loss 0.06109, train_acc 0.9793, valid_acc 0.9140, Time 00:00:30,lr 0.01\n",
      "epoch 144, loss 0.05916, train_acc 0.9798, valid_acc 0.9181, Time 00:00:30,lr 0.01\n",
      "epoch 145, loss 0.06621, train_acc 0.9776, valid_acc 0.9185, Time 00:00:30,lr 0.01\n",
      "epoch 146, loss 0.06132, train_acc 0.9795, valid_acc 0.9203, Time 00:00:30,lr 0.01\n",
      "epoch 147, loss 0.06416, train_acc 0.9784, valid_acc 0.9221, Time 00:00:30,lr 0.01\n",
      "epoch 148, loss 0.06045, train_acc 0.9793, valid_acc 0.9162, Time 00:00:30,lr 0.01\n",
      "epoch 149, loss 0.06016, train_acc 0.9800, valid_acc 0.9142, Time 00:00:30,lr 0.01\n",
      "epoch 150, loss 0.03468, train_acc 0.9895, valid_acc 0.9345, Time 00:00:30,lr 0.001\n",
      "epoch 151, loss 0.02295, train_acc 0.9933, valid_acc 0.9377, Time 00:00:30,lr 0.001\n",
      "epoch 152, loss 0.01791, train_acc 0.9955, valid_acc 0.9364, Time 00:00:30,lr 0.001\n",
      "epoch 153, loss 0.01725, train_acc 0.9955, valid_acc 0.9369, Time 00:00:30,lr 0.001\n",
      "epoch 154, loss 0.01460, train_acc 0.9961, valid_acc 0.9373, Time 00:00:30,lr 0.001\n",
      "epoch 155, loss 0.01362, train_acc 0.9967, valid_acc 0.9381, Time 00:00:30,lr 0.001\n",
      "epoch 156, loss 0.01294, train_acc 0.9970, valid_acc 0.9391, Time 00:00:30,lr 0.001\n",
      "epoch 157, loss 0.01247, train_acc 0.9970, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 158, loss 0.01062, train_acc 0.9974, valid_acc 0.9394, Time 00:00:30,lr 0.001\n",
      "epoch 159, loss 0.01077, train_acc 0.9977, valid_acc 0.9365, Time 00:00:30,lr 0.001\n",
      "epoch 160, loss 0.01017, train_acc 0.9978, valid_acc 0.9392, Time 00:00:30,lr 0.001\n",
      "epoch 161, loss 0.00928, train_acc 0.9981, valid_acc 0.9389, Time 00:00:30,lr 0.001\n",
      "epoch 162, loss 0.00964, train_acc 0.9978, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 163, loss 0.00897, train_acc 0.9982, valid_acc 0.9390, Time 00:00:30,lr 0.001\n",
      "epoch 164, loss 0.00817, train_acc 0.9985, valid_acc 0.9395, Time 00:00:30,lr 0.001\n",
      "epoch 165, loss 0.00879, train_acc 0.9981, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 166, loss 0.00774, train_acc 0.9985, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 167, loss 0.00806, train_acc 0.9982, valid_acc 0.9403, Time 00:00:30,lr 0.001\n",
      "epoch 168, loss 0.00736, train_acc 0.9985, valid_acc 0.9400, Time 00:00:30,lr 0.001\n",
      "epoch 169, loss 0.00760, train_acc 0.9986, valid_acc 0.9401, Time 00:00:30,lr 0.001\n",
      "epoch 170, loss 0.00736, train_acc 0.9986, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 171, loss 0.00706, train_acc 0.9985, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 172, loss 0.00672, train_acc 0.9988, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 173, loss 0.00711, train_acc 0.9985, valid_acc 0.9409, Time 00:00:30,lr 0.001\n",
      "epoch 174, loss 0.00627, train_acc 0.9988, valid_acc 0.9410, Time 00:00:30,lr 0.001\n",
      "epoch 175, loss 0.00647, train_acc 0.9987, valid_acc 0.9391, Time 00:00:33,lr 0.001\n",
      "epoch 176, loss 0.00614, train_acc 0.9990, valid_acc 0.9401, Time 00:00:32,lr 0.001\n",
      "epoch 177, loss 0.00592, train_acc 0.9990, valid_acc 0.9412, Time 00:00:30,lr 0.001\n",
      "epoch 178, loss 0.00610, train_acc 0.9986, valid_acc 0.9389, Time 00:00:30,lr 0.001\n",
      "epoch 179, loss 0.00530, train_acc 0.9992, valid_acc 0.9393, Time 00:00:30,lr 0.001\n",
      "epoch 180, loss 0.00577, train_acc 0.9989, valid_acc 0.9415, Time 00:00:33,lr 0.001\n",
      "epoch 181, loss 0.00612, train_acc 0.9986, valid_acc 0.9392, Time 00:00:35,lr 0.001\n",
      "epoch 182, loss 0.00583, train_acc 0.9987, valid_acc 0.9409, Time 00:00:31,lr 0.001\n",
      "epoch 183, loss 0.00538, train_acc 0.9990, valid_acc 0.9394, Time 00:00:30,lr 0.001\n",
      "epoch 184, loss 0.00512, train_acc 0.9992, valid_acc 0.9414, Time 00:00:33,lr 0.001\n",
      "epoch 185, loss 0.00559, train_acc 0.9990, valid_acc 0.9401, Time 00:00:31,lr 0.001\n",
      "epoch 186, loss 0.00473, train_acc 0.9994, valid_acc 0.9397, Time 00:00:31,lr 0.001\n",
      "epoch 187, loss 0.00465, train_acc 0.9992, valid_acc 0.9387, Time 00:00:35,lr 0.001\n",
      "epoch 188, loss 0.00554, train_acc 0.9988, valid_acc 0.9415, Time 00:00:31,lr 0.001\n",
      "epoch 189, loss 0.00482, train_acc 0.9994, valid_acc 0.9401, Time 00:00:33,lr 0.001\n",
      "epoch 190, loss 0.00493, train_acc 0.9992, valid_acc 0.9410, Time 00:00:36,lr 0.001\n",
      "epoch 191, loss 0.00506, train_acc 0.9990, valid_acc 0.9414, Time 00:00:30,lr 0.001\n",
      "epoch 192, loss 0.00492, train_acc 0.9990, valid_acc 0.9404, Time 00:00:30,lr 0.001\n",
      "epoch 193, loss 0.00497, train_acc 0.9991, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 194, loss 0.00468, train_acc 0.9992, valid_acc 0.9416, Time 00:00:30,lr 0.001\n",
      "epoch 195, loss 0.00445, train_acc 0.9993, valid_acc 0.9425, Time 00:00:30,lr 0.001\n",
      "epoch 196, loss 0.00440, train_acc 0.9994, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 197, loss 0.00445, train_acc 0.9991, valid_acc 0.9427, Time 00:00:30,lr 0.001\n",
      "epoch 198, loss 0.00456, train_acc 0.9992, valid_acc 0.9424, Time 00:00:30,lr 0.001\n",
      "epoch 199, loss 0.00424, train_acc 0.9993, valid_acc 0.9420, Time 00:00:31,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = ResNet(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 try delay pooling resnet arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T03:45:19.563155Z",
     "start_time": "2018-03-05T03:45:19.545584Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_delay_downsample(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_delay_downsample, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(5):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T05:37:46.139687Z",
     "start_time": "2018-03-05T03:45:19.566107Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.74871, train_acc 0.3432, valid_acc 0.4233, Time 00:00:32,lr 0.1\n",
      "epoch 1, loss 1.22673, train_acc 0.5586, valid_acc 0.6011, Time 00:00:33,lr 0.1\n",
      "epoch 2, loss 0.91422, train_acc 0.6805, valid_acc 0.7050, Time 00:00:33,lr 0.1\n",
      "epoch 3, loss 0.77360, train_acc 0.7336, valid_acc 0.6949, Time 00:00:33,lr 0.1\n",
      "epoch 4, loss 0.69274, train_acc 0.7617, valid_acc 0.7435, Time 00:00:33,lr 0.1\n",
      "epoch 5, loss 0.64398, train_acc 0.7797, valid_acc 0.7752, Time 00:00:33,lr 0.1\n",
      "epoch 6, loss 0.60268, train_acc 0.7923, valid_acc 0.7813, Time 00:00:33,lr 0.1\n",
      "epoch 7, loss 0.57284, train_acc 0.8026, valid_acc 0.7886, Time 00:00:33,lr 0.1\n",
      "epoch 8, loss 0.54954, train_acc 0.8118, valid_acc 0.7681, Time 00:00:33,lr 0.1\n",
      "epoch 9, loss 0.52985, train_acc 0.8182, valid_acc 0.8093, Time 00:00:33,lr 0.1\n",
      "epoch 10, loss 0.51509, train_acc 0.8221, valid_acc 0.8017, Time 00:00:33,lr 0.1\n",
      "epoch 11, loss 0.50326, train_acc 0.8282, valid_acc 0.7937, Time 00:00:33,lr 0.1\n",
      "epoch 12, loss 0.48711, train_acc 0.8338, valid_acc 0.8119, Time 00:00:33,lr 0.1\n",
      "epoch 13, loss 0.47811, train_acc 0.8351, valid_acc 0.8344, Time 00:00:33,lr 0.1\n",
      "epoch 14, loss 0.46951, train_acc 0.8389, valid_acc 0.7853, Time 00:00:33,lr 0.1\n",
      "epoch 15, loss 0.46085, train_acc 0.8430, valid_acc 0.8162, Time 00:00:33,lr 0.1\n",
      "epoch 16, loss 0.46285, train_acc 0.8429, valid_acc 0.8195, Time 00:00:33,lr 0.1\n",
      "epoch 17, loss 0.44921, train_acc 0.8452, valid_acc 0.8399, Time 00:00:33,lr 0.1\n",
      "epoch 18, loss 0.44708, train_acc 0.8461, valid_acc 0.8273, Time 00:00:33,lr 0.1\n",
      "epoch 19, loss 0.44015, train_acc 0.8488, valid_acc 0.8203, Time 00:00:33,lr 0.1\n",
      "epoch 20, loss 0.43671, train_acc 0.8511, valid_acc 0.8263, Time 00:00:33,lr 0.1\n",
      "epoch 21, loss 0.43053, train_acc 0.8506, valid_acc 0.8176, Time 00:00:33,lr 0.1\n",
      "epoch 22, loss 0.42708, train_acc 0.8541, valid_acc 0.7993, Time 00:00:33,lr 0.1\n",
      "epoch 23, loss 0.42621, train_acc 0.8544, valid_acc 0.8363, Time 00:00:33,lr 0.1\n",
      "epoch 24, loss 0.41793, train_acc 0.8560, valid_acc 0.8246, Time 00:00:33,lr 0.1\n",
      "epoch 25, loss 0.41721, train_acc 0.8564, valid_acc 0.8430, Time 00:00:33,lr 0.1\n",
      "epoch 26, loss 0.41290, train_acc 0.8587, valid_acc 0.8058, Time 00:00:33,lr 0.1\n",
      "epoch 27, loss 0.41461, train_acc 0.8587, valid_acc 0.8185, Time 00:00:33,lr 0.1\n",
      "epoch 28, loss 0.41093, train_acc 0.8602, valid_acc 0.8380, Time 00:00:33,lr 0.1\n",
      "epoch 29, loss 0.40087, train_acc 0.8614, valid_acc 0.8093, Time 00:00:33,lr 0.1\n",
      "epoch 30, loss 0.40541, train_acc 0.8600, valid_acc 0.8230, Time 00:00:33,lr 0.1\n",
      "epoch 31, loss 0.40255, train_acc 0.8610, valid_acc 0.8079, Time 00:00:33,lr 0.1\n",
      "epoch 32, loss 0.40050, train_acc 0.8628, valid_acc 0.7882, Time 00:00:33,lr 0.1\n",
      "epoch 33, loss 0.39481, train_acc 0.8645, valid_acc 0.8295, Time 00:00:33,lr 0.1\n",
      "epoch 34, loss 0.39867, train_acc 0.8636, valid_acc 0.8184, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.39564, train_acc 0.8642, valid_acc 0.8178, Time 00:00:33,lr 0.1\n",
      "epoch 36, loss 0.39489, train_acc 0.8651, valid_acc 0.8219, Time 00:00:33,lr 0.1\n",
      "epoch 37, loss 0.39592, train_acc 0.8636, valid_acc 0.8539, Time 00:00:33,lr 0.1\n",
      "epoch 38, loss 0.38821, train_acc 0.8660, valid_acc 0.8453, Time 00:00:33,lr 0.1\n",
      "epoch 39, loss 0.39041, train_acc 0.8658, valid_acc 0.7982, Time 00:00:33,lr 0.1\n",
      "epoch 40, loss 0.38574, train_acc 0.8685, valid_acc 0.7816, Time 00:00:33,lr 0.1\n",
      "epoch 41, loss 0.38974, train_acc 0.8665, valid_acc 0.8515, Time 00:00:33,lr 0.1\n",
      "epoch 42, loss 0.38603, train_acc 0.8678, valid_acc 0.8475, Time 00:00:33,lr 0.1\n",
      "epoch 43, loss 0.38327, train_acc 0.8695, valid_acc 0.8285, Time 00:00:33,lr 0.1\n",
      "epoch 44, loss 0.38648, train_acc 0.8657, valid_acc 0.8297, Time 00:00:33,lr 0.1\n",
      "epoch 45, loss 0.38244, train_acc 0.8685, valid_acc 0.8268, Time 00:00:33,lr 0.1\n",
      "epoch 46, loss 0.38622, train_acc 0.8668, valid_acc 0.8441, Time 00:00:33,lr 0.1\n",
      "epoch 47, loss 0.37983, train_acc 0.8692, valid_acc 0.8544, Time 00:00:33,lr 0.1\n",
      "epoch 48, loss 0.38122, train_acc 0.8699, valid_acc 0.8278, Time 00:00:33,lr 0.1\n",
      "epoch 49, loss 0.37919, train_acc 0.8693, valid_acc 0.8447, Time 00:00:33,lr 0.1\n",
      "epoch 50, loss 0.38096, train_acc 0.8695, valid_acc 0.8395, Time 00:00:33,lr 0.1\n",
      "epoch 51, loss 0.38445, train_acc 0.8679, valid_acc 0.8314, Time 00:00:33,lr 0.1\n",
      "epoch 52, loss 0.37595, train_acc 0.8723, valid_acc 0.8600, Time 00:00:33,lr 0.1\n",
      "epoch 53, loss 0.37780, train_acc 0.8703, valid_acc 0.8573, Time 00:00:33,lr 0.1\n",
      "epoch 54, loss 0.37309, train_acc 0.8719, valid_acc 0.8509, Time 00:00:33,lr 0.1\n",
      "epoch 55, loss 0.37771, train_acc 0.8722, valid_acc 0.8346, Time 00:00:33,lr 0.1\n",
      "epoch 56, loss 0.37425, train_acc 0.8728, valid_acc 0.7848, Time 00:00:33,lr 0.1\n",
      "epoch 57, loss 0.37199, train_acc 0.8713, valid_acc 0.8535, Time 00:00:33,lr 0.1\n",
      "epoch 58, loss 0.37070, train_acc 0.8729, valid_acc 0.8559, Time 00:00:33,lr 0.1\n",
      "epoch 59, loss 0.36944, train_acc 0.8733, valid_acc 0.8423, Time 00:00:33,lr 0.1\n",
      "epoch 60, loss 0.37046, train_acc 0.8737, valid_acc 0.8131, Time 00:00:33,lr 0.1\n",
      "epoch 61, loss 0.37506, train_acc 0.8725, valid_acc 0.8269, Time 00:00:33,lr 0.1\n",
      "epoch 62, loss 0.37128, train_acc 0.8726, valid_acc 0.8304, Time 00:00:33,lr 0.1\n",
      "epoch 63, loss 0.37199, train_acc 0.8734, valid_acc 0.8322, Time 00:00:33,lr 0.1\n",
      "epoch 64, loss 0.37266, train_acc 0.8732, valid_acc 0.8573, Time 00:00:33,lr 0.1\n",
      "epoch 65, loss 0.36817, train_acc 0.8733, valid_acc 0.8577, Time 00:00:33,lr 0.1\n",
      "epoch 66, loss 0.36947, train_acc 0.8754, valid_acc 0.8173, Time 00:00:33,lr 0.1\n",
      "epoch 67, loss 0.37241, train_acc 0.8726, valid_acc 0.8525, Time 00:00:33,lr 0.1\n",
      "epoch 68, loss 0.36946, train_acc 0.8737, valid_acc 0.8479, Time 00:00:33,lr 0.1\n",
      "epoch 69, loss 0.36792, train_acc 0.8740, valid_acc 0.8522, Time 00:00:33,lr 0.1\n",
      "epoch 70, loss 0.36331, train_acc 0.8748, valid_acc 0.8248, Time 00:00:33,lr 0.1\n",
      "epoch 71, loss 0.37371, train_acc 0.8709, valid_acc 0.8572, Time 00:00:33,lr 0.1\n",
      "epoch 72, loss 0.36035, train_acc 0.8774, valid_acc 0.8383, Time 00:00:33,lr 0.1\n",
      "epoch 73, loss 0.36753, train_acc 0.8745, valid_acc 0.8642, Time 00:00:33,lr 0.1\n",
      "epoch 74, loss 0.36495, train_acc 0.8748, valid_acc 0.8362, Time 00:00:33,lr 0.1\n",
      "epoch 75, loss 0.36169, train_acc 0.8762, valid_acc 0.8545, Time 00:00:33,lr 0.1\n",
      "epoch 76, loss 0.36737, train_acc 0.8742, valid_acc 0.8660, Time 00:00:33,lr 0.1\n",
      "epoch 77, loss 0.36069, train_acc 0.8762, valid_acc 0.8477, Time 00:00:33,lr 0.1\n",
      "epoch 78, loss 0.35918, train_acc 0.8762, valid_acc 0.7496, Time 00:00:33,lr 0.1\n",
      "epoch 79, loss 0.36596, train_acc 0.8754, valid_acc 0.8702, Time 00:00:33,lr 0.1\n",
      "epoch 80, loss 0.22051, train_acc 0.9255, valid_acc 0.9165, Time 00:00:33,lr 0.01\n",
      "epoch 81, loss 0.16942, train_acc 0.9411, valid_acc 0.9217, Time 00:00:33,lr 0.01\n",
      "epoch 82, loss 0.15287, train_acc 0.9484, valid_acc 0.9220, Time 00:00:33,lr 0.01\n",
      "epoch 83, loss 0.14203, train_acc 0.9515, valid_acc 0.9235, Time 00:00:33,lr 0.01\n",
      "epoch 84, loss 0.13170, train_acc 0.9557, valid_acc 0.9235, Time 00:00:33,lr 0.01\n",
      "epoch 85, loss 0.12316, train_acc 0.9581, valid_acc 0.9272, Time 00:00:33,lr 0.01\n",
      "epoch 86, loss 0.11667, train_acc 0.9605, valid_acc 0.9251, Time 00:00:33,lr 0.01\n",
      "epoch 87, loss 0.11166, train_acc 0.9626, valid_acc 0.9276, Time 00:00:33,lr 0.01\n",
      "epoch 88, loss 0.10688, train_acc 0.9640, valid_acc 0.9276, Time 00:00:33,lr 0.01\n",
      "epoch 89, loss 0.09929, train_acc 0.9659, valid_acc 0.9283, Time 00:00:33,lr 0.01\n",
      "epoch 90, loss 0.09834, train_acc 0.9661, valid_acc 0.9203, Time 00:00:33,lr 0.01\n",
      "epoch 91, loss 0.09401, train_acc 0.9680, valid_acc 0.9287, Time 00:00:33,lr 0.01\n",
      "epoch 92, loss 0.08950, train_acc 0.9696, valid_acc 0.9278, Time 00:00:33,lr 0.01\n",
      "epoch 93, loss 0.08283, train_acc 0.9722, valid_acc 0.9257, Time 00:00:33,lr 0.01\n",
      "epoch 94, loss 0.08354, train_acc 0.9716, valid_acc 0.9247, Time 00:00:33,lr 0.01\n",
      "epoch 95, loss 0.07745, train_acc 0.9740, valid_acc 0.9229, Time 00:00:33,lr 0.01\n",
      "epoch 96, loss 0.07554, train_acc 0.9741, valid_acc 0.9275, Time 00:00:33,lr 0.01\n",
      "epoch 97, loss 0.07464, train_acc 0.9750, valid_acc 0.9286, Time 00:00:33,lr 0.01\n",
      "epoch 98, loss 0.07502, train_acc 0.9737, valid_acc 0.9251, Time 00:00:33,lr 0.01\n",
      "epoch 99, loss 0.07237, train_acc 0.9750, valid_acc 0.9226, Time 00:00:33,lr 0.01\n",
      "epoch 100, loss 0.07316, train_acc 0.9758, valid_acc 0.9264, Time 00:00:33,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.06964, train_acc 0.9760, valid_acc 0.9242, Time 00:00:33,lr 0.01\n",
      "epoch 102, loss 0.06682, train_acc 0.9774, valid_acc 0.9238, Time 00:00:33,lr 0.01\n",
      "epoch 103, loss 0.06689, train_acc 0.9768, valid_acc 0.9242, Time 00:00:33,lr 0.01\n",
      "epoch 104, loss 0.06593, train_acc 0.9776, valid_acc 0.9274, Time 00:00:33,lr 0.01\n",
      "epoch 105, loss 0.06734, train_acc 0.9771, valid_acc 0.9260, Time 00:00:33,lr 0.01\n",
      "epoch 106, loss 0.06837, train_acc 0.9763, valid_acc 0.9233, Time 00:00:33,lr 0.01\n",
      "epoch 107, loss 0.06494, train_acc 0.9778, valid_acc 0.9255, Time 00:00:33,lr 0.01\n",
      "epoch 108, loss 0.06547, train_acc 0.9775, valid_acc 0.9225, Time 00:00:33,lr 0.01\n",
      "epoch 109, loss 0.06840, train_acc 0.9763, valid_acc 0.9247, Time 00:00:33,lr 0.01\n",
      "epoch 110, loss 0.06691, train_acc 0.9776, valid_acc 0.9270, Time 00:00:33,lr 0.01\n",
      "epoch 111, loss 0.06680, train_acc 0.9772, valid_acc 0.9212, Time 00:00:33,lr 0.01\n",
      "epoch 112, loss 0.06807, train_acc 0.9770, valid_acc 0.9235, Time 00:00:33,lr 0.01\n",
      "epoch 113, loss 0.06843, train_acc 0.9765, valid_acc 0.9259, Time 00:00:33,lr 0.01\n",
      "epoch 114, loss 0.06646, train_acc 0.9776, valid_acc 0.9260, Time 00:00:33,lr 0.01\n",
      "epoch 115, loss 0.06611, train_acc 0.9778, valid_acc 0.9272, Time 00:00:33,lr 0.01\n",
      "epoch 116, loss 0.06412, train_acc 0.9782, valid_acc 0.9185, Time 00:00:33,lr 0.01\n",
      "epoch 117, loss 0.06882, train_acc 0.9763, valid_acc 0.9218, Time 00:00:33,lr 0.01\n",
      "epoch 118, loss 0.06780, train_acc 0.9770, valid_acc 0.9180, Time 00:00:37,lr 0.01\n",
      "epoch 119, loss 0.06749, train_acc 0.9765, valid_acc 0.9227, Time 00:00:38,lr 0.01\n",
      "epoch 120, loss 0.07060, train_acc 0.9760, valid_acc 0.9226, Time 00:00:34,lr 0.01\n",
      "epoch 121, loss 0.06805, train_acc 0.9769, valid_acc 0.9198, Time 00:00:33,lr 0.01\n",
      "epoch 122, loss 0.07168, train_acc 0.9755, valid_acc 0.9225, Time 00:00:33,lr 0.01\n",
      "epoch 123, loss 0.07041, train_acc 0.9762, valid_acc 0.9240, Time 00:00:33,lr 0.01\n",
      "epoch 124, loss 0.07179, train_acc 0.9752, valid_acc 0.9199, Time 00:00:33,lr 0.01\n",
      "epoch 125, loss 0.06785, train_acc 0.9778, valid_acc 0.9214, Time 00:00:33,lr 0.01\n",
      "epoch 126, loss 0.07291, train_acc 0.9752, valid_acc 0.9160, Time 00:00:33,lr 0.01\n",
      "epoch 127, loss 0.06623, train_acc 0.9773, valid_acc 0.9237, Time 00:00:33,lr 0.01\n",
      "epoch 128, loss 0.07073, train_acc 0.9759, valid_acc 0.9204, Time 00:00:33,lr 0.01\n",
      "epoch 129, loss 0.06996, train_acc 0.9762, valid_acc 0.9191, Time 00:00:34,lr 0.01\n",
      "epoch 130, loss 0.07506, train_acc 0.9734, valid_acc 0.9173, Time 00:00:33,lr 0.01\n",
      "epoch 131, loss 0.07572, train_acc 0.9737, valid_acc 0.9076, Time 00:00:33,lr 0.01\n",
      "epoch 132, loss 0.07105, train_acc 0.9755, valid_acc 0.9201, Time 00:00:33,lr 0.01\n",
      "epoch 133, loss 0.07461, train_acc 0.9743, valid_acc 0.9150, Time 00:00:33,lr 0.01\n",
      "epoch 134, loss 0.06875, train_acc 0.9768, valid_acc 0.9231, Time 00:00:33,lr 0.01\n",
      "epoch 135, loss 0.06996, train_acc 0.9763, valid_acc 0.9169, Time 00:00:33,lr 0.01\n",
      "epoch 136, loss 0.07493, train_acc 0.9741, valid_acc 0.9214, Time 00:00:33,lr 0.01\n",
      "epoch 137, loss 0.07146, train_acc 0.9747, valid_acc 0.9171, Time 00:00:33,lr 0.01\n",
      "epoch 138, loss 0.07328, train_acc 0.9754, valid_acc 0.9215, Time 00:00:33,lr 0.01\n",
      "epoch 139, loss 0.07751, train_acc 0.9734, valid_acc 0.9269, Time 00:00:33,lr 0.01\n",
      "epoch 140, loss 0.07599, train_acc 0.9738, valid_acc 0.9154, Time 00:00:34,lr 0.01\n",
      "epoch 141, loss 0.06780, train_acc 0.9768, valid_acc 0.9156, Time 00:00:35,lr 0.01\n",
      "epoch 142, loss 0.07360, train_acc 0.9751, valid_acc 0.9236, Time 00:00:33,lr 0.01\n",
      "epoch 143, loss 0.07394, train_acc 0.9740, valid_acc 0.9175, Time 00:00:34,lr 0.01\n",
      "epoch 144, loss 0.07369, train_acc 0.9744, valid_acc 0.9217, Time 00:00:37,lr 0.01\n",
      "epoch 145, loss 0.07437, train_acc 0.9742, valid_acc 0.9182, Time 00:00:33,lr 0.01\n",
      "epoch 146, loss 0.07432, train_acc 0.9737, valid_acc 0.9102, Time 00:00:33,lr 0.01\n",
      "epoch 147, loss 0.07380, train_acc 0.9745, valid_acc 0.9145, Time 00:00:33,lr 0.01\n",
      "epoch 148, loss 0.07292, train_acc 0.9748, valid_acc 0.9212, Time 00:00:33,lr 0.01\n",
      "epoch 149, loss 0.07682, train_acc 0.9734, valid_acc 0.9200, Time 00:00:33,lr 0.01\n",
      "epoch 150, loss 0.04344, train_acc 0.9862, valid_acc 0.9350, Time 00:00:33,lr 0.001\n",
      "epoch 151, loss 0.03075, train_acc 0.9912, valid_acc 0.9356, Time 00:00:33,lr 0.001\n",
      "epoch 152, loss 0.02638, train_acc 0.9927, valid_acc 0.9351, Time 00:00:33,lr 0.001\n",
      "epoch 153, loss 0.02355, train_acc 0.9938, valid_acc 0.9350, Time 00:00:33,lr 0.001\n",
      "epoch 154, loss 0.02280, train_acc 0.9940, valid_acc 0.9359, Time 00:00:34,lr 0.001\n",
      "epoch 155, loss 0.02053, train_acc 0.9951, valid_acc 0.9363, Time 00:00:33,lr 0.001\n",
      "epoch 156, loss 0.01953, train_acc 0.9952, valid_acc 0.9352, Time 00:00:33,lr 0.001\n",
      "epoch 157, loss 0.01891, train_acc 0.9957, valid_acc 0.9354, Time 00:00:33,lr 0.001\n",
      "epoch 158, loss 0.01852, train_acc 0.9954, valid_acc 0.9353, Time 00:00:33,lr 0.001\n",
      "epoch 159, loss 0.01763, train_acc 0.9956, valid_acc 0.9363, Time 00:00:33,lr 0.001\n",
      "epoch 160, loss 0.01674, train_acc 0.9960, valid_acc 0.9366, Time 00:00:33,lr 0.001\n",
      "epoch 161, loss 0.01659, train_acc 0.9961, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 162, loss 0.01573, train_acc 0.9962, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 163, loss 0.01550, train_acc 0.9966, valid_acc 0.9362, Time 00:00:33,lr 0.001\n",
      "epoch 164, loss 0.01424, train_acc 0.9968, valid_acc 0.9367, Time 00:00:33,lr 0.001\n",
      "epoch 165, loss 0.01404, train_acc 0.9969, valid_acc 0.9368, Time 00:00:33,lr 0.001\n",
      "epoch 166, loss 0.01377, train_acc 0.9969, valid_acc 0.9372, Time 00:00:33,lr 0.001\n",
      "epoch 167, loss 0.01356, train_acc 0.9970, valid_acc 0.9376, Time 00:00:33,lr 0.001\n",
      "epoch 168, loss 0.01291, train_acc 0.9973, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 169, loss 0.01344, train_acc 0.9973, valid_acc 0.9378, Time 00:00:33,lr 0.001\n",
      "epoch 170, loss 0.01252, train_acc 0.9973, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 171, loss 0.01335, train_acc 0.9972, valid_acc 0.9366, Time 00:00:33,lr 0.001\n",
      "epoch 172, loss 0.01244, train_acc 0.9973, valid_acc 0.9371, Time 00:00:33,lr 0.001\n",
      "epoch 173, loss 0.01190, train_acc 0.9976, valid_acc 0.9378, Time 00:00:33,lr 0.001\n",
      "epoch 174, loss 0.01216, train_acc 0.9977, valid_acc 0.9380, Time 00:00:33,lr 0.001\n",
      "epoch 175, loss 0.01098, train_acc 0.9982, valid_acc 0.9383, Time 00:00:33,lr 0.001\n",
      "epoch 176, loss 0.01147, train_acc 0.9977, valid_acc 0.9368, Time 00:00:33,lr 0.001\n",
      "epoch 177, loss 0.01014, train_acc 0.9983, valid_acc 0.9380, Time 00:00:33,lr 0.001\n",
      "epoch 178, loss 0.01104, train_acc 0.9978, valid_acc 0.9372, Time 00:00:33,lr 0.001\n",
      "epoch 179, loss 0.01147, train_acc 0.9975, valid_acc 0.9379, Time 00:00:33,lr 0.001\n",
      "epoch 180, loss 0.01022, train_acc 0.9982, valid_acc 0.9375, Time 00:00:33,lr 0.001\n",
      "epoch 181, loss 0.01028, train_acc 0.9980, valid_acc 0.9376, Time 00:00:33,lr 0.001\n",
      "epoch 182, loss 0.00943, train_acc 0.9982, valid_acc 0.9375, Time 00:00:33,lr 0.001\n",
      "epoch 183, loss 0.00947, train_acc 0.9981, valid_acc 0.9381, Time 00:00:33,lr 0.001\n",
      "epoch 184, loss 0.00997, train_acc 0.9979, valid_acc 0.9380, Time 00:00:33,lr 0.001\n",
      "epoch 185, loss 0.00923, train_acc 0.9983, valid_acc 0.9382, Time 00:00:33,lr 0.001\n",
      "epoch 186, loss 0.00944, train_acc 0.9983, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 187, loss 0.00932, train_acc 0.9982, valid_acc 0.9377, Time 00:00:33,lr 0.001\n",
      "epoch 188, loss 0.00963, train_acc 0.9981, valid_acc 0.9372, Time 00:00:33,lr 0.001\n",
      "epoch 189, loss 0.00881, train_acc 0.9984, valid_acc 0.9383, Time 00:00:33,lr 0.001\n",
      "epoch 190, loss 0.00906, train_acc 0.9982, valid_acc 0.9378, Time 00:00:33,lr 0.001\n",
      "epoch 191, loss 0.00878, train_acc 0.9985, valid_acc 0.9385, Time 00:00:33,lr 0.001\n",
      "epoch 192, loss 0.00873, train_acc 0.9985, valid_acc 0.9400, Time 00:00:33,lr 0.001\n",
      "epoch 193, loss 0.00856, train_acc 0.9985, valid_acc 0.9392, Time 00:00:33,lr 0.001\n",
      "epoch 194, loss 0.00905, train_acc 0.9983, valid_acc 0.9387, Time 00:00:33,lr 0.001\n",
      "epoch 195, loss 0.00956, train_acc 0.9983, valid_acc 0.9385, Time 00:00:33,lr 0.001\n",
      "epoch 196, loss 0.00896, train_acc 0.9985, valid_acc 0.9376, Time 00:00:33,lr 0.001\n",
      "epoch 197, loss 0.00819, train_acc 0.9986, valid_acc 0.9386, Time 00:00:33,lr 0.001\n",
      "epoch 198, loss 0.00819, train_acc 0.9986, valid_acc 0.9382, Time 00:00:33,lr 0.001\n",
      "epoch 199, loss 0.00833, train_acc 0.9985, valid_acc 0.9386, Time 00:00:33,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_delay_downsample(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_522_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 to find if width is important, compare to 5.8 double all conv layers' channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:27:01.435710Z",
     "start_time": "2018-03-05T06:27:01.415583Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_522_c64(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_522_c64, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(5):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=256, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=256))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T09:55:35.962114Z",
     "start_time": "2018-03-05T06:27:02.356290Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.80125, train_acc 0.3235, valid_acc 0.4435, Time 00:01:01,lr 0.1\n",
      "epoch 1, loss 1.21529, train_acc 0.5671, valid_acc 0.6560, Time 00:01:02,lr 0.1\n",
      "epoch 2, loss 0.84985, train_acc 0.7038, valid_acc 0.7225, Time 00:01:02,lr 0.1\n",
      "epoch 3, loss 0.70113, train_acc 0.7565, valid_acc 0.7654, Time 00:01:03,lr 0.1\n",
      "epoch 4, loss 0.62073, train_acc 0.7878, valid_acc 0.7823, Time 00:01:03,lr 0.1\n",
      "epoch 5, loss 0.56917, train_acc 0.8041, valid_acc 0.7874, Time 00:01:04,lr 0.1\n",
      "epoch 6, loss 0.52424, train_acc 0.8196, valid_acc 0.7884, Time 00:01:02,lr 0.1\n",
      "epoch 7, loss 0.50291, train_acc 0.8289, valid_acc 0.8291, Time 00:01:03,lr 0.1\n",
      "epoch 8, loss 0.47482, train_acc 0.8385, valid_acc 0.7859, Time 00:01:02,lr 0.1\n",
      "epoch 9, loss 0.44850, train_acc 0.8471, valid_acc 0.8380, Time 00:01:02,lr 0.1\n",
      "epoch 10, loss 0.43947, train_acc 0.8487, valid_acc 0.8139, Time 00:01:02,lr 0.1\n",
      "epoch 11, loss 0.42596, train_acc 0.8529, valid_acc 0.7957, Time 00:01:02,lr 0.1\n",
      "epoch 12, loss 0.41112, train_acc 0.8588, valid_acc 0.8242, Time 00:01:03,lr 0.1\n",
      "epoch 13, loss 0.39968, train_acc 0.8629, valid_acc 0.8341, Time 00:01:02,lr 0.1\n",
      "epoch 14, loss 0.38834, train_acc 0.8670, valid_acc 0.8343, Time 00:01:02,lr 0.1\n",
      "epoch 15, loss 0.38437, train_acc 0.8676, valid_acc 0.8528, Time 00:01:02,lr 0.1\n",
      "epoch 16, loss 0.37885, train_acc 0.8690, valid_acc 0.8522, Time 00:01:02,lr 0.1\n",
      "epoch 17, loss 0.36954, train_acc 0.8750, valid_acc 0.8218, Time 00:01:02,lr 0.1\n",
      "epoch 18, loss 0.36590, train_acc 0.8740, valid_acc 0.8669, Time 00:01:02,lr 0.1\n",
      "epoch 19, loss 0.35680, train_acc 0.8776, valid_acc 0.7886, Time 00:01:02,lr 0.1\n",
      "epoch 20, loss 0.35435, train_acc 0.8782, valid_acc 0.8549, Time 00:01:02,lr 0.1\n",
      "epoch 21, loss 0.35435, train_acc 0.8771, valid_acc 0.8495, Time 00:01:02,lr 0.1\n",
      "epoch 22, loss 0.34705, train_acc 0.8807, valid_acc 0.8506, Time 00:01:02,lr 0.1\n",
      "epoch 23, loss 0.34090, train_acc 0.8825, valid_acc 0.8496, Time 00:01:02,lr 0.1\n",
      "epoch 24, loss 0.34386, train_acc 0.8808, valid_acc 0.8626, Time 00:01:02,lr 0.1\n",
      "epoch 25, loss 0.34245, train_acc 0.8825, valid_acc 0.7985, Time 00:01:02,lr 0.1\n",
      "epoch 26, loss 0.33509, train_acc 0.8857, valid_acc 0.8650, Time 00:01:02,lr 0.1\n",
      "epoch 27, loss 0.33300, train_acc 0.8857, valid_acc 0.8669, Time 00:01:02,lr 0.1\n",
      "epoch 28, loss 0.32566, train_acc 0.8876, valid_acc 0.8621, Time 00:01:02,lr 0.1\n",
      "epoch 29, loss 0.32526, train_acc 0.8898, valid_acc 0.8516, Time 00:01:02,lr 0.1\n",
      "epoch 30, loss 0.32913, train_acc 0.8874, valid_acc 0.8742, Time 00:01:02,lr 0.1\n",
      "epoch 31, loss 0.32649, train_acc 0.8875, valid_acc 0.8690, Time 00:01:02,lr 0.1\n",
      "epoch 32, loss 0.32220, train_acc 0.8900, valid_acc 0.8302, Time 00:01:02,lr 0.1\n",
      "epoch 33, loss 0.32117, train_acc 0.8895, valid_acc 0.8569, Time 00:01:02,lr 0.1\n",
      "epoch 34, loss 0.32370, train_acc 0.8887, valid_acc 0.8769, Time 00:01:02,lr 0.1\n",
      "epoch 35, loss 0.31867, train_acc 0.8903, valid_acc 0.8533, Time 00:01:03,lr 0.1\n",
      "epoch 36, loss 0.31809, train_acc 0.8898, valid_acc 0.8441, Time 00:01:02,lr 0.1\n",
      "epoch 37, loss 0.31201, train_acc 0.8933, valid_acc 0.8406, Time 00:01:02,lr 0.1\n",
      "epoch 38, loss 0.31065, train_acc 0.8922, valid_acc 0.8659, Time 00:01:02,lr 0.1\n",
      "epoch 39, loss 0.30870, train_acc 0.8940, valid_acc 0.8659, Time 00:01:02,lr 0.1\n",
      "epoch 40, loss 0.30990, train_acc 0.8946, valid_acc 0.8653, Time 00:01:02,lr 0.1\n",
      "epoch 41, loss 0.31143, train_acc 0.8916, valid_acc 0.8661, Time 00:01:02,lr 0.1\n",
      "epoch 42, loss 0.30731, train_acc 0.8949, valid_acc 0.8734, Time 00:01:02,lr 0.1\n",
      "epoch 43, loss 0.30617, train_acc 0.8952, valid_acc 0.8598, Time 00:01:02,lr 0.1\n",
      "epoch 44, loss 0.30193, train_acc 0.8966, valid_acc 0.8589, Time 00:01:03,lr 0.1\n",
      "epoch 45, loss 0.30429, train_acc 0.8963, valid_acc 0.8502, Time 00:01:02,lr 0.1\n",
      "epoch 46, loss 0.30656, train_acc 0.8956, valid_acc 0.8829, Time 00:01:02,lr 0.1\n",
      "epoch 47, loss 0.30519, train_acc 0.8958, valid_acc 0.8526, Time 00:01:02,lr 0.1\n",
      "epoch 48, loss 0.30416, train_acc 0.8945, valid_acc 0.8602, Time 00:01:02,lr 0.1\n",
      "epoch 49, loss 0.30313, train_acc 0.8963, valid_acc 0.8058, Time 00:01:02,lr 0.1\n",
      "epoch 50, loss 0.30975, train_acc 0.8951, valid_acc 0.8783, Time 00:01:03,lr 0.1\n",
      "epoch 51, loss 0.29615, train_acc 0.8990, valid_acc 0.8925, Time 00:01:02,lr 0.1\n",
      "epoch 52, loss 0.29919, train_acc 0.8976, valid_acc 0.8604, Time 00:01:02,lr 0.1\n",
      "epoch 53, loss 0.30477, train_acc 0.8962, valid_acc 0.8638, Time 00:01:02,lr 0.1\n",
      "epoch 54, loss 0.29803, train_acc 0.8965, valid_acc 0.8488, Time 00:01:02,lr 0.1\n",
      "epoch 55, loss 0.29738, train_acc 0.8977, valid_acc 0.8526, Time 00:01:02,lr 0.1\n",
      "epoch 56, loss 0.30025, train_acc 0.8967, valid_acc 0.8243, Time 00:01:02,lr 0.1\n",
      "epoch 57, loss 0.29852, train_acc 0.8984, valid_acc 0.8712, Time 00:01:02,lr 0.1\n",
      "epoch 58, loss 0.29340, train_acc 0.8992, valid_acc 0.8735, Time 00:01:02,lr 0.1\n",
      "epoch 59, loss 0.29646, train_acc 0.8973, valid_acc 0.8564, Time 00:01:02,lr 0.1\n",
      "epoch 60, loss 0.29942, train_acc 0.8973, valid_acc 0.8898, Time 00:01:02,lr 0.1\n",
      "epoch 61, loss 0.29024, train_acc 0.8999, valid_acc 0.8471, Time 00:01:02,lr 0.1\n",
      "epoch 62, loss 0.29388, train_acc 0.9000, valid_acc 0.8524, Time 00:01:02,lr 0.1\n",
      "epoch 63, loss 0.29573, train_acc 0.8968, valid_acc 0.8145, Time 00:01:02,lr 0.1\n",
      "epoch 64, loss 0.29297, train_acc 0.9013, valid_acc 0.8845, Time 00:01:02,lr 0.1\n",
      "epoch 65, loss 0.28833, train_acc 0.9022, valid_acc 0.8770, Time 00:01:02,lr 0.1\n",
      "epoch 66, loss 0.29792, train_acc 0.8971, valid_acc 0.8943, Time 00:01:02,lr 0.1\n",
      "epoch 67, loss 0.29277, train_acc 0.9005, valid_acc 0.8739, Time 00:01:02,lr 0.1\n",
      "epoch 68, loss 0.29278, train_acc 0.9000, valid_acc 0.8596, Time 00:01:02,lr 0.1\n",
      "epoch 69, loss 0.29351, train_acc 0.8999, valid_acc 0.8461, Time 00:01:02,lr 0.1\n",
      "epoch 70, loss 0.28908, train_acc 0.9000, valid_acc 0.8510, Time 00:01:02,lr 0.1\n",
      "epoch 71, loss 0.28986, train_acc 0.9004, valid_acc 0.8559, Time 00:01:02,lr 0.1\n",
      "epoch 72, loss 0.29072, train_acc 0.9007, valid_acc 0.8637, Time 00:01:02,lr 0.1\n",
      "epoch 73, loss 0.28822, train_acc 0.9014, valid_acc 0.8428, Time 00:01:02,lr 0.1\n",
      "epoch 74, loss 0.28894, train_acc 0.9002, valid_acc 0.8768, Time 00:01:02,lr 0.1\n",
      "epoch 75, loss 0.28853, train_acc 0.9013, valid_acc 0.8787, Time 00:01:02,lr 0.1\n",
      "epoch 76, loss 0.29077, train_acc 0.9001, valid_acc 0.8747, Time 00:01:02,lr 0.1\n",
      "epoch 77, loss 0.29205, train_acc 0.9006, valid_acc 0.8222, Time 00:01:02,lr 0.1\n",
      "epoch 78, loss 0.28894, train_acc 0.9010, valid_acc 0.8473, Time 00:01:02,lr 0.1\n",
      "epoch 79, loss 0.28275, train_acc 0.9019, valid_acc 0.8289, Time 00:01:02,lr 0.1\n",
      "epoch 80, loss 0.15574, train_acc 0.9475, valid_acc 0.9311, Time 00:01:02,lr 0.01\n",
      "epoch 81, loss 0.10297, train_acc 0.9658, valid_acc 0.9371, Time 00:01:02,lr 0.01\n",
      "epoch 82, loss 0.08857, train_acc 0.9698, valid_acc 0.9369, Time 00:01:02,lr 0.01\n",
      "epoch 83, loss 0.07886, train_acc 0.9731, valid_acc 0.9416, Time 00:01:02,lr 0.01\n",
      "epoch 84, loss 0.07118, train_acc 0.9763, valid_acc 0.9419, Time 00:01:02,lr 0.01\n",
      "epoch 85, loss 0.06399, train_acc 0.9787, valid_acc 0.9402, Time 00:01:02,lr 0.01\n",
      "epoch 86, loss 0.05531, train_acc 0.9816, valid_acc 0.9414, Time 00:01:02,lr 0.01\n",
      "epoch 87, loss 0.04941, train_acc 0.9840, valid_acc 0.9415, Time 00:01:02,lr 0.01\n",
      "epoch 88, loss 0.04808, train_acc 0.9843, valid_acc 0.9430, Time 00:01:02,lr 0.01\n",
      "epoch 89, loss 0.04049, train_acc 0.9871, valid_acc 0.9413, Time 00:01:02,lr 0.01\n",
      "epoch 90, loss 0.04131, train_acc 0.9871, valid_acc 0.9419, Time 00:01:02,lr 0.01\n",
      "epoch 91, loss 0.03723, train_acc 0.9883, valid_acc 0.9424, Time 00:01:02,lr 0.01\n",
      "epoch 92, loss 0.03402, train_acc 0.9892, valid_acc 0.9437, Time 00:01:02,lr 0.01\n",
      "epoch 93, loss 0.03021, train_acc 0.9912, valid_acc 0.9400, Time 00:01:02,lr 0.01\n",
      "epoch 94, loss 0.02831, train_acc 0.9914, valid_acc 0.9430, Time 00:01:02,lr 0.01\n",
      "epoch 95, loss 0.02827, train_acc 0.9916, valid_acc 0.9427, Time 00:01:02,lr 0.01\n",
      "epoch 96, loss 0.02427, train_acc 0.9931, valid_acc 0.9421, Time 00:01:02,lr 0.01\n",
      "epoch 97, loss 0.02600, train_acc 0.9922, valid_acc 0.9424, Time 00:01:02,lr 0.01\n",
      "epoch 98, loss 0.02444, train_acc 0.9927, valid_acc 0.9409, Time 00:01:02,lr 0.01\n",
      "epoch 99, loss 0.02349, train_acc 0.9931, valid_acc 0.9427, Time 00:01:02,lr 0.01\n",
      "epoch 100, loss 0.02360, train_acc 0.9932, valid_acc 0.9429, Time 00:01:02,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.02139, train_acc 0.9936, valid_acc 0.9407, Time 00:01:02,lr 0.01\n",
      "epoch 102, loss 0.02296, train_acc 0.9928, valid_acc 0.9436, Time 00:01:02,lr 0.01\n",
      "epoch 103, loss 0.02055, train_acc 0.9945, valid_acc 0.9414, Time 00:01:02,lr 0.01\n",
      "epoch 104, loss 0.02213, train_acc 0.9933, valid_acc 0.9417, Time 00:01:02,lr 0.01\n",
      "epoch 105, loss 0.01892, train_acc 0.9947, valid_acc 0.9410, Time 00:01:02,lr 0.01\n",
      "epoch 106, loss 0.02145, train_acc 0.9937, valid_acc 0.9418, Time 00:01:02,lr 0.01\n",
      "epoch 107, loss 0.01967, train_acc 0.9943, valid_acc 0.9386, Time 00:01:02,lr 0.01\n",
      "epoch 108, loss 0.01919, train_acc 0.9945, valid_acc 0.9394, Time 00:01:02,lr 0.01\n",
      "epoch 109, loss 0.02001, train_acc 0.9939, valid_acc 0.9409, Time 00:01:02,lr 0.01\n",
      "epoch 110, loss 0.02147, train_acc 0.9938, valid_acc 0.9424, Time 00:01:02,lr 0.01\n",
      "epoch 111, loss 0.02209, train_acc 0.9933, valid_acc 0.9395, Time 00:01:02,lr 0.01\n",
      "epoch 112, loss 0.02179, train_acc 0.9936, valid_acc 0.9397, Time 00:01:02,lr 0.01\n",
      "epoch 113, loss 0.01823, train_acc 0.9951, valid_acc 0.9407, Time 00:01:02,lr 0.01\n",
      "epoch 114, loss 0.02256, train_acc 0.9930, valid_acc 0.9420, Time 00:01:02,lr 0.01\n",
      "epoch 115, loss 0.02210, train_acc 0.9931, valid_acc 0.9397, Time 00:01:02,lr 0.01\n",
      "epoch 116, loss 0.02782, train_acc 0.9908, valid_acc 0.9360, Time 00:01:02,lr 0.01\n",
      "epoch 117, loss 0.02374, train_acc 0.9929, valid_acc 0.9350, Time 00:01:02,lr 0.01\n",
      "epoch 118, loss 0.02712, train_acc 0.9913, valid_acc 0.9391, Time 00:01:02,lr 0.01\n",
      "epoch 119, loss 0.02585, train_acc 0.9920, valid_acc 0.9348, Time 00:01:02,lr 0.01\n",
      "epoch 120, loss 0.02814, train_acc 0.9905, valid_acc 0.9378, Time 00:01:02,lr 0.01\n",
      "epoch 121, loss 0.03003, train_acc 0.9902, valid_acc 0.9358, Time 00:01:02,lr 0.01\n",
      "epoch 122, loss 0.02864, train_acc 0.9912, valid_acc 0.9377, Time 00:01:02,lr 0.01\n",
      "epoch 123, loss 0.02835, train_acc 0.9907, valid_acc 0.9376, Time 00:01:02,lr 0.01\n",
      "epoch 124, loss 0.02621, train_acc 0.9918, valid_acc 0.9327, Time 00:01:02,lr 0.01\n",
      "epoch 125, loss 0.03241, train_acc 0.9895, valid_acc 0.9360, Time 00:01:02,lr 0.01\n",
      "epoch 126, loss 0.03135, train_acc 0.9899, valid_acc 0.9392, Time 00:01:02,lr 0.01\n",
      "epoch 127, loss 0.03428, train_acc 0.9888, valid_acc 0.9312, Time 00:01:02,lr 0.01\n",
      "epoch 128, loss 0.03211, train_acc 0.9897, valid_acc 0.9371, Time 00:01:02,lr 0.01\n",
      "epoch 129, loss 0.03596, train_acc 0.9874, valid_acc 0.9394, Time 00:01:02,lr 0.01\n",
      "epoch 130, loss 0.03371, train_acc 0.9887, valid_acc 0.9314, Time 00:01:02,lr 0.01\n",
      "epoch 131, loss 0.03617, train_acc 0.9882, valid_acc 0.9349, Time 00:01:02,lr 0.01\n",
      "epoch 132, loss 0.03470, train_acc 0.9883, valid_acc 0.9340, Time 00:01:02,lr 0.01\n",
      "epoch 133, loss 0.03657, train_acc 0.9883, valid_acc 0.9276, Time 00:01:02,lr 0.01\n",
      "epoch 134, loss 0.03695, train_acc 0.9878, valid_acc 0.9336, Time 00:01:02,lr 0.01\n",
      "epoch 135, loss 0.03452, train_acc 0.9886, valid_acc 0.9347, Time 00:01:02,lr 0.01\n",
      "epoch 136, loss 0.03210, train_acc 0.9895, valid_acc 0.9336, Time 00:01:02,lr 0.01\n",
      "epoch 137, loss 0.03506, train_acc 0.9886, valid_acc 0.9329, Time 00:01:02,lr 0.01\n",
      "epoch 138, loss 0.03861, train_acc 0.9871, valid_acc 0.9294, Time 00:01:02,lr 0.01\n",
      "epoch 139, loss 0.03809, train_acc 0.9873, valid_acc 0.9319, Time 00:01:02,lr 0.01\n",
      "epoch 140, loss 0.03851, train_acc 0.9873, valid_acc 0.9233, Time 00:01:02,lr 0.01\n",
      "epoch 141, loss 0.04022, train_acc 0.9868, valid_acc 0.9359, Time 00:01:02,lr 0.01\n",
      "epoch 142, loss 0.03846, train_acc 0.9874, valid_acc 0.9306, Time 00:01:02,lr 0.01\n",
      "epoch 143, loss 0.03732, train_acc 0.9877, valid_acc 0.9261, Time 00:01:02,lr 0.01\n",
      "epoch 144, loss 0.03795, train_acc 0.9875, valid_acc 0.9339, Time 00:01:02,lr 0.01\n",
      "epoch 145, loss 0.04298, train_acc 0.9860, valid_acc 0.9302, Time 00:01:02,lr 0.01\n",
      "epoch 146, loss 0.03716, train_acc 0.9884, valid_acc 0.9316, Time 00:01:02,lr 0.01\n",
      "epoch 147, loss 0.04139, train_acc 0.9867, valid_acc 0.9273, Time 00:01:02,lr 0.01\n",
      "epoch 148, loss 0.03711, train_acc 0.9879, valid_acc 0.9309, Time 00:01:02,lr 0.01\n",
      "epoch 149, loss 0.03742, train_acc 0.9880, valid_acc 0.9299, Time 00:01:02,lr 0.01\n",
      "epoch 150, loss 0.02054, train_acc 0.9940, valid_acc 0.9442, Time 00:01:02,lr 0.001\n",
      "epoch 151, loss 0.01223, train_acc 0.9969, valid_acc 0.9461, Time 00:01:02,lr 0.001\n",
      "epoch 152, loss 0.00925, train_acc 0.9981, valid_acc 0.9482, Time 00:01:02,lr 0.001\n",
      "epoch 153, loss 0.00812, train_acc 0.9984, valid_acc 0.9469, Time 00:01:02,lr 0.001\n",
      "epoch 154, loss 0.00758, train_acc 0.9986, valid_acc 0.9478, Time 00:01:02,lr 0.001\n",
      "epoch 155, loss 0.00682, train_acc 0.9988, valid_acc 0.9477, Time 00:01:02,lr 0.001\n",
      "epoch 156, loss 0.00649, train_acc 0.9989, valid_acc 0.9477, Time 00:01:02,lr 0.001\n",
      "epoch 157, loss 0.00602, train_acc 0.9990, valid_acc 0.9478, Time 00:01:02,lr 0.001\n",
      "epoch 158, loss 0.00580, train_acc 0.9990, valid_acc 0.9494, Time 00:01:02,lr 0.001\n",
      "epoch 159, loss 0.00597, train_acc 0.9990, valid_acc 0.9495, Time 00:01:02,lr 0.001\n",
      "epoch 160, loss 0.00537, train_acc 0.9991, valid_acc 0.9493, Time 00:01:02,lr 0.001\n",
      "epoch 161, loss 0.00494, train_acc 0.9993, valid_acc 0.9495, Time 00:01:02,lr 0.001\n",
      "epoch 162, loss 0.00495, train_acc 0.9993, valid_acc 0.9512, Time 00:01:02,lr 0.001\n",
      "epoch 163, loss 0.00436, train_acc 0.9996, valid_acc 0.9504, Time 00:01:02,lr 0.001\n",
      "epoch 164, loss 0.00453, train_acc 0.9993, valid_acc 0.9499, Time 00:01:02,lr 0.001\n",
      "epoch 165, loss 0.00423, train_acc 0.9995, valid_acc 0.9507, Time 00:01:02,lr 0.001\n",
      "epoch 166, loss 0.00424, train_acc 0.9994, valid_acc 0.9496, Time 00:01:02,lr 0.001\n",
      "epoch 167, loss 0.00392, train_acc 0.9995, valid_acc 0.9500, Time 00:01:02,lr 0.001\n",
      "epoch 168, loss 0.00373, train_acc 0.9996, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 169, loss 0.00395, train_acc 0.9993, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 170, loss 0.00376, train_acc 0.9996, valid_acc 0.9512, Time 00:01:02,lr 0.001\n",
      "epoch 171, loss 0.00352, train_acc 0.9996, valid_acc 0.9496, Time 00:01:02,lr 0.001\n",
      "epoch 172, loss 0.00372, train_acc 0.9995, valid_acc 0.9503, Time 00:01:02,lr 0.001\n",
      "epoch 173, loss 0.00341, train_acc 0.9997, valid_acc 0.9516, Time 00:01:02,lr 0.001\n",
      "epoch 174, loss 0.00373, train_acc 0.9994, valid_acc 0.9518, Time 00:01:02,lr 0.001\n",
      "epoch 175, loss 0.00336, train_acc 0.9996, valid_acc 0.9524, Time 00:01:02,lr 0.001\n",
      "epoch 176, loss 0.00351, train_acc 0.9994, valid_acc 0.9515, Time 00:01:02,lr 0.001\n",
      "epoch 177, loss 0.00332, train_acc 0.9995, valid_acc 0.9498, Time 00:01:02,lr 0.001\n",
      "epoch 178, loss 0.00337, train_acc 0.9997, valid_acc 0.9515, Time 00:01:02,lr 0.001\n",
      "epoch 179, loss 0.00315, train_acc 0.9996, valid_acc 0.9511, Time 00:01:02,lr 0.001\n",
      "epoch 180, loss 0.00273, train_acc 0.9998, valid_acc 0.9521, Time 00:01:02,lr 0.001\n",
      "epoch 181, loss 0.00303, train_acc 0.9996, valid_acc 0.9518, Time 00:01:02,lr 0.001\n",
      "epoch 182, loss 0.00319, train_acc 0.9997, valid_acc 0.9507, Time 00:01:02,lr 0.001\n",
      "epoch 183, loss 0.00281, train_acc 0.9997, valid_acc 0.9508, Time 00:01:02,lr 0.001\n",
      "epoch 184, loss 0.00315, train_acc 0.9996, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 185, loss 0.00279, train_acc 0.9998, valid_acc 0.9517, Time 00:01:02,lr 0.001\n",
      "epoch 186, loss 0.00277, train_acc 0.9998, valid_acc 0.9515, Time 00:01:02,lr 0.001\n",
      "epoch 187, loss 0.00271, train_acc 0.9998, valid_acc 0.9520, Time 00:01:02,lr 0.001\n",
      "epoch 188, loss 0.00276, train_acc 0.9998, valid_acc 0.9518, Time 00:01:02,lr 0.001\n",
      "epoch 189, loss 0.00269, train_acc 0.9998, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 190, loss 0.00292, train_acc 0.9997, valid_acc 0.9506, Time 00:01:02,lr 0.001\n",
      "epoch 191, loss 0.00248, train_acc 0.9998, valid_acc 0.9520, Time 00:01:02,lr 0.001\n",
      "epoch 192, loss 0.00255, train_acc 0.9998, valid_acc 0.9511, Time 00:01:02,lr 0.001\n",
      "epoch 193, loss 0.00247, train_acc 0.9998, valid_acc 0.9516, Time 00:01:02,lr 0.001\n",
      "epoch 194, loss 0.00274, train_acc 0.9997, valid_acc 0.9503, Time 00:01:02,lr 0.001\n",
      "epoch 195, loss 0.00255, train_acc 0.9998, valid_acc 0.9511, Time 00:01:02,lr 0.001\n",
      "epoch 196, loss 0.00253, train_acc 0.9998, valid_acc 0.9502, Time 00:01:02,lr 0.001\n",
      "epoch 197, loss 0.00240, train_acc 0.9998, valid_acc 0.9504, Time 00:01:02,lr 0.001\n",
      "epoch 198, loss 0.00272, train_acc 0.9997, valid_acc 0.9517, Time 00:01:02,lr 0.001\n",
      "epoch 199, loss 0.00250, train_acc 0.9999, valid_acc 0.9516, Time 00:01:02,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_522_c64(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_522_c64_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:23:24.183057Z",
     "start_time": "2018-03-05T06:20:05.615Z"
    }
   },
   "source": [
    "## 5.10 to find if width is important, compare to my resnet18 double all conv layers' channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T09:55:35.978942Z",
     "start_time": "2018-03-05T09:55:35.963930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_333_c64(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_333_c64, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=256, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=256))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T13:38:25.724067Z",
     "start_time": "2018-03-05T09:55:35.983801Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.91070, train_acc 0.3031, valid_acc 0.3908, Time 00:01:02,lr 0.1\n",
      "epoch 1, loss 1.49198, train_acc 0.4545, valid_acc 0.5138, Time 00:01:06,lr 0.1\n",
      "epoch 2, loss 1.16327, train_acc 0.5859, valid_acc 0.6698, Time 00:01:06,lr 0.1\n",
      "epoch 3, loss 0.89234, train_acc 0.6859, valid_acc 0.6864, Time 00:01:06,lr 0.1\n",
      "epoch 4, loss 0.74111, train_acc 0.7439, valid_acc 0.6966, Time 00:01:06,lr 0.1\n",
      "epoch 5, loss 0.64906, train_acc 0.7765, valid_acc 0.7626, Time 00:01:06,lr 0.1\n",
      "epoch 6, loss 0.59122, train_acc 0.7965, valid_acc 0.7955, Time 00:01:06,lr 0.1\n",
      "epoch 7, loss 0.54251, train_acc 0.8130, valid_acc 0.7992, Time 00:01:06,lr 0.1\n",
      "epoch 8, loss 0.51203, train_acc 0.8241, valid_acc 0.7816, Time 00:01:06,lr 0.1\n",
      "epoch 9, loss 0.48736, train_acc 0.8330, valid_acc 0.8220, Time 00:01:06,lr 0.1\n",
      "epoch 10, loss 0.46056, train_acc 0.8435, valid_acc 0.8035, Time 00:01:06,lr 0.1\n",
      "epoch 11, loss 0.44692, train_acc 0.8453, valid_acc 0.8289, Time 00:01:06,lr 0.1\n",
      "epoch 12, loss 0.42780, train_acc 0.8531, valid_acc 0.7941, Time 00:01:06,lr 0.1\n",
      "epoch 13, loss 0.41150, train_acc 0.8595, valid_acc 0.8375, Time 00:01:06,lr 0.1\n",
      "epoch 14, loss 0.40133, train_acc 0.8611, valid_acc 0.8405, Time 00:01:06,lr 0.1\n",
      "epoch 15, loss 0.39545, train_acc 0.8640, valid_acc 0.8352, Time 00:01:06,lr 0.1\n",
      "epoch 16, loss 0.37821, train_acc 0.8688, valid_acc 0.8246, Time 00:01:06,lr 0.1\n",
      "epoch 17, loss 0.37287, train_acc 0.8725, valid_acc 0.8544, Time 00:01:06,lr 0.1\n",
      "epoch 18, loss 0.36702, train_acc 0.8742, valid_acc 0.8677, Time 00:01:06,lr 0.1\n",
      "epoch 19, loss 0.36043, train_acc 0.8769, valid_acc 0.8453, Time 00:01:06,lr 0.1\n",
      "epoch 20, loss 0.35116, train_acc 0.8796, valid_acc 0.8630, Time 00:01:06,lr 0.1\n",
      "epoch 21, loss 0.34817, train_acc 0.8812, valid_acc 0.8529, Time 00:01:06,lr 0.1\n",
      "epoch 22, loss 0.34614, train_acc 0.8798, valid_acc 0.8583, Time 00:01:06,lr 0.1\n",
      "epoch 23, loss 0.34516, train_acc 0.8813, valid_acc 0.8291, Time 00:01:06,lr 0.1\n",
      "epoch 24, loss 0.33574, train_acc 0.8840, valid_acc 0.8728, Time 00:01:06,lr 0.1\n",
      "epoch 25, loss 0.32641, train_acc 0.8882, valid_acc 0.8566, Time 00:01:06,lr 0.1\n",
      "epoch 26, loss 0.32412, train_acc 0.8877, valid_acc 0.8509, Time 00:01:06,lr 0.1\n",
      "epoch 27, loss 0.33088, train_acc 0.8869, valid_acc 0.8587, Time 00:01:06,lr 0.1\n",
      "epoch 28, loss 0.32500, train_acc 0.8877, valid_acc 0.8625, Time 00:01:06,lr 0.1\n",
      "epoch 29, loss 0.32184, train_acc 0.8900, valid_acc 0.8426, Time 00:01:06,lr 0.1\n",
      "epoch 30, loss 0.31916, train_acc 0.8906, valid_acc 0.8364, Time 00:01:06,lr 0.1\n",
      "epoch 31, loss 0.31433, train_acc 0.8931, valid_acc 0.8625, Time 00:01:06,lr 0.1\n",
      "epoch 32, loss 0.30779, train_acc 0.8951, valid_acc 0.8342, Time 00:01:06,lr 0.1\n",
      "epoch 33, loss 0.31361, train_acc 0.8931, valid_acc 0.8827, Time 00:01:06,lr 0.1\n",
      "epoch 34, loss 0.30608, train_acc 0.8943, valid_acc 0.8668, Time 00:01:06,lr 0.1\n",
      "epoch 35, loss 0.30962, train_acc 0.8936, valid_acc 0.8702, Time 00:01:06,lr 0.1\n",
      "epoch 36, loss 0.30872, train_acc 0.8944, valid_acc 0.8554, Time 00:01:06,lr 0.1\n",
      "epoch 37, loss 0.30220, train_acc 0.8944, valid_acc 0.8865, Time 00:01:06,lr 0.1\n",
      "epoch 38, loss 0.30488, train_acc 0.8969, valid_acc 0.8845, Time 00:01:06,lr 0.1\n",
      "epoch 39, loss 0.30197, train_acc 0.8975, valid_acc 0.8600, Time 00:01:06,lr 0.1\n",
      "epoch 40, loss 0.30346, train_acc 0.8963, valid_acc 0.8598, Time 00:01:06,lr 0.1\n",
      "epoch 41, loss 0.29583, train_acc 0.8996, valid_acc 0.8866, Time 00:01:06,lr 0.1\n",
      "epoch 42, loss 0.29905, train_acc 0.8973, valid_acc 0.8841, Time 00:01:06,lr 0.1\n",
      "epoch 43, loss 0.29592, train_acc 0.9001, valid_acc 0.8851, Time 00:01:06,lr 0.1\n",
      "epoch 44, loss 0.29321, train_acc 0.9009, valid_acc 0.8920, Time 00:01:07,lr 0.1\n",
      "epoch 45, loss 0.29935, train_acc 0.8979, valid_acc 0.8725, Time 00:01:06,lr 0.1\n",
      "epoch 46, loss 0.29592, train_acc 0.8992, valid_acc 0.8432, Time 00:01:06,lr 0.1\n",
      "epoch 47, loss 0.29559, train_acc 0.8981, valid_acc 0.8579, Time 00:01:06,lr 0.1\n",
      "epoch 48, loss 0.29578, train_acc 0.8989, valid_acc 0.8650, Time 00:01:06,lr 0.1\n",
      "epoch 49, loss 0.29405, train_acc 0.8994, valid_acc 0.8795, Time 00:01:07,lr 0.1\n",
      "epoch 50, loss 0.29210, train_acc 0.8995, valid_acc 0.8885, Time 00:01:06,lr 0.1\n",
      "epoch 51, loss 0.29042, train_acc 0.8995, valid_acc 0.8897, Time 00:01:06,lr 0.1\n",
      "epoch 52, loss 0.28023, train_acc 0.9043, valid_acc 0.8725, Time 00:01:06,lr 0.1\n",
      "epoch 53, loss 0.29078, train_acc 0.9008, valid_acc 0.8650, Time 00:01:06,lr 0.1\n",
      "epoch 54, loss 0.28800, train_acc 0.9012, valid_acc 0.8774, Time 00:01:06,lr 0.1\n",
      "epoch 55, loss 0.28512, train_acc 0.9018, valid_acc 0.8775, Time 00:01:06,lr 0.1\n",
      "epoch 56, loss 0.28262, train_acc 0.9041, valid_acc 0.8773, Time 00:01:06,lr 0.1\n",
      "epoch 57, loss 0.28406, train_acc 0.9025, valid_acc 0.8589, Time 00:01:06,lr 0.1\n",
      "epoch 58, loss 0.28683, train_acc 0.9019, valid_acc 0.8626, Time 00:01:06,lr 0.1\n",
      "epoch 59, loss 0.28206, train_acc 0.9033, valid_acc 0.8879, Time 00:01:06,lr 0.1\n",
      "epoch 60, loss 0.28285, train_acc 0.9028, valid_acc 0.8679, Time 00:01:06,lr 0.1\n",
      "epoch 61, loss 0.28471, train_acc 0.9047, valid_acc 0.8763, Time 00:01:06,lr 0.1\n",
      "epoch 62, loss 0.28389, train_acc 0.9025, valid_acc 0.8808, Time 00:01:06,lr 0.1\n",
      "epoch 63, loss 0.28368, train_acc 0.9015, valid_acc 0.8510, Time 00:01:06,lr 0.1\n",
      "epoch 64, loss 0.28125, train_acc 0.9062, valid_acc 0.8661, Time 00:01:06,lr 0.1\n",
      "epoch 65, loss 0.28093, train_acc 0.9039, valid_acc 0.8646, Time 00:01:06,lr 0.1\n",
      "epoch 66, loss 0.28358, train_acc 0.9046, valid_acc 0.8738, Time 00:01:06,lr 0.1\n",
      "epoch 67, loss 0.27802, train_acc 0.9047, valid_acc 0.8539, Time 00:01:07,lr 0.1\n",
      "epoch 68, loss 0.28430, train_acc 0.9032, valid_acc 0.8884, Time 00:01:08,lr 0.1\n",
      "epoch 69, loss 0.27718, train_acc 0.9048, valid_acc 0.8814, Time 00:01:06,lr 0.1\n",
      "epoch 70, loss 0.28220, train_acc 0.9031, valid_acc 0.8893, Time 00:01:06,lr 0.1\n",
      "epoch 71, loss 0.28048, train_acc 0.9037, valid_acc 0.8666, Time 00:01:06,lr 0.1\n",
      "epoch 72, loss 0.28267, train_acc 0.9032, valid_acc 0.8719, Time 00:01:06,lr 0.1\n",
      "epoch 73, loss 0.27851, train_acc 0.9040, valid_acc 0.8690, Time 00:01:09,lr 0.1\n",
      "epoch 74, loss 0.27998, train_acc 0.9024, valid_acc 0.8920, Time 00:01:06,lr 0.1\n",
      "epoch 75, loss 0.27545, train_acc 0.9062, valid_acc 0.8621, Time 00:01:06,lr 0.1\n",
      "epoch 76, loss 0.28088, train_acc 0.9036, valid_acc 0.8745, Time 00:01:06,lr 0.1\n",
      "epoch 77, loss 0.27670, train_acc 0.9058, valid_acc 0.8773, Time 00:01:06,lr 0.1\n",
      "epoch 78, loss 0.27581, train_acc 0.9069, valid_acc 0.8827, Time 00:01:06,lr 0.1\n",
      "epoch 79, loss 0.27913, train_acc 0.9040, valid_acc 0.8706, Time 00:01:06,lr 0.1\n",
      "epoch 80, loss 0.13957, train_acc 0.9539, valid_acc 0.9370, Time 00:01:06,lr 0.01\n",
      "epoch 81, loss 0.09086, train_acc 0.9695, valid_acc 0.9394, Time 00:01:06,lr 0.01\n",
      "epoch 82, loss 0.07365, train_acc 0.9756, valid_acc 0.9393, Time 00:01:07,lr 0.01\n",
      "epoch 83, loss 0.06506, train_acc 0.9781, valid_acc 0.9417, Time 00:01:06,lr 0.01\n",
      "epoch 84, loss 0.05740, train_acc 0.9813, valid_acc 0.9424, Time 00:01:06,lr 0.01\n",
      "epoch 85, loss 0.05083, train_acc 0.9829, valid_acc 0.9400, Time 00:01:06,lr 0.01\n",
      "epoch 86, loss 0.04590, train_acc 0.9857, valid_acc 0.9419, Time 00:01:06,lr 0.01\n",
      "epoch 87, loss 0.03799, train_acc 0.9875, valid_acc 0.9441, Time 00:01:06,lr 0.01\n",
      "epoch 88, loss 0.03804, train_acc 0.9876, valid_acc 0.9427, Time 00:01:06,lr 0.01\n",
      "epoch 89, loss 0.03164, train_acc 0.9903, valid_acc 0.9441, Time 00:01:06,lr 0.01\n",
      "epoch 90, loss 0.02854, train_acc 0.9905, valid_acc 0.9425, Time 00:01:06,lr 0.01\n",
      "epoch 91, loss 0.02570, train_acc 0.9920, valid_acc 0.9443, Time 00:01:06,lr 0.01\n",
      "epoch 92, loss 0.02691, train_acc 0.9915, valid_acc 0.9447, Time 00:01:06,lr 0.01\n",
      "epoch 93, loss 0.02434, train_acc 0.9921, valid_acc 0.9428, Time 00:01:06,lr 0.01\n",
      "epoch 94, loss 0.02427, train_acc 0.9922, valid_acc 0.9455, Time 00:01:06,lr 0.01\n",
      "epoch 95, loss 0.02132, train_acc 0.9932, valid_acc 0.9446, Time 00:01:06,lr 0.01\n",
      "epoch 96, loss 0.01910, train_acc 0.9945, valid_acc 0.9442, Time 00:01:06,lr 0.01\n",
      "epoch 97, loss 0.01803, train_acc 0.9950, valid_acc 0.9443, Time 00:01:06,lr 0.01\n",
      "epoch 98, loss 0.02039, train_acc 0.9934, valid_acc 0.9434, Time 00:01:06,lr 0.01\n",
      "epoch 99, loss 0.01921, train_acc 0.9939, valid_acc 0.9445, Time 00:01:06,lr 0.01\n",
      "epoch 100, loss 0.01898, train_acc 0.9937, valid_acc 0.9406, Time 00:01:06,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.02047, train_acc 0.9936, valid_acc 0.9447, Time 00:01:06,lr 0.01\n",
      "epoch 102, loss 0.01822, train_acc 0.9943, valid_acc 0.9429, Time 00:01:06,lr 0.01\n",
      "epoch 103, loss 0.01926, train_acc 0.9939, valid_acc 0.9436, Time 00:01:06,lr 0.01\n",
      "epoch 104, loss 0.01883, train_acc 0.9943, valid_acc 0.9438, Time 00:01:06,lr 0.01\n",
      "epoch 105, loss 0.02021, train_acc 0.9935, valid_acc 0.9413, Time 00:01:06,lr 0.01\n",
      "epoch 106, loss 0.01931, train_acc 0.9936, valid_acc 0.9428, Time 00:01:06,lr 0.01\n",
      "epoch 107, loss 0.01908, train_acc 0.9940, valid_acc 0.9422, Time 00:01:06,lr 0.01\n",
      "epoch 108, loss 0.01983, train_acc 0.9935, valid_acc 0.9426, Time 00:01:09,lr 0.01\n",
      "epoch 109, loss 0.01886, train_acc 0.9941, valid_acc 0.9405, Time 00:01:09,lr 0.01\n",
      "epoch 110, loss 0.02233, train_acc 0.9931, valid_acc 0.9411, Time 00:01:08,lr 0.01\n",
      "epoch 111, loss 0.01953, train_acc 0.9939, valid_acc 0.9378, Time 00:01:06,lr 0.01\n",
      "epoch 112, loss 0.02158, train_acc 0.9933, valid_acc 0.9376, Time 00:01:06,lr 0.01\n",
      "epoch 113, loss 0.02251, train_acc 0.9925, valid_acc 0.9409, Time 00:01:06,lr 0.01\n",
      "epoch 114, loss 0.02559, train_acc 0.9916, valid_acc 0.9346, Time 00:01:06,lr 0.01\n",
      "epoch 115, loss 0.02289, train_acc 0.9927, valid_acc 0.9398, Time 00:01:07,lr 0.01\n",
      "epoch 116, loss 0.02405, train_acc 0.9918, valid_acc 0.9409, Time 00:01:06,lr 0.01\n",
      "epoch 117, loss 0.02692, train_acc 0.9910, valid_acc 0.9377, Time 00:01:06,lr 0.01\n",
      "epoch 118, loss 0.02370, train_acc 0.9924, valid_acc 0.9361, Time 00:01:06,lr 0.01\n",
      "epoch 119, loss 0.02839, train_acc 0.9913, valid_acc 0.9372, Time 00:01:06,lr 0.01\n",
      "epoch 120, loss 0.02599, train_acc 0.9913, valid_acc 0.9359, Time 00:01:06,lr 0.01\n",
      "epoch 121, loss 0.02696, train_acc 0.9914, valid_acc 0.9357, Time 00:01:06,lr 0.01\n",
      "epoch 122, loss 0.02899, train_acc 0.9901, valid_acc 0.9317, Time 00:01:06,lr 0.01\n",
      "epoch 123, loss 0.03193, train_acc 0.9896, valid_acc 0.9408, Time 00:01:06,lr 0.01\n",
      "epoch 124, loss 0.02635, train_acc 0.9917, valid_acc 0.9395, Time 00:01:06,lr 0.01\n",
      "epoch 125, loss 0.03074, train_acc 0.9901, valid_acc 0.9381, Time 00:01:06,lr 0.01\n",
      "epoch 126, loss 0.03066, train_acc 0.9894, valid_acc 0.9352, Time 00:01:06,lr 0.01\n",
      "epoch 127, loss 0.03199, train_acc 0.9894, valid_acc 0.9390, Time 00:01:06,lr 0.01\n",
      "epoch 128, loss 0.03443, train_acc 0.9882, valid_acc 0.9352, Time 00:01:06,lr 0.01\n",
      "epoch 129, loss 0.03173, train_acc 0.9892, valid_acc 0.9283, Time 00:01:06,lr 0.01\n",
      "epoch 130, loss 0.03065, train_acc 0.9898, valid_acc 0.9334, Time 00:01:06,lr 0.01\n",
      "epoch 131, loss 0.03048, train_acc 0.9903, valid_acc 0.9392, Time 00:01:06,lr 0.01\n",
      "epoch 132, loss 0.03334, train_acc 0.9890, valid_acc 0.9350, Time 00:01:06,lr 0.01\n",
      "epoch 133, loss 0.03732, train_acc 0.9877, valid_acc 0.9384, Time 00:01:06,lr 0.01\n",
      "epoch 134, loss 0.03484, train_acc 0.9879, valid_acc 0.9261, Time 00:01:06,lr 0.01\n",
      "epoch 135, loss 0.03484, train_acc 0.9882, valid_acc 0.9351, Time 00:01:06,lr 0.01\n",
      "epoch 136, loss 0.03939, train_acc 0.9872, valid_acc 0.9350, Time 00:01:06,lr 0.01\n",
      "epoch 137, loss 0.03319, train_acc 0.9892, valid_acc 0.9326, Time 00:01:06,lr 0.01\n",
      "epoch 138, loss 0.03750, train_acc 0.9875, valid_acc 0.9313, Time 00:01:06,lr 0.01\n",
      "epoch 139, loss 0.03828, train_acc 0.9873, valid_acc 0.9325, Time 00:01:06,lr 0.01\n",
      "epoch 140, loss 0.03764, train_acc 0.9875, valid_acc 0.9328, Time 00:01:06,lr 0.01\n",
      "epoch 141, loss 0.03222, train_acc 0.9899, valid_acc 0.9264, Time 00:01:06,lr 0.01\n",
      "epoch 142, loss 0.03475, train_acc 0.9885, valid_acc 0.9321, Time 00:01:06,lr 0.01\n",
      "epoch 143, loss 0.03488, train_acc 0.9884, valid_acc 0.9293, Time 00:01:06,lr 0.01\n",
      "epoch 144, loss 0.03515, train_acc 0.9885, valid_acc 0.9354, Time 00:01:07,lr 0.01\n",
      "epoch 145, loss 0.03172, train_acc 0.9897, valid_acc 0.9288, Time 00:01:08,lr 0.01\n",
      "epoch 146, loss 0.04010, train_acc 0.9867, valid_acc 0.9282, Time 00:01:13,lr 0.01\n",
      "epoch 147, loss 0.03735, train_acc 0.9873, valid_acc 0.9327, Time 00:01:09,lr 0.01\n",
      "epoch 148, loss 0.03458, train_acc 0.9883, valid_acc 0.9316, Time 00:01:11,lr 0.01\n",
      "epoch 149, loss 0.04376, train_acc 0.9848, valid_acc 0.9363, Time 00:01:06,lr 0.01\n",
      "epoch 150, loss 0.01750, train_acc 0.9948, valid_acc 0.9462, Time 00:01:06,lr 0.001\n",
      "epoch 151, loss 0.01047, train_acc 0.9974, valid_acc 0.9471, Time 00:01:06,lr 0.001\n",
      "epoch 152, loss 0.00795, train_acc 0.9982, valid_acc 0.9488, Time 00:01:06,lr 0.001\n",
      "epoch 153, loss 0.00667, train_acc 0.9984, valid_acc 0.9484, Time 00:01:07,lr 0.001\n",
      "epoch 154, loss 0.00581, train_acc 0.9986, valid_acc 0.9473, Time 00:01:06,lr 0.001\n",
      "epoch 155, loss 0.00485, train_acc 0.9990, valid_acc 0.9505, Time 00:01:06,lr 0.001\n",
      "epoch 156, loss 0.00483, train_acc 0.9987, valid_acc 0.9504, Time 00:01:06,lr 0.001\n",
      "epoch 157, loss 0.00408, train_acc 0.9993, valid_acc 0.9510, Time 00:01:06,lr 0.001\n",
      "epoch 158, loss 0.00388, train_acc 0.9994, valid_acc 0.9500, Time 00:01:06,lr 0.001\n",
      "epoch 159, loss 0.00368, train_acc 0.9993, valid_acc 0.9508, Time 00:01:06,lr 0.001\n",
      "epoch 160, loss 0.00365, train_acc 0.9992, valid_acc 0.9506, Time 00:01:06,lr 0.001\n",
      "epoch 161, loss 0.00294, train_acc 0.9997, valid_acc 0.9511, Time 00:01:06,lr 0.001\n",
      "epoch 162, loss 0.00302, train_acc 0.9996, valid_acc 0.9504, Time 00:01:06,lr 0.001\n",
      "epoch 163, loss 0.00283, train_acc 0.9995, valid_acc 0.9506, Time 00:01:06,lr 0.001\n",
      "epoch 164, loss 0.00246, train_acc 0.9997, valid_acc 0.9514, Time 00:01:06,lr 0.001\n",
      "epoch 165, loss 0.00236, train_acc 0.9996, valid_acc 0.9500, Time 00:01:06,lr 0.001\n",
      "epoch 166, loss 0.00279, train_acc 0.9995, valid_acc 0.9510, Time 00:01:06,lr 0.001\n",
      "epoch 167, loss 0.00224, train_acc 0.9996, valid_acc 0.9521, Time 00:01:06,lr 0.001\n",
      "epoch 168, loss 0.00224, train_acc 0.9998, valid_acc 0.9508, Time 00:01:06,lr 0.001\n",
      "epoch 169, loss 0.00219, train_acc 0.9997, valid_acc 0.9526, Time 00:01:06,lr 0.001\n",
      "epoch 170, loss 0.00219, train_acc 0.9997, valid_acc 0.9520, Time 00:01:06,lr 0.001\n",
      "epoch 171, loss 0.00200, train_acc 0.9997, valid_acc 0.9516, Time 00:01:06,lr 0.001\n",
      "epoch 172, loss 0.00190, train_acc 0.9997, valid_acc 0.9522, Time 00:01:06,lr 0.001\n",
      "epoch 173, loss 0.00208, train_acc 0.9996, valid_acc 0.9507, Time 00:01:06,lr 0.001\n",
      "epoch 174, loss 0.00183, train_acc 0.9998, valid_acc 0.9513, Time 00:01:06,lr 0.001\n",
      "epoch 175, loss 0.00196, train_acc 0.9997, valid_acc 0.9517, Time 00:01:06,lr 0.001\n",
      "epoch 176, loss 0.00178, train_acc 0.9997, valid_acc 0.9512, Time 00:01:06,lr 0.001\n",
      "epoch 177, loss 0.00176, train_acc 0.9997, valid_acc 0.9525, Time 00:01:06,lr 0.001\n",
      "epoch 178, loss 0.00173, train_acc 0.9998, valid_acc 0.9513, Time 00:01:06,lr 0.001\n",
      "epoch 179, loss 0.00203, train_acc 0.9997, valid_acc 0.9531, Time 00:01:06,lr 0.001\n",
      "epoch 180, loss 0.00158, train_acc 0.9998, valid_acc 0.9521, Time 00:01:06,lr 0.001\n",
      "epoch 181, loss 0.00166, train_acc 0.9998, valid_acc 0.9522, Time 00:01:06,lr 0.001\n",
      "epoch 182, loss 0.00158, train_acc 0.9997, valid_acc 0.9518, Time 00:01:06,lr 0.001\n",
      "epoch 183, loss 0.00173, train_acc 0.9997, valid_acc 0.9513, Time 00:01:06,lr 0.001\n",
      "epoch 184, loss 0.00152, train_acc 0.9999, valid_acc 0.9527, Time 00:01:06,lr 0.001\n",
      "epoch 185, loss 0.00155, train_acc 0.9998, valid_acc 0.9529, Time 00:01:06,lr 0.001\n",
      "epoch 186, loss 0.00138, train_acc 0.9998, valid_acc 0.9518, Time 00:01:06,lr 0.001\n",
      "epoch 187, loss 0.00139, train_acc 0.9999, valid_acc 0.9535, Time 00:01:06,lr 0.001\n",
      "epoch 188, loss 0.00149, train_acc 0.9997, valid_acc 0.9529, Time 00:01:06,lr 0.001\n",
      "epoch 189, loss 0.00136, train_acc 0.9999, valid_acc 0.9528, Time 00:01:06,lr 0.001\n",
      "epoch 190, loss 0.00143, train_acc 0.9998, valid_acc 0.9527, Time 00:01:06,lr 0.001\n",
      "epoch 191, loss 0.00152, train_acc 0.9997, valid_acc 0.9536, Time 00:01:06,lr 0.001\n",
      "epoch 192, loss 0.00138, train_acc 0.9998, valid_acc 0.9547, Time 00:01:06,lr 0.001\n",
      "epoch 193, loss 0.00123, train_acc 0.9999, valid_acc 0.9551, Time 00:01:06,lr 0.001\n",
      "epoch 194, loss 0.00131, train_acc 0.9998, valid_acc 0.9532, Time 00:01:07,lr 0.001\n",
      "epoch 195, loss 0.00131, train_acc 0.9998, valid_acc 0.9544, Time 00:01:07,lr 0.001\n",
      "epoch 196, loss 0.00135, train_acc 0.9998, valid_acc 0.9538, Time 00:01:08,lr 0.001\n",
      "epoch 197, loss 0.00130, train_acc 0.9998, valid_acc 0.9536, Time 00:01:06,lr 0.001\n",
      "epoch 198, loss 0.00139, train_acc 0.9998, valid_acc 0.9537, Time 00:01:13,lr 0.001\n",
      "epoch 199, loss 0.00130, train_acc 0.9998, valid_acc 0.9524, Time 00:01:15,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_333_c64(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_333_c64_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12 remove global pooling use flatten to get more infomation\n",
    "\n",
    "more param need smaller lr cause bigger loss<br/>\n",
    "if remove global pooling completely, params will up to 64x, use start lr=0.001 can get loss down(lr=0.1,0.01 can not)<br/>\n",
    "so trade-off, I set global pooling size to 4, and start lr=0.05<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:44:38.277888Z",
     "start_time": "2018-03-09T14:44:38.256242Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_no_avgpooling(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_no_avgpooling, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            #net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.AvgPool2D(pool_size=4))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T16:31:42.369297Z",
     "start_time": "2018-03-09T14:44:38.722549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.86369, train_acc 0.3183, valid_acc 0.4513, Time 00:00:31,lr 0.05\n",
      "epoch 1, loss 1.41296, train_acc 0.4844, valid_acc 0.5312, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 1.09882, train_acc 0.6103, valid_acc 0.6673, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.86951, train_acc 0.6941, valid_acc 0.7282, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.72841, train_acc 0.7466, valid_acc 0.7471, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.64875, train_acc 0.7751, valid_acc 0.7870, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.59037, train_acc 0.7980, valid_acc 0.7901, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.54705, train_acc 0.8113, valid_acc 0.7860, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.51094, train_acc 0.8247, valid_acc 0.8068, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.48787, train_acc 0.8323, valid_acc 0.8220, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.46276, train_acc 0.8392, valid_acc 0.8186, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.44473, train_acc 0.8475, valid_acc 0.8391, Time 00:00:32,lr 0.05\n",
      "epoch 12, loss 0.42401, train_acc 0.8535, valid_acc 0.8411, Time 00:00:31,lr 0.05\n",
      "epoch 13, loss 0.41548, train_acc 0.8551, valid_acc 0.8389, Time 00:00:31,lr 0.05\n",
      "epoch 14, loss 0.39785, train_acc 0.8616, valid_acc 0.8540, Time 00:00:31,lr 0.05\n",
      "epoch 15, loss 0.38835, train_acc 0.8661, valid_acc 0.8544, Time 00:00:31,lr 0.05\n",
      "epoch 16, loss 0.37839, train_acc 0.8692, valid_acc 0.8538, Time 00:00:31,lr 0.05\n",
      "epoch 17, loss 0.36657, train_acc 0.8717, valid_acc 0.8496, Time 00:00:31,lr 0.05\n",
      "epoch 18, loss 0.35731, train_acc 0.8752, valid_acc 0.8531, Time 00:00:31,lr 0.05\n",
      "epoch 19, loss 0.35369, train_acc 0.8787, valid_acc 0.8546, Time 00:00:31,lr 0.05\n",
      "epoch 20, loss 0.34612, train_acc 0.8799, valid_acc 0.8628, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.34034, train_acc 0.8812, valid_acc 0.8433, Time 00:00:31,lr 0.05\n",
      "epoch 22, loss 0.33359, train_acc 0.8841, valid_acc 0.8722, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.33043, train_acc 0.8868, valid_acc 0.8686, Time 00:00:31,lr 0.05\n",
      "epoch 24, loss 0.32236, train_acc 0.8888, valid_acc 0.8731, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.31772, train_acc 0.8895, valid_acc 0.8647, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.31008, train_acc 0.8926, valid_acc 0.8538, Time 00:00:31,lr 0.05\n",
      "epoch 27, loss 0.31079, train_acc 0.8924, valid_acc 0.8575, Time 00:00:31,lr 0.05\n",
      "epoch 28, loss 0.30798, train_acc 0.8940, valid_acc 0.8420, Time 00:00:31,lr 0.05\n",
      "epoch 29, loss 0.29690, train_acc 0.8988, valid_acc 0.8604, Time 00:00:31,lr 0.05\n",
      "epoch 30, loss 0.29969, train_acc 0.8970, valid_acc 0.8636, Time 00:00:32,lr 0.05\n",
      "epoch 31, loss 0.30198, train_acc 0.8961, valid_acc 0.8509, Time 00:00:31,lr 0.05\n",
      "epoch 32, loss 0.29014, train_acc 0.8992, valid_acc 0.8540, Time 00:00:31,lr 0.05\n",
      "epoch 33, loss 0.28759, train_acc 0.8998, valid_acc 0.8803, Time 00:00:31,lr 0.05\n",
      "epoch 34, loss 0.28576, train_acc 0.9018, valid_acc 0.8521, Time 00:00:32,lr 0.05\n",
      "epoch 35, loss 0.28093, train_acc 0.9021, valid_acc 0.8730, Time 00:00:34,lr 0.05\n",
      "epoch 36, loss 0.28297, train_acc 0.9011, valid_acc 0.8515, Time 00:00:33,lr 0.05\n",
      "epoch 37, loss 0.27783, train_acc 0.9031, valid_acc 0.8719, Time 00:00:32,lr 0.05\n",
      "epoch 38, loss 0.28279, train_acc 0.9009, valid_acc 0.8848, Time 00:00:32,lr 0.05\n",
      "epoch 39, loss 0.27545, train_acc 0.9060, valid_acc 0.8570, Time 00:00:31,lr 0.05\n",
      "epoch 40, loss 0.27234, train_acc 0.9038, valid_acc 0.8626, Time 00:00:32,lr 0.05\n",
      "epoch 41, loss 0.27001, train_acc 0.9061, valid_acc 0.8645, Time 00:00:32,lr 0.05\n",
      "epoch 42, loss 0.27082, train_acc 0.9065, valid_acc 0.8616, Time 00:00:32,lr 0.05\n",
      "epoch 43, loss 0.26503, train_acc 0.9087, valid_acc 0.8690, Time 00:00:31,lr 0.05\n",
      "epoch 44, loss 0.26470, train_acc 0.9084, valid_acc 0.8840, Time 00:00:31,lr 0.05\n",
      "epoch 45, loss 0.26786, train_acc 0.9056, valid_acc 0.8561, Time 00:00:32,lr 0.05\n",
      "epoch 46, loss 0.26153, train_acc 0.9100, valid_acc 0.8676, Time 00:00:32,lr 0.05\n",
      "epoch 47, loss 0.26105, train_acc 0.9092, valid_acc 0.8640, Time 00:00:31,lr 0.05\n",
      "epoch 48, loss 0.26102, train_acc 0.9093, valid_acc 0.8787, Time 00:00:31,lr 0.05\n",
      "epoch 49, loss 0.26048, train_acc 0.9102, valid_acc 0.8803, Time 00:00:32,lr 0.05\n",
      "epoch 50, loss 0.26086, train_acc 0.9086, valid_acc 0.8816, Time 00:00:31,lr 0.05\n",
      "epoch 51, loss 0.25517, train_acc 0.9123, valid_acc 0.8743, Time 00:00:32,lr 0.05\n",
      "epoch 52, loss 0.25648, train_acc 0.9114, valid_acc 0.8671, Time 00:00:32,lr 0.05\n",
      "epoch 53, loss 0.25214, train_acc 0.9122, valid_acc 0.8923, Time 00:00:31,lr 0.05\n",
      "epoch 54, loss 0.25421, train_acc 0.9120, valid_acc 0.8528, Time 00:00:31,lr 0.05\n",
      "epoch 55, loss 0.25016, train_acc 0.9134, valid_acc 0.8674, Time 00:00:31,lr 0.05\n",
      "epoch 56, loss 0.24797, train_acc 0.9137, valid_acc 0.8738, Time 00:00:32,lr 0.05\n",
      "epoch 57, loss 0.25224, train_acc 0.9138, valid_acc 0.8915, Time 00:00:31,lr 0.05\n",
      "epoch 58, loss 0.24750, train_acc 0.9144, valid_acc 0.8754, Time 00:00:32,lr 0.05\n",
      "epoch 59, loss 0.25205, train_acc 0.9125, valid_acc 0.8849, Time 00:00:31,lr 0.05\n",
      "epoch 60, loss 0.24709, train_acc 0.9143, valid_acc 0.8655, Time 00:00:32,lr 0.05\n",
      "epoch 61, loss 0.24498, train_acc 0.9162, valid_acc 0.8681, Time 00:00:31,lr 0.05\n",
      "epoch 62, loss 0.24547, train_acc 0.9151, valid_acc 0.8779, Time 00:00:31,lr 0.05\n",
      "epoch 63, loss 0.24523, train_acc 0.9149, valid_acc 0.8876, Time 00:00:32,lr 0.05\n",
      "epoch 64, loss 0.24677, train_acc 0.9144, valid_acc 0.8803, Time 00:00:32,lr 0.05\n",
      "epoch 65, loss 0.24252, train_acc 0.9163, valid_acc 0.8838, Time 00:00:32,lr 0.05\n",
      "epoch 66, loss 0.24278, train_acc 0.9159, valid_acc 0.8731, Time 00:00:34,lr 0.05\n",
      "epoch 67, loss 0.24482, train_acc 0.9152, valid_acc 0.8764, Time 00:00:31,lr 0.05\n",
      "epoch 68, loss 0.24281, train_acc 0.9162, valid_acc 0.8858, Time 00:00:32,lr 0.05\n",
      "epoch 69, loss 0.23871, train_acc 0.9180, valid_acc 0.8865, Time 00:00:32,lr 0.05\n",
      "epoch 70, loss 0.24334, train_acc 0.9157, valid_acc 0.8676, Time 00:00:31,lr 0.05\n",
      "epoch 71, loss 0.24154, train_acc 0.9151, valid_acc 0.8914, Time 00:00:31,lr 0.05\n",
      "epoch 72, loss 0.24128, train_acc 0.9159, valid_acc 0.8880, Time 00:00:32,lr 0.05\n",
      "epoch 73, loss 0.23845, train_acc 0.9174, valid_acc 0.8726, Time 00:00:32,lr 0.05\n",
      "epoch 74, loss 0.23552, train_acc 0.9181, valid_acc 0.8898, Time 00:00:31,lr 0.05\n",
      "epoch 75, loss 0.23641, train_acc 0.9191, valid_acc 0.8718, Time 00:00:32,lr 0.05\n",
      "epoch 76, loss 0.24092, train_acc 0.9162, valid_acc 0.8507, Time 00:00:34,lr 0.05\n",
      "epoch 77, loss 0.23294, train_acc 0.9193, valid_acc 0.8802, Time 00:00:31,lr 0.05\n",
      "epoch 78, loss 0.23625, train_acc 0.9183, valid_acc 0.8936, Time 00:00:32,lr 0.05\n",
      "epoch 79, loss 0.23129, train_acc 0.9189, valid_acc 0.8932, Time 00:00:32,lr 0.05\n",
      "epoch 80, loss 0.12499, train_acc 0.9566, valid_acc 0.9254, Time 00:00:32,lr 0.005\n",
      "epoch 81, loss 0.09201, train_acc 0.9695, valid_acc 0.9301, Time 00:00:32,lr 0.005\n",
      "epoch 82, loss 0.07932, train_acc 0.9734, valid_acc 0.9335, Time 00:00:31,lr 0.005\n",
      "epoch 83, loss 0.07113, train_acc 0.9761, valid_acc 0.9311, Time 00:00:32,lr 0.005\n",
      "epoch 84, loss 0.06533, train_acc 0.9780, valid_acc 0.9320, Time 00:00:32,lr 0.005\n",
      "epoch 85, loss 0.05805, train_acc 0.9810, valid_acc 0.9354, Time 00:00:32,lr 0.005\n",
      "epoch 86, loss 0.05357, train_acc 0.9818, valid_acc 0.9335, Time 00:00:32,lr 0.005\n",
      "epoch 87, loss 0.04943, train_acc 0.9836, valid_acc 0.9341, Time 00:00:32,lr 0.005\n",
      "epoch 88, loss 0.04771, train_acc 0.9843, valid_acc 0.9345, Time 00:00:32,lr 0.005\n",
      "epoch 89, loss 0.04259, train_acc 0.9861, valid_acc 0.9324, Time 00:00:32,lr 0.005\n",
      "epoch 90, loss 0.04138, train_acc 0.9865, valid_acc 0.9352, Time 00:00:32,lr 0.005\n",
      "epoch 91, loss 0.03834, train_acc 0.9870, valid_acc 0.9330, Time 00:00:32,lr 0.005\n",
      "epoch 92, loss 0.03543, train_acc 0.9888, valid_acc 0.9340, Time 00:00:31,lr 0.005\n",
      "epoch 93, loss 0.03302, train_acc 0.9896, valid_acc 0.9353, Time 00:00:32,lr 0.005\n",
      "epoch 94, loss 0.03238, train_acc 0.9895, valid_acc 0.9350, Time 00:00:32,lr 0.005\n",
      "epoch 95, loss 0.02967, train_acc 0.9903, valid_acc 0.9354, Time 00:00:32,lr 0.005\n",
      "epoch 96, loss 0.02814, train_acc 0.9911, valid_acc 0.9355, Time 00:00:31,lr 0.005\n",
      "epoch 97, loss 0.02808, train_acc 0.9911, valid_acc 0.9359, Time 00:00:32,lr 0.005\n",
      "epoch 98, loss 0.02755, train_acc 0.9912, valid_acc 0.9348, Time 00:00:32,lr 0.005\n",
      "epoch 99, loss 0.02720, train_acc 0.9913, valid_acc 0.9355, Time 00:00:32,lr 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.02379, train_acc 0.9925, valid_acc 0.9356, Time 00:00:32,lr 0.005\n",
      "epoch 101, loss 0.02311, train_acc 0.9928, valid_acc 0.9340, Time 00:00:32,lr 0.005\n",
      "epoch 102, loss 0.02330, train_acc 0.9929, valid_acc 0.9349, Time 00:00:32,lr 0.005\n",
      "epoch 103, loss 0.02119, train_acc 0.9933, valid_acc 0.9338, Time 00:00:32,lr 0.005\n",
      "epoch 104, loss 0.02129, train_acc 0.9935, valid_acc 0.9359, Time 00:00:32,lr 0.005\n",
      "epoch 105, loss 0.02065, train_acc 0.9940, valid_acc 0.9342, Time 00:00:32,lr 0.005\n",
      "epoch 106, loss 0.01940, train_acc 0.9942, valid_acc 0.9351, Time 00:00:31,lr 0.005\n",
      "epoch 107, loss 0.02016, train_acc 0.9935, valid_acc 0.9342, Time 00:00:32,lr 0.005\n",
      "epoch 108, loss 0.02009, train_acc 0.9940, valid_acc 0.9345, Time 00:00:32,lr 0.005\n",
      "epoch 109, loss 0.01818, train_acc 0.9946, valid_acc 0.9370, Time 00:00:32,lr 0.005\n",
      "epoch 110, loss 0.01757, train_acc 0.9948, valid_acc 0.9357, Time 00:00:32,lr 0.005\n",
      "epoch 111, loss 0.01770, train_acc 0.9946, valid_acc 0.9345, Time 00:00:31,lr 0.005\n",
      "epoch 112, loss 0.01733, train_acc 0.9949, valid_acc 0.9339, Time 00:00:32,lr 0.005\n",
      "epoch 113, loss 0.01655, train_acc 0.9953, valid_acc 0.9342, Time 00:00:32,lr 0.005\n",
      "epoch 114, loss 0.01659, train_acc 0.9948, valid_acc 0.9340, Time 00:00:32,lr 0.005\n",
      "epoch 115, loss 0.01606, train_acc 0.9956, valid_acc 0.9347, Time 00:00:32,lr 0.005\n",
      "epoch 116, loss 0.01714, train_acc 0.9947, valid_acc 0.9349, Time 00:00:32,lr 0.005\n",
      "epoch 117, loss 0.01472, train_acc 0.9959, valid_acc 0.9351, Time 00:00:32,lr 0.005\n",
      "epoch 118, loss 0.01462, train_acc 0.9959, valid_acc 0.9366, Time 00:00:32,lr 0.005\n",
      "epoch 119, loss 0.01521, train_acc 0.9953, valid_acc 0.9338, Time 00:00:32,lr 0.005\n",
      "epoch 120, loss 0.01750, train_acc 0.9946, valid_acc 0.9371, Time 00:00:32,lr 0.005\n",
      "epoch 121, loss 0.01686, train_acc 0.9951, valid_acc 0.9346, Time 00:00:32,lr 0.005\n",
      "epoch 122, loss 0.01576, train_acc 0.9951, valid_acc 0.9336, Time 00:00:32,lr 0.005\n",
      "epoch 123, loss 0.01641, train_acc 0.9951, valid_acc 0.9358, Time 00:00:32,lr 0.005\n",
      "epoch 124, loss 0.01478, train_acc 0.9957, valid_acc 0.9362, Time 00:00:32,lr 0.005\n",
      "epoch 125, loss 0.01503, train_acc 0.9958, valid_acc 0.9337, Time 00:00:32,lr 0.005\n",
      "epoch 126, loss 0.01427, train_acc 0.9960, valid_acc 0.9355, Time 00:00:32,lr 0.005\n",
      "epoch 127, loss 0.01637, train_acc 0.9951, valid_acc 0.9328, Time 00:00:32,lr 0.005\n",
      "epoch 128, loss 0.01419, train_acc 0.9957, valid_acc 0.9331, Time 00:00:32,lr 0.005\n",
      "epoch 129, loss 0.01513, train_acc 0.9956, valid_acc 0.9337, Time 00:00:32,lr 0.005\n",
      "epoch 130, loss 0.01384, train_acc 0.9958, valid_acc 0.9334, Time 00:00:32,lr 0.005\n",
      "epoch 131, loss 0.01315, train_acc 0.9965, valid_acc 0.9364, Time 00:00:32,lr 0.005\n",
      "epoch 132, loss 0.01436, train_acc 0.9954, valid_acc 0.9354, Time 00:00:32,lr 0.005\n",
      "epoch 133, loss 0.01490, train_acc 0.9956, valid_acc 0.9345, Time 00:00:32,lr 0.005\n",
      "epoch 134, loss 0.01443, train_acc 0.9956, valid_acc 0.9358, Time 00:00:32,lr 0.005\n",
      "epoch 135, loss 0.01281, train_acc 0.9961, valid_acc 0.9341, Time 00:00:32,lr 0.005\n",
      "epoch 136, loss 0.01672, train_acc 0.9947, valid_acc 0.9333, Time 00:00:32,lr 0.005\n",
      "epoch 137, loss 0.01504, train_acc 0.9954, valid_acc 0.9322, Time 00:00:32,lr 0.005\n",
      "epoch 138, loss 0.01695, train_acc 0.9945, valid_acc 0.9328, Time 00:00:33,lr 0.005\n",
      "epoch 139, loss 0.01555, train_acc 0.9955, valid_acc 0.9360, Time 00:00:32,lr 0.005\n",
      "epoch 140, loss 0.01496, train_acc 0.9953, valid_acc 0.9346, Time 00:00:32,lr 0.005\n",
      "epoch 141, loss 0.01565, train_acc 0.9950, valid_acc 0.9342, Time 00:00:32,lr 0.005\n",
      "epoch 142, loss 0.01392, train_acc 0.9958, valid_acc 0.9368, Time 00:00:32,lr 0.005\n",
      "epoch 143, loss 0.01587, train_acc 0.9950, valid_acc 0.9344, Time 00:00:31,lr 0.005\n",
      "epoch 144, loss 0.01565, train_acc 0.9954, valid_acc 0.9346, Time 00:00:32,lr 0.005\n",
      "epoch 145, loss 0.01661, train_acc 0.9946, valid_acc 0.9318, Time 00:00:32,lr 0.005\n",
      "epoch 146, loss 0.01586, train_acc 0.9954, valid_acc 0.9333, Time 00:00:32,lr 0.005\n",
      "epoch 147, loss 0.01866, train_acc 0.9938, valid_acc 0.9348, Time 00:00:32,lr 0.005\n",
      "epoch 148, loss 0.01782, train_acc 0.9941, valid_acc 0.9333, Time 00:00:32,lr 0.005\n",
      "epoch 149, loss 0.01754, train_acc 0.9946, valid_acc 0.9324, Time 00:00:32,lr 0.005\n",
      "epoch 150, loss 0.01265, train_acc 0.9964, valid_acc 0.9367, Time 00:00:32,lr 0.0005\n",
      "epoch 151, loss 0.00918, train_acc 0.9979, valid_acc 0.9380, Time 00:00:32,lr 0.0005\n",
      "epoch 152, loss 0.00856, train_acc 0.9978, valid_acc 0.9392, Time 00:00:33,lr 0.0005\n",
      "epoch 153, loss 0.00722, train_acc 0.9985, valid_acc 0.9394, Time 00:00:32,lr 0.0005\n",
      "epoch 154, loss 0.00695, train_acc 0.9986, valid_acc 0.9408, Time 00:00:31,lr 0.0005\n",
      "epoch 155, loss 0.00634, train_acc 0.9987, valid_acc 0.9401, Time 00:00:32,lr 0.0005\n",
      "epoch 156, loss 0.00589, train_acc 0.9988, valid_acc 0.9390, Time 00:00:31,lr 0.0005\n",
      "epoch 157, loss 0.00578, train_acc 0.9988, valid_acc 0.9407, Time 00:00:32,lr 0.0005\n",
      "epoch 158, loss 0.00610, train_acc 0.9988, valid_acc 0.9400, Time 00:00:31,lr 0.0005\n",
      "epoch 159, loss 0.00541, train_acc 0.9988, valid_acc 0.9399, Time 00:00:31,lr 0.0005\n",
      "epoch 160, loss 0.00581, train_acc 0.9988, valid_acc 0.9420, Time 00:00:31,lr 0.0005\n",
      "epoch 161, loss 0.00533, train_acc 0.9990, valid_acc 0.9408, Time 00:00:32,lr 0.0005\n",
      "epoch 162, loss 0.00517, train_acc 0.9989, valid_acc 0.9405, Time 00:00:32,lr 0.0005\n",
      "epoch 163, loss 0.00501, train_acc 0.9988, valid_acc 0.9406, Time 00:00:31,lr 0.0005\n",
      "epoch 164, loss 0.00445, train_acc 0.9992, valid_acc 0.9397, Time 00:00:31,lr 0.0005\n",
      "epoch 165, loss 0.00476, train_acc 0.9992, valid_acc 0.9412, Time 00:00:32,lr 0.0005\n",
      "epoch 166, loss 0.00492, train_acc 0.9992, valid_acc 0.9414, Time 00:00:31,lr 0.0005\n",
      "epoch 167, loss 0.00465, train_acc 0.9991, valid_acc 0.9410, Time 00:00:31,lr 0.0005\n",
      "epoch 168, loss 0.00429, train_acc 0.9992, valid_acc 0.9408, Time 00:00:32,lr 0.0005\n",
      "epoch 169, loss 0.00439, train_acc 0.9993, valid_acc 0.9404, Time 00:00:32,lr 0.0005\n",
      "epoch 170, loss 0.00433, train_acc 0.9994, valid_acc 0.9411, Time 00:00:31,lr 0.0005\n",
      "epoch 171, loss 0.00413, train_acc 0.9993, valid_acc 0.9422, Time 00:00:31,lr 0.0005\n",
      "epoch 172, loss 0.00436, train_acc 0.9991, valid_acc 0.9417, Time 00:00:32,lr 0.0005\n",
      "epoch 173, loss 0.00403, train_acc 0.9994, valid_acc 0.9407, Time 00:00:31,lr 0.0005\n",
      "epoch 174, loss 0.00434, train_acc 0.9992, valid_acc 0.9408, Time 00:00:31,lr 0.0005\n",
      "epoch 175, loss 0.00427, train_acc 0.9992, valid_acc 0.9409, Time 00:00:32,lr 0.0005\n",
      "epoch 176, loss 0.00408, train_acc 0.9994, valid_acc 0.9398, Time 00:00:32,lr 0.0005\n",
      "epoch 177, loss 0.00393, train_acc 0.9995, valid_acc 0.9415, Time 00:00:31,lr 0.0005\n",
      "epoch 178, loss 0.00401, train_acc 0.9991, valid_acc 0.9417, Time 00:00:31,lr 0.0005\n",
      "epoch 179, loss 0.00382, train_acc 0.9994, valid_acc 0.9403, Time 00:00:31,lr 0.0005\n",
      "epoch 180, loss 0.00371, train_acc 0.9994, valid_acc 0.9416, Time 00:00:32,lr 0.0005\n",
      "epoch 181, loss 0.00402, train_acc 0.9992, valid_acc 0.9420, Time 00:00:31,lr 0.0005\n",
      "epoch 182, loss 0.00379, train_acc 0.9994, valid_acc 0.9415, Time 00:00:31,lr 0.0005\n",
      "epoch 183, loss 0.00376, train_acc 0.9994, valid_acc 0.9417, Time 00:00:31,lr 0.0005\n",
      "epoch 184, loss 0.00328, train_acc 0.9995, valid_acc 0.9412, Time 00:00:31,lr 0.0005\n",
      "epoch 185, loss 0.00321, train_acc 0.9996, valid_acc 0.9420, Time 00:00:31,lr 0.0005\n",
      "epoch 186, loss 0.00375, train_acc 0.9994, valid_acc 0.9412, Time 00:00:32,lr 0.0005\n",
      "epoch 187, loss 0.00350, train_acc 0.9996, valid_acc 0.9421, Time 00:00:32,lr 0.0005\n",
      "epoch 188, loss 0.00338, train_acc 0.9995, valid_acc 0.9416, Time 00:00:31,lr 0.0005\n",
      "epoch 189, loss 0.00361, train_acc 0.9994, valid_acc 0.9419, Time 00:00:31,lr 0.0005\n",
      "epoch 190, loss 0.00312, train_acc 0.9997, valid_acc 0.9419, Time 00:00:31,lr 0.0005\n",
      "epoch 191, loss 0.00351, train_acc 0.9995, valid_acc 0.9414, Time 00:00:31,lr 0.0005\n",
      "epoch 192, loss 0.00369, train_acc 0.9993, valid_acc 0.9410, Time 00:00:31,lr 0.0005\n",
      "epoch 193, loss 0.00352, train_acc 0.9994, valid_acc 0.9422, Time 00:00:31,lr 0.0005\n",
      "epoch 194, loss 0.00325, train_acc 0.9996, valid_acc 0.9405, Time 00:00:31,lr 0.0005\n",
      "epoch 195, loss 0.00320, train_acc 0.9996, valid_acc 0.9405, Time 00:00:31,lr 0.0005\n",
      "epoch 196, loss 0.00296, train_acc 0.9996, valid_acc 0.9396, Time 00:00:31,lr 0.0005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 197, loss 0.00340, train_acc 0.9993, valid_acc 0.9413, Time 00:00:31,lr 0.0005\n",
      "epoch 198, loss 0.00369, train_acc 0.9994, valid_acc 0.9424, Time 00:00:31,lr 0.0005\n",
      "epoch 199, loss 0.00298, train_acc 0.9995, valid_acc 0.9427, Time 00:00:31,lr 0.0005\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_no_avgpooling(10)\n",
    "net.initialize(ctx=ctx)\n",
    "#net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_no_avgpooling_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.13 add a fc layer to preduce more complex feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T16:31:42.385920Z",
     "start_time": "2018-03-09T16:31:42.371030Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_more_fc(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_more_fc, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(256))\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T18:16:07.937867Z",
     "start_time": "2018-03-09T16:31:42.386955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.84084, train_acc 0.3016, valid_acc 0.3551, Time 00:00:28,lr 0.1\n",
      "epoch 1, loss 1.42016, train_acc 0.4837, valid_acc 0.5336, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.07032, train_acc 0.6217, valid_acc 0.6744, Time 00:00:31,lr 0.1\n",
      "epoch 3, loss 0.85827, train_acc 0.7024, valid_acc 0.6448, Time 00:00:31,lr 0.1\n",
      "epoch 4, loss 0.75849, train_acc 0.7396, valid_acc 0.7088, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 0.68497, train_acc 0.7663, valid_acc 0.7673, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 0.65194, train_acc 0.7788, valid_acc 0.7865, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 0.61634, train_acc 0.7906, valid_acc 0.7910, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.59677, train_acc 0.7959, valid_acc 0.8123, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 0.56562, train_acc 0.8072, valid_acc 0.7965, Time 00:00:31,lr 0.1\n",
      "epoch 10, loss 0.54864, train_acc 0.8129, valid_acc 0.7974, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 0.54022, train_acc 0.8156, valid_acc 0.7896, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 0.52684, train_acc 0.8212, valid_acc 0.7972, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.51129, train_acc 0.8251, valid_acc 0.7693, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.49267, train_acc 0.8338, valid_acc 0.7919, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 0.49151, train_acc 0.8330, valid_acc 0.7909, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 0.47818, train_acc 0.8380, valid_acc 0.8131, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 0.47659, train_acc 0.8371, valid_acc 0.8400, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 0.46470, train_acc 0.8435, valid_acc 0.8023, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 0.46479, train_acc 0.8417, valid_acc 0.8185, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 0.45483, train_acc 0.8444, valid_acc 0.8321, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 0.45297, train_acc 0.8476, valid_acc 0.8130, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 0.44549, train_acc 0.8481, valid_acc 0.8214, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.43862, train_acc 0.8501, valid_acc 0.7940, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 0.44332, train_acc 0.8492, valid_acc 0.8516, Time 00:00:34,lr 0.1\n",
      "epoch 25, loss 0.43015, train_acc 0.8543, valid_acc 0.8209, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 0.43308, train_acc 0.8524, valid_acc 0.8355, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 0.42334, train_acc 0.8561, valid_acc 0.8167, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.42647, train_acc 0.8546, valid_acc 0.8329, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 0.42939, train_acc 0.8554, valid_acc 0.8249, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 0.41913, train_acc 0.8591, valid_acc 0.8388, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.41592, train_acc 0.8575, valid_acc 0.8514, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 0.42113, train_acc 0.8583, valid_acc 0.8504, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 0.41423, train_acc 0.8601, valid_acc 0.8166, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 0.41033, train_acc 0.8603, valid_acc 0.8385, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.41056, train_acc 0.8597, valid_acc 0.8245, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 0.40680, train_acc 0.8622, valid_acc 0.8280, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 0.40883, train_acc 0.8613, valid_acc 0.8429, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 0.40213, train_acc 0.8640, valid_acc 0.8499, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 0.40179, train_acc 0.8621, valid_acc 0.8122, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 0.40148, train_acc 0.8641, valid_acc 0.8228, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 0.40658, train_acc 0.8604, valid_acc 0.8372, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 0.39684, train_acc 0.8655, valid_acc 0.8439, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 0.39689, train_acc 0.8653, valid_acc 0.8213, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 0.39331, train_acc 0.8653, valid_acc 0.8270, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 0.38926, train_acc 0.8685, valid_acc 0.8436, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 0.39460, train_acc 0.8660, valid_acc 0.7843, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.39429, train_acc 0.8671, valid_acc 0.8347, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 0.39361, train_acc 0.8659, valid_acc 0.8322, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 0.39343, train_acc 0.8658, valid_acc 0.8452, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 0.39544, train_acc 0.8670, valid_acc 0.8375, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 0.38113, train_acc 0.8705, valid_acc 0.8064, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 0.39157, train_acc 0.8672, valid_acc 0.7971, Time 00:00:31,lr 0.1\n",
      "epoch 53, loss 0.38478, train_acc 0.8679, valid_acc 0.8652, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 0.39092, train_acc 0.8674, valid_acc 0.8151, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 0.39118, train_acc 0.8678, valid_acc 0.8563, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 0.38691, train_acc 0.8691, valid_acc 0.8365, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 0.38456, train_acc 0.8703, valid_acc 0.8668, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 0.38546, train_acc 0.8681, valid_acc 0.8456, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 0.38389, train_acc 0.8697, valid_acc 0.8272, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 0.38263, train_acc 0.8717, valid_acc 0.8393, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 0.38081, train_acc 0.8720, valid_acc 0.8297, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 0.38424, train_acc 0.8699, valid_acc 0.8627, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 0.38171, train_acc 0.8700, valid_acc 0.8329, Time 00:00:32,lr 0.1\n",
      "epoch 64, loss 0.37898, train_acc 0.8714, valid_acc 0.8445, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.37736, train_acc 0.8735, valid_acc 0.8348, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 0.38240, train_acc 0.8694, valid_acc 0.8683, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.37648, train_acc 0.8721, valid_acc 0.8433, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 0.38541, train_acc 0.8685, valid_acc 0.8696, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 0.37955, train_acc 0.8710, valid_acc 0.8425, Time 00:00:31,lr 0.1\n",
      "epoch 70, loss 0.37959, train_acc 0.8707, valid_acc 0.8404, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.38186, train_acc 0.8693, valid_acc 0.8105, Time 00:00:31,lr 0.1\n",
      "epoch 72, loss 0.37423, train_acc 0.8738, valid_acc 0.8484, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 0.37755, train_acc 0.8718, valid_acc 0.8682, Time 00:00:32,lr 0.1\n",
      "epoch 74, loss 0.37839, train_acc 0.8714, valid_acc 0.8089, Time 00:00:32,lr 0.1\n",
      "epoch 75, loss 0.37892, train_acc 0.8716, valid_acc 0.8286, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 0.37683, train_acc 0.8715, valid_acc 0.8524, Time 00:00:31,lr 0.1\n",
      "epoch 77, loss 0.37559, train_acc 0.8720, valid_acc 0.8177, Time 00:00:31,lr 0.1\n",
      "epoch 78, loss 0.37038, train_acc 0.8748, valid_acc 0.8545, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 0.37068, train_acc 0.8739, valid_acc 0.8067, Time 00:00:31,lr 0.1\n",
      "epoch 80, loss 0.20291, train_acc 0.9307, valid_acc 0.9189, Time 00:00:31,lr 0.01\n",
      "epoch 81, loss 0.15385, train_acc 0.9474, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 82, loss 0.13708, train_acc 0.9531, valid_acc 0.9238, Time 00:00:31,lr 0.01\n",
      "epoch 83, loss 0.12160, train_acc 0.9578, valid_acc 0.9270, Time 00:00:31,lr 0.01\n",
      "epoch 84, loss 0.11336, train_acc 0.9614, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 85, loss 0.10344, train_acc 0.9646, valid_acc 0.9318, Time 00:00:31,lr 0.01\n",
      "epoch 86, loss 0.09695, train_acc 0.9665, valid_acc 0.9289, Time 00:00:31,lr 0.01\n",
      "epoch 87, loss 0.09538, train_acc 0.9667, valid_acc 0.9283, Time 00:00:31,lr 0.01\n",
      "epoch 88, loss 0.08728, train_acc 0.9697, valid_acc 0.9312, Time 00:00:31,lr 0.01\n",
      "epoch 89, loss 0.08003, train_acc 0.9726, valid_acc 0.9298, Time 00:00:31,lr 0.01\n",
      "epoch 90, loss 0.07821, train_acc 0.9730, valid_acc 0.9290, Time 00:00:31,lr 0.01\n",
      "epoch 91, loss 0.07061, train_acc 0.9760, valid_acc 0.9322, Time 00:00:31,lr 0.01\n",
      "epoch 92, loss 0.06887, train_acc 0.9760, valid_acc 0.9314, Time 00:00:31,lr 0.01\n",
      "epoch 93, loss 0.06383, train_acc 0.9783, valid_acc 0.9317, Time 00:00:31,lr 0.01\n",
      "epoch 94, loss 0.06293, train_acc 0.9784, valid_acc 0.9330, Time 00:00:31,lr 0.01\n",
      "epoch 95, loss 0.06038, train_acc 0.9789, valid_acc 0.9332, Time 00:00:31,lr 0.01\n",
      "epoch 96, loss 0.05686, train_acc 0.9804, valid_acc 0.9325, Time 00:00:31,lr 0.01\n",
      "epoch 97, loss 0.05618, train_acc 0.9813, valid_acc 0.9272, Time 00:00:31,lr 0.01\n",
      "epoch 98, loss 0.05491, train_acc 0.9804, valid_acc 0.9329, Time 00:00:31,lr 0.01\n",
      "epoch 99, loss 0.05103, train_acc 0.9825, valid_acc 0.9324, Time 00:00:31,lr 0.01\n",
      "epoch 100, loss 0.05291, train_acc 0.9820, valid_acc 0.9289, Time 00:00:31,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.05048, train_acc 0.9827, valid_acc 0.9288, Time 00:00:31,lr 0.01\n",
      "epoch 102, loss 0.04965, train_acc 0.9822, valid_acc 0.9342, Time 00:00:31,lr 0.01\n",
      "epoch 103, loss 0.04977, train_acc 0.9828, valid_acc 0.9318, Time 00:00:31,lr 0.01\n",
      "epoch 104, loss 0.04749, train_acc 0.9835, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 105, loss 0.04683, train_acc 0.9839, valid_acc 0.9288, Time 00:00:31,lr 0.01\n",
      "epoch 106, loss 0.04838, train_acc 0.9836, valid_acc 0.9314, Time 00:00:31,lr 0.01\n",
      "epoch 107, loss 0.04906, train_acc 0.9831, valid_acc 0.9306, Time 00:00:31,lr 0.01\n",
      "epoch 108, loss 0.04616, train_acc 0.9843, valid_acc 0.9316, Time 00:00:31,lr 0.01\n",
      "epoch 109, loss 0.05024, train_acc 0.9823, valid_acc 0.9245, Time 00:00:31,lr 0.01\n",
      "epoch 110, loss 0.04866, train_acc 0.9832, valid_acc 0.9214, Time 00:00:31,lr 0.01\n",
      "epoch 111, loss 0.04861, train_acc 0.9838, valid_acc 0.9251, Time 00:00:31,lr 0.01\n",
      "epoch 112, loss 0.04792, train_acc 0.9832, valid_acc 0.9262, Time 00:00:31,lr 0.01\n",
      "epoch 113, loss 0.04782, train_acc 0.9837, valid_acc 0.9280, Time 00:00:31,lr 0.01\n",
      "epoch 114, loss 0.05068, train_acc 0.9829, valid_acc 0.9261, Time 00:00:31,lr 0.01\n",
      "epoch 115, loss 0.04660, train_acc 0.9840, valid_acc 0.9179, Time 00:00:31,lr 0.01\n",
      "epoch 116, loss 0.04832, train_acc 0.9829, valid_acc 0.9286, Time 00:00:31,lr 0.01\n",
      "epoch 117, loss 0.05213, train_acc 0.9824, valid_acc 0.9272, Time 00:00:31,lr 0.01\n",
      "epoch 118, loss 0.04753, train_acc 0.9839, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 119, loss 0.04884, train_acc 0.9831, valid_acc 0.9263, Time 00:00:31,lr 0.01\n",
      "epoch 120, loss 0.04744, train_acc 0.9838, valid_acc 0.9226, Time 00:00:31,lr 0.01\n",
      "epoch 121, loss 0.05282, train_acc 0.9822, valid_acc 0.9196, Time 00:00:31,lr 0.01\n",
      "epoch 122, loss 0.04887, train_acc 0.9834, valid_acc 0.9208, Time 00:00:31,lr 0.01\n",
      "epoch 123, loss 0.05482, train_acc 0.9812, valid_acc 0.9263, Time 00:00:31,lr 0.01\n",
      "epoch 124, loss 0.05311, train_acc 0.9818, valid_acc 0.9273, Time 00:00:31,lr 0.01\n",
      "epoch 125, loss 0.04994, train_acc 0.9826, valid_acc 0.9252, Time 00:00:31,lr 0.01\n",
      "epoch 126, loss 0.05178, train_acc 0.9823, valid_acc 0.9192, Time 00:00:31,lr 0.01\n",
      "epoch 127, loss 0.05746, train_acc 0.9798, valid_acc 0.9240, Time 00:00:31,lr 0.01\n",
      "epoch 128, loss 0.05325, train_acc 0.9821, valid_acc 0.9217, Time 00:00:31,lr 0.01\n",
      "epoch 129, loss 0.05352, train_acc 0.9822, valid_acc 0.9218, Time 00:00:31,lr 0.01\n",
      "epoch 130, loss 0.05345, train_acc 0.9819, valid_acc 0.9244, Time 00:00:31,lr 0.01\n",
      "epoch 131, loss 0.05321, train_acc 0.9817, valid_acc 0.9235, Time 00:00:31,lr 0.01\n",
      "epoch 132, loss 0.05923, train_acc 0.9796, valid_acc 0.9248, Time 00:00:31,lr 0.01\n",
      "epoch 133, loss 0.05523, train_acc 0.9815, valid_acc 0.9221, Time 00:00:31,lr 0.01\n",
      "epoch 134, loss 0.05554, train_acc 0.9810, valid_acc 0.9274, Time 00:00:31,lr 0.01\n",
      "epoch 135, loss 0.05406, train_acc 0.9812, valid_acc 0.9237, Time 00:00:31,lr 0.01\n",
      "epoch 136, loss 0.05649, train_acc 0.9806, valid_acc 0.9226, Time 00:00:31,lr 0.01\n",
      "epoch 137, loss 0.05934, train_acc 0.9798, valid_acc 0.9132, Time 00:00:31,lr 0.01\n",
      "epoch 138, loss 0.05779, train_acc 0.9800, valid_acc 0.9260, Time 00:00:31,lr 0.01\n",
      "epoch 139, loss 0.05183, train_acc 0.9819, valid_acc 0.9133, Time 00:00:31,lr 0.01\n",
      "epoch 140, loss 0.05781, train_acc 0.9800, valid_acc 0.9199, Time 00:00:31,lr 0.01\n",
      "epoch 141, loss 0.05686, train_acc 0.9805, valid_acc 0.9286, Time 00:00:31,lr 0.01\n",
      "epoch 142, loss 0.05394, train_acc 0.9822, valid_acc 0.9280, Time 00:00:31,lr 0.01\n",
      "epoch 143, loss 0.05580, train_acc 0.9811, valid_acc 0.9157, Time 00:00:31,lr 0.01\n",
      "epoch 144, loss 0.05751, train_acc 0.9805, valid_acc 0.9211, Time 00:00:31,lr 0.01\n",
      "epoch 145, loss 0.05333, train_acc 0.9814, valid_acc 0.9258, Time 00:00:31,lr 0.01\n",
      "epoch 146, loss 0.05588, train_acc 0.9811, valid_acc 0.9233, Time 00:00:31,lr 0.01\n",
      "epoch 147, loss 0.05699, train_acc 0.9810, valid_acc 0.9230, Time 00:00:31,lr 0.01\n",
      "epoch 148, loss 0.05750, train_acc 0.9804, valid_acc 0.9186, Time 00:00:31,lr 0.01\n",
      "epoch 149, loss 0.05856, train_acc 0.9794, valid_acc 0.9221, Time 00:00:31,lr 0.01\n",
      "epoch 150, loss 0.03365, train_acc 0.9893, valid_acc 0.9351, Time 00:00:31,lr 0.001\n",
      "epoch 151, loss 0.02033, train_acc 0.9941, valid_acc 0.9376, Time 00:00:31,lr 0.001\n",
      "epoch 152, loss 0.01665, train_acc 0.9954, valid_acc 0.9393, Time 00:00:31,lr 0.001\n",
      "epoch 153, loss 0.01391, train_acc 0.9964, valid_acc 0.9385, Time 00:00:31,lr 0.001\n",
      "epoch 154, loss 0.01289, train_acc 0.9965, valid_acc 0.9390, Time 00:00:31,lr 0.001\n",
      "epoch 155, loss 0.01123, train_acc 0.9972, valid_acc 0.9394, Time 00:00:31,lr 0.001\n",
      "epoch 156, loss 0.01094, train_acc 0.9972, valid_acc 0.9407, Time 00:00:31,lr 0.001\n",
      "epoch 157, loss 0.00943, train_acc 0.9977, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 158, loss 0.00905, train_acc 0.9979, valid_acc 0.9430, Time 00:00:31,lr 0.001\n",
      "epoch 159, loss 0.00914, train_acc 0.9978, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 160, loss 0.00756, train_acc 0.9985, valid_acc 0.9421, Time 00:00:31,lr 0.001\n",
      "epoch 161, loss 0.00801, train_acc 0.9982, valid_acc 0.9428, Time 00:00:31,lr 0.001\n",
      "epoch 162, loss 0.00760, train_acc 0.9984, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 163, loss 0.00727, train_acc 0.9983, valid_acc 0.9428, Time 00:00:31,lr 0.001\n",
      "epoch 164, loss 0.00701, train_acc 0.9983, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 165, loss 0.00687, train_acc 0.9986, valid_acc 0.9415, Time 00:00:31,lr 0.001\n",
      "epoch 166, loss 0.00622, train_acc 0.9988, valid_acc 0.9420, Time 00:00:31,lr 0.001\n",
      "epoch 167, loss 0.00629, train_acc 0.9987, valid_acc 0.9412, Time 00:00:31,lr 0.001\n",
      "epoch 168, loss 0.00670, train_acc 0.9985, valid_acc 0.9420, Time 00:00:31,lr 0.001\n",
      "epoch 169, loss 0.00619, train_acc 0.9987, valid_acc 0.9425, Time 00:00:31,lr 0.001\n",
      "epoch 170, loss 0.00618, train_acc 0.9986, valid_acc 0.9418, Time 00:00:31,lr 0.001\n",
      "epoch 171, loss 0.00542, train_acc 0.9990, valid_acc 0.9414, Time 00:00:31,lr 0.001\n",
      "epoch 172, loss 0.00555, train_acc 0.9987, valid_acc 0.9423, Time 00:00:31,lr 0.001\n",
      "epoch 173, loss 0.00527, train_acc 0.9989, valid_acc 0.9425, Time 00:00:31,lr 0.001\n",
      "epoch 174, loss 0.00495, train_acc 0.9992, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 175, loss 0.00497, train_acc 0.9991, valid_acc 0.9427, Time 00:00:31,lr 0.001\n",
      "epoch 176, loss 0.00520, train_acc 0.9989, valid_acc 0.9417, Time 00:00:30,lr 0.001\n",
      "epoch 177, loss 0.00522, train_acc 0.9990, valid_acc 0.9428, Time 00:00:31,lr 0.001\n",
      "epoch 178, loss 0.00491, train_acc 0.9989, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 179, loss 0.00445, train_acc 0.9991, valid_acc 0.9423, Time 00:00:30,lr 0.001\n",
      "epoch 180, loss 0.00460, train_acc 0.9990, valid_acc 0.9422, Time 00:00:31,lr 0.001\n",
      "epoch 181, loss 0.00439, train_acc 0.9991, valid_acc 0.9438, Time 00:00:30,lr 0.001\n",
      "epoch 182, loss 0.00478, train_acc 0.9990, valid_acc 0.9439, Time 00:00:31,lr 0.001\n",
      "epoch 183, loss 0.00425, train_acc 0.9991, valid_acc 0.9419, Time 00:00:30,lr 0.001\n",
      "epoch 184, loss 0.00399, train_acc 0.9992, valid_acc 0.9433, Time 00:00:30,lr 0.001\n",
      "epoch 185, loss 0.00447, train_acc 0.9991, valid_acc 0.9418, Time 00:00:31,lr 0.001\n",
      "epoch 186, loss 0.00408, train_acc 0.9992, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 187, loss 0.00414, train_acc 0.9992, valid_acc 0.9428, Time 00:00:30,lr 0.001\n",
      "epoch 188, loss 0.00410, train_acc 0.9992, valid_acc 0.9425, Time 00:00:30,lr 0.001\n",
      "epoch 189, loss 0.00393, train_acc 0.9993, valid_acc 0.9435, Time 00:00:31,lr 0.001\n",
      "epoch 190, loss 0.00365, train_acc 0.9994, valid_acc 0.9430, Time 00:00:31,lr 0.001\n",
      "epoch 191, loss 0.00389, train_acc 0.9994, valid_acc 0.9435, Time 00:00:31,lr 0.001\n",
      "epoch 192, loss 0.00363, train_acc 0.9994, valid_acc 0.9440, Time 00:00:30,lr 0.001\n",
      "epoch 193, loss 0.00337, train_acc 0.9994, valid_acc 0.9425, Time 00:00:30,lr 0.001\n",
      "epoch 194, loss 0.00371, train_acc 0.9995, valid_acc 0.9435, Time 00:00:31,lr 0.001\n",
      "epoch 195, loss 0.00345, train_acc 0.9993, valid_acc 0.9435, Time 00:00:31,lr 0.001\n",
      "epoch 196, loss 0.00354, train_acc 0.9994, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 197, loss 0.00344, train_acc 0.9992, valid_acc 0.9439, Time 00:00:30,lr 0.001\n",
      "epoch 198, loss 0.00367, train_acc 0.9993, valid_acc 0.9437, Time 00:00:30,lr 0.001\n",
      "epoch 199, loss 0.00382, train_acc 0.9992, valid_acc 0.9442, Time 00:00:30,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_more_fc(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_more_fc_e200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T16:18:14.176114Z",
     "start_time": "2018-03-13T16:18:14.153083Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_more_fc_v2(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_more_fc_v2, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(256))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.LeakyReLU(0.1))\n",
    "            net.add(nn.Dropout(0.5))\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:02:18.497294Z",
     "start_time": "2018-03-13T16:18:15.562189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.00073, train_acc 0.2576, valid_acc 0.3418, Time 00:00:29,lr 0.1\n",
      "epoch 1, loss 1.61386, train_acc 0.3917, valid_acc 0.3939, Time 00:00:30,lr 0.1\n",
      "epoch 2, loss 1.35985, train_acc 0.5063, valid_acc 0.5346, Time 00:00:30,lr 0.1\n",
      "epoch 3, loss 1.20042, train_acc 0.5723, valid_acc 0.5073, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 1.09322, train_acc 0.6125, valid_acc 0.6078, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 1.00440, train_acc 0.6488, valid_acc 0.6568, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 0.92570, train_acc 0.6811, valid_acc 0.6783, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 0.85635, train_acc 0.7083, valid_acc 0.6975, Time 00:00:30,lr 0.1\n",
      "epoch 8, loss 0.80856, train_acc 0.7270, valid_acc 0.7123, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 0.75043, train_acc 0.7454, valid_acc 0.7440, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.72465, train_acc 0.7563, valid_acc 0.7775, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 0.69689, train_acc 0.7659, valid_acc 0.7670, Time 00:00:30,lr 0.1\n",
      "epoch 12, loss 0.68061, train_acc 0.7707, valid_acc 0.7782, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.65685, train_acc 0.7784, valid_acc 0.7834, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.64559, train_acc 0.7835, valid_acc 0.7696, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 0.62460, train_acc 0.7912, valid_acc 0.7549, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 0.61036, train_acc 0.7950, valid_acc 0.7727, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 0.59955, train_acc 0.7995, valid_acc 0.7016, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 0.59752, train_acc 0.8002, valid_acc 0.8084, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 0.58358, train_acc 0.8053, valid_acc 0.8234, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 0.57880, train_acc 0.8036, valid_acc 0.8154, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 0.57451, train_acc 0.8094, valid_acc 0.7929, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 0.56095, train_acc 0.8141, valid_acc 0.7753, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.55166, train_acc 0.8163, valid_acc 0.8080, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 0.55101, train_acc 0.8173, valid_acc 0.8184, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 0.54711, train_acc 0.8159, valid_acc 0.7839, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 0.53676, train_acc 0.8203, valid_acc 0.8090, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 0.54151, train_acc 0.8194, valid_acc 0.8228, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.52513, train_acc 0.8252, valid_acc 0.8085, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 0.51619, train_acc 0.8255, valid_acc 0.7979, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 0.52401, train_acc 0.8253, valid_acc 0.8342, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.52383, train_acc 0.8248, valid_acc 0.7965, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 0.51241, train_acc 0.8294, valid_acc 0.8305, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 0.51211, train_acc 0.8288, valid_acc 0.8324, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 0.50378, train_acc 0.8320, valid_acc 0.8289, Time 00:00:31,lr 0.1\n",
      "epoch 35, loss 0.50846, train_acc 0.8300, valid_acc 0.8116, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 0.50149, train_acc 0.8312, valid_acc 0.8295, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 0.49688, train_acc 0.8342, valid_acc 0.7949, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 0.50258, train_acc 0.8324, valid_acc 0.8101, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 0.49238, train_acc 0.8347, valid_acc 0.8354, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 0.49236, train_acc 0.8358, valid_acc 0.8413, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 0.49341, train_acc 0.8353, valid_acc 0.8349, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 0.48956, train_acc 0.8371, valid_acc 0.8297, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 0.48531, train_acc 0.8384, valid_acc 0.7897, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 0.49237, train_acc 0.8363, valid_acc 0.8282, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 0.48191, train_acc 0.8385, valid_acc 0.8315, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 0.48087, train_acc 0.8395, valid_acc 0.8139, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.48068, train_acc 0.8413, valid_acc 0.8455, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 0.47891, train_acc 0.8401, valid_acc 0.8451, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 0.47575, train_acc 0.8424, valid_acc 0.8292, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 0.47623, train_acc 0.8423, valid_acc 0.8520, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 0.46882, train_acc 0.8425, valid_acc 0.8632, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 0.47480, train_acc 0.8412, valid_acc 0.8430, Time 00:00:31,lr 0.1\n",
      "epoch 53, loss 0.46909, train_acc 0.8426, valid_acc 0.8223, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 0.46675, train_acc 0.8451, valid_acc 0.8337, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 0.47103, train_acc 0.8438, valid_acc 0.8455, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 0.46642, train_acc 0.8446, valid_acc 0.8489, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 0.47139, train_acc 0.8429, valid_acc 0.8460, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 0.46614, train_acc 0.8457, valid_acc 0.8272, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 0.46920, train_acc 0.8442, valid_acc 0.8460, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 0.46730, train_acc 0.8458, valid_acc 0.8350, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 0.46690, train_acc 0.8469, valid_acc 0.7937, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 0.46394, train_acc 0.8432, valid_acc 0.8455, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 0.46547, train_acc 0.8449, valid_acc 0.8096, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 0.46452, train_acc 0.8465, valid_acc 0.8268, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.45723, train_acc 0.8478, valid_acc 0.8140, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 0.45878, train_acc 0.8478, valid_acc 0.8431, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 0.45622, train_acc 0.8485, valid_acc 0.8116, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 0.45423, train_acc 0.8465, valid_acc 0.8274, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 0.46137, train_acc 0.8480, valid_acc 0.8255, Time 00:00:31,lr 0.1\n",
      "epoch 70, loss 0.45929, train_acc 0.8473, valid_acc 0.8282, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.45939, train_acc 0.8478, valid_acc 0.8237, Time 00:00:31,lr 0.1\n",
      "epoch 72, loss 0.45660, train_acc 0.8496, valid_acc 0.8362, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 0.45809, train_acc 0.8482, valid_acc 0.8485, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 0.45251, train_acc 0.8507, valid_acc 0.8287, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 0.45656, train_acc 0.8489, valid_acc 0.8270, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 0.45245, train_acc 0.8484, valid_acc 0.8233, Time 00:00:31,lr 0.1\n",
      "epoch 77, loss 0.45207, train_acc 0.8515, valid_acc 0.8037, Time 00:00:31,lr 0.1\n",
      "epoch 78, loss 0.45135, train_acc 0.8504, valid_acc 0.8261, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 0.45028, train_acc 0.8496, valid_acc 0.8059, Time 00:00:31,lr 0.1\n",
      "epoch 80, loss 0.27970, train_acc 0.9058, valid_acc 0.9146, Time 00:00:31,lr 0.01\n",
      "epoch 81, loss 0.21555, train_acc 0.9278, valid_acc 0.9217, Time 00:00:31,lr 0.01\n",
      "epoch 82, loss 0.19625, train_acc 0.9337, valid_acc 0.9219, Time 00:00:31,lr 0.01\n",
      "epoch 83, loss 0.17856, train_acc 0.9396, valid_acc 0.9257, Time 00:00:31,lr 0.01\n",
      "epoch 84, loss 0.16509, train_acc 0.9445, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 85, loss 0.15734, train_acc 0.9467, valid_acc 0.9294, Time 00:00:31,lr 0.01\n",
      "epoch 86, loss 0.14427, train_acc 0.9512, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 87, loss 0.14058, train_acc 0.9528, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 88, loss 0.13016, train_acc 0.9548, valid_acc 0.9272, Time 00:00:31,lr 0.01\n",
      "epoch 89, loss 0.12546, train_acc 0.9560, valid_acc 0.9281, Time 00:00:31,lr 0.01\n",
      "epoch 90, loss 0.12218, train_acc 0.9583, valid_acc 0.9279, Time 00:00:31,lr 0.01\n",
      "epoch 91, loss 0.11370, train_acc 0.9610, valid_acc 0.9269, Time 00:00:31,lr 0.01\n",
      "epoch 92, loss 0.11037, train_acc 0.9613, valid_acc 0.9287, Time 00:00:31,lr 0.01\n",
      "epoch 93, loss 0.10608, train_acc 0.9637, valid_acc 0.9242, Time 00:00:31,lr 0.01\n",
      "epoch 94, loss 0.10247, train_acc 0.9651, valid_acc 0.9293, Time 00:00:31,lr 0.01\n",
      "epoch 95, loss 0.09912, train_acc 0.9664, valid_acc 0.9281, Time 00:00:31,lr 0.01\n",
      "epoch 96, loss 0.09731, train_acc 0.9658, valid_acc 0.9268, Time 00:00:31,lr 0.01\n",
      "epoch 97, loss 0.09295, train_acc 0.9687, valid_acc 0.9278, Time 00:00:31,lr 0.01\n",
      "epoch 98, loss 0.08828, train_acc 0.9695, valid_acc 0.9294, Time 00:00:31,lr 0.01\n",
      "epoch 99, loss 0.09078, train_acc 0.9694, valid_acc 0.9307, Time 00:00:31,lr 0.01\n",
      "epoch 100, loss 0.09008, train_acc 0.9687, valid_acc 0.9214, Time 00:00:31,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.08993, train_acc 0.9693, valid_acc 0.9258, Time 00:00:31,lr 0.01\n",
      "epoch 102, loss 0.08545, train_acc 0.9711, valid_acc 0.9258, Time 00:00:31,lr 0.01\n",
      "epoch 103, loss 0.08577, train_acc 0.9698, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 104, loss 0.08481, train_acc 0.9709, valid_acc 0.9224, Time 00:00:31,lr 0.01\n",
      "epoch 105, loss 0.08369, train_acc 0.9711, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 106, loss 0.07927, train_acc 0.9727, valid_acc 0.9257, Time 00:00:31,lr 0.01\n",
      "epoch 107, loss 0.08274, train_acc 0.9719, valid_acc 0.9215, Time 00:00:31,lr 0.01\n",
      "epoch 108, loss 0.08576, train_acc 0.9704, valid_acc 0.9230, Time 00:00:31,lr 0.01\n",
      "epoch 109, loss 0.07944, train_acc 0.9723, valid_acc 0.9245, Time 00:00:31,lr 0.01\n",
      "epoch 110, loss 0.08079, train_acc 0.9724, valid_acc 0.9221, Time 00:00:31,lr 0.01\n",
      "epoch 111, loss 0.07629, train_acc 0.9734, valid_acc 0.9257, Time 00:00:31,lr 0.01\n",
      "epoch 112, loss 0.07999, train_acc 0.9726, valid_acc 0.9288, Time 00:00:31,lr 0.01\n",
      "epoch 113, loss 0.08064, train_acc 0.9728, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 114, loss 0.07886, train_acc 0.9739, valid_acc 0.9274, Time 00:00:31,lr 0.01\n",
      "epoch 115, loss 0.07992, train_acc 0.9726, valid_acc 0.9273, Time 00:00:31,lr 0.01\n",
      "epoch 116, loss 0.08113, train_acc 0.9718, valid_acc 0.9136, Time 00:00:31,lr 0.01\n",
      "epoch 117, loss 0.07861, train_acc 0.9733, valid_acc 0.9223, Time 00:00:31,lr 0.01\n",
      "epoch 118, loss 0.08072, train_acc 0.9723, valid_acc 0.9240, Time 00:00:31,lr 0.01\n",
      "epoch 119, loss 0.07984, train_acc 0.9721, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 120, loss 0.07906, train_acc 0.9731, valid_acc 0.9222, Time 00:00:31,lr 0.01\n",
      "epoch 121, loss 0.08023, train_acc 0.9720, valid_acc 0.9231, Time 00:00:31,lr 0.01\n",
      "epoch 122, loss 0.08163, train_acc 0.9726, valid_acc 0.9263, Time 00:00:31,lr 0.01\n",
      "epoch 123, loss 0.07641, train_acc 0.9740, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 124, loss 0.08009, train_acc 0.9722, valid_acc 0.9242, Time 00:00:31,lr 0.01\n",
      "epoch 125, loss 0.08629, train_acc 0.9698, valid_acc 0.9156, Time 00:00:31,lr 0.01\n",
      "epoch 126, loss 0.07593, train_acc 0.9746, valid_acc 0.9254, Time 00:00:31,lr 0.01\n",
      "epoch 127, loss 0.07903, train_acc 0.9731, valid_acc 0.9239, Time 00:00:31,lr 0.01\n",
      "epoch 128, loss 0.07729, train_acc 0.9727, valid_acc 0.9245, Time 00:00:31,lr 0.01\n",
      "epoch 129, loss 0.08826, train_acc 0.9694, valid_acc 0.9250, Time 00:00:31,lr 0.01\n",
      "epoch 130, loss 0.07843, train_acc 0.9737, valid_acc 0.9174, Time 00:00:31,lr 0.01\n",
      "epoch 131, loss 0.07892, train_acc 0.9734, valid_acc 0.9204, Time 00:00:31,lr 0.01\n",
      "epoch 132, loss 0.07630, train_acc 0.9739, valid_acc 0.9200, Time 00:00:31,lr 0.01\n",
      "epoch 133, loss 0.08327, train_acc 0.9719, valid_acc 0.9202, Time 00:00:31,lr 0.01\n",
      "epoch 134, loss 0.08164, train_acc 0.9722, valid_acc 0.9232, Time 00:00:31,lr 0.01\n",
      "epoch 135, loss 0.07699, train_acc 0.9732, valid_acc 0.9239, Time 00:00:31,lr 0.01\n",
      "epoch 136, loss 0.07931, train_acc 0.9725, valid_acc 0.9196, Time 00:00:31,lr 0.01\n",
      "epoch 137, loss 0.07859, train_acc 0.9732, valid_acc 0.9195, Time 00:00:31,lr 0.01\n",
      "epoch 138, loss 0.08196, train_acc 0.9718, valid_acc 0.9167, Time 00:00:31,lr 0.01\n",
      "epoch 139, loss 0.08276, train_acc 0.9715, valid_acc 0.9167, Time 00:00:31,lr 0.01\n",
      "epoch 140, loss 0.08000, train_acc 0.9726, valid_acc 0.9248, Time 00:00:31,lr 0.01\n",
      "epoch 141, loss 0.08144, train_acc 0.9724, valid_acc 0.9234, Time 00:00:31,lr 0.01\n",
      "epoch 142, loss 0.08052, train_acc 0.9717, valid_acc 0.9225, Time 00:00:31,lr 0.01\n",
      "epoch 143, loss 0.07770, train_acc 0.9731, valid_acc 0.9205, Time 00:00:31,lr 0.01\n",
      "epoch 144, loss 0.08108, train_acc 0.9717, valid_acc 0.9230, Time 00:00:30,lr 0.01\n",
      "epoch 145, loss 0.08003, train_acc 0.9740, valid_acc 0.9187, Time 00:00:31,lr 0.01\n",
      "epoch 146, loss 0.07216, train_acc 0.9759, valid_acc 0.9246, Time 00:00:31,lr 0.01\n",
      "epoch 147, loss 0.08240, train_acc 0.9715, valid_acc 0.9200, Time 00:00:31,lr 0.01\n",
      "epoch 148, loss 0.07850, train_acc 0.9742, valid_acc 0.9179, Time 00:00:31,lr 0.01\n",
      "epoch 149, loss 0.07673, train_acc 0.9732, valid_acc 0.9272, Time 00:00:31,lr 0.01\n",
      "epoch 150, loss 0.04617, train_acc 0.9849, valid_acc 0.9342, Time 00:00:31,lr 0.001\n",
      "epoch 151, loss 0.02932, train_acc 0.9909, valid_acc 0.9374, Time 00:00:31,lr 0.001\n",
      "epoch 152, loss 0.02423, train_acc 0.9928, valid_acc 0.9369, Time 00:00:31,lr 0.001\n",
      "epoch 153, loss 0.02016, train_acc 0.9938, valid_acc 0.9386, Time 00:00:31,lr 0.001\n",
      "epoch 154, loss 0.01895, train_acc 0.9947, valid_acc 0.9387, Time 00:00:31,lr 0.001\n",
      "epoch 155, loss 0.01889, train_acc 0.9947, valid_acc 0.9383, Time 00:00:31,lr 0.001\n",
      "epoch 156, loss 0.01606, train_acc 0.9955, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 157, loss 0.01441, train_acc 0.9960, valid_acc 0.9393, Time 00:00:31,lr 0.001\n",
      "epoch 158, loss 0.01322, train_acc 0.9963, valid_acc 0.9386, Time 00:00:31,lr 0.001\n",
      "epoch 159, loss 0.01366, train_acc 0.9960, valid_acc 0.9395, Time 00:00:31,lr 0.001\n",
      "epoch 160, loss 0.01316, train_acc 0.9965, valid_acc 0.9410, Time 00:00:31,lr 0.001\n",
      "epoch 161, loss 0.01252, train_acc 0.9966, valid_acc 0.9409, Time 00:00:31,lr 0.001\n",
      "epoch 162, loss 0.01185, train_acc 0.9965, valid_acc 0.9412, Time 00:00:31,lr 0.001\n",
      "epoch 163, loss 0.01146, train_acc 0.9969, valid_acc 0.9405, Time 00:00:31,lr 0.001\n",
      "epoch 164, loss 0.01123, train_acc 0.9966, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 165, loss 0.01011, train_acc 0.9971, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 166, loss 0.01004, train_acc 0.9974, valid_acc 0.9418, Time 00:00:31,lr 0.001\n",
      "epoch 167, loss 0.01025, train_acc 0.9969, valid_acc 0.9421, Time 00:00:31,lr 0.001\n",
      "epoch 168, loss 0.00900, train_acc 0.9974, valid_acc 0.9431, Time 00:00:31,lr 0.001\n",
      "epoch 169, loss 0.00938, train_acc 0.9973, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 170, loss 0.00877, train_acc 0.9976, valid_acc 0.9412, Time 00:00:31,lr 0.001\n",
      "epoch 171, loss 0.00853, train_acc 0.9979, valid_acc 0.9401, Time 00:00:31,lr 0.001\n",
      "epoch 172, loss 0.00879, train_acc 0.9975, valid_acc 0.9405, Time 00:00:31,lr 0.001\n",
      "epoch 173, loss 0.00694, train_acc 0.9984, valid_acc 0.9411, Time 00:00:31,lr 0.001\n",
      "epoch 174, loss 0.00787, train_acc 0.9979, valid_acc 0.9412, Time 00:00:31,lr 0.001\n",
      "epoch 175, loss 0.00749, train_acc 0.9979, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 176, loss 0.00742, train_acc 0.9979, valid_acc 0.9407, Time 00:00:31,lr 0.001\n",
      "epoch 177, loss 0.00709, train_acc 0.9980, valid_acc 0.9404, Time 00:00:31,lr 0.001\n",
      "epoch 178, loss 0.00736, train_acc 0.9980, valid_acc 0.9431, Time 00:00:31,lr 0.001\n",
      "epoch 179, loss 0.00733, train_acc 0.9982, valid_acc 0.9424, Time 00:00:31,lr 0.001\n",
      "epoch 180, loss 0.00715, train_acc 0.9980, valid_acc 0.9410, Time 00:00:31,lr 0.001\n",
      "epoch 181, loss 0.00649, train_acc 0.9984, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 182, loss 0.00619, train_acc 0.9982, valid_acc 0.9415, Time 00:00:31,lr 0.001\n",
      "epoch 183, loss 0.00693, train_acc 0.9983, valid_acc 0.9419, Time 00:00:31,lr 0.001\n",
      "epoch 184, loss 0.00602, train_acc 0.9983, valid_acc 0.9399, Time 00:00:31,lr 0.001\n",
      "epoch 185, loss 0.00650, train_acc 0.9983, valid_acc 0.9419, Time 00:00:31,lr 0.001\n",
      "epoch 186, loss 0.00580, train_acc 0.9984, valid_acc 0.9431, Time 00:00:31,lr 0.001\n",
      "epoch 187, loss 0.00560, train_acc 0.9987, valid_acc 0.9418, Time 00:00:31,lr 0.001\n",
      "epoch 188, loss 0.00543, train_acc 0.9987, valid_acc 0.9411, Time 00:00:31,lr 0.001\n",
      "epoch 189, loss 0.00639, train_acc 0.9983, valid_acc 0.9423, Time 00:00:31,lr 0.001\n",
      "epoch 190, loss 0.00550, train_acc 0.9987, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 191, loss 0.00579, train_acc 0.9984, valid_acc 0.9425, Time 00:00:31,lr 0.001\n",
      "epoch 192, loss 0.00533, train_acc 0.9987, valid_acc 0.9437, Time 00:00:31,lr 0.001\n",
      "epoch 193, loss 0.00488, train_acc 0.9989, valid_acc 0.9416, Time 00:00:31,lr 0.001\n",
      "epoch 194, loss 0.00521, train_acc 0.9987, valid_acc 0.9430, Time 00:00:31,lr 0.001\n",
      "epoch 195, loss 0.00488, train_acc 0.9988, valid_acc 0.9405, Time 00:00:31,lr 0.001\n",
      "epoch 196, loss 0.00500, train_acc 0.9988, valid_acc 0.9428, Time 00:00:31,lr 0.001\n",
      "epoch 197, loss 0.00553, train_acc 0.9985, valid_acc 0.9437, Time 00:00:31,lr 0.001\n",
      "epoch 198, loss 0.00487, train_acc 0.9987, valid_acc 0.9427, Time 00:00:31,lr 0.001\n",
      "epoch 199, loss 0.00476, train_acc 0.9987, valid_acc 0.9427, Time 00:00:31,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_more_fc_v2(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_more_fc_v2_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.14 general lr policy train resnet152_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-13T16:18:20.822Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet152_v2_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet152_v2(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features[0])\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[2:4])\n",
    "        net.add(*resnet.features[5:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [100, 180, 250]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=0)\n",
    "net = get_resnet152_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet152_v2_100e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=0)\n",
    "net = get_resnet152_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "net.load_params(\"../../models/resnet152_v2_100e\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet152_v2_200e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "lr_period = [50]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=0)\n",
    "net = get_resnet152_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "net.load_params(\"../../models/resnet152_v2_200e\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet152_v2_300e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. use rewrite train function to train function to make sure not train code bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.794769Z",
     "start_time": "2018-03-03T17:35:08.750443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def get_lr(e, i, step):\n",
    "    if e >= step[i]:\n",
    "        return i + 1\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, lr_decay, wd, ctx, output_file=None, step={}):\n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "        print \" # valid_acc\", valid_acc\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    step_i = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if epoch > 0:\n",
    "            if len(step.keys()) == 0:\n",
    "                if epoch % lr_period == 0:\n",
    "                    trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            else:\n",
    "                if epoch >= step.keys()[step_i]:\n",
    "                    step_i = step_i + 1\n",
    "                    trainer.set_learning_rate(step[step.keys()[step_i]])\n",
    "        \n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            \n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += utils.accuracy(output, label)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "        \n",
    "        nd.waitall()\n",
    "        \n",
    "    if output_file != sys.stdout:\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:17:26.291786Z",
     "start_time": "2018-03-03T17:35:08.796140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.100838658147\n",
      "epoch 0, loss 2.17650, train_acc 0.1815, valid_acc 0.3233, Time 00:00:29,lr 0.1\n",
      "epoch 1, loss 1.74745, train_acc 0.3639, valid_acc 0.2111, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.48485, train_acc 0.4729, valid_acc 0.5586, Time 00:00:31,lr 0.1\n",
      "epoch 3, loss 1.32675, train_acc 0.5326, valid_acc 0.5165, Time 00:00:31,lr 0.1\n",
      "epoch 4, loss 1.23933, train_acc 0.5676, valid_acc 0.4877, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 1.18796, train_acc 0.5838, valid_acc 0.5885, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.15784, train_acc 0.5958, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 1.12028, train_acc 0.6103, valid_acc 0.6090, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 1.11800, train_acc 0.6141, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 1.09796, train_acc 0.6208, valid_acc 0.6072, Time 00:00:31,lr 0.1\n",
      "epoch 10, loss 1.08225, train_acc 0.6251, valid_acc 0.5205, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 1.07237, train_acc 0.6293, valid_acc 0.5745, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 1.06935, train_acc 0.6309, valid_acc 0.6407, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 1.05872, train_acc 0.6334, valid_acc 0.5826, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 1.04889, train_acc 0.6376, valid_acc 0.6672, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 1.05632, train_acc 0.6361, valid_acc 0.6499, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 1.05060, train_acc 0.6386, valid_acc 0.6274, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 1.04430, train_acc 0.6395, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 1.04184, train_acc 0.6421, valid_acc 0.6441, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 1.04526, train_acc 0.6398, valid_acc 0.5606, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 1.04432, train_acc 0.6404, valid_acc 0.6367, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 1.03036, train_acc 0.6456, valid_acc 0.7069, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 1.03184, train_acc 0.6459, valid_acc 0.6150, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 1.03162, train_acc 0.6419, valid_acc 0.6809, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 1.03152, train_acc 0.6440, valid_acc 0.6096, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 1.02944, train_acc 0.6447, valid_acc 0.6779, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 1.02975, train_acc 0.6446, valid_acc 0.5822, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 1.02655, train_acc 0.6464, valid_acc 0.6957, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 1.02281, train_acc 0.6490, valid_acc 0.6738, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 1.02572, train_acc 0.6455, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 1.01937, train_acc 0.6497, valid_acc 0.5929, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 1.01610, train_acc 0.6499, valid_acc 0.5739, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 1.02582, train_acc 0.6454, valid_acc 0.6808, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 1.02411, train_acc 0.6468, valid_acc 0.7068, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 1.01952, train_acc 0.6492, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 35, loss 1.02947, train_acc 0.6447, valid_acc 0.6387, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 1.01511, train_acc 0.6521, valid_acc 0.5688, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 1.02304, train_acc 0.6474, valid_acc 0.6549, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 1.01642, train_acc 0.6496, valid_acc 0.6774, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 1.01767, train_acc 0.6498, valid_acc 0.5900, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 1.01177, train_acc 0.6501, valid_acc 0.6799, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 1.01933, train_acc 0.6482, valid_acc 0.6359, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 1.01835, train_acc 0.6475, valid_acc 0.6245, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 1.02311, train_acc 0.6466, valid_acc 0.6745, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 1.01084, train_acc 0.6527, valid_acc 0.6729, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 1.01739, train_acc 0.6508, valid_acc 0.6601, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 1.02528, train_acc 0.6453, valid_acc 0.6812, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 1.01437, train_acc 0.6516, valid_acc 0.6579, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 1.02278, train_acc 0.6477, valid_acc 0.6893, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 1.01775, train_acc 0.6479, valid_acc 0.7160, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 1.01944, train_acc 0.6485, valid_acc 0.6020, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 1.02941, train_acc 0.6463, valid_acc 0.6422, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 1.01799, train_acc 0.6497, valid_acc 0.7113, Time 00:00:31,lr 0.1\n",
      "epoch 53, loss 1.01612, train_acc 0.6477, valid_acc 0.6327, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 1.02105, train_acc 0.6492, valid_acc 0.6325, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 1.02410, train_acc 0.6450, valid_acc 0.6888, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 1.01827, train_acc 0.6495, valid_acc 0.6665, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 1.01920, train_acc 0.6502, valid_acc 0.6775, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 1.02319, train_acc 0.6464, valid_acc 0.6765, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 1.01978, train_acc 0.6507, valid_acc 0.5037, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 1.02047, train_acc 0.6486, valid_acc 0.6431, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 1.01323, train_acc 0.6509, valid_acc 0.6778, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 1.01677, train_acc 0.6475, valid_acc 0.6488, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 1.02140, train_acc 0.6477, valid_acc 0.6320, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 1.01649, train_acc 0.6486, valid_acc 0.6985, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 1.01495, train_acc 0.6494, valid_acc 0.6110, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 1.02517, train_acc 0.6482, valid_acc 0.6281, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 1.01754, train_acc 0.6505, valid_acc 0.6933, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 1.01817, train_acc 0.6487, valid_acc 0.5997, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 1.02060, train_acc 0.6480, valid_acc 0.6438, Time 00:00:31,lr 0.1\n",
      "epoch 70, loss 1.01663, train_acc 0.6501, valid_acc 0.6557, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 1.01802, train_acc 0.6506, valid_acc 0.6037, Time 00:00:31,lr 0.1\n",
      "epoch 72, loss 1.02473, train_acc 0.6465, valid_acc 0.6869, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 1.02214, train_acc 0.6481, valid_acc 0.6365, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 1.02075, train_acc 0.6494, valid_acc 0.6727, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 1.02465, train_acc 0.6455, valid_acc 0.6005, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 1.01342, train_acc 0.6509, valid_acc 0.6316, Time 00:00:31,lr 0.1\n",
      "epoch 77, loss 1.02116, train_acc 0.6466, valid_acc 0.6909, Time 00:00:31,lr 0.1\n",
      "epoch 78, loss 1.02252, train_acc 0.6469, valid_acc 0.7035, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 1.01862, train_acc 0.6496, valid_acc 0.7132, Time 00:00:31,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = 80\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:33.999408Z",
     "start_time": "2018-03-03T18:17:26.293466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.713158945687\n",
      "epoch 0, loss 0.89016, train_acc 0.6943, valid_acc 0.6973, Time 00:00:29,lr 0.05\n",
      "epoch 1, loss 0.97071, train_acc 0.6649, valid_acc 0.6228, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.97855, train_acc 0.6619, valid_acc 0.5962, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.98917, train_acc 0.6604, valid_acc 0.6651, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.98467, train_acc 0.6614, valid_acc 0.6855, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.98934, train_acc 0.6582, valid_acc 0.6544, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.99133, train_acc 0.6577, valid_acc 0.6867, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.99419, train_acc 0.6565, valid_acc 0.6996, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.99295, train_acc 0.6574, valid_acc 0.6707, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.98785, train_acc 0.6583, valid_acc 0.6599, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.72795, train_acc 0.7497, valid_acc 0.7739, Time 00:00:31,lr 0.01\n",
      "epoch 11, loss 0.67062, train_acc 0.7695, valid_acc 0.7807, Time 00:00:31,lr 0.01\n",
      "epoch 12, loss 0.66014, train_acc 0.7741, valid_acc 0.8119, Time 00:00:31,lr 0.01\n",
      "epoch 13, loss 0.66186, train_acc 0.7753, valid_acc 0.8148, Time 00:00:31,lr 0.01\n",
      "epoch 14, loss 0.66272, train_acc 0.7717, valid_acc 0.7799, Time 00:00:31,lr 0.01\n",
      "epoch 15, loss 0.64879, train_acc 0.7773, valid_acc 0.8168, Time 00:00:31,lr 0.01\n",
      "epoch 16, loss 0.65313, train_acc 0.7758, valid_acc 0.8103, Time 00:00:31,lr 0.01\n",
      "epoch 17, loss 0.64368, train_acc 0.7803, valid_acc 0.8027, Time 00:00:31,lr 0.01\n",
      "epoch 18, loss 0.64573, train_acc 0.7790, valid_acc 0.7865, Time 00:00:31,lr 0.01\n",
      "epoch 19, loss 0.63965, train_acc 0.7808, valid_acc 0.8113, Time 00:00:31,lr 0.01\n",
      "epoch 20, loss 0.49768, train_acc 0.8307, valid_acc 0.8597, Time 00:00:31,lr 0.002\n",
      "epoch 21, loss 0.45085, train_acc 0.8470, valid_acc 0.8730, Time 00:00:31,lr 0.002\n",
      "epoch 22, loss 0.43494, train_acc 0.8515, valid_acc 0.8760, Time 00:00:31,lr 0.002\n",
      "epoch 23, loss 0.42184, train_acc 0.8574, valid_acc 0.8801, Time 00:00:31,lr 0.002\n",
      "epoch 24, loss 0.41728, train_acc 0.8585, valid_acc 0.8719, Time 00:00:31,lr 0.002\n",
      "epoch 25, loss 0.41803, train_acc 0.8566, valid_acc 0.8713, Time 00:00:31,lr 0.002\n",
      "epoch 26, loss 0.41157, train_acc 0.8586, valid_acc 0.8734, Time 00:00:31,lr 0.002\n",
      "epoch 27, loss 0.40844, train_acc 0.8605, valid_acc 0.8761, Time 00:00:31,lr 0.002\n",
      "epoch 28, loss 0.40713, train_acc 0.8602, valid_acc 0.8781, Time 00:00:31,lr 0.002\n",
      "epoch 29, loss 0.40274, train_acc 0.8636, valid_acc 0.8810, Time 00:00:31,lr 0.002\n",
      "epoch 30, loss 0.34908, train_acc 0.8807, valid_acc 0.8912, Time 00:00:31,lr 0.0004\n",
      "epoch 31, loss 0.32949, train_acc 0.8883, valid_acc 0.8973, Time 00:00:31,lr 0.0004\n",
      "epoch 32, loss 0.31813, train_acc 0.8929, valid_acc 0.8957, Time 00:00:31,lr 0.0004\n",
      "epoch 33, loss 0.31530, train_acc 0.8933, valid_acc 0.8949, Time 00:00:31,lr 0.0004\n",
      "epoch 34, loss 0.30938, train_acc 0.8967, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 35, loss 0.30692, train_acc 0.8977, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 36, loss 0.30172, train_acc 0.8962, valid_acc 0.8979, Time 00:00:31,lr 0.0004\n",
      "epoch 37, loss 0.29754, train_acc 0.9001, valid_acc 0.8978, Time 00:00:31,lr 0.0004\n",
      "epoch 38, loss 0.29589, train_acc 0.8995, valid_acc 0.8993, Time 00:00:31,lr 0.0004\n",
      "epoch 39, loss 0.29325, train_acc 0.9014, valid_acc 0.9010, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = 10\n",
    "lr_decay=0.2\n",
    "log_file=None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug2\", ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:34.023597Z",
     "start_time": "2018-03-03T18:38:34.000689Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.save_params('../../models/resnet18_me_9_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
