{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T06:32:47.406027Z",
     "start_time": "2018-03-01T06:32:42.929058Z"
    },
    "collapsed": true
   },
   "source": [
    "# 0. baseline re-preduce\n",
    "\n",
    "1. 延后第一次（越靠近后面的延缓带来的效果增益越小）的down sample确实会很有效，对于resnet18的效果甚至超过模型宽度(通道数)翻倍的效果; 同时使用实验证明这个性能提升是延缓down sample带来的，而不是卷积核的padding或kernel_size带来的。\n",
    "2. 一组合适的参数保证网络的充分训练和收敛（num_epoch, lr_period, lr_decay), 同时发现过大的weight_decay也会影响（降低）网络的收敛速度和结果\n",
    "```\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. import needed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:26:39.467684Z",
     "start_time": "2018-03-05T06:26:38.956029Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.insert(0, '../../utils')\n",
    "from netlib import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data loader, data argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:26:39.482707Z",
     "start_time": "2018-03-05T06:26:39.469110Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data loader\n",
    "\"\"\"\n",
    "def _transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def data_loader(batch_size, transform_train, transform_test=None, num_workers=0):\n",
    "    if transform_train is None:\n",
    "        transform_train = _transform_train\n",
    "    if transform_test is None:\n",
    "        transform_test = _transform_test\n",
    "        \n",
    "    # flag=1 mean 3 channel image\n",
    "    train_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=True, transform=transform_train)\n",
    "    test_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=False, transform=transform_test)\n",
    "\n",
    "    loader = gluon.data.DataLoader\n",
    "    train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep', num_workers=num_workers)\n",
    "    test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep', num_workers=num_workers)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:26:39.644481Z",
     "start_time": "2018-03-05T06:26:39.587554Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data argument\n",
    "\"\"\"\n",
    "def transform_train_DA1(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def transform_train_DA2(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "    \n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "    \n",
    "\n",
    "random_clip_rate = 0.3\n",
    "def transform_train_DA3(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "#                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "#                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "        \n",
    "    if random.random() > random_clip_rate:\n",
    "        im = im.clip(0, 1)\n",
    "    _aug = image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                   std=np.array([0.2023, 0.1994, 0.2010]),)\n",
    "    im = _aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 data aurgument: mixup\n",
    "1. mixup define\n",
    "2. mixup visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 mixup: define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:26:40.701844Z",
     "start_time": "2018-03-05T06:26:40.691026Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mixup(x1, y1, x2, y2, alpha, num_class):\n",
    "    y1 = nd.one_hot(y1, num_class)\n",
    "    y2 = nd.one_hot(y2, num_class)\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    x = lam * x1 + (1 - lam) * x2\n",
    "    y = lam * y1 + (1 - lam) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 mixup: visulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:26:42.435030Z",
     "start_time": "2018-03-05T06:26:41.546109Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from time import time\n",
    "batch_size = 32\n",
    "transform_train = _transform_test#transform_train_DA1\n",
    "train_data, test_data = data_loader(batch_size, transform_train)\n",
    "mixup_alpha = 1\n",
    "\n",
    "# for x1, y1 in train_data:\n",
    "#     for x2, y2 in mixup_train_data:\n",
    "#         data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "for x, y in train_data:\n",
    "    l = x.shape[0] / 2\n",
    "    data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:26:43.042206Z",
     "start_time": "2018-03-05T06:26:42.538964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAADYCAYAAACwTgnaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f9648b950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'airplane'), (1, 'automobile'), (2, 'bird'), (3, 'cat'), (4, 'deer'), (5, 'dog'), (6, 'frog'), (7, 'horse'), (8, 'ship'), (9, 'truck')]\n",
      "\n",
      "[[[ 0.          0.          0.          0.10898913  0.          0.89101088\n",
      "    0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          1.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.89101088  0.          0.          0.          0.          0.          0.\n",
      "    0.10898913  0.          0.        ]]\n",
      "\n",
      " [[ 0.89101088  0.          0.          0.          0.10898913  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.          0.\n",
      "    0.89101088  0.          0.10898913]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          1.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          1.        ]]\n",
      "\n",
      " [[ 0.10898913  0.          0.89101088  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.10898913  0.          0.          0.89101088  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.89101088  0.10898913  0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]]\n",
      "<NDArray 10x1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from cifar10_utils import show_images\n",
    "%matplotlib inline\n",
    "mean=np.array([0.4914, 0.4822, 0.4465])\n",
    "std=np.array([0.2023, 0.1994, 0.2010])\n",
    "images = data[:10].transpose((0, 2, 3, 1)).asnumpy()\n",
    "images = images * std + mean\n",
    "images = images.transpose((0, 3, 1, 2)) * 255\n",
    "show_images(images)\n",
    "#show_images(data[:9], rgb_mean=mean*255, std=std*255)\n",
    "\n",
    "print [(i, l) for i, l in enumerate(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])]\n",
    "print label[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 mixup: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:26:44.435764Z",
     "start_time": "2018-03-05T06:26:44.394436Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "mixup_test = False\n",
    "\n",
    "if mixup_test:\n",
    "    net = ResNet164_v2(10)\n",
    "    net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    loss_f = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "    num_epochs = 1\n",
    "    learning_rate = 0.1\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    cur_time = time()\n",
    "    iters = 0\n",
    "    \"\"\"\n",
    "    data loader first time run will cost about 3x time than after run.\n",
    "    \"\"\"\n",
    "    for x1, y1 in train_data:\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time\n",
    "        iters += 1\n",
    "    print \"cost time:\", time() - cur_time\n",
    "    print\n",
    "    cur_time = time()\n",
    "\n",
    "    iters = 0\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'momentum': 0.9, 'wd': 1e-4})\n",
    "    for x, y in train_data:\n",
    "        l = x.shape[0] / 2\n",
    "        data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label.as_in_context(ctx))\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time, nd.mean(loss).asscalar()\n",
    "        iters += 1\n",
    "    print iters, time() - cur_time\n",
    "\n",
    "    # load data one by one batch\n",
    "    # iters = 0\n",
    "    # for x1, y1 in train_data:\n",
    "    #     for x2, y2 in mixup_train_data:\n",
    "    #         data, label = mixup(x1, y1, x2[:x1.shape[0]], y2[:y1.shape[0]], mixup_alpha, 10)\n",
    "    #         break\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time\n",
    "    #     iters += 1\n",
    "    # print \"cost time:\", time() - cur_time\n",
    "    # print\n",
    "    # cur_time = time()\n",
    "\n",
    "    # zip will load all datas and then iterate them, too cost memory, will drop speed when memory over.\n",
    "    # iters = 0\n",
    "    # for (x1, y1), (x2, y2) in zip(train_data, mixup_train_data):\n",
    "    #     data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time#, nd.mean(loss).asscalar()\n",
    "    #     iters += 1\n",
    "    # print time() - cur_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:26:45.602040Z",
     "start_time": "2018-03-05T06:26:45.508872Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def abs_mean(W):\n",
    "    return nd.mean(nd.abs(W)).asscalar()\n",
    "\n",
    "def in_list(e, l):\n",
    "    for i in l:\n",
    "        if i == e:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, \n",
    "          lr_decay, wd, ctx, w_key, output_file=None, verbose=False, loss_f=gluon.loss.SoftmaxCrossEntropyLoss(), \n",
    "          use_mixup=False, mixup_alpha=0.2):\n",
    "    def train_batch(data, label, i):\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        _loss = nd.mean(loss).asscalar()\n",
    "        if not use_mixup:\n",
    "            _acc = utils.accuracy(output, label)\n",
    "        else:\n",
    "            _acc = None\n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            print \" # iter\", i,\n",
    "            print \"loss %.5f\" % _loss, \n",
    "            if not use_mixup: print \"acc %.5f\" % _acc,\n",
    "            print \"w (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.data()),\n",
    "            print \") g (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.grad()),\n",
    "            print \")\"\n",
    "        return _loss, _acc\n",
    "            \n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        stdout = sys.stdout\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = output_file\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    if verbose:\n",
    "        print \" #\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "    \n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if in_list(epoch, lr_period):\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            \n",
    "        if not use_mixup:\n",
    "            for data, label in train_data:\n",
    "                _loss, _acc = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                train_acc += _acc\n",
    "                i += 1\n",
    "        else:\n",
    "            for x, y in train_data:\n",
    "                l = x.shape[0] / 2\n",
    "                data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "                _loss, _ = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                i += 1\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        if train_acc < 1e-6:\n",
    "            train_acc = utils.evaluate_accuracy(train_data, net, ctx)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "    if output_file != stdout:\n",
    "        sys.stdout = stdout\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. get net and do EXP\n",
    "```\n",
    "I want to find out the reason that resnet18_v2 get fail baseline in CIFAR10_train2_* script, design follow exp.\n",
    "\n",
    "1. 5.1 train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs.\n",
    "2. 5.2 train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18.\n",
    " there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 re-exp resnet18_v1\n",
    "train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T16:30:21.462308Z",
     "start_time": "2018-03-03T16:30:13.299911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932408146965\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(10)\n",
    "net.load_params('../../models/res18_9', ctx=ctx)\n",
    "valid_acc = utils.evaluate_accuracy(test_data, net, ctx)\n",
    "print valid_acc\n",
    "def get_net(ctx):\n",
    "    num_outputs = 10\n",
    "    net = ResNet(num_outputs)\n",
    "    net.collect_params().initialize(init=init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    return net  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T10:46:50.521728Z",
     "start_time": "2018-03-03T10:05:27.156785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.80346, train_acc 0.3266, valid_acc 0.3598, Time 00:00:28,lr 0.1\n",
      "epoch 1, loss 1.28281, train_acc 0.5411, valid_acc 0.5778, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.01865, train_acc 0.6431, valid_acc 0.6533, Time 00:00:30,lr 0.1\n",
      "epoch 3, loss 0.90869, train_acc 0.6870, valid_acc 0.5823, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 0.85972, train_acc 0.7049, valid_acc 0.6255, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 0.82922, train_acc 0.7158, valid_acc 0.6503, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 0.81416, train_acc 0.7211, valid_acc 0.6621, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 0.79129, train_acc 0.7286, valid_acc 0.6731, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.79221, train_acc 0.7293, valid_acc 0.6960, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 0.77941, train_acc 0.7357, valid_acc 0.7085, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.77304, train_acc 0.7350, valid_acc 0.6773, Time 00:00:33,lr 0.1\n",
      "epoch 11, loss 0.77300, train_acc 0.7362, valid_acc 0.6708, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 0.77379, train_acc 0.7359, valid_acc 0.7297, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.76346, train_acc 0.7400, valid_acc 0.6792, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.75981, train_acc 0.7400, valid_acc 0.7052, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 0.76189, train_acc 0.7409, valid_acc 0.7065, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 0.75300, train_acc 0.7431, valid_acc 0.7030, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 0.74843, train_acc 0.7451, valid_acc 0.7334, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 0.75400, train_acc 0.7439, valid_acc 0.6873, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 0.74822, train_acc 0.7453, valid_acc 0.6994, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 0.74226, train_acc 0.7492, valid_acc 0.6627, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 0.74857, train_acc 0.7449, valid_acc 0.6691, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 0.73817, train_acc 0.7513, valid_acc 0.6690, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.74331, train_acc 0.7469, valid_acc 0.7008, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 0.74029, train_acc 0.7473, valid_acc 0.6862, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 0.73600, train_acc 0.7491, valid_acc 0.7335, Time 00:00:30,lr 0.1\n",
      "epoch 26, loss 0.73561, train_acc 0.7486, valid_acc 0.7344, Time 00:00:30,lr 0.1\n",
      "epoch 27, loss 0.73582, train_acc 0.7491, valid_acc 0.5638, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.73652, train_acc 0.7464, valid_acc 0.5980, Time 00:00:30,lr 0.1\n",
      "epoch 29, loss 0.73622, train_acc 0.7506, valid_acc 0.7386, Time 00:00:30,lr 0.1\n",
      "epoch 30, loss 0.73189, train_acc 0.7487, valid_acc 0.6339, Time 00:00:30,lr 0.1\n",
      "epoch 31, loss 0.73360, train_acc 0.7494, valid_acc 0.6679, Time 00:00:30,lr 0.1\n",
      "epoch 32, loss 0.73711, train_acc 0.7486, valid_acc 0.6363, Time 00:00:30,lr 0.1\n",
      "epoch 33, loss 0.73315, train_acc 0.7495, valid_acc 0.6735, Time 00:00:30,lr 0.1\n",
      "epoch 34, loss 0.73185, train_acc 0.7496, valid_acc 0.7033, Time 00:00:30,lr 0.1\n",
      "epoch 35, loss 0.73066, train_acc 0.7499, valid_acc 0.7409, Time 00:00:30,lr 0.1\n",
      "epoch 36, loss 0.73286, train_acc 0.7492, valid_acc 0.6494, Time 00:00:30,lr 0.1\n",
      "epoch 37, loss 0.72791, train_acc 0.7502, valid_acc 0.6779, Time 00:00:30,lr 0.1\n",
      "epoch 38, loss 0.73055, train_acc 0.7519, valid_acc 0.6367, Time 00:00:30,lr 0.1\n",
      "epoch 39, loss 0.73307, train_acc 0.7487, valid_acc 0.7078, Time 00:00:30,lr 0.1\n",
      "epoch 40, loss 0.72583, train_acc 0.7522, valid_acc 0.6997, Time 00:00:30,lr 0.1\n",
      "epoch 41, loss 0.73065, train_acc 0.7513, valid_acc 0.6822, Time 00:00:30,lr 0.1\n",
      "epoch 42, loss 0.72384, train_acc 0.7533, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 43, loss 0.73071, train_acc 0.7507, valid_acc 0.6380, Time 00:00:30,lr 0.1\n",
      "epoch 44, loss 0.73206, train_acc 0.7511, valid_acc 0.6253, Time 00:00:30,lr 0.1\n",
      "epoch 45, loss 0.72313, train_acc 0.7533, valid_acc 0.6761, Time 00:00:30,lr 0.1\n",
      "epoch 46, loss 0.72663, train_acc 0.7531, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.72931, train_acc 0.7500, valid_acc 0.6451, Time 00:00:30,lr 0.1\n",
      "epoch 48, loss 0.72035, train_acc 0.7536, valid_acc 0.6857, Time 00:00:30,lr 0.1\n",
      "epoch 49, loss 0.73334, train_acc 0.7483, valid_acc 0.6765, Time 00:00:30,lr 0.1\n",
      "epoch 50, loss 0.72158, train_acc 0.7515, valid_acc 0.7363, Time 00:00:30,lr 0.1\n",
      "epoch 51, loss 0.72392, train_acc 0.7522, valid_acc 0.6832, Time 00:00:30,lr 0.1\n",
      "epoch 52, loss 0.72609, train_acc 0.7512, valid_acc 0.7577, Time 00:00:30,lr 0.1\n",
      "epoch 53, loss 0.72894, train_acc 0.7522, valid_acc 0.7357, Time 00:00:30,lr 0.1\n",
      "epoch 54, loss 0.72575, train_acc 0.7526, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 55, loss 0.72233, train_acc 0.7526, valid_acc 0.6178, Time 00:00:30,lr 0.1\n",
      "epoch 56, loss 0.72252, train_acc 0.7532, valid_acc 0.7112, Time 00:00:30,lr 0.1\n",
      "epoch 57, loss 0.72007, train_acc 0.7541, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 58, loss 0.72001, train_acc 0.7533, valid_acc 0.6893, Time 00:00:30,lr 0.1\n",
      "epoch 59, loss 0.72442, train_acc 0.7541, valid_acc 0.7461, Time 00:00:30,lr 0.1\n",
      "epoch 60, loss 0.72184, train_acc 0.7506, valid_acc 0.6913, Time 00:00:30,lr 0.1\n",
      "epoch 61, loss 0.72317, train_acc 0.7547, valid_acc 0.6189, Time 00:00:30,lr 0.1\n",
      "epoch 62, loss 0.72607, train_acc 0.7537, valid_acc 0.7175, Time 00:00:30,lr 0.1\n",
      "epoch 63, loss 0.71946, train_acc 0.7564, valid_acc 0.5596, Time 00:00:30,lr 0.1\n",
      "epoch 64, loss 0.72751, train_acc 0.7509, valid_acc 0.7269, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.72550, train_acc 0.7516, valid_acc 0.6568, Time 00:00:30,lr 0.1\n",
      "epoch 66, loss 0.72351, train_acc 0.7542, valid_acc 0.6292, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.72099, train_acc 0.7540, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 68, loss 0.72052, train_acc 0.7541, valid_acc 0.6509, Time 00:00:30,lr 0.1\n",
      "epoch 69, loss 0.71841, train_acc 0.7530, valid_acc 0.6510, Time 00:00:30,lr 0.1\n",
      "epoch 70, loss 0.72424, train_acc 0.7510, valid_acc 0.6668, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.71802, train_acc 0.7542, valid_acc 0.6872, Time 00:00:30,lr 0.1\n",
      "epoch 72, loss 0.72042, train_acc 0.7525, valid_acc 0.7391, Time 00:00:30,lr 0.1\n",
      "epoch 73, loss 0.72243, train_acc 0.7534, valid_acc 0.6784, Time 00:00:30,lr 0.1\n",
      "epoch 74, loss 0.72777, train_acc 0.7511, valid_acc 0.6956, Time 00:00:30,lr 0.1\n",
      "epoch 75, loss 0.72433, train_acc 0.7536, valid_acc 0.7030, Time 00:00:30,lr 0.1\n",
      "epoch 76, loss 0.72253, train_acc 0.7537, valid_acc 0.6999, Time 00:00:30,lr 0.1\n",
      "epoch 77, loss 0.72566, train_acc 0.7514, valid_acc 0.6808, Time 00:00:30,lr 0.1\n",
      "epoch 78, loss 0.72288, train_acc 0.7538, valid_acc 0.7191, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 0.72042, train_acc 0.7558, valid_acc 0.6245, Time 00:00:30,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.748897Z",
     "start_time": "2018-03-03T16:30:22.024115Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.61285, train_acc 0.7915, valid_acc 0.7278, Time 00:00:31,lr 0.05\n",
      "epoch 1, loss 0.68711, train_acc 0.7663, valid_acc 0.6986, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.70354, train_acc 0.7601, valid_acc 0.7469, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.70402, train_acc 0.7604, valid_acc 0.6044, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.70819, train_acc 0.7573, valid_acc 0.7099, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.71752, train_acc 0.7569, valid_acc 0.7387, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.71241, train_acc 0.7581, valid_acc 0.7282, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.71346, train_acc 0.7582, valid_acc 0.7717, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.71035, train_acc 0.7555, valid_acc 0.7430, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.71255, train_acc 0.7570, valid_acc 0.7069, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.71683, train_acc 0.7539, valid_acc 0.5970, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.71341, train_acc 0.7572, valid_acc 0.7481, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.71346, train_acc 0.7568, valid_acc 0.7049, Time 00:00:38,lr 0.05\n",
      "epoch 13, loss 0.70515, train_acc 0.7603, valid_acc 0.7366, Time 00:00:40,lr 0.05\n",
      "epoch 14, loss 0.71341, train_acc 0.7570, valid_acc 0.6632, Time 00:00:35,lr 0.05\n",
      "epoch 15, loss 0.70901, train_acc 0.7593, valid_acc 0.6943, Time 00:00:39,lr 0.05\n",
      "epoch 16, loss 0.71456, train_acc 0.7563, valid_acc 0.6658, Time 00:00:41,lr 0.05\n",
      "epoch 17, loss 0.71572, train_acc 0.7575, valid_acc 0.6613, Time 00:00:34,lr 0.05\n",
      "epoch 18, loss 0.71535, train_acc 0.7565, valid_acc 0.6999, Time 00:00:37,lr 0.05\n",
      "epoch 19, loss 0.70597, train_acc 0.7611, valid_acc 0.7358, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.71559, train_acc 0.7546, valid_acc 0.6635, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.71545, train_acc 0.7573, valid_acc 0.6657, Time 00:00:32,lr 0.05\n",
      "epoch 22, loss 0.71076, train_acc 0.7595, valid_acc 0.7046, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.71619, train_acc 0.7562, valid_acc 0.6314, Time 00:00:33,lr 0.05\n",
      "epoch 24, loss 0.70736, train_acc 0.7596, valid_acc 0.7478, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.70959, train_acc 0.7582, valid_acc 0.6643, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.71528, train_acc 0.7543, valid_acc 0.6441, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.72031, train_acc 0.7560, valid_acc 0.7425, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.71745, train_acc 0.7578, valid_acc 0.7464, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.70926, train_acc 0.7598, valid_acc 0.6095, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.46802, train_acc 0.8419, valid_acc 0.8559, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.42038, train_acc 0.8549, valid_acc 0.8552, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.41659, train_acc 0.8567, valid_acc 0.8418, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.41601, train_acc 0.8581, valid_acc 0.8321, Time 00:00:33,lr 0.01\n",
      "epoch 34, loss 0.41853, train_acc 0.8580, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.42103, train_acc 0.8563, valid_acc 0.8627, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.41386, train_acc 0.8588, valid_acc 0.8500, Time 00:00:31,lr 0.01\n",
      "epoch 37, loss 0.41497, train_acc 0.8594, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.40632, train_acc 0.8621, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.40439, train_acc 0.8627, valid_acc 0.8521, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.40516, train_acc 0.8633, valid_acc 0.8540, Time 00:00:31,lr 0.01\n",
      "epoch 41, loss 0.40254, train_acc 0.8627, valid_acc 0.8626, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.39611, train_acc 0.8632, valid_acc 0.8397, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.39748, train_acc 0.8639, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.39310, train_acc 0.8676, valid_acc 0.8517, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.38917, train_acc 0.8680, valid_acc 0.8554, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.38661, train_acc 0.8681, valid_acc 0.8591, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.38413, train_acc 0.8689, valid_acc 0.8555, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.37997, train_acc 0.8711, valid_acc 0.8538, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.38270, train_acc 0.8692, valid_acc 0.8484, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.38268, train_acc 0.8702, valid_acc 0.8478, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.38004, train_acc 0.8712, valid_acc 0.8528, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.37946, train_acc 0.8712, valid_acc 0.8603, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.37720, train_acc 0.8728, valid_acc 0.8533, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.37159, train_acc 0.8733, valid_acc 0.8092, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.37223, train_acc 0.8734, valid_acc 0.8453, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.37370, train_acc 0.8742, valid_acc 0.8661, Time 00:00:31,lr 0.01\n",
      "epoch 57, loss 0.37211, train_acc 0.8736, valid_acc 0.8377, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.37039, train_acc 0.8741, valid_acc 0.8310, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.37287, train_acc 0.8743, valid_acc 0.8420, Time 00:00:31,lr 0.01\n",
      "epoch 60, loss 0.23431, train_acc 0.9229, valid_acc 0.9057, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.19372, train_acc 0.9352, valid_acc 0.9090, Time 00:00:31,lr 0.002\n",
      "epoch 62, loss 0.17911, train_acc 0.9395, valid_acc 0.9086, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.17003, train_acc 0.9431, valid_acc 0.9157, Time 00:00:31,lr 0.002\n",
      "epoch 64, loss 0.16376, train_acc 0.9459, valid_acc 0.9081, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.15820, train_acc 0.9475, valid_acc 0.9153, Time 00:00:32,lr 0.002\n",
      "epoch 66, loss 0.15374, train_acc 0.9485, valid_acc 0.9127, Time 00:00:32,lr 0.002\n",
      "epoch 67, loss 0.15401, train_acc 0.9480, valid_acc 0.9136, Time 00:00:31,lr 0.002\n",
      "epoch 68, loss 0.15006, train_acc 0.9496, valid_acc 0.9137, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.15038, train_acc 0.9497, valid_acc 0.9058, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.15246, train_acc 0.9476, valid_acc 0.9106, Time 00:00:31,lr 0.002\n",
      "epoch 71, loss 0.15303, train_acc 0.9474, valid_acc 0.9042, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.15150, train_acc 0.9477, valid_acc 0.9030, Time 00:00:31,lr 0.002\n",
      "epoch 73, loss 0.15295, train_acc 0.9472, valid_acc 0.9058, Time 00:00:32,lr 0.002\n",
      "epoch 74, loss 0.15067, train_acc 0.9496, valid_acc 0.9097, Time 00:00:31,lr 0.002\n",
      "epoch 75, loss 0.15562, train_acc 0.9473, valid_acc 0.9034, Time 00:00:31,lr 0.002\n",
      "epoch 76, loss 0.15555, train_acc 0.9475, valid_acc 0.9029, Time 00:00:31,lr 0.002\n",
      "epoch 77, loss 0.15436, train_acc 0.9484, valid_acc 0.9031, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.15387, train_acc 0.9485, valid_acc 0.9032, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.15457, train_acc 0.9481, valid_acc 0.9065, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.15198, train_acc 0.9484, valid_acc 0.9091, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.15804, train_acc 0.9467, valid_acc 0.9018, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.15374, train_acc 0.9477, valid_acc 0.9049, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.15341, train_acc 0.9491, valid_acc 0.9067, Time 00:00:32,lr 0.002\n",
      "epoch 84, loss 0.15361, train_acc 0.9499, valid_acc 0.9019, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.15058, train_acc 0.9491, valid_acc 0.8981, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.15706, train_acc 0.9464, valid_acc 0.9073, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.15415, train_acc 0.9483, valid_acc 0.9021, Time 00:00:31,lr 0.002\n",
      "epoch 88, loss 0.14994, train_acc 0.9494, valid_acc 0.8976, Time 00:00:32,lr 0.002\n",
      "epoch 89, loss 0.15133, train_acc 0.9490, valid_acc 0.8961, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.08904, train_acc 0.9723, valid_acc 0.9240, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.06755, train_acc 0.9796, valid_acc 0.9256, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.05880, train_acc 0.9830, valid_acc 0.9272, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.05345, train_acc 0.9850, valid_acc 0.9282, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.05022, train_acc 0.9862, valid_acc 0.9306, Time 00:00:31,lr 0.0004\n",
      "epoch 95, loss 0.04724, train_acc 0.9864, valid_acc 0.9297, Time 00:00:31,lr 0.0004\n",
      "epoch 96, loss 0.04417, train_acc 0.9878, valid_acc 0.9303, Time 00:00:31,lr 0.0004\n",
      "epoch 97, loss 0.04230, train_acc 0.9886, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.04097, train_acc 0.9887, valid_acc 0.9272, Time 00:00:32,lr 0.0004\n",
      "epoch 99, loss 0.03953, train_acc 0.9899, valid_acc 0.9291, Time 00:00:31,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.03685, train_acc 0.9899, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.03604, train_acc 0.9903, valid_acc 0.9277, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.03277, train_acc 0.9917, valid_acc 0.9261, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.03316, train_acc 0.9913, valid_acc 0.9253, Time 00:00:32,lr 0.0004\n",
      "epoch 104, loss 0.03211, train_acc 0.9920, valid_acc 0.9288, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.03214, train_acc 0.9913, valid_acc 0.9277, Time 00:00:32,lr 0.0004\n",
      "epoch 106, loss 0.03095, train_acc 0.9922, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 107, loss 0.03007, train_acc 0.9922, valid_acc 0.9275, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.02834, train_acc 0.9926, valid_acc 0.9271, Time 00:00:31,lr 0.0004\n",
      "epoch 109, loss 0.03008, train_acc 0.9920, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.02855, train_acc 0.9928, valid_acc 0.9279, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.02935, train_acc 0.9926, valid_acc 0.9269, Time 00:00:32,lr 0.0004\n",
      "epoch 112, loss 0.02706, train_acc 0.9931, valid_acc 0.9291, Time 00:00:32,lr 0.0004\n",
      "epoch 113, loss 0.02690, train_acc 0.9933, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.02585, train_acc 0.9932, valid_acc 0.9263, Time 00:00:31,lr 0.0004\n",
      "epoch 115, loss 0.02478, train_acc 0.9938, valid_acc 0.9280, Time 00:00:31,lr 0.0004\n",
      "epoch 116, loss 0.02669, train_acc 0.9930, valid_acc 0.9264, Time 00:00:31,lr 0.0004\n",
      "epoch 117, loss 0.02483, train_acc 0.9937, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 118, loss 0.02543, train_acc 0.9936, valid_acc 0.9281, Time 00:00:31,lr 0.0004\n",
      "epoch 119, loss 0.02483, train_acc 0.9939, valid_acc 0.9300, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_me_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 use gluon renset18_v1\n",
    "train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:09.871178Z",
     "start_time": "2018-03-04T08:31:09.864612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and add must in name_scope, or may got name error\n",
    "def get_resnet18_v1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features)\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T01:41:50.461216Z",
     "start_time": "2018-03-04T01:08:08.533086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.51019, train_acc 0.2129, valid_acc 0.3261, Time 00:00:27,lr 0.1\n",
      "epoch 1, loss 1.75673, train_acc 0.3417, valid_acc 0.4151, Time 00:00:25,lr 0.1\n",
      "epoch 2, loss 1.60616, train_acc 0.4121, valid_acc 0.4824, Time 00:00:24,lr 0.1\n",
      "epoch 3, loss 1.49376, train_acc 0.4615, valid_acc 0.4921, Time 00:00:24,lr 0.1\n",
      "epoch 4, loss 1.42743, train_acc 0.4928, valid_acc 0.4964, Time 00:00:25,lr 0.1\n",
      "epoch 5, loss 1.37983, train_acc 0.5144, valid_acc 0.5435, Time 00:00:24,lr 0.1\n",
      "epoch 6, loss 1.36243, train_acc 0.5234, valid_acc 0.5876, Time 00:00:25,lr 0.1\n",
      "epoch 7, loss 1.33671, train_acc 0.5313, valid_acc 0.5255, Time 00:00:25,lr 0.1\n",
      "epoch 8, loss 1.31173, train_acc 0.5461, valid_acc 0.5368, Time 00:00:24,lr 0.1\n",
      "epoch 9, loss 1.27490, train_acc 0.5587, valid_acc 0.5911, Time 00:00:24,lr 0.1\n",
      "epoch 10, loss 1.27032, train_acc 0.5597, valid_acc 0.6087, Time 00:00:28,lr 0.1\n",
      "epoch 11, loss 1.25436, train_acc 0.5670, valid_acc 0.5886, Time 00:00:24,lr 0.1\n",
      "epoch 12, loss 1.24745, train_acc 0.5690, valid_acc 0.5414, Time 00:00:27,lr 0.1\n",
      "epoch 13, loss 1.23173, train_acc 0.5756, valid_acc 0.5944, Time 00:00:25,lr 0.1\n",
      "epoch 14, loss 1.22901, train_acc 0.5782, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 15, loss 1.22526, train_acc 0.5812, valid_acc 0.5576, Time 00:00:24,lr 0.1\n",
      "epoch 16, loss 1.22470, train_acc 0.5828, valid_acc 0.5957, Time 00:00:24,lr 0.1\n",
      "epoch 17, loss 1.21181, train_acc 0.5865, valid_acc 0.6237, Time 00:00:25,lr 0.1\n",
      "epoch 18, loss 1.21252, train_acc 0.5851, valid_acc 0.5971, Time 00:00:25,lr 0.1\n",
      "epoch 19, loss 1.20350, train_acc 0.5899, valid_acc 0.6076, Time 00:00:25,lr 0.1\n",
      "epoch 20, loss 1.20077, train_acc 0.5904, valid_acc 0.6148, Time 00:00:25,lr 0.1\n",
      "epoch 21, loss 1.19665, train_acc 0.5925, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 22, loss 1.19718, train_acc 0.5942, valid_acc 0.5745, Time 00:00:25,lr 0.1\n",
      "epoch 23, loss 1.19424, train_acc 0.5912, valid_acc 0.5711, Time 00:00:25,lr 0.1\n",
      "epoch 24, loss 1.19114, train_acc 0.5953, valid_acc 0.6069, Time 00:00:25,lr 0.1\n",
      "epoch 25, loss 1.19080, train_acc 0.5965, valid_acc 0.6262, Time 00:00:25,lr 0.1\n",
      "epoch 26, loss 1.19219, train_acc 0.5964, valid_acc 0.6112, Time 00:00:25,lr 0.1\n",
      "epoch 27, loss 1.18217, train_acc 0.5991, valid_acc 0.5783, Time 00:00:25,lr 0.1\n",
      "epoch 28, loss 1.19244, train_acc 0.5922, valid_acc 0.6090, Time 00:00:25,lr 0.1\n",
      "epoch 29, loss 1.18442, train_acc 0.5964, valid_acc 0.6002, Time 00:00:27,lr 0.1\n",
      "epoch 30, loss 1.18418, train_acc 0.5984, valid_acc 0.5866, Time 00:00:25,lr 0.1\n",
      "epoch 31, loss 1.18398, train_acc 0.5969, valid_acc 0.5779, Time 00:00:25,lr 0.1\n",
      "epoch 32, loss 1.18966, train_acc 0.5938, valid_acc 0.6059, Time 00:00:25,lr 0.1\n",
      "epoch 33, loss 1.19080, train_acc 0.5951, valid_acc 0.6048, Time 00:00:25,lr 0.1\n",
      "epoch 34, loss 1.18026, train_acc 0.5991, valid_acc 0.6034, Time 00:00:25,lr 0.1\n",
      "epoch 35, loss 1.18109, train_acc 0.5999, valid_acc 0.6194, Time 00:00:25,lr 0.1\n",
      "epoch 36, loss 1.18156, train_acc 0.6008, valid_acc 0.5972, Time 00:00:25,lr 0.1\n",
      "epoch 37, loss 1.19249, train_acc 0.5947, valid_acc 0.5651, Time 00:00:25,lr 0.1\n",
      "epoch 38, loss 1.18308, train_acc 0.5992, valid_acc 0.6097, Time 00:00:25,lr 0.1\n",
      "epoch 39, loss 1.18060, train_acc 0.6001, valid_acc 0.6363, Time 00:00:25,lr 0.1\n",
      "epoch 40, loss 1.17950, train_acc 0.6004, valid_acc 0.6394, Time 00:00:25,lr 0.1\n",
      "epoch 41, loss 1.18485, train_acc 0.5964, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 42, loss 1.18305, train_acc 0.5999, valid_acc 0.6338, Time 00:00:25,lr 0.1\n",
      "epoch 43, loss 1.18409, train_acc 0.5985, valid_acc 0.6278, Time 00:00:24,lr 0.1\n",
      "epoch 44, loss 1.17604, train_acc 0.5990, valid_acc 0.6171, Time 00:00:25,lr 0.1\n",
      "epoch 45, loss 1.18259, train_acc 0.5979, valid_acc 0.6266, Time 00:00:25,lr 0.1\n",
      "epoch 46, loss 1.17980, train_acc 0.6023, valid_acc 0.5782, Time 00:00:25,lr 0.1\n",
      "epoch 47, loss 1.18290, train_acc 0.5980, valid_acc 0.6226, Time 00:00:25,lr 0.1\n",
      "epoch 48, loss 1.17870, train_acc 0.5994, valid_acc 0.5915, Time 00:00:25,lr 0.1\n",
      "epoch 49, loss 1.17969, train_acc 0.5983, valid_acc 0.5877, Time 00:00:24,lr 0.1\n",
      "epoch 50, loss 1.18743, train_acc 0.5986, valid_acc 0.6399, Time 00:00:25,lr 0.1\n",
      "epoch 51, loss 1.17115, train_acc 0.6031, valid_acc 0.5693, Time 00:00:25,lr 0.1\n",
      "epoch 52, loss 1.16321, train_acc 0.6042, valid_acc 0.6509, Time 00:00:24,lr 0.1\n",
      "epoch 53, loss 1.17745, train_acc 0.6004, valid_acc 0.6119, Time 00:00:25,lr 0.1\n",
      "epoch 54, loss 1.18013, train_acc 0.5981, valid_acc 0.5628, Time 00:00:25,lr 0.1\n",
      "epoch 55, loss 1.17807, train_acc 0.5997, valid_acc 0.5863, Time 00:00:26,lr 0.1\n",
      "epoch 56, loss 1.18230, train_acc 0.6013, valid_acc 0.6077, Time 00:00:26,lr 0.1\n",
      "epoch 57, loss 1.17381, train_acc 0.6016, valid_acc 0.6339, Time 00:00:24,lr 0.1\n",
      "epoch 58, loss 1.17931, train_acc 0.5979, valid_acc 0.6215, Time 00:00:24,lr 0.1\n",
      "epoch 59, loss 1.17566, train_acc 0.6000, valid_acc 0.5958, Time 00:00:24,lr 0.1\n",
      "epoch 60, loss 1.16982, train_acc 0.6028, valid_acc 0.6310, Time 00:00:25,lr 0.1\n",
      "epoch 61, loss 1.17248, train_acc 0.6016, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 62, loss 1.17132, train_acc 0.6028, valid_acc 0.6392, Time 00:00:24,lr 0.1\n",
      "epoch 63, loss 1.16930, train_acc 0.6059, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 64, loss 1.17535, train_acc 0.6018, valid_acc 0.6257, Time 00:00:24,lr 0.1\n",
      "epoch 65, loss 1.16613, train_acc 0.6031, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 66, loss 1.17103, train_acc 0.6037, valid_acc 0.5942, Time 00:00:24,lr 0.1\n",
      "epoch 67, loss 1.17368, train_acc 0.6013, valid_acc 0.6163, Time 00:00:25,lr 0.1\n",
      "epoch 68, loss 1.17817, train_acc 0.5984, valid_acc 0.5712, Time 00:00:27,lr 0.1\n",
      "epoch 69, loss 1.18750, train_acc 0.5986, valid_acc 0.6246, Time 00:00:24,lr 0.1\n",
      "epoch 70, loss 1.17631, train_acc 0.6010, valid_acc 0.6244, Time 00:00:24,lr 0.1\n",
      "epoch 71, loss 1.18138, train_acc 0.5989, valid_acc 0.5719, Time 00:00:24,lr 0.1\n",
      "epoch 72, loss 1.17125, train_acc 0.6019, valid_acc 0.6373, Time 00:00:24,lr 0.1\n",
      "epoch 73, loss 1.17880, train_acc 0.6007, valid_acc 0.6064, Time 00:00:24,lr 0.1\n",
      "epoch 74, loss 1.17069, train_acc 0.6043, valid_acc 0.6049, Time 00:00:24,lr 0.1\n",
      "epoch 75, loss 1.17566, train_acc 0.6009, valid_acc 0.5390, Time 00:00:24,lr 0.1\n",
      "epoch 76, loss 1.17658, train_acc 0.5984, valid_acc 0.6027, Time 00:00:24,lr 0.1\n",
      "epoch 77, loss 1.17818, train_acc 0.5994, valid_acc 0.6432, Time 00:00:24,lr 0.1\n",
      "epoch 78, loss 1.18056, train_acc 0.6011, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 79, loss 1.18418, train_acc 0.5995, valid_acc 0.6297, Time 00:00:24,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T02:39:40.682812Z",
     "start_time": "2018-03-04T01:49:21.279182Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.03273, train_acc 0.6475, valid_acc 0.6599, Time 00:00:24,lr 0.05\n",
      "epoch 1, loss 1.10147, train_acc 0.6258, valid_acc 0.6101, Time 00:00:24,lr 0.05\n",
      "epoch 2, loss 1.10971, train_acc 0.6221, valid_acc 0.6303, Time 00:00:24,lr 0.05\n",
      "epoch 3, loss 1.11335, train_acc 0.6201, valid_acc 0.6158, Time 00:00:24,lr 0.05\n",
      "epoch 4, loss 1.11669, train_acc 0.6186, valid_acc 0.6162, Time 00:00:24,lr 0.05\n",
      "epoch 5, loss 1.10626, train_acc 0.6214, valid_acc 0.6486, Time 00:00:24,lr 0.05\n",
      "epoch 6, loss 1.11353, train_acc 0.6192, valid_acc 0.6361, Time 00:00:24,lr 0.05\n",
      "epoch 7, loss 1.10059, train_acc 0.6249, valid_acc 0.6364, Time 00:00:24,lr 0.05\n",
      "epoch 8, loss 1.11230, train_acc 0.6210, valid_acc 0.6317, Time 00:00:24,lr 0.05\n",
      "epoch 9, loss 1.10704, train_acc 0.6200, valid_acc 0.5400, Time 00:00:24,lr 0.05\n",
      "epoch 10, loss 1.10685, train_acc 0.6223, valid_acc 0.6251, Time 00:00:24,lr 0.05\n",
      "epoch 11, loss 1.11229, train_acc 0.6209, valid_acc 0.6055, Time 00:00:24,lr 0.05\n",
      "epoch 12, loss 1.11007, train_acc 0.6232, valid_acc 0.6257, Time 00:00:25,lr 0.05\n",
      "epoch 13, loss 1.10097, train_acc 0.6244, valid_acc 0.6220, Time 00:00:25,lr 0.05\n",
      "epoch 14, loss 1.10608, train_acc 0.6263, valid_acc 0.5887, Time 00:00:24,lr 0.05\n",
      "epoch 15, loss 1.10504, train_acc 0.6224, valid_acc 0.6416, Time 00:00:24,lr 0.05\n",
      "epoch 16, loss 1.10434, train_acc 0.6223, valid_acc 0.6064, Time 00:00:24,lr 0.05\n",
      "epoch 17, loss 1.10511, train_acc 0.6221, valid_acc 0.6365, Time 00:00:24,lr 0.05\n",
      "epoch 18, loss 1.10933, train_acc 0.6206, valid_acc 0.6311, Time 00:00:24,lr 0.05\n",
      "epoch 19, loss 1.10533, train_acc 0.6211, valid_acc 0.6559, Time 00:00:25,lr 0.05\n",
      "epoch 20, loss 1.10336, train_acc 0.6231, valid_acc 0.6529, Time 00:00:24,lr 0.05\n",
      "epoch 21, loss 1.11074, train_acc 0.6202, valid_acc 0.6060, Time 00:00:25,lr 0.05\n",
      "epoch 22, loss 1.10799, train_acc 0.6211, valid_acc 0.6211, Time 00:00:24,lr 0.05\n",
      "epoch 23, loss 1.11143, train_acc 0.6213, valid_acc 0.6449, Time 00:00:24,lr 0.05\n",
      "epoch 24, loss 1.10708, train_acc 0.6227, valid_acc 0.6420, Time 00:00:24,lr 0.05\n",
      "epoch 25, loss 1.11583, train_acc 0.6182, valid_acc 0.6106, Time 00:00:24,lr 0.05\n",
      "epoch 26, loss 1.10713, train_acc 0.6237, valid_acc 0.6260, Time 00:00:25,lr 0.05\n",
      "epoch 27, loss 1.10516, train_acc 0.6226, valid_acc 0.6302, Time 00:00:24,lr 0.05\n",
      "epoch 28, loss 1.10940, train_acc 0.6245, valid_acc 0.6110, Time 00:00:24,lr 0.05\n",
      "epoch 29, loss 1.10750, train_acc 0.6217, valid_acc 0.5529, Time 00:00:24,lr 0.05\n",
      "epoch 30, loss 0.82673, train_acc 0.7164, valid_acc 0.7511, Time 00:00:25,lr 0.01\n",
      "epoch 31, loss 0.76458, train_acc 0.7374, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 32, loss 0.75552, train_acc 0.7419, valid_acc 0.7292, Time 00:00:25,lr 0.01\n",
      "epoch 33, loss 0.75799, train_acc 0.7418, valid_acc 0.7546, Time 00:00:25,lr 0.01\n",
      "epoch 34, loss 0.75636, train_acc 0.7434, valid_acc 0.7697, Time 00:00:24,lr 0.01\n",
      "epoch 35, loss 0.74936, train_acc 0.7466, valid_acc 0.7569, Time 00:00:24,lr 0.01\n",
      "epoch 36, loss 0.74819, train_acc 0.7459, valid_acc 0.7532, Time 00:00:25,lr 0.01\n",
      "epoch 37, loss 0.74411, train_acc 0.7466, valid_acc 0.7663, Time 00:00:24,lr 0.01\n",
      "epoch 38, loss 0.74887, train_acc 0.7453, valid_acc 0.7722, Time 00:00:24,lr 0.01\n",
      "epoch 39, loss 0.73962, train_acc 0.7479, valid_acc 0.7629, Time 00:00:24,lr 0.01\n",
      "epoch 40, loss 0.73451, train_acc 0.7482, valid_acc 0.7582, Time 00:00:24,lr 0.01\n",
      "epoch 41, loss 0.72975, train_acc 0.7514, valid_acc 0.7482, Time 00:00:24,lr 0.01\n",
      "epoch 42, loss 0.72403, train_acc 0.7536, valid_acc 0.7622, Time 00:00:24,lr 0.01\n",
      "epoch 43, loss 0.72834, train_acc 0.7540, valid_acc 0.7566, Time 00:00:24,lr 0.01\n",
      "epoch 44, loss 0.72512, train_acc 0.7522, valid_acc 0.7666, Time 00:00:24,lr 0.01\n",
      "epoch 45, loss 0.71810, train_acc 0.7576, valid_acc 0.7462, Time 00:00:25,lr 0.01\n",
      "epoch 46, loss 0.72037, train_acc 0.7547, valid_acc 0.7660, Time 00:00:25,lr 0.01\n",
      "epoch 47, loss 0.71576, train_acc 0.7564, valid_acc 0.7687, Time 00:00:24,lr 0.01\n",
      "epoch 48, loss 0.71955, train_acc 0.7534, valid_acc 0.7430, Time 00:00:24,lr 0.01\n",
      "epoch 49, loss 0.71184, train_acc 0.7587, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 50, loss 0.71714, train_acc 0.7561, valid_acc 0.7544, Time 00:00:24,lr 0.01\n",
      "epoch 51, loss 0.71158, train_acc 0.7576, valid_acc 0.7566, Time 00:00:25,lr 0.01\n",
      "epoch 52, loss 0.70936, train_acc 0.7582, valid_acc 0.7613, Time 00:00:24,lr 0.01\n",
      "epoch 53, loss 0.70673, train_acc 0.7591, valid_acc 0.7554, Time 00:00:24,lr 0.01\n",
      "epoch 54, loss 0.70670, train_acc 0.7597, valid_acc 0.7607, Time 00:00:24,lr 0.01\n",
      "epoch 55, loss 0.70951, train_acc 0.7566, valid_acc 0.7609, Time 00:00:24,lr 0.01\n",
      "epoch 56, loss 0.70382, train_acc 0.7595, valid_acc 0.7573, Time 00:00:24,lr 0.01\n",
      "epoch 57, loss 0.70531, train_acc 0.7601, valid_acc 0.7356, Time 00:00:24,lr 0.01\n",
      "epoch 58, loss 0.70683, train_acc 0.7595, valid_acc 0.7572, Time 00:00:25,lr 0.01\n",
      "epoch 59, loss 0.70069, train_acc 0.7610, valid_acc 0.7556, Time 00:00:24,lr 0.01\n",
      "epoch 60, loss 0.54787, train_acc 0.8137, valid_acc 0.8178, Time 00:00:24,lr 0.002\n",
      "epoch 61, loss 0.49978, train_acc 0.8285, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 62, loss 0.48376, train_acc 0.8345, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 63, loss 0.47104, train_acc 0.8396, valid_acc 0.8270, Time 00:00:24,lr 0.002\n",
      "epoch 64, loss 0.46549, train_acc 0.8415, valid_acc 0.8376, Time 00:00:25,lr 0.002\n",
      "epoch 65, loss 0.45923, train_acc 0.8430, valid_acc 0.8264, Time 00:00:25,lr 0.002\n",
      "epoch 66, loss 0.45508, train_acc 0.8428, valid_acc 0.8332, Time 00:00:24,lr 0.002\n",
      "epoch 67, loss 0.45490, train_acc 0.8441, valid_acc 0.8351, Time 00:00:24,lr 0.002\n",
      "epoch 68, loss 0.45410, train_acc 0.8435, valid_acc 0.8346, Time 00:00:24,lr 0.002\n",
      "epoch 69, loss 0.44802, train_acc 0.8458, valid_acc 0.8378, Time 00:00:24,lr 0.002\n",
      "epoch 70, loss 0.44962, train_acc 0.8464, valid_acc 0.8339, Time 00:00:24,lr 0.002\n",
      "epoch 71, loss 0.45422, train_acc 0.8447, valid_acc 0.8261, Time 00:00:24,lr 0.002\n",
      "epoch 72, loss 0.44599, train_acc 0.8474, valid_acc 0.8287, Time 00:00:25,lr 0.002\n",
      "epoch 73, loss 0.44833, train_acc 0.8458, valid_acc 0.8317, Time 00:00:24,lr 0.002\n",
      "epoch 74, loss 0.44645, train_acc 0.8466, valid_acc 0.8322, Time 00:00:24,lr 0.002\n",
      "epoch 75, loss 0.45153, train_acc 0.8453, valid_acc 0.8240, Time 00:00:24,lr 0.002\n",
      "epoch 76, loss 0.44996, train_acc 0.8456, valid_acc 0.8233, Time 00:00:24,lr 0.002\n",
      "epoch 77, loss 0.44440, train_acc 0.8473, valid_acc 0.8235, Time 00:00:24,lr 0.002\n",
      "epoch 78, loss 0.44680, train_acc 0.8452, valid_acc 0.8229, Time 00:00:24,lr 0.002\n",
      "epoch 79, loss 0.44781, train_acc 0.8466, valid_acc 0.8210, Time 00:00:24,lr 0.002\n",
      "epoch 80, loss 0.44728, train_acc 0.8452, valid_acc 0.8299, Time 00:00:24,lr 0.002\n",
      "epoch 81, loss 0.44830, train_acc 0.8462, valid_acc 0.8278, Time 00:00:24,lr 0.002\n",
      "epoch 82, loss 0.44525, train_acc 0.8456, valid_acc 0.8325, Time 00:00:24,lr 0.002\n",
      "epoch 83, loss 0.44259, train_acc 0.8477, valid_acc 0.8257, Time 00:00:24,lr 0.002\n",
      "epoch 84, loss 0.44439, train_acc 0.8464, valid_acc 0.8292, Time 00:00:24,lr 0.002\n",
      "epoch 85, loss 0.44217, train_acc 0.8476, valid_acc 0.8280, Time 00:00:24,lr 0.002\n",
      "epoch 86, loss 0.44183, train_acc 0.8487, valid_acc 0.8294, Time 00:00:24,lr 0.002\n",
      "epoch 87, loss 0.43943, train_acc 0.8472, valid_acc 0.8248, Time 00:00:24,lr 0.002\n",
      "epoch 88, loss 0.43877, train_acc 0.8485, valid_acc 0.8164, Time 00:00:26,lr 0.002\n",
      "epoch 89, loss 0.43657, train_acc 0.8498, valid_acc 0.8276, Time 00:00:25,lr 0.002\n",
      "epoch 90, loss 0.35472, train_acc 0.8789, valid_acc 0.8585, Time 00:00:25,lr 0.0004\n",
      "epoch 91, loss 0.32633, train_acc 0.8876, valid_acc 0.8528, Time 00:00:25,lr 0.0004\n",
      "epoch 92, loss 0.31385, train_acc 0.8927, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 93, loss 0.30717, train_acc 0.8945, valid_acc 0.8557, Time 00:00:25,lr 0.0004\n",
      "epoch 94, loss 0.30039, train_acc 0.8969, valid_acc 0.8598, Time 00:00:27,lr 0.0004\n",
      "epoch 95, loss 0.29545, train_acc 0.8995, valid_acc 0.8572, Time 00:00:25,lr 0.0004\n",
      "epoch 96, loss 0.29173, train_acc 0.8990, valid_acc 0.8566, Time 00:00:26,lr 0.0004\n",
      "epoch 97, loss 0.28834, train_acc 0.8994, valid_acc 0.8571, Time 00:00:28,lr 0.0004\n",
      "epoch 98, loss 0.28529, train_acc 0.9006, valid_acc 0.8560, Time 00:00:27,lr 0.0004\n",
      "epoch 99, loss 0.28147, train_acc 0.9024, valid_acc 0.8564, Time 00:00:25,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.27405, train_acc 0.9057, valid_acc 0.8599, Time 00:00:26,lr 0.0004\n",
      "epoch 101, loss 0.27207, train_acc 0.9066, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 102, loss 0.27203, train_acc 0.9067, valid_acc 0.8584, Time 00:00:25,lr 0.0004\n",
      "epoch 103, loss 0.26894, train_acc 0.9070, valid_acc 0.8580, Time 00:00:27,lr 0.0004\n",
      "epoch 104, loss 0.26925, train_acc 0.9077, valid_acc 0.8566, Time 00:00:25,lr 0.0004\n",
      "epoch 105, loss 0.26636, train_acc 0.9088, valid_acc 0.8558, Time 00:00:24,lr 0.0004\n",
      "epoch 106, loss 0.26520, train_acc 0.9085, valid_acc 0.8559, Time 00:00:26,lr 0.0004\n",
      "epoch 107, loss 0.26192, train_acc 0.9094, valid_acc 0.8562, Time 00:00:27,lr 0.0004\n",
      "epoch 108, loss 0.26339, train_acc 0.9091, valid_acc 0.8547, Time 00:00:25,lr 0.0004\n",
      "epoch 109, loss 0.26113, train_acc 0.9096, valid_acc 0.8568, Time 00:00:24,lr 0.0004\n",
      "epoch 110, loss 0.25569, train_acc 0.9102, valid_acc 0.8575, Time 00:00:24,lr 0.0004\n",
      "epoch 111, loss 0.25539, train_acc 0.9117, valid_acc 0.8565, Time 00:00:25,lr 0.0004\n",
      "epoch 112, loss 0.25576, train_acc 0.9120, valid_acc 0.8555, Time 00:00:24,lr 0.0004\n",
      "epoch 113, loss 0.25426, train_acc 0.9129, valid_acc 0.8522, Time 00:00:24,lr 0.0004\n",
      "epoch 114, loss 0.25399, train_acc 0.9125, valid_acc 0.8544, Time 00:00:24,lr 0.0004\n",
      "epoch 115, loss 0.25090, train_acc 0.9127, valid_acc 0.8521, Time 00:00:24,lr 0.0004\n",
      "epoch 116, loss 0.24849, train_acc 0.9130, valid_acc 0.8567, Time 00:00:24,lr 0.0004\n",
      "epoch 117, loss 0.25278, train_acc 0.9111, valid_acc 0.8523, Time 00:00:24,lr 0.0004\n",
      "epoch 118, loss 0.25027, train_acc 0.9130, valid_acc 0.8512, Time 00:00:24,lr 0.0004\n",
      "epoch 119, loss 0.24702, train_acc 0.9142, valid_acc 0.8548, Time 00:00:24,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 gluon resnet18 vs my resnet18\n",
    "```\n",
    "there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:40:01.865661Z",
     "start_time": "2018-03-04T08:40:01.754397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (net): HybridSequential(\n",
      "    (0): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "    (2): Activation(relu)\n",
      "    (3): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (6): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (7): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (8): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (9): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (10): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (11): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (12): AvgPool2D(size=(8, 8), stride=(8, 8), padding=(0, 0), ceil_mode=False)\n",
      "    (13): Flatten\n",
      "    (14): Dense(None -> 10, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net1 = ResNet(10)\n",
    "'''\n",
    "block1: conv(32, 3, 1, 1) + BN + relu\n",
    "block2: Residual_block(32) * 3\n",
    "block3: Resisual_block(64) * 3\n",
    "block4: Residual_block(128) * 3\n",
    "block5: AvgPool(8) + Flatten + Dense(10)\n",
    "\n",
    "actually it has 19 conv + 1 fc, some code name it as resnet20 (each Residual_block has 2 conv or 2 + 1 conv(two line))\n",
    "for cifar10 dataset, most code use this arch (https://github.com/yinglang/pytorch-cifar-models).\n",
    "'''\n",
    "net2 = get_resnet18_v1()\n",
    "'''\n",
    "block1: conv(64, 7, 3, 2) + BN + relu + MaxPool(3, 1, 2)\n",
    "block2: Residual_block(64) * 2\n",
    "block3: Resisual_block(128) * 2\n",
    "block4: Residual_block(256) * 2\n",
    "block5: Residual_block(256) * 2\n",
    "block6: GlobalAvgPool() + Dense(10) \n",
    "\n",
    "has 1 + (2 + 2 + 2 + 2) * 2 = 15 conv + 1 fc, name it as resnet18\n",
    "'''\n",
    "print net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:21.510639Z",
     "start_time": "2018-03-04T08:31:21.405189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): HybridSequential(\n",
      "    (0): Conv2D(3 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n",
      "    (4): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  )\n",
      "  (1): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 gluon resnet18 vs my resnet 18: first conv is conv(k=7,p=3,s=2) vs conv(k=3,p=1,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T03:26:13.432236Z",
     "start_time": "2018-03-04T03:26:13.422231Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T04:08:38.921689Z",
     "start_time": "2018-03-04T03:26:16.924298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.20239, train_acc 0.2669, valid_acc 0.3902, Time 00:00:32,lr 0.1\n",
      "epoch 1, loss 1.65517, train_acc 0.3912, valid_acc 0.4528, Time 00:00:33,lr 0.1\n",
      "epoch 2, loss 1.47576, train_acc 0.4642, valid_acc 0.4337, Time 00:00:33,lr 0.1\n",
      "epoch 3, loss 1.34809, train_acc 0.5176, valid_acc 0.5469, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 1.25687, train_acc 0.5574, valid_acc 0.5480, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 1.20031, train_acc 0.5783, valid_acc 0.5777, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 1.15576, train_acc 0.5958, valid_acc 0.6329, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 1.11209, train_acc 0.6100, valid_acc 0.6637, Time 00:00:30,lr 0.1\n",
      "epoch 8, loss 1.06140, train_acc 0.6298, valid_acc 0.5927, Time 00:00:30,lr 0.1\n",
      "epoch 9, loss 1.02432, train_acc 0.6445, valid_acc 0.6795, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 1.00051, train_acc 0.6551, valid_acc 0.6433, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 0.98188, train_acc 0.6637, valid_acc 0.6974, Time 00:00:30,lr 0.1\n",
      "epoch 12, loss 0.96989, train_acc 0.6664, valid_acc 0.6623, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.95365, train_acc 0.6740, valid_acc 0.6860, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.95321, train_acc 0.6733, valid_acc 0.6337, Time 00:00:30,lr 0.1\n",
      "epoch 15, loss 0.94458, train_acc 0.6777, valid_acc 0.6930, Time 00:00:30,lr 0.1\n",
      "epoch 16, loss 0.93070, train_acc 0.6824, valid_acc 0.7069, Time 00:00:30,lr 0.1\n",
      "epoch 17, loss 0.93221, train_acc 0.6827, valid_acc 0.6382, Time 00:00:30,lr 0.1\n",
      "epoch 18, loss 0.92776, train_acc 0.6840, valid_acc 0.7080, Time 00:00:30,lr 0.1\n",
      "epoch 19, loss 0.92143, train_acc 0.6869, valid_acc 0.6862, Time 00:00:30,lr 0.1\n",
      "epoch 20, loss 0.92236, train_acc 0.6877, valid_acc 0.6391, Time 00:00:30,lr 0.1\n",
      "epoch 21, loss 0.92049, train_acc 0.6863, valid_acc 0.6934, Time 00:00:30,lr 0.1\n",
      "epoch 22, loss 0.91650, train_acc 0.6887, valid_acc 0.6944, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.91073, train_acc 0.6888, valid_acc 0.6668, Time 00:00:30,lr 0.1\n",
      "epoch 24, loss 0.91163, train_acc 0.6908, valid_acc 0.6598, Time 00:00:30,lr 0.1\n",
      "epoch 25, loss 0.90582, train_acc 0.6902, valid_acc 0.6914, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 0.90684, train_acc 0.6901, valid_acc 0.6054, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 0.90768, train_acc 0.6901, valid_acc 0.6733, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.90371, train_acc 0.6923, valid_acc 0.6340, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 0.90607, train_acc 0.6906, valid_acc 0.6989, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 0.90220, train_acc 0.6922, valid_acc 0.5081, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.90443, train_acc 0.6917, valid_acc 0.6913, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 0.89974, train_acc 0.6936, valid_acc 0.6244, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 0.89704, train_acc 0.6960, valid_acc 0.7255, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 0.90115, train_acc 0.6939, valid_acc 0.6980, Time 00:00:32,lr 0.1\n",
      "epoch 35, loss 0.89469, train_acc 0.6961, valid_acc 0.6947, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 0.89557, train_acc 0.6974, valid_acc 0.6494, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 0.89949, train_acc 0.6951, valid_acc 0.7107, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 0.89622, train_acc 0.6963, valid_acc 0.5968, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 0.89347, train_acc 0.6955, valid_acc 0.6612, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 0.89192, train_acc 0.6984, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 0.89004, train_acc 0.6967, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 0.88865, train_acc 0.6992, valid_acc 0.6024, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 0.90129, train_acc 0.6922, valid_acc 0.6781, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 0.89063, train_acc 0.6980, valid_acc 0.6661, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 0.89031, train_acc 0.6983, valid_acc 0.6445, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 0.88648, train_acc 0.6996, valid_acc 0.7091, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.88954, train_acc 0.6960, valid_acc 0.7163, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 0.89009, train_acc 0.6982, valid_acc 0.7027, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 0.88359, train_acc 0.7001, valid_acc 0.6971, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 0.89414, train_acc 0.6958, valid_acc 0.6732, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 0.88717, train_acc 0.6997, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 0.89172, train_acc 0.6956, valid_acc 0.6487, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.89213, train_acc 0.6993, valid_acc 0.6922, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 0.88663, train_acc 0.6969, valid_acc 0.7236, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 0.88679, train_acc 0.6988, valid_acc 0.6824, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 0.88335, train_acc 0.6991, valid_acc 0.6743, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 0.88368, train_acc 0.7007, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 0.88596, train_acc 0.6983, valid_acc 0.7060, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 0.88443, train_acc 0.6993, valid_acc 0.6722, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 0.88799, train_acc 0.6990, valid_acc 0.6508, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 0.88132, train_acc 0.7016, valid_acc 0.6703, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 0.87931, train_acc 0.7012, valid_acc 0.6996, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 0.88957, train_acc 0.6987, valid_acc 0.7010, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 0.88078, train_acc 0.7020, valid_acc 0.6454, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.88306, train_acc 0.7007, valid_acc 0.6412, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 0.88245, train_acc 0.7001, valid_acc 0.6534, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 0.87989, train_acc 0.7020, valid_acc 0.6466, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 0.89072, train_acc 0.6977, valid_acc 0.7111, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 0.88862, train_acc 0.6984, valid_acc 0.6285, Time 00:00:35,lr 0.1\n",
      "epoch 70, loss 0.87356, train_acc 0.7045, valid_acc 0.5976, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.88543, train_acc 0.6981, valid_acc 0.7188, Time 00:00:38,lr 0.1\n",
      "epoch 72, loss 0.88926, train_acc 0.6963, valid_acc 0.7050, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 0.87890, train_acc 0.7006, valid_acc 0.6988, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 0.88213, train_acc 0.6988, valid_acc 0.6710, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 0.89143, train_acc 0.6980, valid_acc 0.6485, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 0.88201, train_acc 0.6990, valid_acc 0.6410, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.87847, train_acc 0.7010, valid_acc 0.6491, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.88841, train_acc 0.6974, valid_acc 0.6903, Time 00:00:35,lr 0.1\n",
      "epoch 79, loss 0.88179, train_acc 0.7004, valid_acc 0.6880, Time 00:00:34,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T05:14:50.043433Z",
     "start_time": "2018-03-04T04:10:53.460734Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.77403, train_acc 0.7361, valid_acc 0.6271, Time 00:00:28,lr 0.05\n",
      "epoch 1, loss 0.84400, train_acc 0.7124, valid_acc 0.6732, Time 00:00:30,lr 0.05\n",
      "epoch 2, loss 0.86126, train_acc 0.7060, valid_acc 0.7009, Time 00:00:30,lr 0.05\n",
      "epoch 3, loss 0.86573, train_acc 0.7063, valid_acc 0.6787, Time 00:00:30,lr 0.05\n",
      "epoch 4, loss 0.85972, train_acc 0.7096, valid_acc 0.6623, Time 00:00:30,lr 0.05\n",
      "epoch 5, loss 0.86412, train_acc 0.7051, valid_acc 0.6821, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.86789, train_acc 0.7018, valid_acc 0.6822, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.86329, train_acc 0.7054, valid_acc 0.6411, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.87379, train_acc 0.7028, valid_acc 0.7026, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.86685, train_acc 0.7050, valid_acc 0.6352, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.86982, train_acc 0.7020, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.86871, train_acc 0.7042, valid_acc 0.5869, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.86923, train_acc 0.7037, valid_acc 0.6458, Time 00:00:31,lr 0.05\n",
      "epoch 13, loss 0.86655, train_acc 0.7048, valid_acc 0.6600, Time 00:00:31,lr 0.05\n",
      "epoch 14, loss 0.86976, train_acc 0.7038, valid_acc 0.7020, Time 00:00:31,lr 0.05\n",
      "epoch 15, loss 0.85832, train_acc 0.7083, valid_acc 0.6998, Time 00:00:31,lr 0.05\n",
      "epoch 16, loss 0.86706, train_acc 0.7046, valid_acc 0.6969, Time 00:00:31,lr 0.05\n",
      "epoch 17, loss 0.86458, train_acc 0.7044, valid_acc 0.6914, Time 00:00:31,lr 0.05\n",
      "epoch 18, loss 0.86462, train_acc 0.7072, valid_acc 0.7017, Time 00:00:31,lr 0.05\n",
      "epoch 19, loss 0.86589, train_acc 0.7067, valid_acc 0.7007, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.87160, train_acc 0.7042, valid_acc 0.7153, Time 00:00:31,lr 0.05\n",
      "epoch 21, loss 0.86976, train_acc 0.7027, valid_acc 0.6829, Time 00:00:31,lr 0.05\n",
      "epoch 22, loss 0.85921, train_acc 0.7075, valid_acc 0.7036, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.86768, train_acc 0.7058, valid_acc 0.7057, Time 00:00:31,lr 0.05\n",
      "epoch 24, loss 0.86215, train_acc 0.7067, valid_acc 0.7118, Time 00:00:31,lr 0.05\n",
      "epoch 25, loss 0.86632, train_acc 0.7029, valid_acc 0.7084, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.87295, train_acc 0.7031, valid_acc 0.6990, Time 00:00:31,lr 0.05\n",
      "epoch 27, loss 0.87456, train_acc 0.7033, valid_acc 0.6865, Time 00:00:31,lr 0.05\n",
      "epoch 28, loss 0.86871, train_acc 0.7044, valid_acc 0.6970, Time 00:00:31,lr 0.05\n",
      "epoch 29, loss 0.86998, train_acc 0.7056, valid_acc 0.6560, Time 00:00:31,lr 0.05\n",
      "epoch 30, loss 0.59346, train_acc 0.7979, valid_acc 0.8265, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.54326, train_acc 0.8155, valid_acc 0.8227, Time 00:00:31,lr 0.01\n",
      "epoch 32, loss 0.53949, train_acc 0.8170, valid_acc 0.8028, Time 00:00:31,lr 0.01\n",
      "epoch 33, loss 0.54204, train_acc 0.8157, valid_acc 0.8154, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.53898, train_acc 0.8156, valid_acc 0.8274, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.54012, train_acc 0.8164, valid_acc 0.8242, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.54208, train_acc 0.8150, valid_acc 0.8189, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.53263, train_acc 0.8171, valid_acc 0.8306, Time 00:00:35,lr 0.01\n",
      "epoch 38, loss 0.53156, train_acc 0.8190, valid_acc 0.8076, Time 00:00:33,lr 0.01\n",
      "epoch 39, loss 0.52680, train_acc 0.8204, valid_acc 0.8144, Time 00:00:33,lr 0.01\n",
      "epoch 40, loss 0.52358, train_acc 0.8228, valid_acc 0.7965, Time 00:00:33,lr 0.01\n",
      "epoch 41, loss 0.52257, train_acc 0.8211, valid_acc 0.8152, Time 00:00:34,lr 0.01\n",
      "epoch 42, loss 0.52220, train_acc 0.8232, valid_acc 0.8375, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.51315, train_acc 0.8264, valid_acc 0.8182, Time 00:00:31,lr 0.01\n",
      "epoch 44, loss 0.51323, train_acc 0.8268, valid_acc 0.8144, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.51435, train_acc 0.8240, valid_acc 0.8080, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.51004, train_acc 0.8262, valid_acc 0.8189, Time 00:00:31,lr 0.01\n",
      "epoch 47, loss 0.50912, train_acc 0.8285, valid_acc 0.8285, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.51028, train_acc 0.8259, valid_acc 0.8171, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.50687, train_acc 0.8291, valid_acc 0.8301, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.50086, train_acc 0.8297, valid_acc 0.8241, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.49983, train_acc 0.8290, valid_acc 0.8161, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.50052, train_acc 0.8278, valid_acc 0.8267, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.49692, train_acc 0.8309, valid_acc 0.8120, Time 00:00:31,lr 0.01\n",
      "epoch 54, loss 0.50006, train_acc 0.8279, valid_acc 0.8300, Time 00:00:31,lr 0.01\n",
      "epoch 55, loss 0.49913, train_acc 0.8295, valid_acc 0.7949, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.49605, train_acc 0.8320, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.49692, train_acc 0.8300, valid_acc 0.8328, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.49206, train_acc 0.8337, valid_acc 0.8387, Time 00:00:31,lr 0.01\n",
      "epoch 59, loss 0.49343, train_acc 0.8311, valid_acc 0.8150, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.34705, train_acc 0.8820, valid_acc 0.8809, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.30127, train_acc 0.8983, valid_acc 0.8846, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.28316, train_acc 0.9037, valid_acc 0.8870, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.27436, train_acc 0.9062, valid_acc 0.8866, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.26887, train_acc 0.9072, valid_acc 0.8916, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.26500, train_acc 0.9096, valid_acc 0.8894, Time 00:00:31,lr 0.002\n",
      "epoch 66, loss 0.26247, train_acc 0.9101, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 67, loss 0.26025, train_acc 0.9112, valid_acc 0.8920, Time 00:00:32,lr 0.002\n",
      "epoch 68, loss 0.25301, train_acc 0.9135, valid_acc 0.8888, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.25118, train_acc 0.9139, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.25089, train_acc 0.9137, valid_acc 0.8884, Time 00:00:32,lr 0.002\n",
      "epoch 71, loss 0.25031, train_acc 0.9147, valid_acc 0.8795, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.25265, train_acc 0.9130, valid_acc 0.8856, Time 00:00:33,lr 0.002\n",
      "epoch 73, loss 0.25271, train_acc 0.9129, valid_acc 0.8883, Time 00:00:31,lr 0.002\n",
      "epoch 74, loss 0.25271, train_acc 0.9128, valid_acc 0.8770, Time 00:00:32,lr 0.002\n",
      "epoch 75, loss 0.25109, train_acc 0.9137, valid_acc 0.8796, Time 00:00:32,lr 0.002\n",
      "epoch 76, loss 0.25106, train_acc 0.9132, valid_acc 0.8824, Time 00:00:32,lr 0.002\n",
      "epoch 77, loss 0.25249, train_acc 0.9136, valid_acc 0.8863, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.25310, train_acc 0.9126, valid_acc 0.8882, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.25188, train_acc 0.9132, valid_acc 0.8809, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.25087, train_acc 0.9143, valid_acc 0.8808, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.25043, train_acc 0.9133, valid_acc 0.8659, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.25028, train_acc 0.9126, valid_acc 0.8830, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.24958, train_acc 0.9140, valid_acc 0.8806, Time 00:00:31,lr 0.002\n",
      "epoch 84, loss 0.25462, train_acc 0.9128, valid_acc 0.8817, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.25416, train_acc 0.9109, valid_acc 0.8797, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.24963, train_acc 0.9134, valid_acc 0.8856, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.25088, train_acc 0.9143, valid_acc 0.8799, Time 00:00:32,lr 0.002\n",
      "epoch 88, loss 0.24692, train_acc 0.9147, valid_acc 0.8874, Time 00:00:31,lr 0.002\n",
      "epoch 89, loss 0.24720, train_acc 0.9146, valid_acc 0.8826, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.17336, train_acc 0.9407, valid_acc 0.9051, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.14650, train_acc 0.9515, valid_acc 0.9075, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.13297, train_acc 0.9564, valid_acc 0.9078, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.12877, train_acc 0.9568, valid_acc 0.9069, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.12118, train_acc 0.9599, valid_acc 0.9086, Time 00:00:32,lr 0.0004\n",
      "epoch 95, loss 0.11531, train_acc 0.9619, valid_acc 0.9090, Time 00:00:32,lr 0.0004\n",
      "epoch 96, loss 0.11811, train_acc 0.9601, valid_acc 0.9075, Time 00:00:35,lr 0.0004\n",
      "epoch 97, loss 0.11135, train_acc 0.9634, valid_acc 0.9066, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.10657, train_acc 0.9653, valid_acc 0.9044, Time 00:00:31,lr 0.0004\n",
      "epoch 99, loss 0.10713, train_acc 0.9637, valid_acc 0.9063, Time 00:00:31,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.10466, train_acc 0.9656, valid_acc 0.9050, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.09925, train_acc 0.9674, valid_acc 0.9043, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.09841, train_acc 0.9671, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.09499, train_acc 0.9692, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 104, loss 0.09518, train_acc 0.9685, valid_acc 0.9089, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.09390, train_acc 0.9696, valid_acc 0.9105, Time 00:00:31,lr 0.0004\n",
      "epoch 106, loss 0.09006, train_acc 0.9702, valid_acc 0.9070, Time 00:00:32,lr 0.0004\n",
      "epoch 107, loss 0.09035, train_acc 0.9709, valid_acc 0.9081, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.08887, train_acc 0.9702, valid_acc 0.9073, Time 00:00:33,lr 0.0004\n",
      "epoch 109, loss 0.08896, train_acc 0.9715, valid_acc 0.9060, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.08627, train_acc 0.9710, valid_acc 0.9103, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.08818, train_acc 0.9706, valid_acc 0.9070, Time 00:00:31,lr 0.0004\n",
      "epoch 112, loss 0.08570, train_acc 0.9718, valid_acc 0.9063, Time 00:00:33,lr 0.0004\n",
      "epoch 113, loss 0.08473, train_acc 0.9719, valid_acc 0.9086, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.08530, train_acc 0.9715, valid_acc 0.9090, Time 00:00:33,lr 0.0004\n",
      "epoch 115, loss 0.08070, train_acc 0.9733, valid_acc 0.9075, Time 00:00:33,lr 0.0004\n",
      "epoch 116, loss 0.07957, train_acc 0.9737, valid_acc 0.9041, Time 00:00:32,lr 0.0004\n",
      "epoch 117, loss 0.07966, train_acc 0.9733, valid_acc 0.9027, Time 00:00:32,lr 0.0004\n",
      "epoch 118, loss 0.08058, train_acc 0.9734, valid_acc 0.9058, Time 00:00:32,lr 0.0004\n",
      "epoch 119, loss 0.08208, train_acc 0.9728, valid_acc 0.9074, Time 00:00:35,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 I wanto find out acc impove may from cancel downsample, so use k=7 but not downsmaple and k=3 but downsample to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:41.147882Z",
     "start_time": "2018-03-04T09:17:41.140250Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k7p3s1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=7, strides=1, padding=3, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T11:07:06.585909Z",
     "start_time": "2018-03-04T09:17:41.978248Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.30376, train_acc 0.2260, valid_acc 0.3361, Time 00:00:31,lr 0.1\n",
      "epoch 1, loss 1.68781, train_acc 0.3729, valid_acc 0.4189, Time 00:00:35,lr 0.1\n",
      "epoch 2, loss 1.52445, train_acc 0.4432, valid_acc 0.5236, Time 00:00:32,lr 0.1\n",
      "epoch 3, loss 1.37861, train_acc 0.5034, valid_acc 0.5116, Time 00:00:32,lr 0.1\n",
      "epoch 4, loss 1.29522, train_acc 0.5379, valid_acc 0.5644, Time 00:00:32,lr 0.1\n",
      "epoch 5, loss 1.23774, train_acc 0.5634, valid_acc 0.5271, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.18491, train_acc 0.5845, valid_acc 0.6008, Time 00:00:32,lr 0.1\n",
      "epoch 7, loss 1.13172, train_acc 0.6043, valid_acc 0.6084, Time 00:00:32,lr 0.1\n",
      "epoch 8, loss 1.10778, train_acc 0.6163, valid_acc 0.6573, Time 00:00:33,lr 0.1\n",
      "epoch 9, loss 1.06816, train_acc 0.6311, valid_acc 0.6118, Time 00:00:32,lr 0.1\n",
      "epoch 10, loss 1.03542, train_acc 0.6430, valid_acc 0.6466, Time 00:00:32,lr 0.1\n",
      "epoch 11, loss 1.02805, train_acc 0.6471, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 1.01376, train_acc 0.6525, valid_acc 0.6565, Time 00:00:33,lr 0.1\n",
      "epoch 13, loss 1.00736, train_acc 0.6535, valid_acc 0.6435, Time 00:00:32,lr 0.1\n",
      "epoch 14, loss 0.99958, train_acc 0.6599, valid_acc 0.6482, Time 00:00:32,lr 0.1\n",
      "epoch 15, loss 0.98714, train_acc 0.6617, valid_acc 0.6799, Time 00:00:32,lr 0.1\n",
      "epoch 16, loss 0.97678, train_acc 0.6662, valid_acc 0.6960, Time 00:00:32,lr 0.1\n",
      "epoch 17, loss 0.97524, train_acc 0.6663, valid_acc 0.6830, Time 00:00:32,lr 0.1\n",
      "epoch 18, loss 0.97347, train_acc 0.6669, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 19, loss 0.97192, train_acc 0.6669, valid_acc 0.6242, Time 00:00:32,lr 0.1\n",
      "epoch 20, loss 0.97201, train_acc 0.6679, valid_acc 0.6656, Time 00:00:32,lr 0.1\n",
      "epoch 21, loss 0.96170, train_acc 0.6740, valid_acc 0.6367, Time 00:00:33,lr 0.1\n",
      "epoch 22, loss 0.96810, train_acc 0.6708, valid_acc 0.6185, Time 00:00:32,lr 0.1\n",
      "epoch 23, loss 0.95803, train_acc 0.6731, valid_acc 0.6805, Time 00:00:33,lr 0.1\n",
      "epoch 24, loss 0.95777, train_acc 0.6729, valid_acc 0.6490, Time 00:00:32,lr 0.1\n",
      "epoch 25, loss 0.96024, train_acc 0.6753, valid_acc 0.6834, Time 00:00:32,lr 0.1\n",
      "epoch 26, loss 0.95364, train_acc 0.6740, valid_acc 0.6864, Time 00:00:32,lr 0.1\n",
      "epoch 27, loss 0.95158, train_acc 0.6774, valid_acc 0.6646, Time 00:00:32,lr 0.1\n",
      "epoch 28, loss 0.94868, train_acc 0.6751, valid_acc 0.6987, Time 00:00:32,lr 0.1\n",
      "epoch 29, loss 0.95404, train_acc 0.6749, valid_acc 0.6318, Time 00:00:32,lr 0.1\n",
      "epoch 30, loss 0.95379, train_acc 0.6741, valid_acc 0.6917, Time 00:00:32,lr 0.1\n",
      "epoch 31, loss 0.94801, train_acc 0.6744, valid_acc 0.6283, Time 00:00:32,lr 0.1\n",
      "epoch 32, loss 0.95098, train_acc 0.6751, valid_acc 0.6732, Time 00:00:33,lr 0.1\n",
      "epoch 33, loss 0.94359, train_acc 0.6791, valid_acc 0.6295, Time 00:00:34,lr 0.1\n",
      "epoch 34, loss 0.94709, train_acc 0.6774, valid_acc 0.6200, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.94835, train_acc 0.6774, valid_acc 0.6354, Time 00:00:32,lr 0.1\n",
      "epoch 36, loss 0.94460, train_acc 0.6784, valid_acc 0.6478, Time 00:00:32,lr 0.1\n",
      "epoch 37, loss 0.94238, train_acc 0.6794, valid_acc 0.6423, Time 00:00:32,lr 0.1\n",
      "epoch 38, loss 0.94366, train_acc 0.6784, valid_acc 0.5573, Time 00:00:32,lr 0.1\n",
      "epoch 39, loss 0.93595, train_acc 0.6804, valid_acc 0.6669, Time 00:00:32,lr 0.1\n",
      "epoch 40, loss 0.94138, train_acc 0.6788, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 41, loss 0.94039, train_acc 0.6797, valid_acc 0.6339, Time 00:00:34,lr 0.1\n",
      "epoch 42, loss 0.93921, train_acc 0.6797, valid_acc 0.6167, Time 00:00:32,lr 0.1\n",
      "epoch 43, loss 0.93964, train_acc 0.6790, valid_acc 0.6928, Time 00:00:32,lr 0.1\n",
      "epoch 44, loss 0.93891, train_acc 0.6801, valid_acc 0.6485, Time 00:00:32,lr 0.1\n",
      "epoch 45, loss 0.93846, train_acc 0.6814, valid_acc 0.5983, Time 00:00:32,lr 0.1\n",
      "epoch 46, loss 0.93314, train_acc 0.6822, valid_acc 0.6216, Time 00:00:32,lr 0.1\n",
      "epoch 47, loss 0.93256, train_acc 0.6823, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 48, loss 0.94286, train_acc 0.6781, valid_acc 0.6123, Time 00:00:32,lr 0.1\n",
      "epoch 49, loss 0.93333, train_acc 0.6848, valid_acc 0.6390, Time 00:00:32,lr 0.1\n",
      "epoch 50, loss 0.93153, train_acc 0.6829, valid_acc 0.6682, Time 00:00:32,lr 0.1\n",
      "epoch 51, loss 0.93787, train_acc 0.6817, valid_acc 0.7057, Time 00:00:32,lr 0.1\n",
      "epoch 52, loss 0.93450, train_acc 0.6823, valid_acc 0.6080, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.93795, train_acc 0.6822, valid_acc 0.6733, Time 00:00:32,lr 0.1\n",
      "epoch 54, loss 0.92976, train_acc 0.6836, valid_acc 0.6498, Time 00:00:32,lr 0.1\n",
      "epoch 55, loss 0.93671, train_acc 0.6794, valid_acc 0.6334, Time 00:00:32,lr 0.1\n",
      "epoch 56, loss 0.93371, train_acc 0.6807, valid_acc 0.6849, Time 00:00:32,lr 0.1\n",
      "epoch 57, loss 0.93283, train_acc 0.6830, valid_acc 0.6737, Time 00:00:32,lr 0.1\n",
      "epoch 58, loss 0.94008, train_acc 0.6813, valid_acc 0.6681, Time 00:00:32,lr 0.1\n",
      "epoch 59, loss 0.92572, train_acc 0.6860, valid_acc 0.6837, Time 00:00:32,lr 0.1\n",
      "epoch 60, loss 0.93572, train_acc 0.6810, valid_acc 0.6855, Time 00:00:32,lr 0.1\n",
      "epoch 61, loss 0.92939, train_acc 0.6856, valid_acc 0.6800, Time 00:00:32,lr 0.1\n",
      "epoch 62, loss 0.92851, train_acc 0.6848, valid_acc 0.6933, Time 00:00:32,lr 0.1\n",
      "epoch 63, loss 0.93826, train_acc 0.6798, valid_acc 0.5912, Time 00:00:32,lr 0.1\n",
      "epoch 64, loss 0.92919, train_acc 0.6859, valid_acc 0.6371, Time 00:00:32,lr 0.1\n",
      "epoch 65, loss 0.92681, train_acc 0.6829, valid_acc 0.6720, Time 00:00:32,lr 0.1\n",
      "epoch 66, loss 0.92689, train_acc 0.6861, valid_acc 0.6765, Time 00:00:32,lr 0.1\n",
      "epoch 67, loss 0.92941, train_acc 0.6836, valid_acc 0.6244, Time 00:00:32,lr 0.1\n",
      "epoch 68, loss 0.92887, train_acc 0.6831, valid_acc 0.6602, Time 00:00:32,lr 0.1\n",
      "epoch 69, loss 0.93088, train_acc 0.6826, valid_acc 0.6989, Time 00:00:32,lr 0.1\n",
      "epoch 70, loss 0.92295, train_acc 0.6847, valid_acc 0.7020, Time 00:00:32,lr 0.1\n",
      "epoch 71, loss 0.92583, train_acc 0.6863, valid_acc 0.6810, Time 00:00:32,lr 0.1\n",
      "epoch 72, loss 0.92959, train_acc 0.6839, valid_acc 0.6843, Time 00:00:32,lr 0.1\n",
      "epoch 73, loss 0.93152, train_acc 0.6827, valid_acc 0.6619, Time 00:00:32,lr 0.1\n",
      "epoch 74, loss 0.92909, train_acc 0.6839, valid_acc 0.6328, Time 00:00:32,lr 0.1\n",
      "epoch 75, loss 0.92891, train_acc 0.6839, valid_acc 0.6598, Time 00:00:32,lr 0.1\n",
      "epoch 76, loss 0.93157, train_acc 0.6843, valid_acc 0.6716, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.93235, train_acc 0.6823, valid_acc 0.6870, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.92986, train_acc 0.6865, valid_acc 0.6388, Time 00:00:32,lr 0.1\n",
      "epoch 79, loss 0.92761, train_acc 0.6849, valid_acc 0.6662, Time 00:00:32,lr 0.1\n",
      "epoch 0, loss 0.80896, train_acc 0.7271, valid_acc 0.6203, Time 00:00:30,lr 0.05\n",
      "epoch 1, loss 0.88513, train_acc 0.6974, valid_acc 0.6184, Time 00:00:32,lr 0.05\n",
      "epoch 2, loss 0.90256, train_acc 0.6931, valid_acc 0.7099, Time 00:00:32,lr 0.05\n",
      "epoch 3, loss 0.90922, train_acc 0.6893, valid_acc 0.6401, Time 00:00:32,lr 0.05\n",
      "epoch 4, loss 0.91353, train_acc 0.6878, valid_acc 0.6681, Time 00:00:32,lr 0.05\n",
      "epoch 5, loss 0.90272, train_acc 0.6925, valid_acc 0.6439, Time 00:00:32,lr 0.05\n",
      "epoch 6, loss 0.91182, train_acc 0.6895, valid_acc 0.7033, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.91077, train_acc 0.6885, valid_acc 0.6915, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.91324, train_acc 0.6878, valid_acc 0.6665, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.91369, train_acc 0.6868, valid_acc 0.6593, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.91246, train_acc 0.6879, valid_acc 0.6637, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.91111, train_acc 0.6916, valid_acc 0.7026, Time 00:00:32,lr 0.05\n",
      "epoch 12, loss 0.91954, train_acc 0.6874, valid_acc 0.6634, Time 00:00:32,lr 0.05\n",
      "epoch 13, loss 0.91728, train_acc 0.6863, valid_acc 0.6654, Time 00:00:32,lr 0.05\n",
      "epoch 14, loss 0.91430, train_acc 0.6878, valid_acc 0.6514, Time 00:00:32,lr 0.05\n",
      "epoch 15, loss 0.91699, train_acc 0.6876, valid_acc 0.6709, Time 00:00:32,lr 0.05\n",
      "epoch 16, loss 0.91586, train_acc 0.6865, valid_acc 0.6684, Time 00:00:32,lr 0.05\n",
      "epoch 17, loss 0.90909, train_acc 0.6914, valid_acc 0.6667, Time 00:00:32,lr 0.05\n",
      "epoch 18, loss 0.90777, train_acc 0.6873, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 19, loss 0.91364, train_acc 0.6880, valid_acc 0.6613, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.91116, train_acc 0.6876, valid_acc 0.6495, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.90711, train_acc 0.6921, valid_acc 0.6621, Time 00:00:32,lr 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss 0.91377, train_acc 0.6874, valid_acc 0.7030, Time 00:00:32,lr 0.05\n",
      "epoch 23, loss 0.91145, train_acc 0.6884, valid_acc 0.6387, Time 00:00:32,lr 0.05\n",
      "epoch 24, loss 0.91180, train_acc 0.6891, valid_acc 0.7214, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.91215, train_acc 0.6880, valid_acc 0.6815, Time 00:00:32,lr 0.05\n",
      "epoch 26, loss 0.90945, train_acc 0.6892, valid_acc 0.6991, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.91134, train_acc 0.6903, valid_acc 0.6897, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.91023, train_acc 0.6889, valid_acc 0.6460, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.90890, train_acc 0.6892, valid_acc 0.6105, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.62314, train_acc 0.7879, valid_acc 0.8119, Time 00:00:32,lr 0.01\n",
      "epoch 31, loss 0.57048, train_acc 0.8056, valid_acc 0.8057, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.56539, train_acc 0.8086, valid_acc 0.8094, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.56050, train_acc 0.8097, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.56481, train_acc 0.8090, valid_acc 0.7986, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.56655, train_acc 0.8061, valid_acc 0.8173, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.55780, train_acc 0.8100, valid_acc 0.7797, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.55701, train_acc 0.8100, valid_acc 0.8271, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.55273, train_acc 0.8132, valid_acc 0.8075, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.55390, train_acc 0.8106, valid_acc 0.8073, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.54441, train_acc 0.8154, valid_acc 0.8010, Time 00:00:32,lr 0.01\n",
      "epoch 41, loss 0.54516, train_acc 0.8139, valid_acc 0.8111, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.53969, train_acc 0.8148, valid_acc 0.8155, Time 00:00:32,lr 0.01\n",
      "epoch 43, loss 0.53975, train_acc 0.8157, valid_acc 0.8236, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.53627, train_acc 0.8176, valid_acc 0.8247, Time 00:00:32,lr 0.01\n",
      "epoch 45, loss 0.53035, train_acc 0.8196, valid_acc 0.8118, Time 00:00:32,lr 0.01\n",
      "epoch 46, loss 0.53219, train_acc 0.8190, valid_acc 0.8203, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.53024, train_acc 0.8185, valid_acc 0.8109, Time 00:00:32,lr 0.01\n",
      "epoch 48, loss 0.52706, train_acc 0.8210, valid_acc 0.8323, Time 00:00:32,lr 0.01\n",
      "epoch 49, loss 0.52530, train_acc 0.8220, valid_acc 0.7924, Time 00:00:32,lr 0.01\n",
      "epoch 50, loss 0.52793, train_acc 0.8215, valid_acc 0.8195, Time 00:00:32,lr 0.01\n",
      "epoch 51, loss 0.52560, train_acc 0.8216, valid_acc 0.8287, Time 00:00:32,lr 0.01\n",
      "epoch 52, loss 0.51781, train_acc 0.8240, valid_acc 0.8058, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.51830, train_acc 0.8228, valid_acc 0.8306, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.51420, train_acc 0.8254, valid_acc 0.8258, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.51919, train_acc 0.8242, valid_acc 0.8067, Time 00:00:32,lr 0.01\n",
      "epoch 56, loss 0.51772, train_acc 0.8231, valid_acc 0.8133, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.50818, train_acc 0.8279, valid_acc 0.8285, Time 00:00:32,lr 0.01\n",
      "epoch 58, loss 0.51014, train_acc 0.8252, valid_acc 0.8101, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.51187, train_acc 0.8270, valid_acc 0.8182, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.35304, train_acc 0.8818, valid_acc 0.8777, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.31041, train_acc 0.8950, valid_acc 0.8848, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.29029, train_acc 0.9032, valid_acc 0.8857, Time 00:00:32,lr 0.002\n",
      "epoch 63, loss 0.28051, train_acc 0.9045, valid_acc 0.8852, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.27056, train_acc 0.9079, valid_acc 0.8854, Time 00:00:32,lr 0.002\n",
      "epoch 65, loss 0.26793, train_acc 0.9099, valid_acc 0.8821, Time 00:00:33,lr 0.002\n",
      "epoch 66, loss 0.26292, train_acc 0.9110, valid_acc 0.8788, Time 00:00:35,lr 0.002\n",
      "epoch 67, loss 0.25597, train_acc 0.9123, valid_acc 0.8886, Time 00:00:33,lr 0.002\n",
      "epoch 68, loss 0.25621, train_acc 0.9120, valid_acc 0.8809, Time 00:00:34,lr 0.002\n",
      "epoch 69, loss 0.25495, train_acc 0.9139, valid_acc 0.8817, Time 00:00:34,lr 0.002\n",
      "epoch 70, loss 0.25461, train_acc 0.9128, valid_acc 0.8820, Time 00:00:35,lr 0.002\n",
      "epoch 71, loss 0.25337, train_acc 0.9143, valid_acc 0.8872, Time 00:00:33,lr 0.002\n",
      "epoch 72, loss 0.25769, train_acc 0.9113, valid_acc 0.8756, Time 00:00:32,lr 0.002\n",
      "epoch 73, loss 0.25496, train_acc 0.9130, valid_acc 0.8843, Time 00:00:33,lr 0.002\n",
      "epoch 74, loss 0.25236, train_acc 0.9132, valid_acc 0.8803, Time 00:00:32,lr 0.002\n",
      "epoch 75, loss 0.25699, train_acc 0.9118, valid_acc 0.8812, Time 00:00:33,lr 0.002\n",
      "epoch 76, loss 0.25216, train_acc 0.9150, valid_acc 0.8855, Time 00:00:34,lr 0.002\n",
      "epoch 77, loss 0.25850, train_acc 0.9112, valid_acc 0.8700, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.24882, train_acc 0.9136, valid_acc 0.8773, Time 00:00:33,lr 0.002\n",
      "epoch 79, loss 0.25507, train_acc 0.9135, valid_acc 0.8743, Time 00:00:33,lr 0.002\n",
      "epoch 80, loss 0.25569, train_acc 0.9138, valid_acc 0.8738, Time 00:00:32,lr 0.002\n",
      "epoch 81, loss 0.25226, train_acc 0.9132, valid_acc 0.8836, Time 00:00:32,lr 0.002\n",
      "epoch 82, loss 0.24877, train_acc 0.9143, valid_acc 0.8696, Time 00:00:32,lr 0.002\n",
      "epoch 83, loss 0.24857, train_acc 0.9146, valid_acc 0.8788, Time 00:00:33,lr 0.002\n",
      "epoch 84, loss 0.25128, train_acc 0.9141, valid_acc 0.8837, Time 00:00:33,lr 0.002\n",
      "epoch 85, loss 0.24834, train_acc 0.9156, valid_acc 0.8790, Time 00:00:32,lr 0.002\n",
      "epoch 86, loss 0.24991, train_acc 0.9156, valid_acc 0.8798, Time 00:00:33,lr 0.002\n",
      "epoch 87, loss 0.24662, train_acc 0.9153, valid_acc 0.8713, Time 00:00:36,lr 0.002\n",
      "epoch 88, loss 0.25288, train_acc 0.9130, valid_acc 0.8833, Time 00:00:34,lr 0.002\n",
      "epoch 89, loss 0.24991, train_acc 0.9138, valid_acc 0.8753, Time 00:00:32,lr 0.002\n",
      "epoch 90, loss 0.16786, train_acc 0.9432, valid_acc 0.9054, Time 00:00:33,lr 0.0004\n",
      "epoch 91, loss 0.14234, train_acc 0.9528, valid_acc 0.9077, Time 00:00:33,lr 0.0004\n",
      "epoch 92, loss 0.13349, train_acc 0.9558, valid_acc 0.9064, Time 00:00:33,lr 0.0004\n",
      "epoch 93, loss 0.12081, train_acc 0.9609, valid_acc 0.9081, Time 00:00:32,lr 0.0004\n",
      "epoch 94, loss 0.11819, train_acc 0.9613, valid_acc 0.9066, Time 00:00:33,lr 0.0004\n",
      "epoch 95, loss 0.11401, train_acc 0.9624, valid_acc 0.9075, Time 00:00:33,lr 0.0004\n",
      "epoch 96, loss 0.10900, train_acc 0.9646, valid_acc 0.9096, Time 00:00:33,lr 0.0004\n",
      "epoch 97, loss 0.10655, train_acc 0.9661, valid_acc 0.9090, Time 00:00:32,lr 0.0004\n",
      "epoch 98, loss 0.10042, train_acc 0.9670, valid_acc 0.9089, Time 00:00:33,lr 0.0004\n",
      "epoch 99, loss 0.09990, train_acc 0.9671, valid_acc 0.9062, Time 00:00:33,lr 0.0004\n",
      "epoch 100, loss 0.09451, train_acc 0.9689, valid_acc 0.9094, Time 00:00:33,lr 0.0004\n",
      "epoch 101, loss 0.09150, train_acc 0.9703, valid_acc 0.9071, Time 00:00:33,lr 0.0004\n",
      "epoch 102, loss 0.09121, train_acc 0.9704, valid_acc 0.9092, Time 00:00:33,lr 0.0004\n",
      "epoch 103, loss 0.08833, train_acc 0.9716, valid_acc 0.9052, Time 00:00:32,lr 0.0004\n",
      "epoch 104, loss 0.08499, train_acc 0.9719, valid_acc 0.9048, Time 00:00:35,lr 0.0004\n",
      "epoch 105, loss 0.08679, train_acc 0.9713, valid_acc 0.9080, Time 00:00:33,lr 0.0004\n",
      "epoch 106, loss 0.08194, train_acc 0.9739, valid_acc 0.9084, Time 00:00:32,lr 0.0004\n",
      "epoch 107, loss 0.08463, train_acc 0.9727, valid_acc 0.9057, Time 00:00:32,lr 0.0004\n",
      "epoch 108, loss 0.08137, train_acc 0.9731, valid_acc 0.9066, Time 00:00:32,lr 0.0004\n",
      "epoch 109, loss 0.07945, train_acc 0.9746, valid_acc 0.9056, Time 00:00:33,lr 0.0004\n",
      "epoch 110, loss 0.07836, train_acc 0.9753, valid_acc 0.9054, Time 00:00:33,lr 0.0004\n",
      "epoch 111, loss 0.07749, train_acc 0.9753, valid_acc 0.9080, Time 00:00:34,lr 0.0004\n",
      "epoch 112, loss 0.07876, train_acc 0.9742, valid_acc 0.9073, Time 00:00:34,lr 0.0004\n",
      "epoch 113, loss 0.07657, train_acc 0.9749, valid_acc 0.9067, Time 00:00:33,lr 0.0004\n",
      "epoch 114, loss 0.07602, train_acc 0.9756, valid_acc 0.9083, Time 00:00:32,lr 0.0004\n",
      "epoch 115, loss 0.07467, train_acc 0.9759, valid_acc 0.9054, Time 00:00:33,lr 0.0004\n",
      "epoch 116, loss 0.07413, train_acc 0.9758, valid_acc 0.9057, Time 00:00:34,lr 0.0004\n",
      "epoch 117, loss 0.07005, train_acc 0.9773, valid_acc 0.9093, Time 00:00:32,lr 0.0004\n",
      "epoch 118, loss 0.07078, train_acc 0.9771, valid_acc 0.9078, Time 00:00:32,lr 0.0004\n",
      "epoch 119, loss 0.07212, train_acc 0.9771, valid_acc 0.9074, Time 00:00:32,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k7p3s1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k7p3s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T11:07:06.593926Z",
     "start_time": "2018-03-04T11:07:06.587399Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k3p1s2(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=2, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T12:30:35.597723Z",
     "start_time": "2018-03-04T11:07:06.595085Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.50262, train_acc 0.2350, valid_acc 0.3242, Time 00:00:23,lr 0.1\n",
      "epoch 1, loss 1.78462, train_acc 0.3364, valid_acc 0.4290, Time 00:00:24,lr 0.1\n",
      "epoch 2, loss 1.62260, train_acc 0.4035, valid_acc 0.4589, Time 00:00:24,lr 0.1\n",
      "epoch 3, loss 1.54967, train_acc 0.4401, valid_acc 0.4553, Time 00:00:25,lr 0.1\n",
      "epoch 4, loss 1.47895, train_acc 0.4708, valid_acc 0.4980, Time 00:00:28,lr 0.1\n",
      "epoch 5, loss 1.42545, train_acc 0.4961, valid_acc 0.5139, Time 00:00:25,lr 0.1\n",
      "epoch 6, loss 1.38317, train_acc 0.5145, valid_acc 0.5491, Time 00:00:25,lr 0.1\n",
      "epoch 7, loss 1.33647, train_acc 0.5331, valid_acc 0.5664, Time 00:00:26,lr 0.1\n",
      "epoch 8, loss 1.30906, train_acc 0.5418, valid_acc 0.5851, Time 00:00:26,lr 0.1\n",
      "epoch 9, loss 1.29921, train_acc 0.5507, valid_acc 0.5837, Time 00:00:25,lr 0.1\n",
      "epoch 10, loss 1.28389, train_acc 0.5545, valid_acc 0.5244, Time 00:00:26,lr 0.1\n",
      "epoch 11, loss 1.25926, train_acc 0.5661, valid_acc 0.5465, Time 00:00:25,lr 0.1\n",
      "epoch 12, loss 1.25738, train_acc 0.5654, valid_acc 0.5883, Time 00:00:27,lr 0.1\n",
      "epoch 13, loss 1.24461, train_acc 0.5716, valid_acc 0.5379, Time 00:00:28,lr 0.1\n",
      "epoch 14, loss 1.24978, train_acc 0.5737, valid_acc 0.6021, Time 00:00:26,lr 0.1\n",
      "epoch 15, loss 1.22359, train_acc 0.5794, valid_acc 0.5540, Time 00:00:28,lr 0.1\n",
      "epoch 16, loss 1.23121, train_acc 0.5796, valid_acc 0.5539, Time 00:00:25,lr 0.1\n",
      "epoch 17, loss 1.21983, train_acc 0.5814, valid_acc 0.5995, Time 00:00:26,lr 0.1\n",
      "epoch 18, loss 1.21139, train_acc 0.5877, valid_acc 0.5875, Time 00:00:24,lr 0.1\n",
      "epoch 19, loss 1.22130, train_acc 0.5828, valid_acc 0.5786, Time 00:00:24,lr 0.1\n",
      "epoch 20, loss 1.21126, train_acc 0.5862, valid_acc 0.6172, Time 00:00:24,lr 0.1\n",
      "epoch 21, loss 1.20354, train_acc 0.5890, valid_acc 0.6017, Time 00:00:24,lr 0.1\n",
      "epoch 22, loss 1.20505, train_acc 0.5921, valid_acc 0.5667, Time 00:00:24,lr 0.1\n",
      "epoch 23, loss 1.20091, train_acc 0.5939, valid_acc 0.6174, Time 00:00:24,lr 0.1\n",
      "epoch 24, loss 1.20286, train_acc 0.5919, valid_acc 0.5933, Time 00:00:24,lr 0.1\n",
      "epoch 25, loss 1.19615, train_acc 0.5933, valid_acc 0.6055, Time 00:00:24,lr 0.1\n",
      "epoch 26, loss 1.18975, train_acc 0.5981, valid_acc 0.5990, Time 00:00:24,lr 0.1\n",
      "epoch 27, loss 1.19463, train_acc 0.5954, valid_acc 0.6119, Time 00:00:24,lr 0.1\n",
      "epoch 28, loss 1.19693, train_acc 0.5941, valid_acc 0.5994, Time 00:00:24,lr 0.1\n",
      "epoch 29, loss 1.19191, train_acc 0.5949, valid_acc 0.5786, Time 00:00:25,lr 0.1\n",
      "epoch 30, loss 1.19812, train_acc 0.5941, valid_acc 0.6081, Time 00:00:26,lr 0.1\n",
      "epoch 31, loss 1.19063, train_acc 0.5957, valid_acc 0.6407, Time 00:00:24,lr 0.1\n",
      "epoch 32, loss 1.19470, train_acc 0.5954, valid_acc 0.5912, Time 00:00:24,lr 0.1\n",
      "epoch 33, loss 1.18433, train_acc 0.5977, valid_acc 0.5361, Time 00:00:25,lr 0.1\n",
      "epoch 34, loss 1.18371, train_acc 0.5981, valid_acc 0.6132, Time 00:00:26,lr 0.1\n",
      "epoch 35, loss 1.19304, train_acc 0.5930, valid_acc 0.5312, Time 00:00:24,lr 0.1\n",
      "epoch 36, loss 1.18548, train_acc 0.5992, valid_acc 0.6146, Time 00:00:24,lr 0.1\n",
      "epoch 37, loss 1.19769, train_acc 0.5952, valid_acc 0.5817, Time 00:00:24,lr 0.1\n",
      "epoch 38, loss 1.17540, train_acc 0.6027, valid_acc 0.5742, Time 00:00:24,lr 0.1\n",
      "epoch 39, loss 1.18462, train_acc 0.5991, valid_acc 0.5887, Time 00:00:24,lr 0.1\n",
      "epoch 40, loss 1.18646, train_acc 0.5946, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 41, loss 1.19338, train_acc 0.5940, valid_acc 0.5486, Time 00:00:25,lr 0.1\n",
      "epoch 42, loss 1.18848, train_acc 0.5971, valid_acc 0.6175, Time 00:00:26,lr 0.1\n",
      "epoch 43, loss 1.19524, train_acc 0.5988, valid_acc 0.6187, Time 00:00:29,lr 0.1\n",
      "epoch 44, loss 1.18320, train_acc 0.5983, valid_acc 0.6223, Time 00:00:26,lr 0.1\n",
      "epoch 45, loss 1.17820, train_acc 0.6003, valid_acc 0.5944, Time 00:00:24,lr 0.1\n",
      "epoch 46, loss 1.17758, train_acc 0.6003, valid_acc 0.5844, Time 00:00:24,lr 0.1\n",
      "epoch 47, loss 1.18451, train_acc 0.5996, valid_acc 0.5785, Time 00:00:24,lr 0.1\n",
      "epoch 48, loss 1.18422, train_acc 0.6030, valid_acc 0.6022, Time 00:00:24,lr 0.1\n",
      "epoch 49, loss 1.18552, train_acc 0.5979, valid_acc 0.5716, Time 00:00:24,lr 0.1\n",
      "epoch 50, loss 1.18247, train_acc 0.5989, valid_acc 0.6341, Time 00:00:25,lr 0.1\n",
      "epoch 51, loss 1.18669, train_acc 0.5968, valid_acc 0.5807, Time 00:00:26,lr 0.1\n",
      "epoch 52, loss 1.18293, train_acc 0.5982, valid_acc 0.5857, Time 00:00:25,lr 0.1\n",
      "epoch 53, loss 1.18378, train_acc 0.6022, valid_acc 0.6234, Time 00:00:25,lr 0.1\n",
      "epoch 54, loss 1.17740, train_acc 0.6008, valid_acc 0.5988, Time 00:00:24,lr 0.1\n",
      "epoch 55, loss 1.17670, train_acc 0.5992, valid_acc 0.6202, Time 00:00:24,lr 0.1\n",
      "epoch 56, loss 1.18444, train_acc 0.6006, valid_acc 0.6178, Time 00:00:24,lr 0.1\n",
      "epoch 57, loss 1.18786, train_acc 0.5994, valid_acc 0.5740, Time 00:00:25,lr 0.1\n",
      "epoch 58, loss 1.18102, train_acc 0.6033, valid_acc 0.5980, Time 00:00:27,lr 0.1\n",
      "epoch 59, loss 1.17959, train_acc 0.6037, valid_acc 0.6219, Time 00:00:25,lr 0.1\n",
      "epoch 60, loss 1.18183, train_acc 0.5997, valid_acc 0.5931, Time 00:00:24,lr 0.1\n",
      "epoch 61, loss 1.18130, train_acc 0.5995, valid_acc 0.6252, Time 00:00:24,lr 0.1\n",
      "epoch 62, loss 1.18737, train_acc 0.5990, valid_acc 0.5961, Time 00:00:24,lr 0.1\n",
      "epoch 63, loss 1.18772, train_acc 0.5994, valid_acc 0.6248, Time 00:00:24,lr 0.1\n",
      "epoch 64, loss 1.18769, train_acc 0.5959, valid_acc 0.5748, Time 00:00:25,lr 0.1\n",
      "epoch 65, loss 1.18745, train_acc 0.5981, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 66, loss 1.17686, train_acc 0.6023, valid_acc 0.6277, Time 00:00:25,lr 0.1\n",
      "epoch 67, loss 1.18466, train_acc 0.5998, valid_acc 0.6262, Time 00:00:26,lr 0.1\n",
      "epoch 68, loss 1.18047, train_acc 0.6003, valid_acc 0.6232, Time 00:00:25,lr 0.1\n",
      "epoch 69, loss 1.18025, train_acc 0.5982, valid_acc 0.5672, Time 00:00:25,lr 0.1\n",
      "epoch 70, loss 1.18145, train_acc 0.5999, valid_acc 0.5342, Time 00:00:25,lr 0.1\n",
      "epoch 71, loss 1.18055, train_acc 0.6016, valid_acc 0.6221, Time 00:00:25,lr 0.1\n",
      "epoch 72, loss 1.18384, train_acc 0.5982, valid_acc 0.6094, Time 00:00:24,lr 0.1\n",
      "epoch 73, loss 1.17971, train_acc 0.6009, valid_acc 0.6354, Time 00:00:24,lr 0.1\n",
      "epoch 74, loss 1.18438, train_acc 0.6002, valid_acc 0.6417, Time 00:00:24,lr 0.1\n",
      "epoch 75, loss 1.17807, train_acc 0.5993, valid_acc 0.6025, Time 00:00:24,lr 0.1\n",
      "epoch 76, loss 1.18557, train_acc 0.5996, valid_acc 0.6152, Time 00:00:24,lr 0.1\n",
      "epoch 77, loss 1.18545, train_acc 0.5993, valid_acc 0.5532, Time 00:00:24,lr 0.1\n",
      "epoch 78, loss 1.17480, train_acc 0.5997, valid_acc 0.5950, Time 00:00:24,lr 0.1\n",
      "epoch 79, loss 1.18309, train_acc 0.5995, valid_acc 0.6056, Time 00:00:24,lr 0.1\n",
      "epoch 0, loss 1.03273, train_acc 0.6483, valid_acc 0.6740, Time 00:00:22,lr 0.05\n",
      "epoch 1, loss 1.09674, train_acc 0.6251, valid_acc 0.6415, Time 00:00:24,lr 0.05\n",
      "epoch 2, loss 1.10671, train_acc 0.6231, valid_acc 0.6117, Time 00:00:24,lr 0.05\n",
      "epoch 3, loss 1.11485, train_acc 0.6204, valid_acc 0.6554, Time 00:00:24,lr 0.05\n",
      "epoch 4, loss 1.11149, train_acc 0.6221, valid_acc 0.6662, Time 00:00:26,lr 0.05\n",
      "epoch 5, loss 1.11029, train_acc 0.6228, valid_acc 0.6449, Time 00:00:25,lr 0.05\n",
      "epoch 6, loss 1.10534, train_acc 0.6226, valid_acc 0.6285, Time 00:00:24,lr 0.05\n",
      "epoch 7, loss 1.11431, train_acc 0.6230, valid_acc 0.5960, Time 00:00:24,lr 0.05\n",
      "epoch 8, loss 1.11183, train_acc 0.6227, valid_acc 0.6311, Time 00:00:24,lr 0.05\n",
      "epoch 9, loss 1.11037, train_acc 0.6222, valid_acc 0.6452, Time 00:00:24,lr 0.05\n",
      "epoch 10, loss 1.11295, train_acc 0.6200, valid_acc 0.5944, Time 00:00:24,lr 0.05\n",
      "epoch 11, loss 1.12221, train_acc 0.6177, valid_acc 0.6159, Time 00:00:25,lr 0.05\n",
      "epoch 12, loss 1.11013, train_acc 0.6216, valid_acc 0.6719, Time 00:00:25,lr 0.05\n",
      "epoch 13, loss 1.10863, train_acc 0.6211, valid_acc 0.6305, Time 00:00:29,lr 0.05\n",
      "epoch 14, loss 1.10922, train_acc 0.6231, valid_acc 0.6379, Time 00:00:25,lr 0.05\n",
      "epoch 15, loss 1.11353, train_acc 0.6212, valid_acc 0.6409, Time 00:00:28,lr 0.05\n",
      "epoch 16, loss 1.11767, train_acc 0.6190, valid_acc 0.6056, Time 00:00:24,lr 0.05\n",
      "epoch 17, loss 1.10862, train_acc 0.6219, valid_acc 0.6377, Time 00:00:26,lr 0.05\n",
      "epoch 18, loss 1.10578, train_acc 0.6234, valid_acc 0.6432, Time 00:00:27,lr 0.05\n",
      "epoch 19, loss 1.11750, train_acc 0.6168, valid_acc 0.5950, Time 00:00:25,lr 0.05\n",
      "epoch 20, loss 1.11371, train_acc 0.6195, valid_acc 0.6746, Time 00:00:25,lr 0.05\n",
      "epoch 21, loss 1.10867, train_acc 0.6208, valid_acc 0.5958, Time 00:00:25,lr 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss 1.11496, train_acc 0.6212, valid_acc 0.6180, Time 00:00:24,lr 0.05\n",
      "epoch 23, loss 1.11273, train_acc 0.6213, valid_acc 0.6098, Time 00:00:24,lr 0.05\n",
      "epoch 24, loss 1.11474, train_acc 0.6186, valid_acc 0.6466, Time 00:00:24,lr 0.05\n",
      "epoch 25, loss 1.11465, train_acc 0.6209, valid_acc 0.6061, Time 00:00:24,lr 0.05\n",
      "epoch 26, loss 1.11365, train_acc 0.6206, valid_acc 0.6113, Time 00:00:25,lr 0.05\n",
      "epoch 27, loss 1.11495, train_acc 0.6179, valid_acc 0.5848, Time 00:00:26,lr 0.05\n",
      "epoch 28, loss 1.10766, train_acc 0.6230, valid_acc 0.6258, Time 00:00:25,lr 0.05\n",
      "epoch 29, loss 1.11466, train_acc 0.6197, valid_acc 0.6257, Time 00:00:24,lr 0.05\n",
      "epoch 30, loss 0.83832, train_acc 0.7133, valid_acc 0.7441, Time 00:00:24,lr 0.01\n",
      "epoch 31, loss 0.77775, train_acc 0.7343, valid_acc 0.7425, Time 00:00:24,lr 0.01\n",
      "epoch 32, loss 0.78149, train_acc 0.7322, valid_acc 0.7436, Time 00:00:25,lr 0.01\n",
      "epoch 33, loss 0.77605, train_acc 0.7342, valid_acc 0.7492, Time 00:00:24,lr 0.01\n",
      "epoch 34, loss 0.77268, train_acc 0.7366, valid_acc 0.7493, Time 00:00:24,lr 0.01\n",
      "epoch 35, loss 0.77276, train_acc 0.7349, valid_acc 0.7450, Time 00:00:24,lr 0.01\n",
      "epoch 36, loss 0.77670, train_acc 0.7335, valid_acc 0.7512, Time 00:00:24,lr 0.01\n",
      "epoch 37, loss 0.76556, train_acc 0.7388, valid_acc 0.7374, Time 00:00:24,lr 0.01\n",
      "epoch 38, loss 0.76203, train_acc 0.7419, valid_acc 0.7737, Time 00:00:24,lr 0.01\n",
      "epoch 39, loss 0.76107, train_acc 0.7383, valid_acc 0.7531, Time 00:00:24,lr 0.01\n",
      "epoch 40, loss 0.75353, train_acc 0.7417, valid_acc 0.7496, Time 00:00:24,lr 0.01\n",
      "epoch 41, loss 0.75731, train_acc 0.7428, valid_acc 0.7497, Time 00:00:24,lr 0.01\n",
      "epoch 42, loss 0.75287, train_acc 0.7437, valid_acc 0.7447, Time 00:00:24,lr 0.01\n",
      "epoch 43, loss 0.74855, train_acc 0.7449, valid_acc 0.7481, Time 00:00:24,lr 0.01\n",
      "epoch 44, loss 0.74202, train_acc 0.7472, valid_acc 0.7389, Time 00:00:24,lr 0.01\n",
      "epoch 45, loss 0.74359, train_acc 0.7445, valid_acc 0.7470, Time 00:00:24,lr 0.01\n",
      "epoch 46, loss 0.73820, train_acc 0.7492, valid_acc 0.7462, Time 00:00:24,lr 0.01\n",
      "epoch 47, loss 0.73883, train_acc 0.7470, valid_acc 0.7622, Time 00:00:24,lr 0.01\n",
      "epoch 48, loss 0.73880, train_acc 0.7491, valid_acc 0.7629, Time 00:00:24,lr 0.01\n",
      "epoch 49, loss 0.73663, train_acc 0.7489, valid_acc 0.7739, Time 00:00:25,lr 0.01\n",
      "epoch 50, loss 0.73340, train_acc 0.7486, valid_acc 0.7617, Time 00:00:24,lr 0.01\n",
      "epoch 51, loss 0.73506, train_acc 0.7482, valid_acc 0.7459, Time 00:00:26,lr 0.01\n",
      "epoch 52, loss 0.73561, train_acc 0.7481, valid_acc 0.7577, Time 00:00:25,lr 0.01\n",
      "epoch 53, loss 0.73704, train_acc 0.7482, valid_acc 0.7637, Time 00:00:24,lr 0.01\n",
      "epoch 54, loss 0.72151, train_acc 0.7545, valid_acc 0.7602, Time 00:00:24,lr 0.01\n",
      "epoch 55, loss 0.72973, train_acc 0.7516, valid_acc 0.7717, Time 00:00:24,lr 0.01\n",
      "epoch 56, loss 0.72597, train_acc 0.7522, valid_acc 0.7561, Time 00:00:24,lr 0.01\n",
      "epoch 57, loss 0.72498, train_acc 0.7512, valid_acc 0.7513, Time 00:00:25,lr 0.01\n",
      "epoch 58, loss 0.72307, train_acc 0.7534, valid_acc 0.7575, Time 00:00:24,lr 0.01\n",
      "epoch 59, loss 0.72512, train_acc 0.7530, valid_acc 0.7345, Time 00:00:24,lr 0.01\n",
      "epoch 60, loss 0.57424, train_acc 0.8031, valid_acc 0.8110, Time 00:00:25,lr 0.002\n",
      "epoch 61, loss 0.53185, train_acc 0.8193, valid_acc 0.8204, Time 00:00:24,lr 0.002\n",
      "epoch 62, loss 0.52125, train_acc 0.8227, valid_acc 0.8243, Time 00:00:24,lr 0.002\n",
      "epoch 63, loss 0.50715, train_acc 0.8265, valid_acc 0.8171, Time 00:00:26,lr 0.002\n",
      "epoch 64, loss 0.50566, train_acc 0.8266, valid_acc 0.8231, Time 00:00:24,lr 0.002\n",
      "epoch 65, loss 0.49758, train_acc 0.8306, valid_acc 0.8185, Time 00:00:24,lr 0.002\n",
      "epoch 66, loss 0.49151, train_acc 0.8314, valid_acc 0.8244, Time 00:00:24,lr 0.002\n",
      "epoch 67, loss 0.49002, train_acc 0.8309, valid_acc 0.8193, Time 00:00:24,lr 0.002\n",
      "epoch 68, loss 0.49131, train_acc 0.8323, valid_acc 0.8247, Time 00:00:24,lr 0.002\n",
      "epoch 69, loss 0.48864, train_acc 0.8319, valid_acc 0.8245, Time 00:00:26,lr 0.002\n",
      "epoch 70, loss 0.48861, train_acc 0.8329, valid_acc 0.8152, Time 00:00:26,lr 0.002\n",
      "epoch 71, loss 0.49203, train_acc 0.8310, valid_acc 0.8241, Time 00:00:24,lr 0.002\n",
      "epoch 72, loss 0.48694, train_acc 0.8326, valid_acc 0.8207, Time 00:00:25,lr 0.002\n",
      "epoch 73, loss 0.48480, train_acc 0.8329, valid_acc 0.8162, Time 00:00:26,lr 0.002\n",
      "epoch 74, loss 0.49028, train_acc 0.8328, valid_acc 0.8217, Time 00:00:24,lr 0.002\n",
      "epoch 75, loss 0.48933, train_acc 0.8325, valid_acc 0.8256, Time 00:00:24,lr 0.002\n",
      "epoch 76, loss 0.48938, train_acc 0.8311, valid_acc 0.8240, Time 00:00:24,lr 0.002\n",
      "epoch 77, loss 0.48646, train_acc 0.8330, valid_acc 0.8162, Time 00:00:24,lr 0.002\n",
      "epoch 78, loss 0.48905, train_acc 0.8328, valid_acc 0.8183, Time 00:00:24,lr 0.002\n",
      "epoch 79, loss 0.48690, train_acc 0.8318, valid_acc 0.8164, Time 00:00:24,lr 0.002\n",
      "epoch 80, loss 0.48914, train_acc 0.8336, valid_acc 0.8162, Time 00:00:24,lr 0.002\n",
      "epoch 81, loss 0.49061, train_acc 0.8331, valid_acc 0.8200, Time 00:00:24,lr 0.002\n",
      "epoch 82, loss 0.48518, train_acc 0.8342, valid_acc 0.8108, Time 00:00:24,lr 0.002\n",
      "epoch 83, loss 0.48351, train_acc 0.8340, valid_acc 0.8190, Time 00:00:24,lr 0.002\n",
      "epoch 84, loss 0.48935, train_acc 0.8316, valid_acc 0.8246, Time 00:00:24,lr 0.002\n",
      "epoch 85, loss 0.48496, train_acc 0.8341, valid_acc 0.8153, Time 00:00:24,lr 0.002\n",
      "epoch 86, loss 0.48636, train_acc 0.8330, valid_acc 0.8215, Time 00:00:24,lr 0.002\n",
      "epoch 87, loss 0.48367, train_acc 0.8343, valid_acc 0.8203, Time 00:00:24,lr 0.002\n",
      "epoch 88, loss 0.48439, train_acc 0.8336, valid_acc 0.8209, Time 00:00:24,lr 0.002\n",
      "epoch 89, loss 0.48604, train_acc 0.8307, valid_acc 0.8233, Time 00:00:24,lr 0.002\n",
      "epoch 90, loss 0.40677, train_acc 0.8612, valid_acc 0.8445, Time 00:00:24,lr 0.0004\n",
      "epoch 91, loss 0.37624, train_acc 0.8704, valid_acc 0.8463, Time 00:00:24,lr 0.0004\n",
      "epoch 92, loss 0.36739, train_acc 0.8739, valid_acc 0.8504, Time 00:00:24,lr 0.0004\n",
      "epoch 93, loss 0.35553, train_acc 0.8765, valid_acc 0.8514, Time 00:00:27,lr 0.0004\n",
      "epoch 94, loss 0.35360, train_acc 0.8777, valid_acc 0.8497, Time 00:00:24,lr 0.0004\n",
      "epoch 95, loss 0.34861, train_acc 0.8797, valid_acc 0.8496, Time 00:00:24,lr 0.0004\n",
      "epoch 96, loss 0.34631, train_acc 0.8805, valid_acc 0.8536, Time 00:00:24,lr 0.0004\n",
      "epoch 97, loss 0.34272, train_acc 0.8825, valid_acc 0.8508, Time 00:00:24,lr 0.0004\n",
      "epoch 98, loss 0.33808, train_acc 0.8834, valid_acc 0.8506, Time 00:00:24,lr 0.0004\n",
      "epoch 99, loss 0.33606, train_acc 0.8850, valid_acc 0.8530, Time 00:00:24,lr 0.0004\n",
      "epoch 100, loss 0.33146, train_acc 0.8853, valid_acc 0.8521, Time 00:00:25,lr 0.0004\n",
      "epoch 101, loss 0.32776, train_acc 0.8870, valid_acc 0.8508, Time 00:00:26,lr 0.0004\n",
      "epoch 102, loss 0.32878, train_acc 0.8876, valid_acc 0.8494, Time 00:00:24,lr 0.0004\n",
      "epoch 103, loss 0.32975, train_acc 0.8859, valid_acc 0.8497, Time 00:00:25,lr 0.0004\n",
      "epoch 104, loss 0.32299, train_acc 0.8885, valid_acc 0.8508, Time 00:00:24,lr 0.0004\n",
      "epoch 105, loss 0.32167, train_acc 0.8903, valid_acc 0.8495, Time 00:00:24,lr 0.0004\n",
      "epoch 106, loss 0.32123, train_acc 0.8899, valid_acc 0.8451, Time 00:00:24,lr 0.0004\n",
      "epoch 107, loss 0.31717, train_acc 0.8901, valid_acc 0.8499, Time 00:00:24,lr 0.0004\n",
      "epoch 108, loss 0.32094, train_acc 0.8888, valid_acc 0.8480, Time 00:00:24,lr 0.0004\n",
      "epoch 109, loss 0.31830, train_acc 0.8905, valid_acc 0.8485, Time 00:00:24,lr 0.0004\n",
      "epoch 110, loss 0.31645, train_acc 0.8901, valid_acc 0.8507, Time 00:00:24,lr 0.0004\n",
      "epoch 111, loss 0.31573, train_acc 0.8911, valid_acc 0.8499, Time 00:00:24,lr 0.0004\n",
      "epoch 112, loss 0.31620, train_acc 0.8903, valid_acc 0.8511, Time 00:00:24,lr 0.0004\n",
      "epoch 113, loss 0.31348, train_acc 0.8922, valid_acc 0.8513, Time 00:00:24,lr 0.0004\n",
      "epoch 114, loss 0.31418, train_acc 0.8923, valid_acc 0.8472, Time 00:00:24,lr 0.0004\n",
      "epoch 115, loss 0.31038, train_acc 0.8922, valid_acc 0.8499, Time 00:00:24,lr 0.0004\n",
      "epoch 116, loss 0.31282, train_acc 0.8913, valid_acc 0.8484, Time 00:00:24,lr 0.0004\n",
      "epoch 117, loss 0.30687, train_acc 0.8954, valid_acc 0.8487, Time 00:00:24,lr 0.0004\n",
      "epoch 118, loss 0.31341, train_acc 0.8917, valid_acc 0.8544, Time 00:00:25,lr 0.0004\n",
      "epoch 119, loss 0.30818, train_acc 0.8922, valid_acc 0.8467, Time 00:00:24,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k3p1s2()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k3p1s2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 gluon resnet18 vs my resnet 18: use max pooling vs no max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T15:04:54.477541Z",
     "start_time": "2018-03-04T15:04:54.449458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:3])\n",
    "        net.add(*resnet.features[4:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:31:03.467325Z",
     "start_time": "2018-03-04T05:14:50.124830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.38056, train_acc 0.1657, valid_acc 0.2737, Time 00:00:56,lr 0.1\n",
      "epoch 1, loss 1.72326, train_acc 0.3529, valid_acc 0.3568, Time 00:00:57,lr 0.1\n",
      "epoch 2, loss 1.41782, train_acc 0.4814, valid_acc 0.5539, Time 00:00:56,lr 0.1\n",
      "epoch 3, loss 1.18895, train_acc 0.5769, valid_acc 0.4499, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.06284, train_acc 0.6269, valid_acc 0.6079, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 0.94214, train_acc 0.6724, valid_acc 0.6066, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.88145, train_acc 0.6946, valid_acc 0.5705, Time 00:00:57,lr 0.1\n",
      "epoch 7, loss 0.84403, train_acc 0.7099, valid_acc 0.5912, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.81196, train_acc 0.7204, valid_acc 0.7287, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.79473, train_acc 0.7280, valid_acc 0.6951, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.78191, train_acc 0.7316, valid_acc 0.6971, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.77675, train_acc 0.7329, valid_acc 0.7301, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.75665, train_acc 0.7406, valid_acc 0.6933, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.75395, train_acc 0.7421, valid_acc 0.6948, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.74696, train_acc 0.7435, valid_acc 0.7115, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.73717, train_acc 0.7483, valid_acc 0.6977, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.73225, train_acc 0.7511, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 17, loss 0.73271, train_acc 0.7495, valid_acc 0.7008, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.72847, train_acc 0.7509, valid_acc 0.6897, Time 00:00:56,lr 0.1\n",
      "epoch 19, loss 0.72694, train_acc 0.7517, valid_acc 0.5876, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.72430, train_acc 0.7523, valid_acc 0.7296, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.71836, train_acc 0.7536, valid_acc 0.6899, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.71967, train_acc 0.7531, valid_acc 0.6885, Time 00:00:56,lr 0.1\n",
      "epoch 23, loss 0.71953, train_acc 0.7555, valid_acc 0.7130, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.70939, train_acc 0.7559, valid_acc 0.6943, Time 00:00:56,lr 0.1\n",
      "epoch 25, loss 0.71292, train_acc 0.7573, valid_acc 0.7349, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.71025, train_acc 0.7568, valid_acc 0.7351, Time 00:00:56,lr 0.1\n",
      "epoch 27, loss 0.71876, train_acc 0.7551, valid_acc 0.7187, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.70902, train_acc 0.7583, valid_acc 0.7208, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.70804, train_acc 0.7554, valid_acc 0.7308, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.69986, train_acc 0.7599, valid_acc 0.7021, Time 00:00:56,lr 0.1\n",
      "epoch 31, loss 0.70725, train_acc 0.7590, valid_acc 0.7294, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.70756, train_acc 0.7607, valid_acc 0.7474, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.70504, train_acc 0.7586, valid_acc 0.7125, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.70649, train_acc 0.7594, valid_acc 0.6955, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.70067, train_acc 0.7614, valid_acc 0.7556, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.70107, train_acc 0.7616, valid_acc 0.5762, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.69916, train_acc 0.7612, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.70580, train_acc 0.7588, valid_acc 0.6723, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.69736, train_acc 0.7607, valid_acc 0.7138, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.70295, train_acc 0.7603, valid_acc 0.7250, Time 00:00:56,lr 0.1\n",
      "epoch 41, loss 0.69599, train_acc 0.7622, valid_acc 0.6537, Time 00:00:57,lr 0.1\n",
      "epoch 42, loss 0.70267, train_acc 0.7622, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 43, loss 0.70145, train_acc 0.7609, valid_acc 0.6835, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.69758, train_acc 0.7642, valid_acc 0.6921, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.69759, train_acc 0.7621, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.69861, train_acc 0.7599, valid_acc 0.6791, Time 00:00:56,lr 0.1\n",
      "epoch 47, loss 0.69245, train_acc 0.7646, valid_acc 0.7376, Time 00:00:58,lr 0.1\n",
      "epoch 48, loss 0.70034, train_acc 0.7613, valid_acc 0.6330, Time 00:01:02,lr 0.1\n",
      "epoch 49, loss 0.69684, train_acc 0.7638, valid_acc 0.7106, Time 00:00:59,lr 0.1\n",
      "epoch 50, loss 0.69907, train_acc 0.7642, valid_acc 0.7437, Time 00:01:02,lr 0.1\n",
      "epoch 51, loss 0.69313, train_acc 0.7632, valid_acc 0.7042, Time 00:01:03,lr 0.1\n",
      "epoch 52, loss 0.69796, train_acc 0.7624, valid_acc 0.6378, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.69387, train_acc 0.7634, valid_acc 0.7631, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.69179, train_acc 0.7639, valid_acc 0.6815, Time 00:00:56,lr 0.1\n",
      "epoch 55, loss 0.70082, train_acc 0.7620, valid_acc 0.7479, Time 00:00:56,lr 0.1\n",
      "epoch 56, loss 0.69588, train_acc 0.7631, valid_acc 0.7191, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.69733, train_acc 0.7639, valid_acc 0.7269, Time 00:00:57,lr 0.1\n",
      "epoch 58, loss 0.69333, train_acc 0.7659, valid_acc 0.6627, Time 00:00:56,lr 0.1\n",
      "epoch 59, loss 0.69576, train_acc 0.7629, valid_acc 0.7450, Time 00:00:56,lr 0.1\n",
      "epoch 60, loss 0.69494, train_acc 0.7634, valid_acc 0.7199, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.69445, train_acc 0.7622, valid_acc 0.7363, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.69379, train_acc 0.7620, valid_acc 0.7078, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.69461, train_acc 0.7638, valid_acc 0.7163, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.69229, train_acc 0.7644, valid_acc 0.7119, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.69231, train_acc 0.7630, valid_acc 0.7010, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.69897, train_acc 0.7628, valid_acc 0.6015, Time 00:01:00,lr 0.1\n",
      "epoch 67, loss 0.69971, train_acc 0.7618, valid_acc 0.7291, Time 00:01:00,lr 0.1\n",
      "epoch 68, loss 0.69992, train_acc 0.7627, valid_acc 0.6869, Time 00:00:59,lr 0.1\n",
      "epoch 69, loss 0.69482, train_acc 0.7612, valid_acc 0.7649, Time 00:01:03,lr 0.1\n",
      "epoch 70, loss 0.69304, train_acc 0.7629, valid_acc 0.6619, Time 00:00:57,lr 0.1\n",
      "epoch 71, loss 0.69240, train_acc 0.7654, valid_acc 0.7104, Time 00:00:56,lr 0.1\n",
      "epoch 72, loss 0.69880, train_acc 0.7608, valid_acc 0.7013, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.69195, train_acc 0.7630, valid_acc 0.6779, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.69141, train_acc 0.7646, valid_acc 0.6973, Time 00:00:59,lr 0.1\n",
      "epoch 75, loss 0.68878, train_acc 0.7666, valid_acc 0.6534, Time 00:00:59,lr 0.1\n",
      "epoch 76, loss 0.69063, train_acc 0.7644, valid_acc 0.6998, Time 00:01:00,lr 0.1\n",
      "epoch 77, loss 0.69243, train_acc 0.7637, valid_acc 0.7247, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.70374, train_acc 0.7610, valid_acc 0.6934, Time 00:00:57,lr 0.1\n",
      "epoch 79, loss 0.69154, train_acc 0.7635, valid_acc 0.7347, Time 00:01:01,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:28:52.313297Z",
     "start_time": "2018-03-04T06:33:29.684980Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.58195, train_acc 0.8029, valid_acc 0.7700, Time 00:00:52,lr 0.05\n",
      "epoch 1, loss 0.65821, train_acc 0.7764, valid_acc 0.7590, Time 00:00:55,lr 0.05\n",
      "epoch 2, loss 0.68045, train_acc 0.7693, valid_acc 0.4827, Time 00:00:56,lr 0.05\n",
      "epoch 3, loss 0.68091, train_acc 0.7667, valid_acc 0.7464, Time 00:00:56,lr 0.05\n",
      "epoch 4, loss 0.68455, train_acc 0.7671, valid_acc 0.7237, Time 00:00:56,lr 0.05\n",
      "epoch 5, loss 0.68415, train_acc 0.7694, valid_acc 0.7485, Time 00:00:57,lr 0.05\n",
      "epoch 6, loss 0.68346, train_acc 0.7678, valid_acc 0.5743, Time 00:00:58,lr 0.05\n",
      "epoch 7, loss 0.68119, train_acc 0.7682, valid_acc 0.6967, Time 00:00:56,lr 0.05\n",
      "epoch 8, loss 0.68399, train_acc 0.7670, valid_acc 0.6504, Time 00:00:58,lr 0.05\n",
      "epoch 9, loss 0.68451, train_acc 0.7676, valid_acc 0.6430, Time 00:00:57,lr 0.05\n",
      "epoch 10, loss 0.68556, train_acc 0.7673, valid_acc 0.6783, Time 00:00:59,lr 0.05\n",
      "epoch 11, loss 0.68058, train_acc 0.7669, valid_acc 0.6660, Time 00:00:57,lr 0.05\n",
      "epoch 12, loss 0.69183, train_acc 0.7643, valid_acc 0.6486, Time 00:00:57,lr 0.05\n",
      "epoch 13, loss 0.68374, train_acc 0.7672, valid_acc 0.7040, Time 00:00:57,lr 0.05\n",
      "epoch 14, loss 0.68587, train_acc 0.7647, valid_acc 0.7557, Time 00:00:58,lr 0.05\n",
      "epoch 15, loss 0.68559, train_acc 0.7704, valid_acc 0.6823, Time 00:00:57,lr 0.05\n",
      "epoch 16, loss 0.68928, train_acc 0.7640, valid_acc 0.6444, Time 00:00:59,lr 0.05\n",
      "epoch 17, loss 0.68680, train_acc 0.7662, valid_acc 0.5039, Time 00:00:58,lr 0.05\n",
      "epoch 18, loss 0.68851, train_acc 0.7660, valid_acc 0.6354, Time 00:01:01,lr 0.05\n",
      "epoch 19, loss 0.68628, train_acc 0.7667, valid_acc 0.6202, Time 00:00:59,lr 0.05\n",
      "epoch 20, loss 0.69103, train_acc 0.7665, valid_acc 0.7290, Time 00:01:01,lr 0.05\n",
      "epoch 21, loss 0.68566, train_acc 0.7666, valid_acc 0.6471, Time 00:00:59,lr 0.05\n",
      "epoch 22, loss 0.69282, train_acc 0.7654, valid_acc 0.7272, Time 00:01:01,lr 0.05\n",
      "epoch 23, loss 0.68336, train_acc 0.7706, valid_acc 0.6115, Time 00:01:02,lr 0.05\n",
      "epoch 24, loss 0.68217, train_acc 0.7695, valid_acc 0.7322, Time 00:01:00,lr 0.05\n",
      "epoch 25, loss 0.68338, train_acc 0.7686, valid_acc 0.7349, Time 00:01:01,lr 0.05\n",
      "epoch 26, loss 0.68748, train_acc 0.7667, valid_acc 0.7034, Time 00:01:04,lr 0.05\n",
      "epoch 27, loss 0.69171, train_acc 0.7651, valid_acc 0.7070, Time 00:00:59,lr 0.05\n",
      "epoch 28, loss 0.68284, train_acc 0.7680, valid_acc 0.7419, Time 00:00:57,lr 0.05\n",
      "epoch 29, loss 0.68182, train_acc 0.7672, valid_acc 0.6750, Time 00:00:57,lr 0.05\n",
      "epoch 30, loss 0.43710, train_acc 0.8514, valid_acc 0.8458, Time 00:00:56,lr 0.01\n",
      "epoch 31, loss 0.38423, train_acc 0.8686, valid_acc 0.8636, Time 00:00:56,lr 0.01\n",
      "epoch 32, loss 0.38351, train_acc 0.8686, valid_acc 0.8521, Time 00:01:01,lr 0.01\n",
      "epoch 33, loss 0.38735, train_acc 0.8679, valid_acc 0.8384, Time 00:01:00,lr 0.01\n",
      "epoch 34, loss 0.38082, train_acc 0.8711, valid_acc 0.8641, Time 00:00:56,lr 0.01\n",
      "epoch 35, loss 0.38738, train_acc 0.8672, valid_acc 0.8248, Time 00:00:55,lr 0.01\n",
      "epoch 36, loss 0.38062, train_acc 0.8709, valid_acc 0.8618, Time 00:00:55,lr 0.01\n",
      "epoch 37, loss 0.38049, train_acc 0.8718, valid_acc 0.8291, Time 00:00:57,lr 0.01\n",
      "epoch 38, loss 0.37551, train_acc 0.8724, valid_acc 0.8629, Time 00:00:58,lr 0.01\n",
      "epoch 39, loss 0.37463, train_acc 0.8717, valid_acc 0.8566, Time 00:01:00,lr 0.01\n",
      "epoch 40, loss 0.37176, train_acc 0.8730, valid_acc 0.8559, Time 00:01:00,lr 0.01\n",
      "epoch 41, loss 0.36386, train_acc 0.8775, valid_acc 0.8362, Time 00:00:56,lr 0.01\n",
      "epoch 42, loss 0.36582, train_acc 0.8751, valid_acc 0.8740, Time 00:00:56,lr 0.01\n",
      "epoch 43, loss 0.36425, train_acc 0.8759, valid_acc 0.8561, Time 00:00:56,lr 0.01\n",
      "epoch 44, loss 0.35750, train_acc 0.8785, valid_acc 0.8442, Time 00:00:56,lr 0.01\n",
      "epoch 45, loss 0.35708, train_acc 0.8781, valid_acc 0.8600, Time 00:00:57,lr 0.01\n",
      "epoch 46, loss 0.35498, train_acc 0.8784, valid_acc 0.8639, Time 00:00:56,lr 0.01\n",
      "epoch 47, loss 0.35540, train_acc 0.8789, valid_acc 0.8602, Time 00:00:56,lr 0.01\n",
      "epoch 48, loss 0.35173, train_acc 0.8803, valid_acc 0.8466, Time 00:00:56,lr 0.01\n",
      "epoch 49, loss 0.35126, train_acc 0.8813, valid_acc 0.8660, Time 00:00:56,lr 0.01\n",
      "epoch 50, loss 0.34843, train_acc 0.8821, valid_acc 0.8473, Time 00:00:56,lr 0.01\n",
      "epoch 51, loss 0.34383, train_acc 0.8816, valid_acc 0.8726, Time 00:00:57,lr 0.01\n",
      "epoch 52, loss 0.34999, train_acc 0.8809, valid_acc 0.8602, Time 00:01:01,lr 0.01\n",
      "epoch 53, loss 0.34417, train_acc 0.8835, valid_acc 0.8419, Time 00:00:56,lr 0.01\n",
      "epoch 54, loss 0.34483, train_acc 0.8828, valid_acc 0.8654, Time 00:00:56,lr 0.01\n",
      "epoch 55, loss 0.34106, train_acc 0.8856, valid_acc 0.8673, Time 00:00:56,lr 0.01\n",
      "epoch 56, loss 0.34146, train_acc 0.8839, valid_acc 0.8758, Time 00:01:01,lr 0.01\n",
      "epoch 57, loss 0.34200, train_acc 0.8854, valid_acc 0.8456, Time 00:00:58,lr 0.01\n",
      "epoch 58, loss 0.34399, train_acc 0.8828, valid_acc 0.8639, Time 00:00:58,lr 0.01\n",
      "epoch 59, loss 0.34053, train_acc 0.8847, valid_acc 0.8597, Time 00:00:58,lr 0.01\n",
      "epoch 60, loss 0.20173, train_acc 0.9333, valid_acc 0.9198, Time 00:00:56,lr 0.002\n",
      "epoch 61, loss 0.15869, train_acc 0.9455, valid_acc 0.9215, Time 00:00:56,lr 0.002\n",
      "epoch 62, loss 0.14265, train_acc 0.9519, valid_acc 0.9222, Time 00:00:56,lr 0.002\n",
      "epoch 63, loss 0.13336, train_acc 0.9559, valid_acc 0.9245, Time 00:00:58,lr 0.002\n",
      "epoch 64, loss 0.12715, train_acc 0.9568, valid_acc 0.9178, Time 00:00:56,lr 0.002\n",
      "epoch 65, loss 0.12213, train_acc 0.9591, valid_acc 0.9210, Time 00:00:58,lr 0.002\n",
      "epoch 66, loss 0.11907, train_acc 0.9592, valid_acc 0.9224, Time 00:00:57,lr 0.002\n",
      "epoch 67, loss 0.11596, train_acc 0.9613, valid_acc 0.9201, Time 00:00:56,lr 0.002\n",
      "epoch 68, loss 0.11954, train_acc 0.9596, valid_acc 0.9202, Time 00:00:56,lr 0.002\n",
      "epoch 69, loss 0.11487, train_acc 0.9614, valid_acc 0.9212, Time 00:00:56,lr 0.002\n",
      "epoch 70, loss 0.11803, train_acc 0.9598, valid_acc 0.9105, Time 00:00:56,lr 0.002\n",
      "epoch 71, loss 0.11419, train_acc 0.9616, valid_acc 0.9124, Time 00:00:56,lr 0.002\n",
      "epoch 72, loss 0.11577, train_acc 0.9608, valid_acc 0.9108, Time 00:00:56,lr 0.002\n",
      "epoch 73, loss 0.11624, train_acc 0.9604, valid_acc 0.9160, Time 00:00:56,lr 0.002\n",
      "epoch 74, loss 0.11776, train_acc 0.9603, valid_acc 0.9185, Time 00:00:56,lr 0.002\n",
      "epoch 75, loss 0.11450, train_acc 0.9622, valid_acc 0.9117, Time 00:00:56,lr 0.002\n",
      "epoch 76, loss 0.11900, train_acc 0.9613, valid_acc 0.9097, Time 00:01:00,lr 0.002\n",
      "epoch 77, loss 0.12027, train_acc 0.9592, valid_acc 0.9142, Time 00:00:57,lr 0.002\n",
      "epoch 78, loss 0.12142, train_acc 0.9593, valid_acc 0.9121, Time 00:00:56,lr 0.002\n",
      "epoch 79, loss 0.12121, train_acc 0.9592, valid_acc 0.9147, Time 00:00:57,lr 0.002\n",
      "epoch 80, loss 0.12328, train_acc 0.9580, valid_acc 0.9057, Time 00:00:57,lr 0.002\n",
      "epoch 81, loss 0.12675, train_acc 0.9566, valid_acc 0.9083, Time 00:00:56,lr 0.002\n",
      "epoch 82, loss 0.11973, train_acc 0.9583, valid_acc 0.9099, Time 00:00:58,lr 0.002\n",
      "epoch 83, loss 0.12241, train_acc 0.9586, valid_acc 0.9161, Time 00:00:55,lr 0.002\n",
      "epoch 84, loss 0.11883, train_acc 0.9593, valid_acc 0.9097, Time 00:00:56,lr 0.002\n",
      "epoch 85, loss 0.12814, train_acc 0.9568, valid_acc 0.9161, Time 00:00:56,lr 0.002\n",
      "epoch 86, loss 0.11971, train_acc 0.9590, valid_acc 0.9075, Time 00:00:55,lr 0.002\n",
      "epoch 87, loss 0.12493, train_acc 0.9573, valid_acc 0.9143, Time 00:00:56,lr 0.002\n",
      "epoch 88, loss 0.11748, train_acc 0.9599, valid_acc 0.9079, Time 00:01:00,lr 0.002\n",
      "epoch 89, loss 0.12178, train_acc 0.9585, valid_acc 0.9043, Time 00:00:55,lr 0.002\n",
      "epoch 90, loss 0.06359, train_acc 0.9809, valid_acc 0.9297, Time 00:00:56,lr 0.0004\n",
      "epoch 91, loss 0.04226, train_acc 0.9878, valid_acc 0.9347, Time 00:00:55,lr 0.0004\n",
      "epoch 92, loss 0.03643, train_acc 0.9901, valid_acc 0.9339, Time 00:00:56,lr 0.0004\n",
      "epoch 93, loss 0.03193, train_acc 0.9917, valid_acc 0.9344, Time 00:00:55,lr 0.0004\n",
      "epoch 94, loss 0.03020, train_acc 0.9919, valid_acc 0.9354, Time 00:00:56,lr 0.0004\n",
      "epoch 95, loss 0.02928, train_acc 0.9922, valid_acc 0.9342, Time 00:00:55,lr 0.0004\n",
      "epoch 96, loss 0.02602, train_acc 0.9928, valid_acc 0.9355, Time 00:00:55,lr 0.0004\n",
      "epoch 97, loss 0.02409, train_acc 0.9935, valid_acc 0.9350, Time 00:00:55,lr 0.0004\n",
      "epoch 98, loss 0.02386, train_acc 0.9939, valid_acc 0.9329, Time 00:00:56,lr 0.0004\n",
      "epoch 99, loss 0.01987, train_acc 0.9951, valid_acc 0.9358, Time 00:00:56,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.02084, train_acc 0.9946, valid_acc 0.9337, Time 00:00:57,lr 0.0004\n",
      "epoch 101, loss 0.01851, train_acc 0.9954, valid_acc 0.9360, Time 00:00:58,lr 0.0004\n",
      "epoch 102, loss 0.01870, train_acc 0.9954, valid_acc 0.9357, Time 00:00:56,lr 0.0004\n",
      "epoch 103, loss 0.01807, train_acc 0.9956, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 104, loss 0.01849, train_acc 0.9954, valid_acc 0.9336, Time 00:00:56,lr 0.0004\n",
      "epoch 105, loss 0.01738, train_acc 0.9961, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 106, loss 0.01575, train_acc 0.9961, valid_acc 0.9359, Time 00:01:01,lr 0.0004\n",
      "epoch 107, loss 0.01618, train_acc 0.9963, valid_acc 0.9350, Time 00:00:56,lr 0.0004\n",
      "epoch 108, loss 0.01708, train_acc 0.9954, valid_acc 0.9334, Time 00:00:56,lr 0.0004\n",
      "epoch 109, loss 0.01544, train_acc 0.9964, valid_acc 0.9364, Time 00:00:56,lr 0.0004\n",
      "epoch 110, loss 0.01496, train_acc 0.9965, valid_acc 0.9346, Time 00:00:57,lr 0.0004\n",
      "epoch 111, loss 0.01420, train_acc 0.9966, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 112, loss 0.01472, train_acc 0.9962, valid_acc 0.9342, Time 00:00:56,lr 0.0004\n",
      "epoch 113, loss 0.01399, train_acc 0.9968, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 114, loss 0.01392, train_acc 0.9969, valid_acc 0.9344, Time 00:00:57,lr 0.0004\n",
      "epoch 115, loss 0.01281, train_acc 0.9975, valid_acc 0.9342, Time 00:00:58,lr 0.0004\n",
      "epoch 116, loss 0.01356, train_acc 0.9968, valid_acc 0.9359, Time 00:00:56,lr 0.0004\n",
      "epoch 117, loss 0.01356, train_acc 0.9971, valid_acc 0.9362, Time 00:00:58,lr 0.0004\n",
      "epoch 118, loss 0.01320, train_acc 0.9970, valid_acc 0.9353, Time 00:00:59,lr 0.0004\n",
      "epoch 119, loss 0.01289, train_acc 0.9971, valid_acc 0.9349, Time 00:01:00,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\", ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3_no_max_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 general lr policy train resnet18_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T18:12:46.252720Z",
     "start_time": "2018-03-04T15:04:58.881087Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.15846, train_acc 0.2636, valid_acc 0.3974, Time 00:00:50,lr 0.1\n",
      "epoch 1, loss 1.62803, train_acc 0.3977, valid_acc 0.4551, Time 00:00:54,lr 0.1\n",
      "epoch 2, loss 1.41918, train_acc 0.4863, valid_acc 0.5159, Time 00:00:55,lr 0.1\n",
      "epoch 3, loss 1.19825, train_acc 0.5719, valid_acc 0.6059, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.01131, train_acc 0.6444, valid_acc 0.6491, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 0.88996, train_acc 0.6878, valid_acc 0.7103, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.78305, train_acc 0.7267, valid_acc 0.7235, Time 00:00:57,lr 0.1\n",
      "epoch 7, loss 0.69410, train_acc 0.7577, valid_acc 0.7858, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.63683, train_acc 0.7805, valid_acc 0.7856, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.60257, train_acc 0.7929, valid_acc 0.7577, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.57003, train_acc 0.8039, valid_acc 0.7868, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.54821, train_acc 0.8109, valid_acc 0.8210, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.52209, train_acc 0.8195, valid_acc 0.8023, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.50669, train_acc 0.8247, valid_acc 0.8284, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.49168, train_acc 0.8307, valid_acc 0.7976, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.48094, train_acc 0.8356, valid_acc 0.8335, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.46743, train_acc 0.8393, valid_acc 0.8340, Time 00:00:55,lr 0.1\n",
      "epoch 17, loss 0.45708, train_acc 0.8422, valid_acc 0.8399, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.44915, train_acc 0.8447, valid_acc 0.8338, Time 00:00:55,lr 0.1\n",
      "epoch 19, loss 0.44107, train_acc 0.8492, valid_acc 0.8440, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.42885, train_acc 0.8522, valid_acc 0.8271, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.41722, train_acc 0.8576, valid_acc 0.8403, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.41695, train_acc 0.8535, valid_acc 0.8352, Time 00:00:55,lr 0.1\n",
      "epoch 23, loss 0.40497, train_acc 0.8610, valid_acc 0.8293, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.40128, train_acc 0.8627, valid_acc 0.8544, Time 00:00:55,lr 0.1\n",
      "epoch 25, loss 0.39778, train_acc 0.8637, valid_acc 0.8255, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.38924, train_acc 0.8654, valid_acc 0.8475, Time 00:00:55,lr 0.1\n",
      "epoch 27, loss 0.38375, train_acc 0.8672, valid_acc 0.8315, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.38154, train_acc 0.8695, valid_acc 0.8524, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.37514, train_acc 0.8702, valid_acc 0.8548, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.37547, train_acc 0.8701, valid_acc 0.8264, Time 00:00:55,lr 0.1\n",
      "epoch 31, loss 0.36880, train_acc 0.8730, valid_acc 0.8621, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.36823, train_acc 0.8750, valid_acc 0.8452, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.36444, train_acc 0.8735, valid_acc 0.8324, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.35993, train_acc 0.8753, valid_acc 0.8566, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.35871, train_acc 0.8765, valid_acc 0.8386, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.35150, train_acc 0.8794, valid_acc 0.8577, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.35466, train_acc 0.8782, valid_acc 0.8438, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.34956, train_acc 0.8793, valid_acc 0.8389, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.34803, train_acc 0.8814, valid_acc 0.8594, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.34794, train_acc 0.8803, valid_acc 0.8586, Time 00:00:55,lr 0.1\n",
      "epoch 41, loss 0.33894, train_acc 0.8836, valid_acc 0.8612, Time 00:00:55,lr 0.1\n",
      "epoch 42, loss 0.33981, train_acc 0.8850, valid_acc 0.8690, Time 00:00:55,lr 0.1\n",
      "epoch 43, loss 0.34106, train_acc 0.8833, valid_acc 0.8577, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.33637, train_acc 0.8857, valid_acc 0.8618, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.33335, train_acc 0.8851, valid_acc 0.8739, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.33010, train_acc 0.8878, valid_acc 0.8711, Time 00:00:55,lr 0.1\n",
      "epoch 47, loss 0.33107, train_acc 0.8860, valid_acc 0.8427, Time 00:00:56,lr 0.1\n",
      "epoch 48, loss 0.33052, train_acc 0.8873, valid_acc 0.8533, Time 00:00:56,lr 0.1\n",
      "epoch 49, loss 0.32679, train_acc 0.8881, valid_acc 0.8696, Time 00:00:56,lr 0.1\n",
      "epoch 50, loss 0.33123, train_acc 0.8852, valid_acc 0.8655, Time 00:00:56,lr 0.1\n",
      "epoch 51, loss 0.33064, train_acc 0.8860, valid_acc 0.8566, Time 00:00:56,lr 0.1\n",
      "epoch 52, loss 0.32772, train_acc 0.8872, valid_acc 0.8538, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.32102, train_acc 0.8909, valid_acc 0.8782, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.32483, train_acc 0.8884, valid_acc 0.8752, Time 00:00:55,lr 0.1\n",
      "epoch 55, loss 0.32031, train_acc 0.8922, valid_acc 0.8608, Time 00:00:58,lr 0.1\n",
      "epoch 56, loss 0.32091, train_acc 0.8904, valid_acc 0.8685, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.31825, train_acc 0.8908, valid_acc 0.8731, Time 00:00:56,lr 0.1\n",
      "epoch 58, loss 0.32040, train_acc 0.8905, valid_acc 0.8439, Time 00:00:59,lr 0.1\n",
      "epoch 59, loss 0.31838, train_acc 0.8906, valid_acc 0.8543, Time 00:01:01,lr 0.1\n",
      "epoch 60, loss 0.31636, train_acc 0.8924, valid_acc 0.8622, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.31622, train_acc 0.8912, valid_acc 0.8482, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.31287, train_acc 0.8932, valid_acc 0.8822, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.30961, train_acc 0.8930, valid_acc 0.8839, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.31417, train_acc 0.8945, valid_acc 0.8728, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.31620, train_acc 0.8919, valid_acc 0.8675, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.30825, train_acc 0.8954, valid_acc 0.8811, Time 00:00:56,lr 0.1\n",
      "epoch 67, loss 0.30862, train_acc 0.8947, valid_acc 0.8865, Time 00:00:56,lr 0.1\n",
      "epoch 68, loss 0.30983, train_acc 0.8933, valid_acc 0.8414, Time 00:00:56,lr 0.1\n",
      "epoch 69, loss 0.30705, train_acc 0.8954, valid_acc 0.8583, Time 00:00:56,lr 0.1\n",
      "epoch 70, loss 0.30614, train_acc 0.8944, valid_acc 0.8689, Time 00:00:59,lr 0.1\n",
      "epoch 71, loss 0.30487, train_acc 0.8951, valid_acc 0.8686, Time 00:00:57,lr 0.1\n",
      "epoch 72, loss 0.30635, train_acc 0.8959, valid_acc 0.8605, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.30478, train_acc 0.8955, valid_acc 0.8763, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.30377, train_acc 0.8953, valid_acc 0.8621, Time 00:01:01,lr 0.1\n",
      "epoch 75, loss 0.30006, train_acc 0.8972, valid_acc 0.8836, Time 00:00:58,lr 0.1\n",
      "epoch 76, loss 0.30860, train_acc 0.8948, valid_acc 0.8890, Time 00:00:56,lr 0.1\n",
      "epoch 77, loss 0.30585, train_acc 0.8953, valid_acc 0.8825, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.30411, train_acc 0.8961, valid_acc 0.8783, Time 00:00:56,lr 0.1\n",
      "epoch 79, loss 0.30075, train_acc 0.8967, valid_acc 0.8733, Time 00:00:55,lr 0.1\n",
      "epoch 80, loss 0.16171, train_acc 0.9455, valid_acc 0.9280, Time 00:00:58,lr 0.01\n",
      "epoch 81, loss 0.12098, train_acc 0.9590, valid_acc 0.9312, Time 00:00:57,lr 0.01\n",
      "epoch 82, loss 0.10316, train_acc 0.9647, valid_acc 0.9346, Time 00:00:56,lr 0.01\n",
      "epoch 83, loss 0.08788, train_acc 0.9699, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 84, loss 0.08168, train_acc 0.9721, valid_acc 0.9355, Time 00:00:56,lr 0.01\n",
      "epoch 85, loss 0.07426, train_acc 0.9743, valid_acc 0.9345, Time 00:00:56,lr 0.01\n",
      "epoch 86, loss 0.06671, train_acc 0.9770, valid_acc 0.9364, Time 00:00:57,lr 0.01\n",
      "epoch 87, loss 0.06325, train_acc 0.9785, valid_acc 0.9351, Time 00:00:56,lr 0.01\n",
      "epoch 88, loss 0.05709, train_acc 0.9806, valid_acc 0.9365, Time 00:00:56,lr 0.01\n",
      "epoch 89, loss 0.05294, train_acc 0.9820, valid_acc 0.9345, Time 00:00:56,lr 0.01\n",
      "epoch 90, loss 0.05128, train_acc 0.9830, valid_acc 0.9374, Time 00:00:56,lr 0.01\n",
      "epoch 91, loss 0.04591, train_acc 0.9843, valid_acc 0.9338, Time 00:00:56,lr 0.01\n",
      "epoch 92, loss 0.04429, train_acc 0.9850, valid_acc 0.9394, Time 00:00:56,lr 0.01\n",
      "epoch 93, loss 0.04115, train_acc 0.9867, valid_acc 0.9374, Time 00:00:56,lr 0.01\n",
      "epoch 94, loss 0.04127, train_acc 0.9853, valid_acc 0.9354, Time 00:00:56,lr 0.01\n",
      "epoch 95, loss 0.03846, train_acc 0.9870, valid_acc 0.9360, Time 00:00:56,lr 0.01\n",
      "epoch 96, loss 0.03880, train_acc 0.9865, valid_acc 0.9355, Time 00:00:56,lr 0.01\n",
      "epoch 97, loss 0.03761, train_acc 0.9873, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 98, loss 0.03572, train_acc 0.9885, valid_acc 0.9355, Time 00:00:56,lr 0.01\n",
      "epoch 99, loss 0.03503, train_acc 0.9886, valid_acc 0.9352, Time 00:00:56,lr 0.01\n",
      "epoch 100, loss 0.03311, train_acc 0.9891, valid_acc 0.9379, Time 00:00:56,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.03337, train_acc 0.9887, valid_acc 0.9390, Time 00:00:56,lr 0.01\n",
      "epoch 102, loss 0.03445, train_acc 0.9890, valid_acc 0.9373, Time 00:00:56,lr 0.01\n",
      "epoch 103, loss 0.03368, train_acc 0.9888, valid_acc 0.9336, Time 00:00:56,lr 0.01\n",
      "epoch 104, loss 0.03256, train_acc 0.9898, valid_acc 0.9330, Time 00:00:56,lr 0.01\n",
      "epoch 105, loss 0.03219, train_acc 0.9896, valid_acc 0.9352, Time 00:00:56,lr 0.01\n",
      "epoch 106, loss 0.03368, train_acc 0.9877, valid_acc 0.9321, Time 00:00:56,lr 0.01\n",
      "epoch 107, loss 0.03187, train_acc 0.9894, valid_acc 0.9285, Time 00:00:56,lr 0.01\n",
      "epoch 108, loss 0.03688, train_acc 0.9875, valid_acc 0.9326, Time 00:00:56,lr 0.01\n",
      "epoch 109, loss 0.03780, train_acc 0.9873, valid_acc 0.9341, Time 00:00:56,lr 0.01\n",
      "epoch 110, loss 0.03586, train_acc 0.9875, valid_acc 0.9356, Time 00:00:56,lr 0.01\n",
      "epoch 111, loss 0.03375, train_acc 0.9887, valid_acc 0.9339, Time 00:00:56,lr 0.01\n",
      "epoch 112, loss 0.03716, train_acc 0.9871, valid_acc 0.9310, Time 00:00:56,lr 0.01\n",
      "epoch 113, loss 0.03485, train_acc 0.9883, valid_acc 0.9332, Time 00:00:56,lr 0.01\n",
      "epoch 114, loss 0.03535, train_acc 0.9884, valid_acc 0.9346, Time 00:00:56,lr 0.01\n",
      "epoch 115, loss 0.03670, train_acc 0.9874, valid_acc 0.9343, Time 00:00:56,lr 0.01\n",
      "epoch 116, loss 0.03895, train_acc 0.9865, valid_acc 0.9309, Time 00:00:56,lr 0.01\n",
      "epoch 117, loss 0.03772, train_acc 0.9875, valid_acc 0.9293, Time 00:00:56,lr 0.01\n",
      "epoch 118, loss 0.03680, train_acc 0.9874, valid_acc 0.9319, Time 00:00:56,lr 0.01\n",
      "epoch 119, loss 0.03773, train_acc 0.9870, valid_acc 0.9334, Time 00:00:56,lr 0.01\n",
      "epoch 120, loss 0.04006, train_acc 0.9861, valid_acc 0.9331, Time 00:00:56,lr 0.01\n",
      "epoch 121, loss 0.04120, train_acc 0.9861, valid_acc 0.9315, Time 00:00:56,lr 0.01\n",
      "epoch 122, loss 0.04192, train_acc 0.9860, valid_acc 0.9294, Time 00:00:56,lr 0.01\n",
      "epoch 123, loss 0.04074, train_acc 0.9864, valid_acc 0.9280, Time 00:00:56,lr 0.01\n",
      "epoch 124, loss 0.04274, train_acc 0.9852, valid_acc 0.9279, Time 00:00:56,lr 0.01\n",
      "epoch 125, loss 0.04207, train_acc 0.9860, valid_acc 0.9332, Time 00:00:56,lr 0.01\n",
      "epoch 126, loss 0.04055, train_acc 0.9862, valid_acc 0.9292, Time 00:00:56,lr 0.01\n",
      "epoch 127, loss 0.04287, train_acc 0.9857, valid_acc 0.9241, Time 00:00:56,lr 0.01\n",
      "epoch 128, loss 0.04564, train_acc 0.9842, valid_acc 0.9270, Time 00:00:56,lr 0.01\n",
      "epoch 129, loss 0.04434, train_acc 0.9847, valid_acc 0.9281, Time 00:00:56,lr 0.01\n",
      "epoch 130, loss 0.04602, train_acc 0.9845, valid_acc 0.9340, Time 00:00:56,lr 0.01\n",
      "epoch 131, loss 0.04299, train_acc 0.9857, valid_acc 0.9290, Time 00:00:56,lr 0.01\n",
      "epoch 132, loss 0.04678, train_acc 0.9846, valid_acc 0.9289, Time 00:00:56,lr 0.01\n",
      "epoch 133, loss 0.04226, train_acc 0.9860, valid_acc 0.9291, Time 00:00:55,lr 0.01\n",
      "epoch 134, loss 0.04568, train_acc 0.9842, valid_acc 0.9266, Time 00:00:56,lr 0.01\n",
      "epoch 135, loss 0.04841, train_acc 0.9839, valid_acc 0.9294, Time 00:00:55,lr 0.01\n",
      "epoch 136, loss 0.04353, train_acc 0.9854, valid_acc 0.9302, Time 00:00:55,lr 0.01\n",
      "epoch 137, loss 0.04479, train_acc 0.9853, valid_acc 0.9284, Time 00:00:55,lr 0.01\n",
      "epoch 138, loss 0.04460, train_acc 0.9849, valid_acc 0.9290, Time 00:00:56,lr 0.01\n",
      "epoch 139, loss 0.05203, train_acc 0.9821, valid_acc 0.9315, Time 00:00:55,lr 0.01\n",
      "epoch 140, loss 0.04833, train_acc 0.9832, valid_acc 0.9216, Time 00:00:55,lr 0.01\n",
      "epoch 141, loss 0.04433, train_acc 0.9851, valid_acc 0.9331, Time 00:00:55,lr 0.01\n",
      "epoch 142, loss 0.04645, train_acc 0.9843, valid_acc 0.9234, Time 00:00:56,lr 0.01\n",
      "epoch 143, loss 0.04340, train_acc 0.9854, valid_acc 0.9286, Time 00:00:55,lr 0.01\n",
      "epoch 144, loss 0.04866, train_acc 0.9828, valid_acc 0.9302, Time 00:00:56,lr 0.01\n",
      "epoch 145, loss 0.05015, train_acc 0.9834, valid_acc 0.9280, Time 00:00:55,lr 0.01\n",
      "epoch 146, loss 0.04580, train_acc 0.9843, valid_acc 0.9310, Time 00:00:56,lr 0.01\n",
      "epoch 147, loss 0.04804, train_acc 0.9835, valid_acc 0.9258, Time 00:00:56,lr 0.01\n",
      "epoch 148, loss 0.04823, train_acc 0.9837, valid_acc 0.9259, Time 00:00:56,lr 0.01\n",
      "epoch 149, loss 0.04857, train_acc 0.9837, valid_acc 0.9285, Time 00:00:56,lr 0.01\n",
      "epoch 150, loss 0.02528, train_acc 0.9924, valid_acc 0.9435, Time 00:00:55,lr 0.001\n",
      "epoch 151, loss 0.01440, train_acc 0.9963, valid_acc 0.9431, Time 00:00:56,lr 0.001\n",
      "epoch 152, loss 0.01215, train_acc 0.9965, valid_acc 0.9442, Time 00:00:55,lr 0.001\n",
      "epoch 153, loss 0.00935, train_acc 0.9976, valid_acc 0.9453, Time 00:00:56,lr 0.001\n",
      "epoch 154, loss 0.00826, train_acc 0.9983, valid_acc 0.9451, Time 00:00:56,lr 0.001\n",
      "epoch 155, loss 0.00747, train_acc 0.9985, valid_acc 0.9453, Time 00:00:55,lr 0.001\n",
      "epoch 156, loss 0.00651, train_acc 0.9986, valid_acc 0.9444, Time 00:00:55,lr 0.001\n",
      "epoch 157, loss 0.00666, train_acc 0.9987, valid_acc 0.9455, Time 00:00:55,lr 0.001\n",
      "epoch 158, loss 0.00589, train_acc 0.9987, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 159, loss 0.00533, train_acc 0.9989, valid_acc 0.9463, Time 00:00:56,lr 0.001\n",
      "epoch 160, loss 0.00520, train_acc 0.9987, valid_acc 0.9466, Time 00:00:56,lr 0.001\n",
      "epoch 161, loss 0.00459, train_acc 0.9991, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 162, loss 0.00516, train_acc 0.9989, valid_acc 0.9456, Time 00:00:55,lr 0.001\n",
      "epoch 163, loss 0.00442, train_acc 0.9991, valid_acc 0.9462, Time 00:00:56,lr 0.001\n",
      "epoch 164, loss 0.00472, train_acc 0.9990, valid_acc 0.9467, Time 00:00:55,lr 0.001\n",
      "epoch 165, loss 0.00432, train_acc 0.9992, valid_acc 0.9462, Time 00:00:56,lr 0.001\n",
      "epoch 166, loss 0.00408, train_acc 0.9993, valid_acc 0.9473, Time 00:00:56,lr 0.001\n",
      "epoch 167, loss 0.00390, train_acc 0.9991, valid_acc 0.9471, Time 00:00:55,lr 0.001\n",
      "epoch 168, loss 0.00405, train_acc 0.9991, valid_acc 0.9457, Time 00:00:56,lr 0.001\n",
      "epoch 169, loss 0.00342, train_acc 0.9994, valid_acc 0.9453, Time 00:00:55,lr 0.001\n",
      "epoch 170, loss 0.00392, train_acc 0.9991, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 171, loss 0.00307, train_acc 0.9994, valid_acc 0.9472, Time 00:00:56,lr 0.001\n",
      "epoch 172, loss 0.00323, train_acc 0.9995, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 173, loss 0.00349, train_acc 0.9992, valid_acc 0.9480, Time 00:00:55,lr 0.001\n",
      "epoch 174, loss 0.00285, train_acc 0.9996, valid_acc 0.9486, Time 00:00:56,lr 0.001\n",
      "epoch 175, loss 0.00360, train_acc 0.9992, valid_acc 0.9475, Time 00:00:56,lr 0.001\n",
      "epoch 176, loss 0.00316, train_acc 0.9996, valid_acc 0.9482, Time 00:00:56,lr 0.001\n",
      "epoch 177, loss 0.00330, train_acc 0.9993, valid_acc 0.9486, Time 00:00:56,lr 0.001\n",
      "epoch 178, loss 0.00291, train_acc 0.9994, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 179, loss 0.00328, train_acc 0.9993, valid_acc 0.9477, Time 00:00:55,lr 0.001\n",
      "epoch 180, loss 0.00271, train_acc 0.9995, valid_acc 0.9472, Time 00:00:56,lr 0.001\n",
      "epoch 181, loss 0.00288, train_acc 0.9994, valid_acc 0.9485, Time 00:00:56,lr 0.001\n",
      "epoch 182, loss 0.00273, train_acc 0.9995, valid_acc 0.9481, Time 00:00:56,lr 0.001\n",
      "epoch 183, loss 0.00283, train_acc 0.9993, valid_acc 0.9480, Time 00:00:55,lr 0.001\n",
      "epoch 184, loss 0.00248, train_acc 0.9996, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 185, loss 0.00256, train_acc 0.9995, valid_acc 0.9480, Time 00:00:56,lr 0.001\n",
      "epoch 186, loss 0.00240, train_acc 0.9995, valid_acc 0.9469, Time 00:00:55,lr 0.001\n",
      "epoch 187, loss 0.00271, train_acc 0.9996, valid_acc 0.9491, Time 00:00:55,lr 0.001\n",
      "epoch 188, loss 0.00239, train_acc 0.9996, valid_acc 0.9476, Time 00:00:56,lr 0.001\n",
      "epoch 189, loss 0.00219, train_acc 0.9997, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 190, loss 0.00211, train_acc 0.9997, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 191, loss 0.00261, train_acc 0.9995, valid_acc 0.9468, Time 00:00:56,lr 0.001\n",
      "epoch 192, loss 0.00210, train_acc 0.9997, valid_acc 0.9472, Time 00:00:55,lr 0.001\n",
      "epoch 193, loss 0.00229, train_acc 0.9996, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 194, loss 0.00222, train_acc 0.9996, valid_acc 0.9486, Time 00:00:56,lr 0.001\n",
      "epoch 195, loss 0.00208, train_acc 0.9996, valid_acc 0.9490, Time 00:00:55,lr 0.001\n",
      "epoch 196, loss 0.00211, train_acc 0.9997, valid_acc 0.9479, Time 00:00:56,lr 0.001\n",
      "epoch 197, loss 0.00225, train_acc 0.9996, valid_acc 0.9476, Time 00:00:56,lr 0.001\n",
      "epoch 198, loss 0.00184, train_acc 0.9997, valid_acc 0.9473, Time 00:00:56,lr 0.001\n",
      "epoch 199, loss 0.00202, train_acc 0.9997, valid_acc 0.9479, Time 00:00:56,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 general lr policy train resnet18_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T18:12:46.262239Z",
     "start_time": "2018-03-04T18:12:46.254255Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v2_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v2(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features[0])\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[2:4])\n",
    "        net.add(*resnet.features[5:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T21:20:09.948413Z",
     "start_time": "2018-03-04T18:12:46.263723Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.44951, train_acc 0.1700, valid_acc 0.2141, Time 00:00:52,lr 0.1\n",
      "epoch 1, loss 1.86926, train_acc 0.2854, valid_acc 0.3249, Time 00:00:56,lr 0.1\n",
      "epoch 2, loss 1.68936, train_acc 0.3693, valid_acc 0.4108, Time 00:00:56,lr 0.1\n",
      "epoch 3, loss 1.49349, train_acc 0.4522, valid_acc 0.4833, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.28756, train_acc 0.5355, valid_acc 0.4992, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 1.08968, train_acc 0.6136, valid_acc 0.6383, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.91630, train_acc 0.6771, valid_acc 0.6946, Time 00:00:56,lr 0.1\n",
      "epoch 7, loss 0.79485, train_acc 0.7227, valid_acc 0.7397, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.71892, train_acc 0.7512, valid_acc 0.7302, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.66048, train_acc 0.7693, valid_acc 0.7255, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.62220, train_acc 0.7848, valid_acc 0.7608, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.59447, train_acc 0.7961, valid_acc 0.7898, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.56015, train_acc 0.8065, valid_acc 0.8082, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.53974, train_acc 0.8164, valid_acc 0.8009, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.51667, train_acc 0.8223, valid_acc 0.8149, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.50768, train_acc 0.8256, valid_acc 0.7942, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.49144, train_acc 0.8322, valid_acc 0.8099, Time 00:00:56,lr 0.1\n",
      "epoch 17, loss 0.47786, train_acc 0.8355, valid_acc 0.8245, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.49227, train_acc 0.8324, valid_acc 0.8198, Time 00:00:56,lr 0.1\n",
      "epoch 19, loss 0.45751, train_acc 0.8445, valid_acc 0.8412, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.45357, train_acc 0.8442, valid_acc 0.8123, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.44426, train_acc 0.8465, valid_acc 0.8173, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.42997, train_acc 0.8532, valid_acc 0.8631, Time 00:00:56,lr 0.1\n",
      "epoch 23, loss 0.42412, train_acc 0.8549, valid_acc 0.8350, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.42291, train_acc 0.8548, valid_acc 0.8455, Time 00:00:56,lr 0.1\n",
      "epoch 25, loss 0.41066, train_acc 0.8580, valid_acc 0.8565, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.39995, train_acc 0.8625, valid_acc 0.8302, Time 00:00:56,lr 0.1\n",
      "epoch 27, loss 0.39947, train_acc 0.8624, valid_acc 0.8342, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.39504, train_acc 0.8641, valid_acc 0.8619, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.38650, train_acc 0.8688, valid_acc 0.8529, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.38075, train_acc 0.8688, valid_acc 0.8649, Time 00:00:56,lr 0.1\n",
      "epoch 31, loss 0.38200, train_acc 0.8699, valid_acc 0.8489, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.37619, train_acc 0.8716, valid_acc 0.8620, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.36805, train_acc 0.8738, valid_acc 0.8418, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.36680, train_acc 0.8751, valid_acc 0.8586, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.36398, train_acc 0.8745, valid_acc 0.8426, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.36144, train_acc 0.8759, valid_acc 0.8554, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.36382, train_acc 0.8748, valid_acc 0.8499, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.35524, train_acc 0.8783, valid_acc 0.8435, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.35648, train_acc 0.8791, valid_acc 0.8483, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.35342, train_acc 0.8771, valid_acc 0.8595, Time 00:00:56,lr 0.1\n",
      "epoch 41, loss 0.34319, train_acc 0.8819, valid_acc 0.8546, Time 00:00:56,lr 0.1\n",
      "epoch 42, loss 0.34849, train_acc 0.8813, valid_acc 0.8763, Time 00:00:56,lr 0.1\n",
      "epoch 43, loss 0.34387, train_acc 0.8830, valid_acc 0.8559, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.34304, train_acc 0.8825, valid_acc 0.8425, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.34640, train_acc 0.8812, valid_acc 0.8577, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.34386, train_acc 0.8830, valid_acc 0.8669, Time 00:00:56,lr 0.1\n",
      "epoch 47, loss 0.34541, train_acc 0.8797, valid_acc 0.8602, Time 00:00:56,lr 0.1\n",
      "epoch 48, loss 0.33837, train_acc 0.8826, valid_acc 0.8406, Time 00:00:56,lr 0.1\n",
      "epoch 49, loss 0.33817, train_acc 0.8843, valid_acc 0.8596, Time 00:00:56,lr 0.1\n",
      "epoch 50, loss 0.33068, train_acc 0.8861, valid_acc 0.8688, Time 00:00:56,lr 0.1\n",
      "epoch 51, loss 0.33042, train_acc 0.8865, valid_acc 0.8732, Time 00:00:56,lr 0.1\n",
      "epoch 52, loss 0.32935, train_acc 0.8885, valid_acc 0.8560, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.33289, train_acc 0.8855, valid_acc 0.8453, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.32762, train_acc 0.8890, valid_acc 0.8629, Time 00:00:56,lr 0.1\n",
      "epoch 55, loss 0.32562, train_acc 0.8886, valid_acc 0.8745, Time 00:00:56,lr 0.1\n",
      "epoch 56, loss 0.32622, train_acc 0.8895, valid_acc 0.8809, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.32513, train_acc 0.8890, valid_acc 0.8741, Time 00:00:56,lr 0.1\n",
      "epoch 58, loss 0.32801, train_acc 0.8882, valid_acc 0.8769, Time 00:00:56,lr 0.1\n",
      "epoch 59, loss 0.32038, train_acc 0.8905, valid_acc 0.8665, Time 00:00:56,lr 0.1\n",
      "epoch 60, loss 0.32088, train_acc 0.8897, valid_acc 0.8747, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.32201, train_acc 0.8899, valid_acc 0.8699, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.32008, train_acc 0.8901, valid_acc 0.8592, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.31851, train_acc 0.8914, valid_acc 0.8809, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.31505, train_acc 0.8915, valid_acc 0.8750, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.31955, train_acc 0.8915, valid_acc 0.8677, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.31765, train_acc 0.8909, valid_acc 0.8683, Time 00:00:56,lr 0.1\n",
      "epoch 67, loss 0.32005, train_acc 0.8906, valid_acc 0.8666, Time 00:00:56,lr 0.1\n",
      "epoch 68, loss 0.31254, train_acc 0.8927, valid_acc 0.8861, Time 00:00:56,lr 0.1\n",
      "epoch 69, loss 0.30988, train_acc 0.8934, valid_acc 0.8846, Time 00:00:56,lr 0.1\n",
      "epoch 70, loss 0.31350, train_acc 0.8937, valid_acc 0.8795, Time 00:00:56,lr 0.1\n",
      "epoch 71, loss 0.31071, train_acc 0.8928, valid_acc 0.8512, Time 00:00:56,lr 0.1\n",
      "epoch 72, loss 0.31702, train_acc 0.8909, valid_acc 0.8794, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.30821, train_acc 0.8951, valid_acc 0.8532, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.30979, train_acc 0.8936, valid_acc 0.7775, Time 00:00:56,lr 0.1\n",
      "epoch 75, loss 0.30905, train_acc 0.8950, valid_acc 0.8663, Time 00:00:56,lr 0.1\n",
      "epoch 76, loss 0.30563, train_acc 0.8947, valid_acc 0.8680, Time 00:00:56,lr 0.1\n",
      "epoch 77, loss 0.30650, train_acc 0.8947, valid_acc 0.8465, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.30130, train_acc 0.8974, valid_acc 0.8824, Time 00:00:56,lr 0.1\n",
      "epoch 79, loss 0.30596, train_acc 0.8963, valid_acc 0.8652, Time 00:00:56,lr 0.1\n",
      "epoch 80, loss 0.17533, train_acc 0.9401, valid_acc 0.9288, Time 00:00:56,lr 0.01\n",
      "epoch 81, loss 0.12621, train_acc 0.9566, valid_acc 0.9311, Time 00:00:56,lr 0.01\n",
      "epoch 82, loss 0.10747, train_acc 0.9627, valid_acc 0.9352, Time 00:00:56,lr 0.01\n",
      "epoch 83, loss 0.09768, train_acc 0.9671, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 84, loss 0.08784, train_acc 0.9699, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 85, loss 0.07791, train_acc 0.9734, valid_acc 0.9379, Time 00:00:56,lr 0.01\n",
      "epoch 86, loss 0.07193, train_acc 0.9752, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 87, loss 0.06827, train_acc 0.9765, valid_acc 0.9379, Time 00:00:56,lr 0.01\n",
      "epoch 88, loss 0.06251, train_acc 0.9786, valid_acc 0.9407, Time 00:00:56,lr 0.01\n",
      "epoch 89, loss 0.05615, train_acc 0.9811, valid_acc 0.9362, Time 00:00:56,lr 0.01\n",
      "epoch 90, loss 0.05426, train_acc 0.9811, valid_acc 0.9380, Time 00:00:56,lr 0.01\n",
      "epoch 91, loss 0.05197, train_acc 0.9825, valid_acc 0.9376, Time 00:00:56,lr 0.01\n",
      "epoch 92, loss 0.04575, train_acc 0.9843, valid_acc 0.9390, Time 00:00:56,lr 0.01\n",
      "epoch 93, loss 0.04560, train_acc 0.9843, valid_acc 0.9365, Time 00:00:56,lr 0.01\n",
      "epoch 94, loss 0.04354, train_acc 0.9850, valid_acc 0.9388, Time 00:00:56,lr 0.01\n",
      "epoch 95, loss 0.04189, train_acc 0.9855, valid_acc 0.9404, Time 00:00:56,lr 0.01\n",
      "epoch 96, loss 0.04202, train_acc 0.9863, valid_acc 0.9385, Time 00:00:56,lr 0.01\n",
      "epoch 97, loss 0.03752, train_acc 0.9877, valid_acc 0.9356, Time 00:00:56,lr 0.01\n",
      "epoch 98, loss 0.03980, train_acc 0.9863, valid_acc 0.9395, Time 00:00:56,lr 0.01\n",
      "epoch 99, loss 0.03526, train_acc 0.9883, valid_acc 0.9376, Time 00:00:56,lr 0.01\n",
      "epoch 100, loss 0.03696, train_acc 0.9875, valid_acc 0.9352, Time 00:00:56,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.03855, train_acc 0.9868, valid_acc 0.9370, Time 00:00:56,lr 0.01\n",
      "epoch 102, loss 0.03788, train_acc 0.9867, valid_acc 0.9364, Time 00:00:56,lr 0.01\n",
      "epoch 103, loss 0.03558, train_acc 0.9879, valid_acc 0.9353, Time 00:00:56,lr 0.01\n",
      "epoch 104, loss 0.03517, train_acc 0.9886, valid_acc 0.9349, Time 00:00:56,lr 0.01\n",
      "epoch 105, loss 0.03818, train_acc 0.9870, valid_acc 0.9386, Time 00:00:56,lr 0.01\n",
      "epoch 106, loss 0.03449, train_acc 0.9884, valid_acc 0.9323, Time 00:00:56,lr 0.01\n",
      "epoch 107, loss 0.03584, train_acc 0.9881, valid_acc 0.9361, Time 00:00:56,lr 0.01\n",
      "epoch 108, loss 0.03502, train_acc 0.9877, valid_acc 0.9363, Time 00:00:56,lr 0.01\n",
      "epoch 109, loss 0.03759, train_acc 0.9875, valid_acc 0.9258, Time 00:00:56,lr 0.01\n",
      "epoch 110, loss 0.03876, train_acc 0.9870, valid_acc 0.9316, Time 00:00:56,lr 0.01\n",
      "epoch 111, loss 0.03911, train_acc 0.9871, valid_acc 0.9317, Time 00:00:56,lr 0.01\n",
      "epoch 112, loss 0.03818, train_acc 0.9868, valid_acc 0.9322, Time 00:00:56,lr 0.01\n",
      "epoch 113, loss 0.03854, train_acc 0.9872, valid_acc 0.9324, Time 00:00:56,lr 0.01\n",
      "epoch 114, loss 0.03957, train_acc 0.9870, valid_acc 0.9312, Time 00:00:56,lr 0.01\n",
      "epoch 115, loss 0.03727, train_acc 0.9877, valid_acc 0.9328, Time 00:00:56,lr 0.01\n",
      "epoch 116, loss 0.03950, train_acc 0.9867, valid_acc 0.9341, Time 00:00:56,lr 0.01\n",
      "epoch 117, loss 0.03904, train_acc 0.9873, valid_acc 0.9369, Time 00:00:56,lr 0.01\n",
      "epoch 118, loss 0.04037, train_acc 0.9864, valid_acc 0.9323, Time 00:00:56,lr 0.01\n",
      "epoch 119, loss 0.04153, train_acc 0.9859, valid_acc 0.9333, Time 00:00:56,lr 0.01\n",
      "epoch 120, loss 0.04340, train_acc 0.9856, valid_acc 0.9303, Time 00:00:56,lr 0.01\n",
      "epoch 121, loss 0.03868, train_acc 0.9870, valid_acc 0.9308, Time 00:00:56,lr 0.01\n",
      "epoch 122, loss 0.04523, train_acc 0.9845, valid_acc 0.9321, Time 00:00:56,lr 0.01\n",
      "epoch 123, loss 0.04460, train_acc 0.9852, valid_acc 0.9260, Time 00:00:56,lr 0.01\n",
      "epoch 124, loss 0.04559, train_acc 0.9850, valid_acc 0.9319, Time 00:00:56,lr 0.01\n",
      "epoch 125, loss 0.04768, train_acc 0.9842, valid_acc 0.9292, Time 00:00:56,lr 0.01\n",
      "epoch 126, loss 0.04399, train_acc 0.9851, valid_acc 0.9295, Time 00:00:56,lr 0.01\n",
      "epoch 127, loss 0.04340, train_acc 0.9852, valid_acc 0.9296, Time 00:00:56,lr 0.01\n",
      "epoch 128, loss 0.04687, train_acc 0.9844, valid_acc 0.9247, Time 00:00:56,lr 0.01\n",
      "epoch 129, loss 0.04552, train_acc 0.9849, valid_acc 0.9326, Time 00:00:56,lr 0.01\n",
      "epoch 130, loss 0.04505, train_acc 0.9845, valid_acc 0.9259, Time 00:00:56,lr 0.01\n",
      "epoch 131, loss 0.04696, train_acc 0.9838, valid_acc 0.9226, Time 00:00:56,lr 0.01\n",
      "epoch 132, loss 0.04853, train_acc 0.9839, valid_acc 0.9269, Time 00:00:56,lr 0.01\n",
      "epoch 133, loss 0.05106, train_acc 0.9828, valid_acc 0.9255, Time 00:00:56,lr 0.01\n",
      "epoch 134, loss 0.04962, train_acc 0.9829, valid_acc 0.9282, Time 00:00:56,lr 0.01\n",
      "epoch 135, loss 0.04603, train_acc 0.9844, valid_acc 0.9287, Time 00:00:56,lr 0.01\n",
      "epoch 136, loss 0.05080, train_acc 0.9827, valid_acc 0.9321, Time 00:00:56,lr 0.01\n",
      "epoch 137, loss 0.04662, train_acc 0.9839, valid_acc 0.9287, Time 00:00:56,lr 0.01\n",
      "epoch 138, loss 0.04839, train_acc 0.9832, valid_acc 0.9236, Time 00:00:56,lr 0.01\n",
      "epoch 139, loss 0.05286, train_acc 0.9814, valid_acc 0.9283, Time 00:00:56,lr 0.01\n",
      "epoch 140, loss 0.04797, train_acc 0.9834, valid_acc 0.9238, Time 00:00:56,lr 0.01\n",
      "epoch 141, loss 0.04590, train_acc 0.9841, valid_acc 0.9296, Time 00:00:56,lr 0.01\n",
      "epoch 142, loss 0.05083, train_acc 0.9826, valid_acc 0.9292, Time 00:00:56,lr 0.01\n",
      "epoch 143, loss 0.04744, train_acc 0.9837, valid_acc 0.9221, Time 00:00:56,lr 0.01\n",
      "epoch 144, loss 0.04808, train_acc 0.9835, valid_acc 0.9270, Time 00:00:56,lr 0.01\n",
      "epoch 145, loss 0.05711, train_acc 0.9806, valid_acc 0.9244, Time 00:00:56,lr 0.01\n",
      "epoch 146, loss 0.05352, train_acc 0.9817, valid_acc 0.9268, Time 00:00:56,lr 0.01\n",
      "epoch 147, loss 0.05069, train_acc 0.9826, valid_acc 0.9252, Time 00:00:56,lr 0.01\n",
      "epoch 148, loss 0.05457, train_acc 0.9812, valid_acc 0.9238, Time 00:00:56,lr 0.01\n",
      "epoch 149, loss 0.04820, train_acc 0.9835, valid_acc 0.9207, Time 00:00:56,lr 0.01\n",
      "epoch 150, loss 0.02426, train_acc 0.9927, valid_acc 0.9386, Time 00:00:56,lr 0.001\n",
      "epoch 151, loss 0.01617, train_acc 0.9954, valid_acc 0.9411, Time 00:00:56,lr 0.001\n",
      "epoch 152, loss 0.01236, train_acc 0.9969, valid_acc 0.9421, Time 00:00:56,lr 0.001\n",
      "epoch 153, loss 0.01000, train_acc 0.9978, valid_acc 0.9432, Time 00:00:56,lr 0.001\n",
      "epoch 154, loss 0.00899, train_acc 0.9979, valid_acc 0.9432, Time 00:00:56,lr 0.001\n",
      "epoch 155, loss 0.00864, train_acc 0.9978, valid_acc 0.9441, Time 00:00:56,lr 0.001\n",
      "epoch 156, loss 0.00746, train_acc 0.9982, valid_acc 0.9453, Time 00:00:56,lr 0.001\n",
      "epoch 157, loss 0.00767, train_acc 0.9983, valid_acc 0.9448, Time 00:00:56,lr 0.001\n",
      "epoch 158, loss 0.00699, train_acc 0.9985, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 159, loss 0.00615, train_acc 0.9988, valid_acc 0.9464, Time 00:00:56,lr 0.001\n",
      "epoch 160, loss 0.00609, train_acc 0.9987, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 161, loss 0.00539, train_acc 0.9989, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 162, loss 0.00604, train_acc 0.9984, valid_acc 0.9447, Time 00:00:56,lr 0.001\n",
      "epoch 163, loss 0.00496, train_acc 0.9990, valid_acc 0.9453, Time 00:00:56,lr 0.001\n",
      "epoch 164, loss 0.00543, train_acc 0.9988, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 165, loss 0.00462, train_acc 0.9989, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 166, loss 0.00484, train_acc 0.9989, valid_acc 0.9454, Time 00:00:56,lr 0.001\n",
      "epoch 167, loss 0.00405, train_acc 0.9992, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 168, loss 0.00447, train_acc 0.9990, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 169, loss 0.00409, train_acc 0.9991, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 170, loss 0.00435, train_acc 0.9990, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 171, loss 0.00335, train_acc 0.9993, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 172, loss 0.00361, train_acc 0.9993, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 173, loss 0.00369, train_acc 0.9994, valid_acc 0.9470, Time 00:00:56,lr 0.001\n",
      "epoch 174, loss 0.00371, train_acc 0.9992, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 175, loss 0.00349, train_acc 0.9993, valid_acc 0.9463, Time 00:00:56,lr 0.001\n",
      "epoch 176, loss 0.00300, train_acc 0.9995, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 177, loss 0.00323, train_acc 0.9994, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 178, loss 0.00298, train_acc 0.9994, valid_acc 0.9462, Time 00:00:55,lr 0.001\n",
      "epoch 179, loss 0.00293, train_acc 0.9995, valid_acc 0.9467, Time 00:00:56,lr 0.001\n",
      "epoch 180, loss 0.00275, train_acc 0.9996, valid_acc 0.9463, Time 00:00:56,lr 0.001\n",
      "epoch 181, loss 0.00334, train_acc 0.9992, valid_acc 0.9469, Time 00:00:56,lr 0.001\n",
      "epoch 182, loss 0.00327, train_acc 0.9994, valid_acc 0.9464, Time 00:00:56,lr 0.001\n",
      "epoch 183, loss 0.00269, train_acc 0.9995, valid_acc 0.9459, Time 00:00:55,lr 0.001\n",
      "epoch 184, loss 0.00283, train_acc 0.9995, valid_acc 0.9472, Time 00:00:56,lr 0.001\n",
      "epoch 185, loss 0.00267, train_acc 0.9996, valid_acc 0.9475, Time 00:00:56,lr 0.001\n",
      "epoch 186, loss 0.00273, train_acc 0.9996, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 187, loss 0.00273, train_acc 0.9995, valid_acc 0.9479, Time 00:00:56,lr 0.001\n",
      "epoch 188, loss 0.00232, train_acc 0.9997, valid_acc 0.9450, Time 00:00:56,lr 0.001\n",
      "epoch 189, loss 0.00264, train_acc 0.9995, valid_acc 0.9464, Time 00:00:56,lr 0.001\n",
      "epoch 190, loss 0.00256, train_acc 0.9994, valid_acc 0.9470, Time 00:00:56,lr 0.001\n",
      "epoch 191, loss 0.00225, train_acc 0.9997, valid_acc 0.9462, Time 00:00:56,lr 0.001\n",
      "epoch 192, loss 0.00242, train_acc 0.9997, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 193, loss 0.00261, train_acc 0.9996, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 194, loss 0.00224, train_acc 0.9997, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 195, loss 0.00238, train_acc 0.9995, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 196, loss 0.00232, train_acc 0.9996, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 197, loss 0.00220, train_acc 0.9996, valid_acc 0.9468, Time 00:00:56,lr 0.001\n",
      "epoch 198, loss 0.00265, train_acc 0.9995, valid_acc 0.9454, Time 00:00:56,lr 0.001\n",
      "epoch 199, loss 0.00222, train_acc 0.9996, valid_acc 0.9473, Time 00:00:56,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v2_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 general lr policy train my resnet18v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T03:45:19.543171Z",
     "start_time": "2018-03-05T02:01:38.930989Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.75249, train_acc 0.3416, valid_acc 0.4678, Time 00:00:31,lr 0.1\n",
      "epoch 1, loss 1.23604, train_acc 0.5568, valid_acc 0.5724, Time 00:00:32,lr 0.1\n",
      "epoch 2, loss 0.93259, train_acc 0.6715, valid_acc 0.7011, Time 00:00:32,lr 0.1\n",
      "epoch 3, loss 0.78639, train_acc 0.7277, valid_acc 0.6987, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 0.68671, train_acc 0.7642, valid_acc 0.7434, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 0.62936, train_acc 0.7856, valid_acc 0.7396, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 0.59543, train_acc 0.7951, valid_acc 0.7949, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 0.56563, train_acc 0.8073, valid_acc 0.7511, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.54190, train_acc 0.8125, valid_acc 0.7714, Time 00:00:30,lr 0.1\n",
      "epoch 9, loss 0.51894, train_acc 0.8220, valid_acc 0.8141, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.50352, train_acc 0.8259, valid_acc 0.7803, Time 00:00:30,lr 0.1\n",
      "epoch 11, loss 0.49073, train_acc 0.8327, valid_acc 0.8127, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 0.47917, train_acc 0.8354, valid_acc 0.8078, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.47001, train_acc 0.8364, valid_acc 0.8317, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.45984, train_acc 0.8415, valid_acc 0.8142, Time 00:00:30,lr 0.1\n",
      "epoch 15, loss 0.45590, train_acc 0.8441, valid_acc 0.7748, Time 00:00:30,lr 0.1\n",
      "epoch 16, loss 0.44058, train_acc 0.8479, valid_acc 0.8230, Time 00:00:30,lr 0.1\n",
      "epoch 17, loss 0.43728, train_acc 0.8481, valid_acc 0.8120, Time 00:00:30,lr 0.1\n",
      "epoch 18, loss 0.43228, train_acc 0.8511, valid_acc 0.7777, Time 00:00:30,lr 0.1\n",
      "epoch 19, loss 0.42770, train_acc 0.8548, valid_acc 0.8265, Time 00:00:30,lr 0.1\n",
      "epoch 20, loss 0.42149, train_acc 0.8558, valid_acc 0.8383, Time 00:00:30,lr 0.1\n",
      "epoch 21, loss 0.41593, train_acc 0.8559, valid_acc 0.8471, Time 00:00:30,lr 0.1\n",
      "epoch 22, loss 0.41718, train_acc 0.8565, valid_acc 0.8438, Time 00:00:30,lr 0.1\n",
      "epoch 23, loss 0.41097, train_acc 0.8591, valid_acc 0.8104, Time 00:00:30,lr 0.1\n",
      "epoch 24, loss 0.40569, train_acc 0.8603, valid_acc 0.8265, Time 00:00:30,lr 0.1\n",
      "epoch 25, loss 0.39741, train_acc 0.8625, valid_acc 0.8375, Time 00:00:30,lr 0.1\n",
      "epoch 26, loss 0.39868, train_acc 0.8648, valid_acc 0.8310, Time 00:00:30,lr 0.1\n",
      "epoch 27, loss 0.39217, train_acc 0.8654, valid_acc 0.8222, Time 00:00:30,lr 0.1\n",
      "epoch 28, loss 0.39285, train_acc 0.8661, valid_acc 0.8436, Time 00:00:30,lr 0.1\n",
      "epoch 29, loss 0.38964, train_acc 0.8661, valid_acc 0.8419, Time 00:00:30,lr 0.1\n",
      "epoch 30, loss 0.38233, train_acc 0.8675, valid_acc 0.8302, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.38502, train_acc 0.8687, valid_acc 0.8512, Time 00:00:30,lr 0.1\n",
      "epoch 32, loss 0.37983, train_acc 0.8693, valid_acc 0.8574, Time 00:00:30,lr 0.1\n",
      "epoch 33, loss 0.37806, train_acc 0.8686, valid_acc 0.8553, Time 00:00:30,lr 0.1\n",
      "epoch 34, loss 0.37425, train_acc 0.8709, valid_acc 0.8625, Time 00:00:30,lr 0.1\n",
      "epoch 35, loss 0.37156, train_acc 0.8724, valid_acc 0.8599, Time 00:00:30,lr 0.1\n",
      "epoch 36, loss 0.37042, train_acc 0.8747, valid_acc 0.8524, Time 00:00:30,lr 0.1\n",
      "epoch 37, loss 0.37193, train_acc 0.8734, valid_acc 0.8277, Time 00:00:30,lr 0.1\n",
      "epoch 38, loss 0.37142, train_acc 0.8734, valid_acc 0.8223, Time 00:00:30,lr 0.1\n",
      "epoch 39, loss 0.36700, train_acc 0.8739, valid_acc 0.8126, Time 00:00:30,lr 0.1\n",
      "epoch 40, loss 0.36533, train_acc 0.8738, valid_acc 0.8582, Time 00:00:30,lr 0.1\n",
      "epoch 41, loss 0.36709, train_acc 0.8759, valid_acc 0.8450, Time 00:00:30,lr 0.1\n",
      "epoch 42, loss 0.36452, train_acc 0.8751, valid_acc 0.8551, Time 00:00:30,lr 0.1\n",
      "epoch 43, loss 0.36363, train_acc 0.8752, valid_acc 0.8559, Time 00:00:30,lr 0.1\n",
      "epoch 44, loss 0.36318, train_acc 0.8768, valid_acc 0.8331, Time 00:00:30,lr 0.1\n",
      "epoch 45, loss 0.36180, train_acc 0.8765, valid_acc 0.8542, Time 00:00:30,lr 0.1\n",
      "epoch 46, loss 0.35892, train_acc 0.8770, valid_acc 0.8296, Time 00:00:30,lr 0.1\n",
      "epoch 47, loss 0.36113, train_acc 0.8765, valid_acc 0.8493, Time 00:00:30,lr 0.1\n",
      "epoch 48, loss 0.35691, train_acc 0.8780, valid_acc 0.8250, Time 00:00:30,lr 0.1\n",
      "epoch 49, loss 0.35613, train_acc 0.8781, valid_acc 0.8409, Time 00:00:30,lr 0.1\n",
      "epoch 50, loss 0.35971, train_acc 0.8755, valid_acc 0.8462, Time 00:00:30,lr 0.1\n",
      "epoch 51, loss 0.35123, train_acc 0.8807, valid_acc 0.8555, Time 00:00:30,lr 0.1\n",
      "epoch 52, loss 0.35600, train_acc 0.8809, valid_acc 0.8634, Time 00:00:30,lr 0.1\n",
      "epoch 53, loss 0.34899, train_acc 0.8804, valid_acc 0.8415, Time 00:00:30,lr 0.1\n",
      "epoch 54, loss 0.35340, train_acc 0.8780, valid_acc 0.8486, Time 00:00:30,lr 0.1\n",
      "epoch 55, loss 0.35170, train_acc 0.8798, valid_acc 0.8252, Time 00:00:30,lr 0.1\n",
      "epoch 56, loss 0.35164, train_acc 0.8809, valid_acc 0.8500, Time 00:00:30,lr 0.1\n",
      "epoch 57, loss 0.34906, train_acc 0.8793, valid_acc 0.8432, Time 00:00:30,lr 0.1\n",
      "epoch 58, loss 0.35115, train_acc 0.8790, valid_acc 0.8634, Time 00:00:30,lr 0.1\n",
      "epoch 59, loss 0.35323, train_acc 0.8779, valid_acc 0.8571, Time 00:00:30,lr 0.1\n",
      "epoch 60, loss 0.34844, train_acc 0.8792, valid_acc 0.8580, Time 00:00:30,lr 0.1\n",
      "epoch 61, loss 0.34465, train_acc 0.8826, valid_acc 0.8559, Time 00:00:30,lr 0.1\n",
      "epoch 62, loss 0.34941, train_acc 0.8816, valid_acc 0.8703, Time 00:00:30,lr 0.1\n",
      "epoch 63, loss 0.35071, train_acc 0.8794, valid_acc 0.8159, Time 00:00:30,lr 0.1\n",
      "epoch 64, loss 0.34600, train_acc 0.8824, valid_acc 0.8438, Time 00:00:30,lr 0.1\n",
      "epoch 65, loss 0.34546, train_acc 0.8836, valid_acc 0.8596, Time 00:00:30,lr 0.1\n",
      "epoch 66, loss 0.34340, train_acc 0.8827, valid_acc 0.8552, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.34738, train_acc 0.8812, valid_acc 0.8569, Time 00:00:30,lr 0.1\n",
      "epoch 68, loss 0.34896, train_acc 0.8806, valid_acc 0.8501, Time 00:00:30,lr 0.1\n",
      "epoch 69, loss 0.34608, train_acc 0.8815, valid_acc 0.8504, Time 00:00:30,lr 0.1\n",
      "epoch 70, loss 0.34501, train_acc 0.8821, valid_acc 0.8615, Time 00:00:30,lr 0.1\n",
      "epoch 71, loss 0.34624, train_acc 0.8812, valid_acc 0.8541, Time 00:00:30,lr 0.1\n",
      "epoch 72, loss 0.34812, train_acc 0.8805, valid_acc 0.8727, Time 00:00:30,lr 0.1\n",
      "epoch 73, loss 0.34509, train_acc 0.8817, valid_acc 0.8514, Time 00:00:30,lr 0.1\n",
      "epoch 74, loss 0.34422, train_acc 0.8802, valid_acc 0.8745, Time 00:00:30,lr 0.1\n",
      "epoch 75, loss 0.34185, train_acc 0.8834, valid_acc 0.8565, Time 00:00:30,lr 0.1\n",
      "epoch 76, loss 0.34527, train_acc 0.8807, valid_acc 0.8577, Time 00:00:30,lr 0.1\n",
      "epoch 77, loss 0.33983, train_acc 0.8838, valid_acc 0.8720, Time 00:00:30,lr 0.1\n",
      "epoch 78, loss 0.33928, train_acc 0.8850, valid_acc 0.8603, Time 00:00:30,lr 0.1\n",
      "epoch 79, loss 0.34337, train_acc 0.8827, valid_acc 0.8280, Time 00:00:30,lr 0.1\n",
      "epoch 80, loss 0.19619, train_acc 0.9336, valid_acc 0.9207, Time 00:00:30,lr 0.01\n",
      "epoch 81, loss 0.14760, train_acc 0.9501, valid_acc 0.9249, Time 00:00:30,lr 0.01\n",
      "epoch 82, loss 0.12989, train_acc 0.9549, valid_acc 0.9265, Time 00:00:30,lr 0.01\n",
      "epoch 83, loss 0.11951, train_acc 0.9593, valid_acc 0.9292, Time 00:00:30,lr 0.01\n",
      "epoch 84, loss 0.10738, train_acc 0.9632, valid_acc 0.9288, Time 00:00:30,lr 0.01\n",
      "epoch 85, loss 0.10071, train_acc 0.9657, valid_acc 0.9295, Time 00:00:30,lr 0.01\n",
      "epoch 86, loss 0.09417, train_acc 0.9681, valid_acc 0.9312, Time 00:00:30,lr 0.01\n",
      "epoch 87, loss 0.08605, train_acc 0.9708, valid_acc 0.9300, Time 00:00:30,lr 0.01\n",
      "epoch 88, loss 0.08259, train_acc 0.9722, valid_acc 0.9313, Time 00:00:30,lr 0.01\n",
      "epoch 89, loss 0.07684, train_acc 0.9739, valid_acc 0.9288, Time 00:00:31,lr 0.01\n",
      "epoch 90, loss 0.07300, train_acc 0.9756, valid_acc 0.9294, Time 00:00:30,lr 0.01\n",
      "epoch 91, loss 0.07099, train_acc 0.9757, valid_acc 0.9294, Time 00:00:30,lr 0.01\n",
      "epoch 92, loss 0.06592, train_acc 0.9776, valid_acc 0.9317, Time 00:00:30,lr 0.01\n",
      "epoch 93, loss 0.06440, train_acc 0.9783, valid_acc 0.9258, Time 00:00:30,lr 0.01\n",
      "epoch 94, loss 0.06085, train_acc 0.9799, valid_acc 0.9287, Time 00:00:30,lr 0.01\n",
      "epoch 95, loss 0.06026, train_acc 0.9803, valid_acc 0.9313, Time 00:00:30,lr 0.01\n",
      "epoch 96, loss 0.05759, train_acc 0.9805, valid_acc 0.9257, Time 00:00:30,lr 0.01\n",
      "epoch 97, loss 0.05517, train_acc 0.9812, valid_acc 0.9270, Time 00:00:30,lr 0.01\n",
      "epoch 98, loss 0.05672, train_acc 0.9807, valid_acc 0.9301, Time 00:00:30,lr 0.01\n",
      "epoch 99, loss 0.05231, train_acc 0.9819, valid_acc 0.9292, Time 00:00:30,lr 0.01\n",
      "epoch 100, loss 0.05057, train_acc 0.9828, valid_acc 0.9245, Time 00:00:30,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.05123, train_acc 0.9828, valid_acc 0.9259, Time 00:00:30,lr 0.01\n",
      "epoch 102, loss 0.04977, train_acc 0.9830, valid_acc 0.9254, Time 00:00:30,lr 0.01\n",
      "epoch 103, loss 0.05176, train_acc 0.9827, valid_acc 0.9254, Time 00:00:30,lr 0.01\n",
      "epoch 104, loss 0.05073, train_acc 0.9826, valid_acc 0.9237, Time 00:00:30,lr 0.01\n",
      "epoch 105, loss 0.05045, train_acc 0.9829, valid_acc 0.9311, Time 00:00:30,lr 0.01\n",
      "epoch 106, loss 0.04769, train_acc 0.9841, valid_acc 0.9267, Time 00:00:35,lr 0.01\n",
      "epoch 107, loss 0.05069, train_acc 0.9823, valid_acc 0.9261, Time 00:00:34,lr 0.01\n",
      "epoch 108, loss 0.04908, train_acc 0.9833, valid_acc 0.9308, Time 00:00:31,lr 0.01\n",
      "epoch 109, loss 0.05262, train_acc 0.9825, valid_acc 0.9267, Time 00:00:30,lr 0.01\n",
      "epoch 110, loss 0.05038, train_acc 0.9829, valid_acc 0.9237, Time 00:00:30,lr 0.01\n",
      "epoch 111, loss 0.04573, train_acc 0.9848, valid_acc 0.9262, Time 00:00:30,lr 0.01\n",
      "epoch 112, loss 0.05370, train_acc 0.9811, valid_acc 0.9257, Time 00:00:30,lr 0.01\n",
      "epoch 113, loss 0.05240, train_acc 0.9824, valid_acc 0.9216, Time 00:00:30,lr 0.01\n",
      "epoch 114, loss 0.05037, train_acc 0.9833, valid_acc 0.9265, Time 00:00:30,lr 0.01\n",
      "epoch 115, loss 0.04898, train_acc 0.9839, valid_acc 0.9200, Time 00:00:30,lr 0.01\n",
      "epoch 116, loss 0.05494, train_acc 0.9815, valid_acc 0.9220, Time 00:00:30,lr 0.01\n",
      "epoch 117, loss 0.05175, train_acc 0.9825, valid_acc 0.9261, Time 00:00:30,lr 0.01\n",
      "epoch 118, loss 0.05293, train_acc 0.9816, valid_acc 0.9246, Time 00:00:31,lr 0.01\n",
      "epoch 119, loss 0.05280, train_acc 0.9816, valid_acc 0.9216, Time 00:00:30,lr 0.01\n",
      "epoch 120, loss 0.05557, train_acc 0.9807, valid_acc 0.9241, Time 00:00:30,lr 0.01\n",
      "epoch 121, loss 0.05531, train_acc 0.9812, valid_acc 0.9272, Time 00:00:30,lr 0.01\n",
      "epoch 122, loss 0.05505, train_acc 0.9816, valid_acc 0.9233, Time 00:00:30,lr 0.01\n",
      "epoch 123, loss 0.05293, train_acc 0.9821, valid_acc 0.9178, Time 00:00:30,lr 0.01\n",
      "epoch 124, loss 0.05691, train_acc 0.9807, valid_acc 0.9250, Time 00:00:30,lr 0.01\n",
      "epoch 125, loss 0.05509, train_acc 0.9815, valid_acc 0.9178, Time 00:00:30,lr 0.01\n",
      "epoch 126, loss 0.05544, train_acc 0.9806, valid_acc 0.9197, Time 00:00:30,lr 0.01\n",
      "epoch 127, loss 0.05728, train_acc 0.9800, valid_acc 0.9217, Time 00:00:30,lr 0.01\n",
      "epoch 128, loss 0.05760, train_acc 0.9804, valid_acc 0.9254, Time 00:00:30,lr 0.01\n",
      "epoch 129, loss 0.05628, train_acc 0.9812, valid_acc 0.9191, Time 00:00:30,lr 0.01\n",
      "epoch 130, loss 0.05874, train_acc 0.9795, valid_acc 0.9190, Time 00:00:30,lr 0.01\n",
      "epoch 131, loss 0.05753, train_acc 0.9803, valid_acc 0.9250, Time 00:00:30,lr 0.01\n",
      "epoch 132, loss 0.05882, train_acc 0.9794, valid_acc 0.9200, Time 00:00:30,lr 0.01\n",
      "epoch 133, loss 0.06110, train_acc 0.9796, valid_acc 0.9173, Time 00:00:30,lr 0.01\n",
      "epoch 134, loss 0.06017, train_acc 0.9798, valid_acc 0.9207, Time 00:00:30,lr 0.01\n",
      "epoch 135, loss 0.05784, train_acc 0.9803, valid_acc 0.9088, Time 00:00:30,lr 0.01\n",
      "epoch 136, loss 0.06184, train_acc 0.9790, valid_acc 0.9193, Time 00:00:30,lr 0.01\n",
      "epoch 137, loss 0.05722, train_acc 0.9803, valid_acc 0.9279, Time 00:00:30,lr 0.01\n",
      "epoch 138, loss 0.05880, train_acc 0.9800, valid_acc 0.9203, Time 00:00:30,lr 0.01\n",
      "epoch 139, loss 0.05727, train_acc 0.9806, valid_acc 0.9189, Time 00:00:30,lr 0.01\n",
      "epoch 140, loss 0.06348, train_acc 0.9784, valid_acc 0.9242, Time 00:00:30,lr 0.01\n",
      "epoch 141, loss 0.05603, train_acc 0.9808, valid_acc 0.9251, Time 00:00:30,lr 0.01\n",
      "epoch 142, loss 0.05919, train_acc 0.9797, valid_acc 0.9173, Time 00:00:30,lr 0.01\n",
      "epoch 143, loss 0.06109, train_acc 0.9793, valid_acc 0.9140, Time 00:00:30,lr 0.01\n",
      "epoch 144, loss 0.05916, train_acc 0.9798, valid_acc 0.9181, Time 00:00:30,lr 0.01\n",
      "epoch 145, loss 0.06621, train_acc 0.9776, valid_acc 0.9185, Time 00:00:30,lr 0.01\n",
      "epoch 146, loss 0.06132, train_acc 0.9795, valid_acc 0.9203, Time 00:00:30,lr 0.01\n",
      "epoch 147, loss 0.06416, train_acc 0.9784, valid_acc 0.9221, Time 00:00:30,lr 0.01\n",
      "epoch 148, loss 0.06045, train_acc 0.9793, valid_acc 0.9162, Time 00:00:30,lr 0.01\n",
      "epoch 149, loss 0.06016, train_acc 0.9800, valid_acc 0.9142, Time 00:00:30,lr 0.01\n",
      "epoch 150, loss 0.03468, train_acc 0.9895, valid_acc 0.9345, Time 00:00:30,lr 0.001\n",
      "epoch 151, loss 0.02295, train_acc 0.9933, valid_acc 0.9377, Time 00:00:30,lr 0.001\n",
      "epoch 152, loss 0.01791, train_acc 0.9955, valid_acc 0.9364, Time 00:00:30,lr 0.001\n",
      "epoch 153, loss 0.01725, train_acc 0.9955, valid_acc 0.9369, Time 00:00:30,lr 0.001\n",
      "epoch 154, loss 0.01460, train_acc 0.9961, valid_acc 0.9373, Time 00:00:30,lr 0.001\n",
      "epoch 155, loss 0.01362, train_acc 0.9967, valid_acc 0.9381, Time 00:00:30,lr 0.001\n",
      "epoch 156, loss 0.01294, train_acc 0.9970, valid_acc 0.9391, Time 00:00:30,lr 0.001\n",
      "epoch 157, loss 0.01247, train_acc 0.9970, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 158, loss 0.01062, train_acc 0.9974, valid_acc 0.9394, Time 00:00:30,lr 0.001\n",
      "epoch 159, loss 0.01077, train_acc 0.9977, valid_acc 0.9365, Time 00:00:30,lr 0.001\n",
      "epoch 160, loss 0.01017, train_acc 0.9978, valid_acc 0.9392, Time 00:00:30,lr 0.001\n",
      "epoch 161, loss 0.00928, train_acc 0.9981, valid_acc 0.9389, Time 00:00:30,lr 0.001\n",
      "epoch 162, loss 0.00964, train_acc 0.9978, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 163, loss 0.00897, train_acc 0.9982, valid_acc 0.9390, Time 00:00:30,lr 0.001\n",
      "epoch 164, loss 0.00817, train_acc 0.9985, valid_acc 0.9395, Time 00:00:30,lr 0.001\n",
      "epoch 165, loss 0.00879, train_acc 0.9981, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 166, loss 0.00774, train_acc 0.9985, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 167, loss 0.00806, train_acc 0.9982, valid_acc 0.9403, Time 00:00:30,lr 0.001\n",
      "epoch 168, loss 0.00736, train_acc 0.9985, valid_acc 0.9400, Time 00:00:30,lr 0.001\n",
      "epoch 169, loss 0.00760, train_acc 0.9986, valid_acc 0.9401, Time 00:00:30,lr 0.001\n",
      "epoch 170, loss 0.00736, train_acc 0.9986, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 171, loss 0.00706, train_acc 0.9985, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 172, loss 0.00672, train_acc 0.9988, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 173, loss 0.00711, train_acc 0.9985, valid_acc 0.9409, Time 00:00:30,lr 0.001\n",
      "epoch 174, loss 0.00627, train_acc 0.9988, valid_acc 0.9410, Time 00:00:30,lr 0.001\n",
      "epoch 175, loss 0.00647, train_acc 0.9987, valid_acc 0.9391, Time 00:00:33,lr 0.001\n",
      "epoch 176, loss 0.00614, train_acc 0.9990, valid_acc 0.9401, Time 00:00:32,lr 0.001\n",
      "epoch 177, loss 0.00592, train_acc 0.9990, valid_acc 0.9412, Time 00:00:30,lr 0.001\n",
      "epoch 178, loss 0.00610, train_acc 0.9986, valid_acc 0.9389, Time 00:00:30,lr 0.001\n",
      "epoch 179, loss 0.00530, train_acc 0.9992, valid_acc 0.9393, Time 00:00:30,lr 0.001\n",
      "epoch 180, loss 0.00577, train_acc 0.9989, valid_acc 0.9415, Time 00:00:33,lr 0.001\n",
      "epoch 181, loss 0.00612, train_acc 0.9986, valid_acc 0.9392, Time 00:00:35,lr 0.001\n",
      "epoch 182, loss 0.00583, train_acc 0.9987, valid_acc 0.9409, Time 00:00:31,lr 0.001\n",
      "epoch 183, loss 0.00538, train_acc 0.9990, valid_acc 0.9394, Time 00:00:30,lr 0.001\n",
      "epoch 184, loss 0.00512, train_acc 0.9992, valid_acc 0.9414, Time 00:00:33,lr 0.001\n",
      "epoch 185, loss 0.00559, train_acc 0.9990, valid_acc 0.9401, Time 00:00:31,lr 0.001\n",
      "epoch 186, loss 0.00473, train_acc 0.9994, valid_acc 0.9397, Time 00:00:31,lr 0.001\n",
      "epoch 187, loss 0.00465, train_acc 0.9992, valid_acc 0.9387, Time 00:00:35,lr 0.001\n",
      "epoch 188, loss 0.00554, train_acc 0.9988, valid_acc 0.9415, Time 00:00:31,lr 0.001\n",
      "epoch 189, loss 0.00482, train_acc 0.9994, valid_acc 0.9401, Time 00:00:33,lr 0.001\n",
      "epoch 190, loss 0.00493, train_acc 0.9992, valid_acc 0.9410, Time 00:00:36,lr 0.001\n",
      "epoch 191, loss 0.00506, train_acc 0.9990, valid_acc 0.9414, Time 00:00:30,lr 0.001\n",
      "epoch 192, loss 0.00492, train_acc 0.9990, valid_acc 0.9404, Time 00:00:30,lr 0.001\n",
      "epoch 193, loss 0.00497, train_acc 0.9991, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 194, loss 0.00468, train_acc 0.9992, valid_acc 0.9416, Time 00:00:30,lr 0.001\n",
      "epoch 195, loss 0.00445, train_acc 0.9993, valid_acc 0.9425, Time 00:00:30,lr 0.001\n",
      "epoch 196, loss 0.00440, train_acc 0.9994, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 197, loss 0.00445, train_acc 0.9991, valid_acc 0.9427, Time 00:00:30,lr 0.001\n",
      "epoch 198, loss 0.00456, train_acc 0.9992, valid_acc 0.9424, Time 00:00:30,lr 0.001\n",
      "epoch 199, loss 0.00424, train_acc 0.9993, valid_acc 0.9420, Time 00:00:31,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = ResNet(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 try delay pooling resnet arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T03:45:19.563155Z",
     "start_time": "2018-03-05T03:45:19.545584Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_delay_downsample(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_delay_downsample, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(5):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T05:37:46.139687Z",
     "start_time": "2018-03-05T03:45:19.566107Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.74871, train_acc 0.3432, valid_acc 0.4233, Time 00:00:32,lr 0.1\n",
      "epoch 1, loss 1.22673, train_acc 0.5586, valid_acc 0.6011, Time 00:00:33,lr 0.1\n",
      "epoch 2, loss 0.91422, train_acc 0.6805, valid_acc 0.7050, Time 00:00:33,lr 0.1\n",
      "epoch 3, loss 0.77360, train_acc 0.7336, valid_acc 0.6949, Time 00:00:33,lr 0.1\n",
      "epoch 4, loss 0.69274, train_acc 0.7617, valid_acc 0.7435, Time 00:00:33,lr 0.1\n",
      "epoch 5, loss 0.64398, train_acc 0.7797, valid_acc 0.7752, Time 00:00:33,lr 0.1\n",
      "epoch 6, loss 0.60268, train_acc 0.7923, valid_acc 0.7813, Time 00:00:33,lr 0.1\n",
      "epoch 7, loss 0.57284, train_acc 0.8026, valid_acc 0.7886, Time 00:00:33,lr 0.1\n",
      "epoch 8, loss 0.54954, train_acc 0.8118, valid_acc 0.7681, Time 00:00:33,lr 0.1\n",
      "epoch 9, loss 0.52985, train_acc 0.8182, valid_acc 0.8093, Time 00:00:33,lr 0.1\n",
      "epoch 10, loss 0.51509, train_acc 0.8221, valid_acc 0.8017, Time 00:00:33,lr 0.1\n",
      "epoch 11, loss 0.50326, train_acc 0.8282, valid_acc 0.7937, Time 00:00:33,lr 0.1\n",
      "epoch 12, loss 0.48711, train_acc 0.8338, valid_acc 0.8119, Time 00:00:33,lr 0.1\n",
      "epoch 13, loss 0.47811, train_acc 0.8351, valid_acc 0.8344, Time 00:00:33,lr 0.1\n",
      "epoch 14, loss 0.46951, train_acc 0.8389, valid_acc 0.7853, Time 00:00:33,lr 0.1\n",
      "epoch 15, loss 0.46085, train_acc 0.8430, valid_acc 0.8162, Time 00:00:33,lr 0.1\n",
      "epoch 16, loss 0.46285, train_acc 0.8429, valid_acc 0.8195, Time 00:00:33,lr 0.1\n",
      "epoch 17, loss 0.44921, train_acc 0.8452, valid_acc 0.8399, Time 00:00:33,lr 0.1\n",
      "epoch 18, loss 0.44708, train_acc 0.8461, valid_acc 0.8273, Time 00:00:33,lr 0.1\n",
      "epoch 19, loss 0.44015, train_acc 0.8488, valid_acc 0.8203, Time 00:00:33,lr 0.1\n",
      "epoch 20, loss 0.43671, train_acc 0.8511, valid_acc 0.8263, Time 00:00:33,lr 0.1\n",
      "epoch 21, loss 0.43053, train_acc 0.8506, valid_acc 0.8176, Time 00:00:33,lr 0.1\n",
      "epoch 22, loss 0.42708, train_acc 0.8541, valid_acc 0.7993, Time 00:00:33,lr 0.1\n",
      "epoch 23, loss 0.42621, train_acc 0.8544, valid_acc 0.8363, Time 00:00:33,lr 0.1\n",
      "epoch 24, loss 0.41793, train_acc 0.8560, valid_acc 0.8246, Time 00:00:33,lr 0.1\n",
      "epoch 25, loss 0.41721, train_acc 0.8564, valid_acc 0.8430, Time 00:00:33,lr 0.1\n",
      "epoch 26, loss 0.41290, train_acc 0.8587, valid_acc 0.8058, Time 00:00:33,lr 0.1\n",
      "epoch 27, loss 0.41461, train_acc 0.8587, valid_acc 0.8185, Time 00:00:33,lr 0.1\n",
      "epoch 28, loss 0.41093, train_acc 0.8602, valid_acc 0.8380, Time 00:00:33,lr 0.1\n",
      "epoch 29, loss 0.40087, train_acc 0.8614, valid_acc 0.8093, Time 00:00:33,lr 0.1\n",
      "epoch 30, loss 0.40541, train_acc 0.8600, valid_acc 0.8230, Time 00:00:33,lr 0.1\n",
      "epoch 31, loss 0.40255, train_acc 0.8610, valid_acc 0.8079, Time 00:00:33,lr 0.1\n",
      "epoch 32, loss 0.40050, train_acc 0.8628, valid_acc 0.7882, Time 00:00:33,lr 0.1\n",
      "epoch 33, loss 0.39481, train_acc 0.8645, valid_acc 0.8295, Time 00:00:33,lr 0.1\n",
      "epoch 34, loss 0.39867, train_acc 0.8636, valid_acc 0.8184, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.39564, train_acc 0.8642, valid_acc 0.8178, Time 00:00:33,lr 0.1\n",
      "epoch 36, loss 0.39489, train_acc 0.8651, valid_acc 0.8219, Time 00:00:33,lr 0.1\n",
      "epoch 37, loss 0.39592, train_acc 0.8636, valid_acc 0.8539, Time 00:00:33,lr 0.1\n",
      "epoch 38, loss 0.38821, train_acc 0.8660, valid_acc 0.8453, Time 00:00:33,lr 0.1\n",
      "epoch 39, loss 0.39041, train_acc 0.8658, valid_acc 0.7982, Time 00:00:33,lr 0.1\n",
      "epoch 40, loss 0.38574, train_acc 0.8685, valid_acc 0.7816, Time 00:00:33,lr 0.1\n",
      "epoch 41, loss 0.38974, train_acc 0.8665, valid_acc 0.8515, Time 00:00:33,lr 0.1\n",
      "epoch 42, loss 0.38603, train_acc 0.8678, valid_acc 0.8475, Time 00:00:33,lr 0.1\n",
      "epoch 43, loss 0.38327, train_acc 0.8695, valid_acc 0.8285, Time 00:00:33,lr 0.1\n",
      "epoch 44, loss 0.38648, train_acc 0.8657, valid_acc 0.8297, Time 00:00:33,lr 0.1\n",
      "epoch 45, loss 0.38244, train_acc 0.8685, valid_acc 0.8268, Time 00:00:33,lr 0.1\n",
      "epoch 46, loss 0.38622, train_acc 0.8668, valid_acc 0.8441, Time 00:00:33,lr 0.1\n",
      "epoch 47, loss 0.37983, train_acc 0.8692, valid_acc 0.8544, Time 00:00:33,lr 0.1\n",
      "epoch 48, loss 0.38122, train_acc 0.8699, valid_acc 0.8278, Time 00:00:33,lr 0.1\n",
      "epoch 49, loss 0.37919, train_acc 0.8693, valid_acc 0.8447, Time 00:00:33,lr 0.1\n",
      "epoch 50, loss 0.38096, train_acc 0.8695, valid_acc 0.8395, Time 00:00:33,lr 0.1\n",
      "epoch 51, loss 0.38445, train_acc 0.8679, valid_acc 0.8314, Time 00:00:33,lr 0.1\n",
      "epoch 52, loss 0.37595, train_acc 0.8723, valid_acc 0.8600, Time 00:00:33,lr 0.1\n",
      "epoch 53, loss 0.37780, train_acc 0.8703, valid_acc 0.8573, Time 00:00:33,lr 0.1\n",
      "epoch 54, loss 0.37309, train_acc 0.8719, valid_acc 0.8509, Time 00:00:33,lr 0.1\n",
      "epoch 55, loss 0.37771, train_acc 0.8722, valid_acc 0.8346, Time 00:00:33,lr 0.1\n",
      "epoch 56, loss 0.37425, train_acc 0.8728, valid_acc 0.7848, Time 00:00:33,lr 0.1\n",
      "epoch 57, loss 0.37199, train_acc 0.8713, valid_acc 0.8535, Time 00:00:33,lr 0.1\n",
      "epoch 58, loss 0.37070, train_acc 0.8729, valid_acc 0.8559, Time 00:00:33,lr 0.1\n",
      "epoch 59, loss 0.36944, train_acc 0.8733, valid_acc 0.8423, Time 00:00:33,lr 0.1\n",
      "epoch 60, loss 0.37046, train_acc 0.8737, valid_acc 0.8131, Time 00:00:33,lr 0.1\n",
      "epoch 61, loss 0.37506, train_acc 0.8725, valid_acc 0.8269, Time 00:00:33,lr 0.1\n",
      "epoch 62, loss 0.37128, train_acc 0.8726, valid_acc 0.8304, Time 00:00:33,lr 0.1\n",
      "epoch 63, loss 0.37199, train_acc 0.8734, valid_acc 0.8322, Time 00:00:33,lr 0.1\n",
      "epoch 64, loss 0.37266, train_acc 0.8732, valid_acc 0.8573, Time 00:00:33,lr 0.1\n",
      "epoch 65, loss 0.36817, train_acc 0.8733, valid_acc 0.8577, Time 00:00:33,lr 0.1\n",
      "epoch 66, loss 0.36947, train_acc 0.8754, valid_acc 0.8173, Time 00:00:33,lr 0.1\n",
      "epoch 67, loss 0.37241, train_acc 0.8726, valid_acc 0.8525, Time 00:00:33,lr 0.1\n",
      "epoch 68, loss 0.36946, train_acc 0.8737, valid_acc 0.8479, Time 00:00:33,lr 0.1\n",
      "epoch 69, loss 0.36792, train_acc 0.8740, valid_acc 0.8522, Time 00:00:33,lr 0.1\n",
      "epoch 70, loss 0.36331, train_acc 0.8748, valid_acc 0.8248, Time 00:00:33,lr 0.1\n",
      "epoch 71, loss 0.37371, train_acc 0.8709, valid_acc 0.8572, Time 00:00:33,lr 0.1\n",
      "epoch 72, loss 0.36035, train_acc 0.8774, valid_acc 0.8383, Time 00:00:33,lr 0.1\n",
      "epoch 73, loss 0.36753, train_acc 0.8745, valid_acc 0.8642, Time 00:00:33,lr 0.1\n",
      "epoch 74, loss 0.36495, train_acc 0.8748, valid_acc 0.8362, Time 00:00:33,lr 0.1\n",
      "epoch 75, loss 0.36169, train_acc 0.8762, valid_acc 0.8545, Time 00:00:33,lr 0.1\n",
      "epoch 76, loss 0.36737, train_acc 0.8742, valid_acc 0.8660, Time 00:00:33,lr 0.1\n",
      "epoch 77, loss 0.36069, train_acc 0.8762, valid_acc 0.8477, Time 00:00:33,lr 0.1\n",
      "epoch 78, loss 0.35918, train_acc 0.8762, valid_acc 0.7496, Time 00:00:33,lr 0.1\n",
      "epoch 79, loss 0.36596, train_acc 0.8754, valid_acc 0.8702, Time 00:00:33,lr 0.1\n",
      "epoch 80, loss 0.22051, train_acc 0.9255, valid_acc 0.9165, Time 00:00:33,lr 0.01\n",
      "epoch 81, loss 0.16942, train_acc 0.9411, valid_acc 0.9217, Time 00:00:33,lr 0.01\n",
      "epoch 82, loss 0.15287, train_acc 0.9484, valid_acc 0.9220, Time 00:00:33,lr 0.01\n",
      "epoch 83, loss 0.14203, train_acc 0.9515, valid_acc 0.9235, Time 00:00:33,lr 0.01\n",
      "epoch 84, loss 0.13170, train_acc 0.9557, valid_acc 0.9235, Time 00:00:33,lr 0.01\n",
      "epoch 85, loss 0.12316, train_acc 0.9581, valid_acc 0.9272, Time 00:00:33,lr 0.01\n",
      "epoch 86, loss 0.11667, train_acc 0.9605, valid_acc 0.9251, Time 00:00:33,lr 0.01\n",
      "epoch 87, loss 0.11166, train_acc 0.9626, valid_acc 0.9276, Time 00:00:33,lr 0.01\n",
      "epoch 88, loss 0.10688, train_acc 0.9640, valid_acc 0.9276, Time 00:00:33,lr 0.01\n",
      "epoch 89, loss 0.09929, train_acc 0.9659, valid_acc 0.9283, Time 00:00:33,lr 0.01\n",
      "epoch 90, loss 0.09834, train_acc 0.9661, valid_acc 0.9203, Time 00:00:33,lr 0.01\n",
      "epoch 91, loss 0.09401, train_acc 0.9680, valid_acc 0.9287, Time 00:00:33,lr 0.01\n",
      "epoch 92, loss 0.08950, train_acc 0.9696, valid_acc 0.9278, Time 00:00:33,lr 0.01\n",
      "epoch 93, loss 0.08283, train_acc 0.9722, valid_acc 0.9257, Time 00:00:33,lr 0.01\n",
      "epoch 94, loss 0.08354, train_acc 0.9716, valid_acc 0.9247, Time 00:00:33,lr 0.01\n",
      "epoch 95, loss 0.07745, train_acc 0.9740, valid_acc 0.9229, Time 00:00:33,lr 0.01\n",
      "epoch 96, loss 0.07554, train_acc 0.9741, valid_acc 0.9275, Time 00:00:33,lr 0.01\n",
      "epoch 97, loss 0.07464, train_acc 0.9750, valid_acc 0.9286, Time 00:00:33,lr 0.01\n",
      "epoch 98, loss 0.07502, train_acc 0.9737, valid_acc 0.9251, Time 00:00:33,lr 0.01\n",
      "epoch 99, loss 0.07237, train_acc 0.9750, valid_acc 0.9226, Time 00:00:33,lr 0.01\n",
      "epoch 100, loss 0.07316, train_acc 0.9758, valid_acc 0.9264, Time 00:00:33,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.06964, train_acc 0.9760, valid_acc 0.9242, Time 00:00:33,lr 0.01\n",
      "epoch 102, loss 0.06682, train_acc 0.9774, valid_acc 0.9238, Time 00:00:33,lr 0.01\n",
      "epoch 103, loss 0.06689, train_acc 0.9768, valid_acc 0.9242, Time 00:00:33,lr 0.01\n",
      "epoch 104, loss 0.06593, train_acc 0.9776, valid_acc 0.9274, Time 00:00:33,lr 0.01\n",
      "epoch 105, loss 0.06734, train_acc 0.9771, valid_acc 0.9260, Time 00:00:33,lr 0.01\n",
      "epoch 106, loss 0.06837, train_acc 0.9763, valid_acc 0.9233, Time 00:00:33,lr 0.01\n",
      "epoch 107, loss 0.06494, train_acc 0.9778, valid_acc 0.9255, Time 00:00:33,lr 0.01\n",
      "epoch 108, loss 0.06547, train_acc 0.9775, valid_acc 0.9225, Time 00:00:33,lr 0.01\n",
      "epoch 109, loss 0.06840, train_acc 0.9763, valid_acc 0.9247, Time 00:00:33,lr 0.01\n",
      "epoch 110, loss 0.06691, train_acc 0.9776, valid_acc 0.9270, Time 00:00:33,lr 0.01\n",
      "epoch 111, loss 0.06680, train_acc 0.9772, valid_acc 0.9212, Time 00:00:33,lr 0.01\n",
      "epoch 112, loss 0.06807, train_acc 0.9770, valid_acc 0.9235, Time 00:00:33,lr 0.01\n",
      "epoch 113, loss 0.06843, train_acc 0.9765, valid_acc 0.9259, Time 00:00:33,lr 0.01\n",
      "epoch 114, loss 0.06646, train_acc 0.9776, valid_acc 0.9260, Time 00:00:33,lr 0.01\n",
      "epoch 115, loss 0.06611, train_acc 0.9778, valid_acc 0.9272, Time 00:00:33,lr 0.01\n",
      "epoch 116, loss 0.06412, train_acc 0.9782, valid_acc 0.9185, Time 00:00:33,lr 0.01\n",
      "epoch 117, loss 0.06882, train_acc 0.9763, valid_acc 0.9218, Time 00:00:33,lr 0.01\n",
      "epoch 118, loss 0.06780, train_acc 0.9770, valid_acc 0.9180, Time 00:00:37,lr 0.01\n",
      "epoch 119, loss 0.06749, train_acc 0.9765, valid_acc 0.9227, Time 00:00:38,lr 0.01\n",
      "epoch 120, loss 0.07060, train_acc 0.9760, valid_acc 0.9226, Time 00:00:34,lr 0.01\n",
      "epoch 121, loss 0.06805, train_acc 0.9769, valid_acc 0.9198, Time 00:00:33,lr 0.01\n",
      "epoch 122, loss 0.07168, train_acc 0.9755, valid_acc 0.9225, Time 00:00:33,lr 0.01\n",
      "epoch 123, loss 0.07041, train_acc 0.9762, valid_acc 0.9240, Time 00:00:33,lr 0.01\n",
      "epoch 124, loss 0.07179, train_acc 0.9752, valid_acc 0.9199, Time 00:00:33,lr 0.01\n",
      "epoch 125, loss 0.06785, train_acc 0.9778, valid_acc 0.9214, Time 00:00:33,lr 0.01\n",
      "epoch 126, loss 0.07291, train_acc 0.9752, valid_acc 0.9160, Time 00:00:33,lr 0.01\n",
      "epoch 127, loss 0.06623, train_acc 0.9773, valid_acc 0.9237, Time 00:00:33,lr 0.01\n",
      "epoch 128, loss 0.07073, train_acc 0.9759, valid_acc 0.9204, Time 00:00:33,lr 0.01\n",
      "epoch 129, loss 0.06996, train_acc 0.9762, valid_acc 0.9191, Time 00:00:34,lr 0.01\n",
      "epoch 130, loss 0.07506, train_acc 0.9734, valid_acc 0.9173, Time 00:00:33,lr 0.01\n",
      "epoch 131, loss 0.07572, train_acc 0.9737, valid_acc 0.9076, Time 00:00:33,lr 0.01\n",
      "epoch 132, loss 0.07105, train_acc 0.9755, valid_acc 0.9201, Time 00:00:33,lr 0.01\n",
      "epoch 133, loss 0.07461, train_acc 0.9743, valid_acc 0.9150, Time 00:00:33,lr 0.01\n",
      "epoch 134, loss 0.06875, train_acc 0.9768, valid_acc 0.9231, Time 00:00:33,lr 0.01\n",
      "epoch 135, loss 0.06996, train_acc 0.9763, valid_acc 0.9169, Time 00:00:33,lr 0.01\n",
      "epoch 136, loss 0.07493, train_acc 0.9741, valid_acc 0.9214, Time 00:00:33,lr 0.01\n",
      "epoch 137, loss 0.07146, train_acc 0.9747, valid_acc 0.9171, Time 00:00:33,lr 0.01\n",
      "epoch 138, loss 0.07328, train_acc 0.9754, valid_acc 0.9215, Time 00:00:33,lr 0.01\n",
      "epoch 139, loss 0.07751, train_acc 0.9734, valid_acc 0.9269, Time 00:00:33,lr 0.01\n",
      "epoch 140, loss 0.07599, train_acc 0.9738, valid_acc 0.9154, Time 00:00:34,lr 0.01\n",
      "epoch 141, loss 0.06780, train_acc 0.9768, valid_acc 0.9156, Time 00:00:35,lr 0.01\n",
      "epoch 142, loss 0.07360, train_acc 0.9751, valid_acc 0.9236, Time 00:00:33,lr 0.01\n",
      "epoch 143, loss 0.07394, train_acc 0.9740, valid_acc 0.9175, Time 00:00:34,lr 0.01\n",
      "epoch 144, loss 0.07369, train_acc 0.9744, valid_acc 0.9217, Time 00:00:37,lr 0.01\n",
      "epoch 145, loss 0.07437, train_acc 0.9742, valid_acc 0.9182, Time 00:00:33,lr 0.01\n",
      "epoch 146, loss 0.07432, train_acc 0.9737, valid_acc 0.9102, Time 00:00:33,lr 0.01\n",
      "epoch 147, loss 0.07380, train_acc 0.9745, valid_acc 0.9145, Time 00:00:33,lr 0.01\n",
      "epoch 148, loss 0.07292, train_acc 0.9748, valid_acc 0.9212, Time 00:00:33,lr 0.01\n",
      "epoch 149, loss 0.07682, train_acc 0.9734, valid_acc 0.9200, Time 00:00:33,lr 0.01\n",
      "epoch 150, loss 0.04344, train_acc 0.9862, valid_acc 0.9350, Time 00:00:33,lr 0.001\n",
      "epoch 151, loss 0.03075, train_acc 0.9912, valid_acc 0.9356, Time 00:00:33,lr 0.001\n",
      "epoch 152, loss 0.02638, train_acc 0.9927, valid_acc 0.9351, Time 00:00:33,lr 0.001\n",
      "epoch 153, loss 0.02355, train_acc 0.9938, valid_acc 0.9350, Time 00:00:33,lr 0.001\n",
      "epoch 154, loss 0.02280, train_acc 0.9940, valid_acc 0.9359, Time 00:00:34,lr 0.001\n",
      "epoch 155, loss 0.02053, train_acc 0.9951, valid_acc 0.9363, Time 00:00:33,lr 0.001\n",
      "epoch 156, loss 0.01953, train_acc 0.9952, valid_acc 0.9352, Time 00:00:33,lr 0.001\n",
      "epoch 157, loss 0.01891, train_acc 0.9957, valid_acc 0.9354, Time 00:00:33,lr 0.001\n",
      "epoch 158, loss 0.01852, train_acc 0.9954, valid_acc 0.9353, Time 00:00:33,lr 0.001\n",
      "epoch 159, loss 0.01763, train_acc 0.9956, valid_acc 0.9363, Time 00:00:33,lr 0.001\n",
      "epoch 160, loss 0.01674, train_acc 0.9960, valid_acc 0.9366, Time 00:00:33,lr 0.001\n",
      "epoch 161, loss 0.01659, train_acc 0.9961, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 162, loss 0.01573, train_acc 0.9962, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 163, loss 0.01550, train_acc 0.9966, valid_acc 0.9362, Time 00:00:33,lr 0.001\n",
      "epoch 164, loss 0.01424, train_acc 0.9968, valid_acc 0.9367, Time 00:00:33,lr 0.001\n",
      "epoch 165, loss 0.01404, train_acc 0.9969, valid_acc 0.9368, Time 00:00:33,lr 0.001\n",
      "epoch 166, loss 0.01377, train_acc 0.9969, valid_acc 0.9372, Time 00:00:33,lr 0.001\n",
      "epoch 167, loss 0.01356, train_acc 0.9970, valid_acc 0.9376, Time 00:00:33,lr 0.001\n",
      "epoch 168, loss 0.01291, train_acc 0.9973, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 169, loss 0.01344, train_acc 0.9973, valid_acc 0.9378, Time 00:00:33,lr 0.001\n",
      "epoch 170, loss 0.01252, train_acc 0.9973, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 171, loss 0.01335, train_acc 0.9972, valid_acc 0.9366, Time 00:00:33,lr 0.001\n",
      "epoch 172, loss 0.01244, train_acc 0.9973, valid_acc 0.9371, Time 00:00:33,lr 0.001\n",
      "epoch 173, loss 0.01190, train_acc 0.9976, valid_acc 0.9378, Time 00:00:33,lr 0.001\n",
      "epoch 174, loss 0.01216, train_acc 0.9977, valid_acc 0.9380, Time 00:00:33,lr 0.001\n",
      "epoch 175, loss 0.01098, train_acc 0.9982, valid_acc 0.9383, Time 00:00:33,lr 0.001\n",
      "epoch 176, loss 0.01147, train_acc 0.9977, valid_acc 0.9368, Time 00:00:33,lr 0.001\n",
      "epoch 177, loss 0.01014, train_acc 0.9983, valid_acc 0.9380, Time 00:00:33,lr 0.001\n",
      "epoch 178, loss 0.01104, train_acc 0.9978, valid_acc 0.9372, Time 00:00:33,lr 0.001\n",
      "epoch 179, loss 0.01147, train_acc 0.9975, valid_acc 0.9379, Time 00:00:33,lr 0.001\n",
      "epoch 180, loss 0.01022, train_acc 0.9982, valid_acc 0.9375, Time 00:00:33,lr 0.001\n",
      "epoch 181, loss 0.01028, train_acc 0.9980, valid_acc 0.9376, Time 00:00:33,lr 0.001\n",
      "epoch 182, loss 0.00943, train_acc 0.9982, valid_acc 0.9375, Time 00:00:33,lr 0.001\n",
      "epoch 183, loss 0.00947, train_acc 0.9981, valid_acc 0.9381, Time 00:00:33,lr 0.001\n",
      "epoch 184, loss 0.00997, train_acc 0.9979, valid_acc 0.9380, Time 00:00:33,lr 0.001\n",
      "epoch 185, loss 0.00923, train_acc 0.9983, valid_acc 0.9382, Time 00:00:33,lr 0.001\n",
      "epoch 186, loss 0.00944, train_acc 0.9983, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 187, loss 0.00932, train_acc 0.9982, valid_acc 0.9377, Time 00:00:33,lr 0.001\n",
      "epoch 188, loss 0.00963, train_acc 0.9981, valid_acc 0.9372, Time 00:00:33,lr 0.001\n",
      "epoch 189, loss 0.00881, train_acc 0.9984, valid_acc 0.9383, Time 00:00:33,lr 0.001\n",
      "epoch 190, loss 0.00906, train_acc 0.9982, valid_acc 0.9378, Time 00:00:33,lr 0.001\n",
      "epoch 191, loss 0.00878, train_acc 0.9985, valid_acc 0.9385, Time 00:00:33,lr 0.001\n",
      "epoch 192, loss 0.00873, train_acc 0.9985, valid_acc 0.9400, Time 00:00:33,lr 0.001\n",
      "epoch 193, loss 0.00856, train_acc 0.9985, valid_acc 0.9392, Time 00:00:33,lr 0.001\n",
      "epoch 194, loss 0.00905, train_acc 0.9983, valid_acc 0.9387, Time 00:00:33,lr 0.001\n",
      "epoch 195, loss 0.00956, train_acc 0.9983, valid_acc 0.9385, Time 00:00:33,lr 0.001\n",
      "epoch 196, loss 0.00896, train_acc 0.9985, valid_acc 0.9376, Time 00:00:33,lr 0.001\n",
      "epoch 197, loss 0.00819, train_acc 0.9986, valid_acc 0.9386, Time 00:00:33,lr 0.001\n",
      "epoch 198, loss 0.00819, train_acc 0.9986, valid_acc 0.9382, Time 00:00:33,lr 0.001\n",
      "epoch 199, loss 0.00833, train_acc 0.9985, valid_acc 0.9386, Time 00:00:33,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_delay_downsample(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_522_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 to find if width is important, compare to 5.8 double all conv layers' channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:27:01.435710Z",
     "start_time": "2018-03-05T06:27:01.415583Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_522_c64(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_522_c64, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(5):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=256, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=256))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T09:55:35.962114Z",
     "start_time": "2018-03-05T06:27:02.356290Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.80125, train_acc 0.3235, valid_acc 0.4435, Time 00:01:01,lr 0.1\n",
      "epoch 1, loss 1.21529, train_acc 0.5671, valid_acc 0.6560, Time 00:01:02,lr 0.1\n",
      "epoch 2, loss 0.84985, train_acc 0.7038, valid_acc 0.7225, Time 00:01:02,lr 0.1\n",
      "epoch 3, loss 0.70113, train_acc 0.7565, valid_acc 0.7654, Time 00:01:03,lr 0.1\n",
      "epoch 4, loss 0.62073, train_acc 0.7878, valid_acc 0.7823, Time 00:01:03,lr 0.1\n",
      "epoch 5, loss 0.56917, train_acc 0.8041, valid_acc 0.7874, Time 00:01:04,lr 0.1\n",
      "epoch 6, loss 0.52424, train_acc 0.8196, valid_acc 0.7884, Time 00:01:02,lr 0.1\n",
      "epoch 7, loss 0.50291, train_acc 0.8289, valid_acc 0.8291, Time 00:01:03,lr 0.1\n",
      "epoch 8, loss 0.47482, train_acc 0.8385, valid_acc 0.7859, Time 00:01:02,lr 0.1\n",
      "epoch 9, loss 0.44850, train_acc 0.8471, valid_acc 0.8380, Time 00:01:02,lr 0.1\n",
      "epoch 10, loss 0.43947, train_acc 0.8487, valid_acc 0.8139, Time 00:01:02,lr 0.1\n",
      "epoch 11, loss 0.42596, train_acc 0.8529, valid_acc 0.7957, Time 00:01:02,lr 0.1\n",
      "epoch 12, loss 0.41112, train_acc 0.8588, valid_acc 0.8242, Time 00:01:03,lr 0.1\n",
      "epoch 13, loss 0.39968, train_acc 0.8629, valid_acc 0.8341, Time 00:01:02,lr 0.1\n",
      "epoch 14, loss 0.38834, train_acc 0.8670, valid_acc 0.8343, Time 00:01:02,lr 0.1\n",
      "epoch 15, loss 0.38437, train_acc 0.8676, valid_acc 0.8528, Time 00:01:02,lr 0.1\n",
      "epoch 16, loss 0.37885, train_acc 0.8690, valid_acc 0.8522, Time 00:01:02,lr 0.1\n",
      "epoch 17, loss 0.36954, train_acc 0.8750, valid_acc 0.8218, Time 00:01:02,lr 0.1\n",
      "epoch 18, loss 0.36590, train_acc 0.8740, valid_acc 0.8669, Time 00:01:02,lr 0.1\n",
      "epoch 19, loss 0.35680, train_acc 0.8776, valid_acc 0.7886, Time 00:01:02,lr 0.1\n",
      "epoch 20, loss 0.35435, train_acc 0.8782, valid_acc 0.8549, Time 00:01:02,lr 0.1\n",
      "epoch 21, loss 0.35435, train_acc 0.8771, valid_acc 0.8495, Time 00:01:02,lr 0.1\n",
      "epoch 22, loss 0.34705, train_acc 0.8807, valid_acc 0.8506, Time 00:01:02,lr 0.1\n",
      "epoch 23, loss 0.34090, train_acc 0.8825, valid_acc 0.8496, Time 00:01:02,lr 0.1\n",
      "epoch 24, loss 0.34386, train_acc 0.8808, valid_acc 0.8626, Time 00:01:02,lr 0.1\n",
      "epoch 25, loss 0.34245, train_acc 0.8825, valid_acc 0.7985, Time 00:01:02,lr 0.1\n",
      "epoch 26, loss 0.33509, train_acc 0.8857, valid_acc 0.8650, Time 00:01:02,lr 0.1\n",
      "epoch 27, loss 0.33300, train_acc 0.8857, valid_acc 0.8669, Time 00:01:02,lr 0.1\n",
      "epoch 28, loss 0.32566, train_acc 0.8876, valid_acc 0.8621, Time 00:01:02,lr 0.1\n",
      "epoch 29, loss 0.32526, train_acc 0.8898, valid_acc 0.8516, Time 00:01:02,lr 0.1\n",
      "epoch 30, loss 0.32913, train_acc 0.8874, valid_acc 0.8742, Time 00:01:02,lr 0.1\n",
      "epoch 31, loss 0.32649, train_acc 0.8875, valid_acc 0.8690, Time 00:01:02,lr 0.1\n",
      "epoch 32, loss 0.32220, train_acc 0.8900, valid_acc 0.8302, Time 00:01:02,lr 0.1\n",
      "epoch 33, loss 0.32117, train_acc 0.8895, valid_acc 0.8569, Time 00:01:02,lr 0.1\n",
      "epoch 34, loss 0.32370, train_acc 0.8887, valid_acc 0.8769, Time 00:01:02,lr 0.1\n",
      "epoch 35, loss 0.31867, train_acc 0.8903, valid_acc 0.8533, Time 00:01:03,lr 0.1\n",
      "epoch 36, loss 0.31809, train_acc 0.8898, valid_acc 0.8441, Time 00:01:02,lr 0.1\n",
      "epoch 37, loss 0.31201, train_acc 0.8933, valid_acc 0.8406, Time 00:01:02,lr 0.1\n",
      "epoch 38, loss 0.31065, train_acc 0.8922, valid_acc 0.8659, Time 00:01:02,lr 0.1\n",
      "epoch 39, loss 0.30870, train_acc 0.8940, valid_acc 0.8659, Time 00:01:02,lr 0.1\n",
      "epoch 40, loss 0.30990, train_acc 0.8946, valid_acc 0.8653, Time 00:01:02,lr 0.1\n",
      "epoch 41, loss 0.31143, train_acc 0.8916, valid_acc 0.8661, Time 00:01:02,lr 0.1\n",
      "epoch 42, loss 0.30731, train_acc 0.8949, valid_acc 0.8734, Time 00:01:02,lr 0.1\n",
      "epoch 43, loss 0.30617, train_acc 0.8952, valid_acc 0.8598, Time 00:01:02,lr 0.1\n",
      "epoch 44, loss 0.30193, train_acc 0.8966, valid_acc 0.8589, Time 00:01:03,lr 0.1\n",
      "epoch 45, loss 0.30429, train_acc 0.8963, valid_acc 0.8502, Time 00:01:02,lr 0.1\n",
      "epoch 46, loss 0.30656, train_acc 0.8956, valid_acc 0.8829, Time 00:01:02,lr 0.1\n",
      "epoch 47, loss 0.30519, train_acc 0.8958, valid_acc 0.8526, Time 00:01:02,lr 0.1\n",
      "epoch 48, loss 0.30416, train_acc 0.8945, valid_acc 0.8602, Time 00:01:02,lr 0.1\n",
      "epoch 49, loss 0.30313, train_acc 0.8963, valid_acc 0.8058, Time 00:01:02,lr 0.1\n",
      "epoch 50, loss 0.30975, train_acc 0.8951, valid_acc 0.8783, Time 00:01:03,lr 0.1\n",
      "epoch 51, loss 0.29615, train_acc 0.8990, valid_acc 0.8925, Time 00:01:02,lr 0.1\n",
      "epoch 52, loss 0.29919, train_acc 0.8976, valid_acc 0.8604, Time 00:01:02,lr 0.1\n",
      "epoch 53, loss 0.30477, train_acc 0.8962, valid_acc 0.8638, Time 00:01:02,lr 0.1\n",
      "epoch 54, loss 0.29803, train_acc 0.8965, valid_acc 0.8488, Time 00:01:02,lr 0.1\n",
      "epoch 55, loss 0.29738, train_acc 0.8977, valid_acc 0.8526, Time 00:01:02,lr 0.1\n",
      "epoch 56, loss 0.30025, train_acc 0.8967, valid_acc 0.8243, Time 00:01:02,lr 0.1\n",
      "epoch 57, loss 0.29852, train_acc 0.8984, valid_acc 0.8712, Time 00:01:02,lr 0.1\n",
      "epoch 58, loss 0.29340, train_acc 0.8992, valid_acc 0.8735, Time 00:01:02,lr 0.1\n",
      "epoch 59, loss 0.29646, train_acc 0.8973, valid_acc 0.8564, Time 00:01:02,lr 0.1\n",
      "epoch 60, loss 0.29942, train_acc 0.8973, valid_acc 0.8898, Time 00:01:02,lr 0.1\n",
      "epoch 61, loss 0.29024, train_acc 0.8999, valid_acc 0.8471, Time 00:01:02,lr 0.1\n",
      "epoch 62, loss 0.29388, train_acc 0.9000, valid_acc 0.8524, Time 00:01:02,lr 0.1\n",
      "epoch 63, loss 0.29573, train_acc 0.8968, valid_acc 0.8145, Time 00:01:02,lr 0.1\n",
      "epoch 64, loss 0.29297, train_acc 0.9013, valid_acc 0.8845, Time 00:01:02,lr 0.1\n",
      "epoch 65, loss 0.28833, train_acc 0.9022, valid_acc 0.8770, Time 00:01:02,lr 0.1\n",
      "epoch 66, loss 0.29792, train_acc 0.8971, valid_acc 0.8943, Time 00:01:02,lr 0.1\n",
      "epoch 67, loss 0.29277, train_acc 0.9005, valid_acc 0.8739, Time 00:01:02,lr 0.1\n",
      "epoch 68, loss 0.29278, train_acc 0.9000, valid_acc 0.8596, Time 00:01:02,lr 0.1\n",
      "epoch 69, loss 0.29351, train_acc 0.8999, valid_acc 0.8461, Time 00:01:02,lr 0.1\n",
      "epoch 70, loss 0.28908, train_acc 0.9000, valid_acc 0.8510, Time 00:01:02,lr 0.1\n",
      "epoch 71, loss 0.28986, train_acc 0.9004, valid_acc 0.8559, Time 00:01:02,lr 0.1\n",
      "epoch 72, loss 0.29072, train_acc 0.9007, valid_acc 0.8637, Time 00:01:02,lr 0.1\n",
      "epoch 73, loss 0.28822, train_acc 0.9014, valid_acc 0.8428, Time 00:01:02,lr 0.1\n",
      "epoch 74, loss 0.28894, train_acc 0.9002, valid_acc 0.8768, Time 00:01:02,lr 0.1\n",
      "epoch 75, loss 0.28853, train_acc 0.9013, valid_acc 0.8787, Time 00:01:02,lr 0.1\n",
      "epoch 76, loss 0.29077, train_acc 0.9001, valid_acc 0.8747, Time 00:01:02,lr 0.1\n",
      "epoch 77, loss 0.29205, train_acc 0.9006, valid_acc 0.8222, Time 00:01:02,lr 0.1\n",
      "epoch 78, loss 0.28894, train_acc 0.9010, valid_acc 0.8473, Time 00:01:02,lr 0.1\n",
      "epoch 79, loss 0.28275, train_acc 0.9019, valid_acc 0.8289, Time 00:01:02,lr 0.1\n",
      "epoch 80, loss 0.15574, train_acc 0.9475, valid_acc 0.9311, Time 00:01:02,lr 0.01\n",
      "epoch 81, loss 0.10297, train_acc 0.9658, valid_acc 0.9371, Time 00:01:02,lr 0.01\n",
      "epoch 82, loss 0.08857, train_acc 0.9698, valid_acc 0.9369, Time 00:01:02,lr 0.01\n",
      "epoch 83, loss 0.07886, train_acc 0.9731, valid_acc 0.9416, Time 00:01:02,lr 0.01\n",
      "epoch 84, loss 0.07118, train_acc 0.9763, valid_acc 0.9419, Time 00:01:02,lr 0.01\n",
      "epoch 85, loss 0.06399, train_acc 0.9787, valid_acc 0.9402, Time 00:01:02,lr 0.01\n",
      "epoch 86, loss 0.05531, train_acc 0.9816, valid_acc 0.9414, Time 00:01:02,lr 0.01\n",
      "epoch 87, loss 0.04941, train_acc 0.9840, valid_acc 0.9415, Time 00:01:02,lr 0.01\n",
      "epoch 88, loss 0.04808, train_acc 0.9843, valid_acc 0.9430, Time 00:01:02,lr 0.01\n",
      "epoch 89, loss 0.04049, train_acc 0.9871, valid_acc 0.9413, Time 00:01:02,lr 0.01\n",
      "epoch 90, loss 0.04131, train_acc 0.9871, valid_acc 0.9419, Time 00:01:02,lr 0.01\n",
      "epoch 91, loss 0.03723, train_acc 0.9883, valid_acc 0.9424, Time 00:01:02,lr 0.01\n",
      "epoch 92, loss 0.03402, train_acc 0.9892, valid_acc 0.9437, Time 00:01:02,lr 0.01\n",
      "epoch 93, loss 0.03021, train_acc 0.9912, valid_acc 0.9400, Time 00:01:02,lr 0.01\n",
      "epoch 94, loss 0.02831, train_acc 0.9914, valid_acc 0.9430, Time 00:01:02,lr 0.01\n",
      "epoch 95, loss 0.02827, train_acc 0.9916, valid_acc 0.9427, Time 00:01:02,lr 0.01\n",
      "epoch 96, loss 0.02427, train_acc 0.9931, valid_acc 0.9421, Time 00:01:02,lr 0.01\n",
      "epoch 97, loss 0.02600, train_acc 0.9922, valid_acc 0.9424, Time 00:01:02,lr 0.01\n",
      "epoch 98, loss 0.02444, train_acc 0.9927, valid_acc 0.9409, Time 00:01:02,lr 0.01\n",
      "epoch 99, loss 0.02349, train_acc 0.9931, valid_acc 0.9427, Time 00:01:02,lr 0.01\n",
      "epoch 100, loss 0.02360, train_acc 0.9932, valid_acc 0.9429, Time 00:01:02,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.02139, train_acc 0.9936, valid_acc 0.9407, Time 00:01:02,lr 0.01\n",
      "epoch 102, loss 0.02296, train_acc 0.9928, valid_acc 0.9436, Time 00:01:02,lr 0.01\n",
      "epoch 103, loss 0.02055, train_acc 0.9945, valid_acc 0.9414, Time 00:01:02,lr 0.01\n",
      "epoch 104, loss 0.02213, train_acc 0.9933, valid_acc 0.9417, Time 00:01:02,lr 0.01\n",
      "epoch 105, loss 0.01892, train_acc 0.9947, valid_acc 0.9410, Time 00:01:02,lr 0.01\n",
      "epoch 106, loss 0.02145, train_acc 0.9937, valid_acc 0.9418, Time 00:01:02,lr 0.01\n",
      "epoch 107, loss 0.01967, train_acc 0.9943, valid_acc 0.9386, Time 00:01:02,lr 0.01\n",
      "epoch 108, loss 0.01919, train_acc 0.9945, valid_acc 0.9394, Time 00:01:02,lr 0.01\n",
      "epoch 109, loss 0.02001, train_acc 0.9939, valid_acc 0.9409, Time 00:01:02,lr 0.01\n",
      "epoch 110, loss 0.02147, train_acc 0.9938, valid_acc 0.9424, Time 00:01:02,lr 0.01\n",
      "epoch 111, loss 0.02209, train_acc 0.9933, valid_acc 0.9395, Time 00:01:02,lr 0.01\n",
      "epoch 112, loss 0.02179, train_acc 0.9936, valid_acc 0.9397, Time 00:01:02,lr 0.01\n",
      "epoch 113, loss 0.01823, train_acc 0.9951, valid_acc 0.9407, Time 00:01:02,lr 0.01\n",
      "epoch 114, loss 0.02256, train_acc 0.9930, valid_acc 0.9420, Time 00:01:02,lr 0.01\n",
      "epoch 115, loss 0.02210, train_acc 0.9931, valid_acc 0.9397, Time 00:01:02,lr 0.01\n",
      "epoch 116, loss 0.02782, train_acc 0.9908, valid_acc 0.9360, Time 00:01:02,lr 0.01\n",
      "epoch 117, loss 0.02374, train_acc 0.9929, valid_acc 0.9350, Time 00:01:02,lr 0.01\n",
      "epoch 118, loss 0.02712, train_acc 0.9913, valid_acc 0.9391, Time 00:01:02,lr 0.01\n",
      "epoch 119, loss 0.02585, train_acc 0.9920, valid_acc 0.9348, Time 00:01:02,lr 0.01\n",
      "epoch 120, loss 0.02814, train_acc 0.9905, valid_acc 0.9378, Time 00:01:02,lr 0.01\n",
      "epoch 121, loss 0.03003, train_acc 0.9902, valid_acc 0.9358, Time 00:01:02,lr 0.01\n",
      "epoch 122, loss 0.02864, train_acc 0.9912, valid_acc 0.9377, Time 00:01:02,lr 0.01\n",
      "epoch 123, loss 0.02835, train_acc 0.9907, valid_acc 0.9376, Time 00:01:02,lr 0.01\n",
      "epoch 124, loss 0.02621, train_acc 0.9918, valid_acc 0.9327, Time 00:01:02,lr 0.01\n",
      "epoch 125, loss 0.03241, train_acc 0.9895, valid_acc 0.9360, Time 00:01:02,lr 0.01\n",
      "epoch 126, loss 0.03135, train_acc 0.9899, valid_acc 0.9392, Time 00:01:02,lr 0.01\n",
      "epoch 127, loss 0.03428, train_acc 0.9888, valid_acc 0.9312, Time 00:01:02,lr 0.01\n",
      "epoch 128, loss 0.03211, train_acc 0.9897, valid_acc 0.9371, Time 00:01:02,lr 0.01\n",
      "epoch 129, loss 0.03596, train_acc 0.9874, valid_acc 0.9394, Time 00:01:02,lr 0.01\n",
      "epoch 130, loss 0.03371, train_acc 0.9887, valid_acc 0.9314, Time 00:01:02,lr 0.01\n",
      "epoch 131, loss 0.03617, train_acc 0.9882, valid_acc 0.9349, Time 00:01:02,lr 0.01\n",
      "epoch 132, loss 0.03470, train_acc 0.9883, valid_acc 0.9340, Time 00:01:02,lr 0.01\n",
      "epoch 133, loss 0.03657, train_acc 0.9883, valid_acc 0.9276, Time 00:01:02,lr 0.01\n",
      "epoch 134, loss 0.03695, train_acc 0.9878, valid_acc 0.9336, Time 00:01:02,lr 0.01\n",
      "epoch 135, loss 0.03452, train_acc 0.9886, valid_acc 0.9347, Time 00:01:02,lr 0.01\n",
      "epoch 136, loss 0.03210, train_acc 0.9895, valid_acc 0.9336, Time 00:01:02,lr 0.01\n",
      "epoch 137, loss 0.03506, train_acc 0.9886, valid_acc 0.9329, Time 00:01:02,lr 0.01\n",
      "epoch 138, loss 0.03861, train_acc 0.9871, valid_acc 0.9294, Time 00:01:02,lr 0.01\n",
      "epoch 139, loss 0.03809, train_acc 0.9873, valid_acc 0.9319, Time 00:01:02,lr 0.01\n",
      "epoch 140, loss 0.03851, train_acc 0.9873, valid_acc 0.9233, Time 00:01:02,lr 0.01\n",
      "epoch 141, loss 0.04022, train_acc 0.9868, valid_acc 0.9359, Time 00:01:02,lr 0.01\n",
      "epoch 142, loss 0.03846, train_acc 0.9874, valid_acc 0.9306, Time 00:01:02,lr 0.01\n",
      "epoch 143, loss 0.03732, train_acc 0.9877, valid_acc 0.9261, Time 00:01:02,lr 0.01\n",
      "epoch 144, loss 0.03795, train_acc 0.9875, valid_acc 0.9339, Time 00:01:02,lr 0.01\n",
      "epoch 145, loss 0.04298, train_acc 0.9860, valid_acc 0.9302, Time 00:01:02,lr 0.01\n",
      "epoch 146, loss 0.03716, train_acc 0.9884, valid_acc 0.9316, Time 00:01:02,lr 0.01\n",
      "epoch 147, loss 0.04139, train_acc 0.9867, valid_acc 0.9273, Time 00:01:02,lr 0.01\n",
      "epoch 148, loss 0.03711, train_acc 0.9879, valid_acc 0.9309, Time 00:01:02,lr 0.01\n",
      "epoch 149, loss 0.03742, train_acc 0.9880, valid_acc 0.9299, Time 00:01:02,lr 0.01\n",
      "epoch 150, loss 0.02054, train_acc 0.9940, valid_acc 0.9442, Time 00:01:02,lr 0.001\n",
      "epoch 151, loss 0.01223, train_acc 0.9969, valid_acc 0.9461, Time 00:01:02,lr 0.001\n",
      "epoch 152, loss 0.00925, train_acc 0.9981, valid_acc 0.9482, Time 00:01:02,lr 0.001\n",
      "epoch 153, loss 0.00812, train_acc 0.9984, valid_acc 0.9469, Time 00:01:02,lr 0.001\n",
      "epoch 154, loss 0.00758, train_acc 0.9986, valid_acc 0.9478, Time 00:01:02,lr 0.001\n",
      "epoch 155, loss 0.00682, train_acc 0.9988, valid_acc 0.9477, Time 00:01:02,lr 0.001\n",
      "epoch 156, loss 0.00649, train_acc 0.9989, valid_acc 0.9477, Time 00:01:02,lr 0.001\n",
      "epoch 157, loss 0.00602, train_acc 0.9990, valid_acc 0.9478, Time 00:01:02,lr 0.001\n",
      "epoch 158, loss 0.00580, train_acc 0.9990, valid_acc 0.9494, Time 00:01:02,lr 0.001\n",
      "epoch 159, loss 0.00597, train_acc 0.9990, valid_acc 0.9495, Time 00:01:02,lr 0.001\n",
      "epoch 160, loss 0.00537, train_acc 0.9991, valid_acc 0.9493, Time 00:01:02,lr 0.001\n",
      "epoch 161, loss 0.00494, train_acc 0.9993, valid_acc 0.9495, Time 00:01:02,lr 0.001\n",
      "epoch 162, loss 0.00495, train_acc 0.9993, valid_acc 0.9512, Time 00:01:02,lr 0.001\n",
      "epoch 163, loss 0.00436, train_acc 0.9996, valid_acc 0.9504, Time 00:01:02,lr 0.001\n",
      "epoch 164, loss 0.00453, train_acc 0.9993, valid_acc 0.9499, Time 00:01:02,lr 0.001\n",
      "epoch 165, loss 0.00423, train_acc 0.9995, valid_acc 0.9507, Time 00:01:02,lr 0.001\n",
      "epoch 166, loss 0.00424, train_acc 0.9994, valid_acc 0.9496, Time 00:01:02,lr 0.001\n",
      "epoch 167, loss 0.00392, train_acc 0.9995, valid_acc 0.9500, Time 00:01:02,lr 0.001\n",
      "epoch 168, loss 0.00373, train_acc 0.9996, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 169, loss 0.00395, train_acc 0.9993, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 170, loss 0.00376, train_acc 0.9996, valid_acc 0.9512, Time 00:01:02,lr 0.001\n",
      "epoch 171, loss 0.00352, train_acc 0.9996, valid_acc 0.9496, Time 00:01:02,lr 0.001\n",
      "epoch 172, loss 0.00372, train_acc 0.9995, valid_acc 0.9503, Time 00:01:02,lr 0.001\n",
      "epoch 173, loss 0.00341, train_acc 0.9997, valid_acc 0.9516, Time 00:01:02,lr 0.001\n",
      "epoch 174, loss 0.00373, train_acc 0.9994, valid_acc 0.9518, Time 00:01:02,lr 0.001\n",
      "epoch 175, loss 0.00336, train_acc 0.9996, valid_acc 0.9524, Time 00:01:02,lr 0.001\n",
      "epoch 176, loss 0.00351, train_acc 0.9994, valid_acc 0.9515, Time 00:01:02,lr 0.001\n",
      "epoch 177, loss 0.00332, train_acc 0.9995, valid_acc 0.9498, Time 00:01:02,lr 0.001\n",
      "epoch 178, loss 0.00337, train_acc 0.9997, valid_acc 0.9515, Time 00:01:02,lr 0.001\n",
      "epoch 179, loss 0.00315, train_acc 0.9996, valid_acc 0.9511, Time 00:01:02,lr 0.001\n",
      "epoch 180, loss 0.00273, train_acc 0.9998, valid_acc 0.9521, Time 00:01:02,lr 0.001\n",
      "epoch 181, loss 0.00303, train_acc 0.9996, valid_acc 0.9518, Time 00:01:02,lr 0.001\n",
      "epoch 182, loss 0.00319, train_acc 0.9997, valid_acc 0.9507, Time 00:01:02,lr 0.001\n",
      "epoch 183, loss 0.00281, train_acc 0.9997, valid_acc 0.9508, Time 00:01:02,lr 0.001\n",
      "epoch 184, loss 0.00315, train_acc 0.9996, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 185, loss 0.00279, train_acc 0.9998, valid_acc 0.9517, Time 00:01:02,lr 0.001\n",
      "epoch 186, loss 0.00277, train_acc 0.9998, valid_acc 0.9515, Time 00:01:02,lr 0.001\n",
      "epoch 187, loss 0.00271, train_acc 0.9998, valid_acc 0.9520, Time 00:01:02,lr 0.001\n",
      "epoch 188, loss 0.00276, train_acc 0.9998, valid_acc 0.9518, Time 00:01:02,lr 0.001\n",
      "epoch 189, loss 0.00269, train_acc 0.9998, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 190, loss 0.00292, train_acc 0.9997, valid_acc 0.9506, Time 00:01:02,lr 0.001\n",
      "epoch 191, loss 0.00248, train_acc 0.9998, valid_acc 0.9520, Time 00:01:02,lr 0.001\n",
      "epoch 192, loss 0.00255, train_acc 0.9998, valid_acc 0.9511, Time 00:01:02,lr 0.001\n",
      "epoch 193, loss 0.00247, train_acc 0.9998, valid_acc 0.9516, Time 00:01:02,lr 0.001\n",
      "epoch 194, loss 0.00274, train_acc 0.9997, valid_acc 0.9503, Time 00:01:02,lr 0.001\n",
      "epoch 195, loss 0.00255, train_acc 0.9998, valid_acc 0.9511, Time 00:01:02,lr 0.001\n",
      "epoch 196, loss 0.00253, train_acc 0.9998, valid_acc 0.9502, Time 00:01:02,lr 0.001\n",
      "epoch 197, loss 0.00240, train_acc 0.9998, valid_acc 0.9504, Time 00:01:02,lr 0.001\n",
      "epoch 198, loss 0.00272, train_acc 0.9997, valid_acc 0.9517, Time 00:01:02,lr 0.001\n",
      "epoch 199, loss 0.00250, train_acc 0.9999, valid_acc 0.9516, Time 00:01:02,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_522_c64(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_522_c64_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:23:24.183057Z",
     "start_time": "2018-03-05T06:20:05.615Z"
    }
   },
   "source": [
    "## 5.10 to find if width is important, compare to my resnet18 double all conv layers' channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T09:55:35.978942Z",
     "start_time": "2018-03-05T09:55:35.963930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_333_c64(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_333_c64, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=256, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=256))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T13:38:25.724067Z",
     "start_time": "2018-03-05T09:55:35.983801Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.91070, train_acc 0.3031, valid_acc 0.3908, Time 00:01:02,lr 0.1\n",
      "epoch 1, loss 1.49198, train_acc 0.4545, valid_acc 0.5138, Time 00:01:06,lr 0.1\n",
      "epoch 2, loss 1.16327, train_acc 0.5859, valid_acc 0.6698, Time 00:01:06,lr 0.1\n",
      "epoch 3, loss 0.89234, train_acc 0.6859, valid_acc 0.6864, Time 00:01:06,lr 0.1\n",
      "epoch 4, loss 0.74111, train_acc 0.7439, valid_acc 0.6966, Time 00:01:06,lr 0.1\n",
      "epoch 5, loss 0.64906, train_acc 0.7765, valid_acc 0.7626, Time 00:01:06,lr 0.1\n",
      "epoch 6, loss 0.59122, train_acc 0.7965, valid_acc 0.7955, Time 00:01:06,lr 0.1\n",
      "epoch 7, loss 0.54251, train_acc 0.8130, valid_acc 0.7992, Time 00:01:06,lr 0.1\n",
      "epoch 8, loss 0.51203, train_acc 0.8241, valid_acc 0.7816, Time 00:01:06,lr 0.1\n",
      "epoch 9, loss 0.48736, train_acc 0.8330, valid_acc 0.8220, Time 00:01:06,lr 0.1\n",
      "epoch 10, loss 0.46056, train_acc 0.8435, valid_acc 0.8035, Time 00:01:06,lr 0.1\n",
      "epoch 11, loss 0.44692, train_acc 0.8453, valid_acc 0.8289, Time 00:01:06,lr 0.1\n",
      "epoch 12, loss 0.42780, train_acc 0.8531, valid_acc 0.7941, Time 00:01:06,lr 0.1\n",
      "epoch 13, loss 0.41150, train_acc 0.8595, valid_acc 0.8375, Time 00:01:06,lr 0.1\n",
      "epoch 14, loss 0.40133, train_acc 0.8611, valid_acc 0.8405, Time 00:01:06,lr 0.1\n",
      "epoch 15, loss 0.39545, train_acc 0.8640, valid_acc 0.8352, Time 00:01:06,lr 0.1\n",
      "epoch 16, loss 0.37821, train_acc 0.8688, valid_acc 0.8246, Time 00:01:06,lr 0.1\n",
      "epoch 17, loss 0.37287, train_acc 0.8725, valid_acc 0.8544, Time 00:01:06,lr 0.1\n",
      "epoch 18, loss 0.36702, train_acc 0.8742, valid_acc 0.8677, Time 00:01:06,lr 0.1\n",
      "epoch 19, loss 0.36043, train_acc 0.8769, valid_acc 0.8453, Time 00:01:06,lr 0.1\n",
      "epoch 20, loss 0.35116, train_acc 0.8796, valid_acc 0.8630, Time 00:01:06,lr 0.1\n",
      "epoch 21, loss 0.34817, train_acc 0.8812, valid_acc 0.8529, Time 00:01:06,lr 0.1\n",
      "epoch 22, loss 0.34614, train_acc 0.8798, valid_acc 0.8583, Time 00:01:06,lr 0.1\n",
      "epoch 23, loss 0.34516, train_acc 0.8813, valid_acc 0.8291, Time 00:01:06,lr 0.1\n",
      "epoch 24, loss 0.33574, train_acc 0.8840, valid_acc 0.8728, Time 00:01:06,lr 0.1\n",
      "epoch 25, loss 0.32641, train_acc 0.8882, valid_acc 0.8566, Time 00:01:06,lr 0.1\n",
      "epoch 26, loss 0.32412, train_acc 0.8877, valid_acc 0.8509, Time 00:01:06,lr 0.1\n",
      "epoch 27, loss 0.33088, train_acc 0.8869, valid_acc 0.8587, Time 00:01:06,lr 0.1\n",
      "epoch 28, loss 0.32500, train_acc 0.8877, valid_acc 0.8625, Time 00:01:06,lr 0.1\n",
      "epoch 29, loss 0.32184, train_acc 0.8900, valid_acc 0.8426, Time 00:01:06,lr 0.1\n",
      "epoch 30, loss 0.31916, train_acc 0.8906, valid_acc 0.8364, Time 00:01:06,lr 0.1\n",
      "epoch 31, loss 0.31433, train_acc 0.8931, valid_acc 0.8625, Time 00:01:06,lr 0.1\n",
      "epoch 32, loss 0.30779, train_acc 0.8951, valid_acc 0.8342, Time 00:01:06,lr 0.1\n",
      "epoch 33, loss 0.31361, train_acc 0.8931, valid_acc 0.8827, Time 00:01:06,lr 0.1\n",
      "epoch 34, loss 0.30608, train_acc 0.8943, valid_acc 0.8668, Time 00:01:06,lr 0.1\n",
      "epoch 35, loss 0.30962, train_acc 0.8936, valid_acc 0.8702, Time 00:01:06,lr 0.1\n",
      "epoch 36, loss 0.30872, train_acc 0.8944, valid_acc 0.8554, Time 00:01:06,lr 0.1\n",
      "epoch 37, loss 0.30220, train_acc 0.8944, valid_acc 0.8865, Time 00:01:06,lr 0.1\n",
      "epoch 38, loss 0.30488, train_acc 0.8969, valid_acc 0.8845, Time 00:01:06,lr 0.1\n",
      "epoch 39, loss 0.30197, train_acc 0.8975, valid_acc 0.8600, Time 00:01:06,lr 0.1\n",
      "epoch 40, loss 0.30346, train_acc 0.8963, valid_acc 0.8598, Time 00:01:06,lr 0.1\n",
      "epoch 41, loss 0.29583, train_acc 0.8996, valid_acc 0.8866, Time 00:01:06,lr 0.1\n",
      "epoch 42, loss 0.29905, train_acc 0.8973, valid_acc 0.8841, Time 00:01:06,lr 0.1\n",
      "epoch 43, loss 0.29592, train_acc 0.9001, valid_acc 0.8851, Time 00:01:06,lr 0.1\n",
      "epoch 44, loss 0.29321, train_acc 0.9009, valid_acc 0.8920, Time 00:01:07,lr 0.1\n",
      "epoch 45, loss 0.29935, train_acc 0.8979, valid_acc 0.8725, Time 00:01:06,lr 0.1\n",
      "epoch 46, loss 0.29592, train_acc 0.8992, valid_acc 0.8432, Time 00:01:06,lr 0.1\n",
      "epoch 47, loss 0.29559, train_acc 0.8981, valid_acc 0.8579, Time 00:01:06,lr 0.1\n",
      "epoch 48, loss 0.29578, train_acc 0.8989, valid_acc 0.8650, Time 00:01:06,lr 0.1\n",
      "epoch 49, loss 0.29405, train_acc 0.8994, valid_acc 0.8795, Time 00:01:07,lr 0.1\n",
      "epoch 50, loss 0.29210, train_acc 0.8995, valid_acc 0.8885, Time 00:01:06,lr 0.1\n",
      "epoch 51, loss 0.29042, train_acc 0.8995, valid_acc 0.8897, Time 00:01:06,lr 0.1\n",
      "epoch 52, loss 0.28023, train_acc 0.9043, valid_acc 0.8725, Time 00:01:06,lr 0.1\n",
      "epoch 53, loss 0.29078, train_acc 0.9008, valid_acc 0.8650, Time 00:01:06,lr 0.1\n",
      "epoch 54, loss 0.28800, train_acc 0.9012, valid_acc 0.8774, Time 00:01:06,lr 0.1\n",
      "epoch 55, loss 0.28512, train_acc 0.9018, valid_acc 0.8775, Time 00:01:06,lr 0.1\n",
      "epoch 56, loss 0.28262, train_acc 0.9041, valid_acc 0.8773, Time 00:01:06,lr 0.1\n",
      "epoch 57, loss 0.28406, train_acc 0.9025, valid_acc 0.8589, Time 00:01:06,lr 0.1\n",
      "epoch 58, loss 0.28683, train_acc 0.9019, valid_acc 0.8626, Time 00:01:06,lr 0.1\n",
      "epoch 59, loss 0.28206, train_acc 0.9033, valid_acc 0.8879, Time 00:01:06,lr 0.1\n",
      "epoch 60, loss 0.28285, train_acc 0.9028, valid_acc 0.8679, Time 00:01:06,lr 0.1\n",
      "epoch 61, loss 0.28471, train_acc 0.9047, valid_acc 0.8763, Time 00:01:06,lr 0.1\n",
      "epoch 62, loss 0.28389, train_acc 0.9025, valid_acc 0.8808, Time 00:01:06,lr 0.1\n",
      "epoch 63, loss 0.28368, train_acc 0.9015, valid_acc 0.8510, Time 00:01:06,lr 0.1\n",
      "epoch 64, loss 0.28125, train_acc 0.9062, valid_acc 0.8661, Time 00:01:06,lr 0.1\n",
      "epoch 65, loss 0.28093, train_acc 0.9039, valid_acc 0.8646, Time 00:01:06,lr 0.1\n",
      "epoch 66, loss 0.28358, train_acc 0.9046, valid_acc 0.8738, Time 00:01:06,lr 0.1\n",
      "epoch 67, loss 0.27802, train_acc 0.9047, valid_acc 0.8539, Time 00:01:07,lr 0.1\n",
      "epoch 68, loss 0.28430, train_acc 0.9032, valid_acc 0.8884, Time 00:01:08,lr 0.1\n",
      "epoch 69, loss 0.27718, train_acc 0.9048, valid_acc 0.8814, Time 00:01:06,lr 0.1\n",
      "epoch 70, loss 0.28220, train_acc 0.9031, valid_acc 0.8893, Time 00:01:06,lr 0.1\n",
      "epoch 71, loss 0.28048, train_acc 0.9037, valid_acc 0.8666, Time 00:01:06,lr 0.1\n",
      "epoch 72, loss 0.28267, train_acc 0.9032, valid_acc 0.8719, Time 00:01:06,lr 0.1\n",
      "epoch 73, loss 0.27851, train_acc 0.9040, valid_acc 0.8690, Time 00:01:09,lr 0.1\n",
      "epoch 74, loss 0.27998, train_acc 0.9024, valid_acc 0.8920, Time 00:01:06,lr 0.1\n",
      "epoch 75, loss 0.27545, train_acc 0.9062, valid_acc 0.8621, Time 00:01:06,lr 0.1\n",
      "epoch 76, loss 0.28088, train_acc 0.9036, valid_acc 0.8745, Time 00:01:06,lr 0.1\n",
      "epoch 77, loss 0.27670, train_acc 0.9058, valid_acc 0.8773, Time 00:01:06,lr 0.1\n",
      "epoch 78, loss 0.27581, train_acc 0.9069, valid_acc 0.8827, Time 00:01:06,lr 0.1\n",
      "epoch 79, loss 0.27913, train_acc 0.9040, valid_acc 0.8706, Time 00:01:06,lr 0.1\n",
      "epoch 80, loss 0.13957, train_acc 0.9539, valid_acc 0.9370, Time 00:01:06,lr 0.01\n",
      "epoch 81, loss 0.09086, train_acc 0.9695, valid_acc 0.9394, Time 00:01:06,lr 0.01\n",
      "epoch 82, loss 0.07365, train_acc 0.9756, valid_acc 0.9393, Time 00:01:07,lr 0.01\n",
      "epoch 83, loss 0.06506, train_acc 0.9781, valid_acc 0.9417, Time 00:01:06,lr 0.01\n",
      "epoch 84, loss 0.05740, train_acc 0.9813, valid_acc 0.9424, Time 00:01:06,lr 0.01\n",
      "epoch 85, loss 0.05083, train_acc 0.9829, valid_acc 0.9400, Time 00:01:06,lr 0.01\n",
      "epoch 86, loss 0.04590, train_acc 0.9857, valid_acc 0.9419, Time 00:01:06,lr 0.01\n",
      "epoch 87, loss 0.03799, train_acc 0.9875, valid_acc 0.9441, Time 00:01:06,lr 0.01\n",
      "epoch 88, loss 0.03804, train_acc 0.9876, valid_acc 0.9427, Time 00:01:06,lr 0.01\n",
      "epoch 89, loss 0.03164, train_acc 0.9903, valid_acc 0.9441, Time 00:01:06,lr 0.01\n",
      "epoch 90, loss 0.02854, train_acc 0.9905, valid_acc 0.9425, Time 00:01:06,lr 0.01\n",
      "epoch 91, loss 0.02570, train_acc 0.9920, valid_acc 0.9443, Time 00:01:06,lr 0.01\n",
      "epoch 92, loss 0.02691, train_acc 0.9915, valid_acc 0.9447, Time 00:01:06,lr 0.01\n",
      "epoch 93, loss 0.02434, train_acc 0.9921, valid_acc 0.9428, Time 00:01:06,lr 0.01\n",
      "epoch 94, loss 0.02427, train_acc 0.9922, valid_acc 0.9455, Time 00:01:06,lr 0.01\n",
      "epoch 95, loss 0.02132, train_acc 0.9932, valid_acc 0.9446, Time 00:01:06,lr 0.01\n",
      "epoch 96, loss 0.01910, train_acc 0.9945, valid_acc 0.9442, Time 00:01:06,lr 0.01\n",
      "epoch 97, loss 0.01803, train_acc 0.9950, valid_acc 0.9443, Time 00:01:06,lr 0.01\n",
      "epoch 98, loss 0.02039, train_acc 0.9934, valid_acc 0.9434, Time 00:01:06,lr 0.01\n",
      "epoch 99, loss 0.01921, train_acc 0.9939, valid_acc 0.9445, Time 00:01:06,lr 0.01\n",
      "epoch 100, loss 0.01898, train_acc 0.9937, valid_acc 0.9406, Time 00:01:06,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.02047, train_acc 0.9936, valid_acc 0.9447, Time 00:01:06,lr 0.01\n",
      "epoch 102, loss 0.01822, train_acc 0.9943, valid_acc 0.9429, Time 00:01:06,lr 0.01\n",
      "epoch 103, loss 0.01926, train_acc 0.9939, valid_acc 0.9436, Time 00:01:06,lr 0.01\n",
      "epoch 104, loss 0.01883, train_acc 0.9943, valid_acc 0.9438, Time 00:01:06,lr 0.01\n",
      "epoch 105, loss 0.02021, train_acc 0.9935, valid_acc 0.9413, Time 00:01:06,lr 0.01\n",
      "epoch 106, loss 0.01931, train_acc 0.9936, valid_acc 0.9428, Time 00:01:06,lr 0.01\n",
      "epoch 107, loss 0.01908, train_acc 0.9940, valid_acc 0.9422, Time 00:01:06,lr 0.01\n",
      "epoch 108, loss 0.01983, train_acc 0.9935, valid_acc 0.9426, Time 00:01:09,lr 0.01\n",
      "epoch 109, loss 0.01886, train_acc 0.9941, valid_acc 0.9405, Time 00:01:09,lr 0.01\n",
      "epoch 110, loss 0.02233, train_acc 0.9931, valid_acc 0.9411, Time 00:01:08,lr 0.01\n",
      "epoch 111, loss 0.01953, train_acc 0.9939, valid_acc 0.9378, Time 00:01:06,lr 0.01\n",
      "epoch 112, loss 0.02158, train_acc 0.9933, valid_acc 0.9376, Time 00:01:06,lr 0.01\n",
      "epoch 113, loss 0.02251, train_acc 0.9925, valid_acc 0.9409, Time 00:01:06,lr 0.01\n",
      "epoch 114, loss 0.02559, train_acc 0.9916, valid_acc 0.9346, Time 00:01:06,lr 0.01\n",
      "epoch 115, loss 0.02289, train_acc 0.9927, valid_acc 0.9398, Time 00:01:07,lr 0.01\n",
      "epoch 116, loss 0.02405, train_acc 0.9918, valid_acc 0.9409, Time 00:01:06,lr 0.01\n",
      "epoch 117, loss 0.02692, train_acc 0.9910, valid_acc 0.9377, Time 00:01:06,lr 0.01\n",
      "epoch 118, loss 0.02370, train_acc 0.9924, valid_acc 0.9361, Time 00:01:06,lr 0.01\n",
      "epoch 119, loss 0.02839, train_acc 0.9913, valid_acc 0.9372, Time 00:01:06,lr 0.01\n",
      "epoch 120, loss 0.02599, train_acc 0.9913, valid_acc 0.9359, Time 00:01:06,lr 0.01\n",
      "epoch 121, loss 0.02696, train_acc 0.9914, valid_acc 0.9357, Time 00:01:06,lr 0.01\n",
      "epoch 122, loss 0.02899, train_acc 0.9901, valid_acc 0.9317, Time 00:01:06,lr 0.01\n",
      "epoch 123, loss 0.03193, train_acc 0.9896, valid_acc 0.9408, Time 00:01:06,lr 0.01\n",
      "epoch 124, loss 0.02635, train_acc 0.9917, valid_acc 0.9395, Time 00:01:06,lr 0.01\n",
      "epoch 125, loss 0.03074, train_acc 0.9901, valid_acc 0.9381, Time 00:01:06,lr 0.01\n",
      "epoch 126, loss 0.03066, train_acc 0.9894, valid_acc 0.9352, Time 00:01:06,lr 0.01\n",
      "epoch 127, loss 0.03199, train_acc 0.9894, valid_acc 0.9390, Time 00:01:06,lr 0.01\n",
      "epoch 128, loss 0.03443, train_acc 0.9882, valid_acc 0.9352, Time 00:01:06,lr 0.01\n",
      "epoch 129, loss 0.03173, train_acc 0.9892, valid_acc 0.9283, Time 00:01:06,lr 0.01\n",
      "epoch 130, loss 0.03065, train_acc 0.9898, valid_acc 0.9334, Time 00:01:06,lr 0.01\n",
      "epoch 131, loss 0.03048, train_acc 0.9903, valid_acc 0.9392, Time 00:01:06,lr 0.01\n",
      "epoch 132, loss 0.03334, train_acc 0.9890, valid_acc 0.9350, Time 00:01:06,lr 0.01\n",
      "epoch 133, loss 0.03732, train_acc 0.9877, valid_acc 0.9384, Time 00:01:06,lr 0.01\n",
      "epoch 134, loss 0.03484, train_acc 0.9879, valid_acc 0.9261, Time 00:01:06,lr 0.01\n",
      "epoch 135, loss 0.03484, train_acc 0.9882, valid_acc 0.9351, Time 00:01:06,lr 0.01\n",
      "epoch 136, loss 0.03939, train_acc 0.9872, valid_acc 0.9350, Time 00:01:06,lr 0.01\n",
      "epoch 137, loss 0.03319, train_acc 0.9892, valid_acc 0.9326, Time 00:01:06,lr 0.01\n",
      "epoch 138, loss 0.03750, train_acc 0.9875, valid_acc 0.9313, Time 00:01:06,lr 0.01\n",
      "epoch 139, loss 0.03828, train_acc 0.9873, valid_acc 0.9325, Time 00:01:06,lr 0.01\n",
      "epoch 140, loss 0.03764, train_acc 0.9875, valid_acc 0.9328, Time 00:01:06,lr 0.01\n",
      "epoch 141, loss 0.03222, train_acc 0.9899, valid_acc 0.9264, Time 00:01:06,lr 0.01\n",
      "epoch 142, loss 0.03475, train_acc 0.9885, valid_acc 0.9321, Time 00:01:06,lr 0.01\n",
      "epoch 143, loss 0.03488, train_acc 0.9884, valid_acc 0.9293, Time 00:01:06,lr 0.01\n",
      "epoch 144, loss 0.03515, train_acc 0.9885, valid_acc 0.9354, Time 00:01:07,lr 0.01\n",
      "epoch 145, loss 0.03172, train_acc 0.9897, valid_acc 0.9288, Time 00:01:08,lr 0.01\n",
      "epoch 146, loss 0.04010, train_acc 0.9867, valid_acc 0.9282, Time 00:01:13,lr 0.01\n",
      "epoch 147, loss 0.03735, train_acc 0.9873, valid_acc 0.9327, Time 00:01:09,lr 0.01\n",
      "epoch 148, loss 0.03458, train_acc 0.9883, valid_acc 0.9316, Time 00:01:11,lr 0.01\n",
      "epoch 149, loss 0.04376, train_acc 0.9848, valid_acc 0.9363, Time 00:01:06,lr 0.01\n",
      "epoch 150, loss 0.01750, train_acc 0.9948, valid_acc 0.9462, Time 00:01:06,lr 0.001\n",
      "epoch 151, loss 0.01047, train_acc 0.9974, valid_acc 0.9471, Time 00:01:06,lr 0.001\n",
      "epoch 152, loss 0.00795, train_acc 0.9982, valid_acc 0.9488, Time 00:01:06,lr 0.001\n",
      "epoch 153, loss 0.00667, train_acc 0.9984, valid_acc 0.9484, Time 00:01:07,lr 0.001\n",
      "epoch 154, loss 0.00581, train_acc 0.9986, valid_acc 0.9473, Time 00:01:06,lr 0.001\n",
      "epoch 155, loss 0.00485, train_acc 0.9990, valid_acc 0.9505, Time 00:01:06,lr 0.001\n",
      "epoch 156, loss 0.00483, train_acc 0.9987, valid_acc 0.9504, Time 00:01:06,lr 0.001\n",
      "epoch 157, loss 0.00408, train_acc 0.9993, valid_acc 0.9510, Time 00:01:06,lr 0.001\n",
      "epoch 158, loss 0.00388, train_acc 0.9994, valid_acc 0.9500, Time 00:01:06,lr 0.001\n",
      "epoch 159, loss 0.00368, train_acc 0.9993, valid_acc 0.9508, Time 00:01:06,lr 0.001\n",
      "epoch 160, loss 0.00365, train_acc 0.9992, valid_acc 0.9506, Time 00:01:06,lr 0.001\n",
      "epoch 161, loss 0.00294, train_acc 0.9997, valid_acc 0.9511, Time 00:01:06,lr 0.001\n",
      "epoch 162, loss 0.00302, train_acc 0.9996, valid_acc 0.9504, Time 00:01:06,lr 0.001\n",
      "epoch 163, loss 0.00283, train_acc 0.9995, valid_acc 0.9506, Time 00:01:06,lr 0.001\n",
      "epoch 164, loss 0.00246, train_acc 0.9997, valid_acc 0.9514, Time 00:01:06,lr 0.001\n",
      "epoch 165, loss 0.00236, train_acc 0.9996, valid_acc 0.9500, Time 00:01:06,lr 0.001\n",
      "epoch 166, loss 0.00279, train_acc 0.9995, valid_acc 0.9510, Time 00:01:06,lr 0.001\n",
      "epoch 167, loss 0.00224, train_acc 0.9996, valid_acc 0.9521, Time 00:01:06,lr 0.001\n",
      "epoch 168, loss 0.00224, train_acc 0.9998, valid_acc 0.9508, Time 00:01:06,lr 0.001\n",
      "epoch 169, loss 0.00219, train_acc 0.9997, valid_acc 0.9526, Time 00:01:06,lr 0.001\n",
      "epoch 170, loss 0.00219, train_acc 0.9997, valid_acc 0.9520, Time 00:01:06,lr 0.001\n",
      "epoch 171, loss 0.00200, train_acc 0.9997, valid_acc 0.9516, Time 00:01:06,lr 0.001\n",
      "epoch 172, loss 0.00190, train_acc 0.9997, valid_acc 0.9522, Time 00:01:06,lr 0.001\n",
      "epoch 173, loss 0.00208, train_acc 0.9996, valid_acc 0.9507, Time 00:01:06,lr 0.001\n",
      "epoch 174, loss 0.00183, train_acc 0.9998, valid_acc 0.9513, Time 00:01:06,lr 0.001\n",
      "epoch 175, loss 0.00196, train_acc 0.9997, valid_acc 0.9517, Time 00:01:06,lr 0.001\n",
      "epoch 176, loss 0.00178, train_acc 0.9997, valid_acc 0.9512, Time 00:01:06,lr 0.001\n",
      "epoch 177, loss 0.00176, train_acc 0.9997, valid_acc 0.9525, Time 00:01:06,lr 0.001\n",
      "epoch 178, loss 0.00173, train_acc 0.9998, valid_acc 0.9513, Time 00:01:06,lr 0.001\n",
      "epoch 179, loss 0.00203, train_acc 0.9997, valid_acc 0.9531, Time 00:01:06,lr 0.001\n",
      "epoch 180, loss 0.00158, train_acc 0.9998, valid_acc 0.9521, Time 00:01:06,lr 0.001\n",
      "epoch 181, loss 0.00166, train_acc 0.9998, valid_acc 0.9522, Time 00:01:06,lr 0.001\n",
      "epoch 182, loss 0.00158, train_acc 0.9997, valid_acc 0.9518, Time 00:01:06,lr 0.001\n",
      "epoch 183, loss 0.00173, train_acc 0.9997, valid_acc 0.9513, Time 00:01:06,lr 0.001\n",
      "epoch 184, loss 0.00152, train_acc 0.9999, valid_acc 0.9527, Time 00:01:06,lr 0.001\n",
      "epoch 185, loss 0.00155, train_acc 0.9998, valid_acc 0.9529, Time 00:01:06,lr 0.001\n",
      "epoch 186, loss 0.00138, train_acc 0.9998, valid_acc 0.9518, Time 00:01:06,lr 0.001\n",
      "epoch 187, loss 0.00139, train_acc 0.9999, valid_acc 0.9535, Time 00:01:06,lr 0.001\n",
      "epoch 188, loss 0.00149, train_acc 0.9997, valid_acc 0.9529, Time 00:01:06,lr 0.001\n",
      "epoch 189, loss 0.00136, train_acc 0.9999, valid_acc 0.9528, Time 00:01:06,lr 0.001\n",
      "epoch 190, loss 0.00143, train_acc 0.9998, valid_acc 0.9527, Time 00:01:06,lr 0.001\n",
      "epoch 191, loss 0.00152, train_acc 0.9997, valid_acc 0.9536, Time 00:01:06,lr 0.001\n",
      "epoch 192, loss 0.00138, train_acc 0.9998, valid_acc 0.9547, Time 00:01:06,lr 0.001\n",
      "epoch 193, loss 0.00123, train_acc 0.9999, valid_acc 0.9551, Time 00:01:06,lr 0.001\n",
      "epoch 194, loss 0.00131, train_acc 0.9998, valid_acc 0.9532, Time 00:01:07,lr 0.001\n",
      "epoch 195, loss 0.00131, train_acc 0.9998, valid_acc 0.9544, Time 00:01:07,lr 0.001\n",
      "epoch 196, loss 0.00135, train_acc 0.9998, valid_acc 0.9538, Time 00:01:08,lr 0.001\n",
      "epoch 197, loss 0.00130, train_acc 0.9998, valid_acc 0.9536, Time 00:01:06,lr 0.001\n",
      "epoch 198, loss 0.00139, train_acc 0.9998, valid_acc 0.9537, Time 00:01:13,lr 0.001\n",
      "epoch 199, loss 0.00130, train_acc 0.9998, valid_acc 0.9524, Time 00:01:15,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_333_c64(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_333_c64_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11 general lr policy train resnet152_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T13:44:19.846663Z",
     "start_time": "2018-03-05T13:44:19.824497Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet152_v2_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet152_v2(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features[0])\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[2:4])\n",
    "        net.add(*resnet.features[5:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T13:52:47.969940Z",
     "start_time": "2018-03-05T13:44:22.629059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.02409, train_acc 0.2941, valid_acc 0.4078, Time 00:06:37,lr 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3209:\n",
      "Process Process-3212:\n",
      "Process Process-3211:\n",
      "Process Process-3210:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mxnet/gluon/data/dataloader.py\", line 120, in worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mxnet/gluon/data/dataloader.py\", line 120, in worker_loop\n",
      "    self.run()\n",
      "    data_queue.put((idx, batch))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mxnet/gluon/data/dataloader.py\", line 120, in worker_loop\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 101, in put\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    data_queue.put((idx, batch))\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    data_queue.put((idx, batch))\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 101, in put\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 101, in put\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mxnet/gluon/data/dataloader.py\", line 120, in worker_loop\n",
      "    data_queue.put((idx, batch))\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 101, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-707519fca34b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhybridize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n\u001b[0;32m---> 13\u001b[0;31m       [], log_file, False, loss_f)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../models/resnet152_v2_300e\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-26786e1da2ef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_data, valid_data, num_epochs, lr, lr_period, lr_decay, wd, ctx, w_key, output_file, verbose, loss_f, use_mixup, mixup_alpha)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-26786e1da2ef>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(data, label, i)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0m_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet/ndarray/ndarray.pyc\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet/ndarray/ndarray.pyc\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1815\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [100, 180, 250]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=2)\n",
    "net = get_resnet152_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet152_v2_300e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. use rewrite train function to train function to make sure not train code bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.794769Z",
     "start_time": "2018-03-03T17:35:08.750443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def get_lr(e, i, step):\n",
    "    if e >= step[i]:\n",
    "        return i + 1\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, lr_decay, wd, ctx, output_file=None, step={}):\n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "        print \" # valid_acc\", valid_acc\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    step_i = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if epoch > 0:\n",
    "            if len(step.keys()) == 0:\n",
    "                if epoch % lr_period == 0:\n",
    "                    trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            else:\n",
    "                if epoch >= step.keys()[step_i]:\n",
    "                    step_i = step_i + 1\n",
    "                    trainer.set_learning_rate(step[step.keys()[step_i]])\n",
    "        \n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            \n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += utils.accuracy(output, label)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "        \n",
    "        nd.waitall()\n",
    "        \n",
    "    if output_file != sys.stdout:\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:17:26.291786Z",
     "start_time": "2018-03-03T17:35:08.796140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.100838658147\n",
      "epoch 0, loss 2.17650, train_acc 0.1815, valid_acc 0.3233, Time 00:00:29,lr 0.1\n",
      "epoch 1, loss 1.74745, train_acc 0.3639, valid_acc 0.2111, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.48485, train_acc 0.4729, valid_acc 0.5586, Time 00:00:31,lr 0.1\n",
      "epoch 3, loss 1.32675, train_acc 0.5326, valid_acc 0.5165, Time 00:00:31,lr 0.1\n",
      "epoch 4, loss 1.23933, train_acc 0.5676, valid_acc 0.4877, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 1.18796, train_acc 0.5838, valid_acc 0.5885, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.15784, train_acc 0.5958, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 1.12028, train_acc 0.6103, valid_acc 0.6090, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 1.11800, train_acc 0.6141, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 1.09796, train_acc 0.6208, valid_acc 0.6072, Time 00:00:31,lr 0.1\n",
      "epoch 10, loss 1.08225, train_acc 0.6251, valid_acc 0.5205, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 1.07237, train_acc 0.6293, valid_acc 0.5745, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 1.06935, train_acc 0.6309, valid_acc 0.6407, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 1.05872, train_acc 0.6334, valid_acc 0.5826, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 1.04889, train_acc 0.6376, valid_acc 0.6672, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 1.05632, train_acc 0.6361, valid_acc 0.6499, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 1.05060, train_acc 0.6386, valid_acc 0.6274, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 1.04430, train_acc 0.6395, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 1.04184, train_acc 0.6421, valid_acc 0.6441, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 1.04526, train_acc 0.6398, valid_acc 0.5606, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 1.04432, train_acc 0.6404, valid_acc 0.6367, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 1.03036, train_acc 0.6456, valid_acc 0.7069, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 1.03184, train_acc 0.6459, valid_acc 0.6150, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 1.03162, train_acc 0.6419, valid_acc 0.6809, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 1.03152, train_acc 0.6440, valid_acc 0.6096, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 1.02944, train_acc 0.6447, valid_acc 0.6779, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 1.02975, train_acc 0.6446, valid_acc 0.5822, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 1.02655, train_acc 0.6464, valid_acc 0.6957, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 1.02281, train_acc 0.6490, valid_acc 0.6738, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 1.02572, train_acc 0.6455, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 1.01937, train_acc 0.6497, valid_acc 0.5929, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 1.01610, train_acc 0.6499, valid_acc 0.5739, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 1.02582, train_acc 0.6454, valid_acc 0.6808, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 1.02411, train_acc 0.6468, valid_acc 0.7068, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 1.01952, train_acc 0.6492, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 35, loss 1.02947, train_acc 0.6447, valid_acc 0.6387, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 1.01511, train_acc 0.6521, valid_acc 0.5688, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 1.02304, train_acc 0.6474, valid_acc 0.6549, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 1.01642, train_acc 0.6496, valid_acc 0.6774, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 1.01767, train_acc 0.6498, valid_acc 0.5900, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 1.01177, train_acc 0.6501, valid_acc 0.6799, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 1.01933, train_acc 0.6482, valid_acc 0.6359, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 1.01835, train_acc 0.6475, valid_acc 0.6245, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 1.02311, train_acc 0.6466, valid_acc 0.6745, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 1.01084, train_acc 0.6527, valid_acc 0.6729, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 1.01739, train_acc 0.6508, valid_acc 0.6601, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 1.02528, train_acc 0.6453, valid_acc 0.6812, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 1.01437, train_acc 0.6516, valid_acc 0.6579, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 1.02278, train_acc 0.6477, valid_acc 0.6893, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 1.01775, train_acc 0.6479, valid_acc 0.7160, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 1.01944, train_acc 0.6485, valid_acc 0.6020, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 1.02941, train_acc 0.6463, valid_acc 0.6422, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 1.01799, train_acc 0.6497, valid_acc 0.7113, Time 00:00:31,lr 0.1\n",
      "epoch 53, loss 1.01612, train_acc 0.6477, valid_acc 0.6327, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 1.02105, train_acc 0.6492, valid_acc 0.6325, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 1.02410, train_acc 0.6450, valid_acc 0.6888, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 1.01827, train_acc 0.6495, valid_acc 0.6665, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 1.01920, train_acc 0.6502, valid_acc 0.6775, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 1.02319, train_acc 0.6464, valid_acc 0.6765, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 1.01978, train_acc 0.6507, valid_acc 0.5037, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 1.02047, train_acc 0.6486, valid_acc 0.6431, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 1.01323, train_acc 0.6509, valid_acc 0.6778, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 1.01677, train_acc 0.6475, valid_acc 0.6488, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 1.02140, train_acc 0.6477, valid_acc 0.6320, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 1.01649, train_acc 0.6486, valid_acc 0.6985, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 1.01495, train_acc 0.6494, valid_acc 0.6110, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 1.02517, train_acc 0.6482, valid_acc 0.6281, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 1.01754, train_acc 0.6505, valid_acc 0.6933, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 1.01817, train_acc 0.6487, valid_acc 0.5997, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 1.02060, train_acc 0.6480, valid_acc 0.6438, Time 00:00:31,lr 0.1\n",
      "epoch 70, loss 1.01663, train_acc 0.6501, valid_acc 0.6557, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 1.01802, train_acc 0.6506, valid_acc 0.6037, Time 00:00:31,lr 0.1\n",
      "epoch 72, loss 1.02473, train_acc 0.6465, valid_acc 0.6869, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 1.02214, train_acc 0.6481, valid_acc 0.6365, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 1.02075, train_acc 0.6494, valid_acc 0.6727, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 1.02465, train_acc 0.6455, valid_acc 0.6005, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 1.01342, train_acc 0.6509, valid_acc 0.6316, Time 00:00:31,lr 0.1\n",
      "epoch 77, loss 1.02116, train_acc 0.6466, valid_acc 0.6909, Time 00:00:31,lr 0.1\n",
      "epoch 78, loss 1.02252, train_acc 0.6469, valid_acc 0.7035, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 1.01862, train_acc 0.6496, valid_acc 0.7132, Time 00:00:31,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = 80\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:33.999408Z",
     "start_time": "2018-03-03T18:17:26.293466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.713158945687\n",
      "epoch 0, loss 0.89016, train_acc 0.6943, valid_acc 0.6973, Time 00:00:29,lr 0.05\n",
      "epoch 1, loss 0.97071, train_acc 0.6649, valid_acc 0.6228, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.97855, train_acc 0.6619, valid_acc 0.5962, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.98917, train_acc 0.6604, valid_acc 0.6651, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.98467, train_acc 0.6614, valid_acc 0.6855, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.98934, train_acc 0.6582, valid_acc 0.6544, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.99133, train_acc 0.6577, valid_acc 0.6867, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.99419, train_acc 0.6565, valid_acc 0.6996, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.99295, train_acc 0.6574, valid_acc 0.6707, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.98785, train_acc 0.6583, valid_acc 0.6599, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.72795, train_acc 0.7497, valid_acc 0.7739, Time 00:00:31,lr 0.01\n",
      "epoch 11, loss 0.67062, train_acc 0.7695, valid_acc 0.7807, Time 00:00:31,lr 0.01\n",
      "epoch 12, loss 0.66014, train_acc 0.7741, valid_acc 0.8119, Time 00:00:31,lr 0.01\n",
      "epoch 13, loss 0.66186, train_acc 0.7753, valid_acc 0.8148, Time 00:00:31,lr 0.01\n",
      "epoch 14, loss 0.66272, train_acc 0.7717, valid_acc 0.7799, Time 00:00:31,lr 0.01\n",
      "epoch 15, loss 0.64879, train_acc 0.7773, valid_acc 0.8168, Time 00:00:31,lr 0.01\n",
      "epoch 16, loss 0.65313, train_acc 0.7758, valid_acc 0.8103, Time 00:00:31,lr 0.01\n",
      "epoch 17, loss 0.64368, train_acc 0.7803, valid_acc 0.8027, Time 00:00:31,lr 0.01\n",
      "epoch 18, loss 0.64573, train_acc 0.7790, valid_acc 0.7865, Time 00:00:31,lr 0.01\n",
      "epoch 19, loss 0.63965, train_acc 0.7808, valid_acc 0.8113, Time 00:00:31,lr 0.01\n",
      "epoch 20, loss 0.49768, train_acc 0.8307, valid_acc 0.8597, Time 00:00:31,lr 0.002\n",
      "epoch 21, loss 0.45085, train_acc 0.8470, valid_acc 0.8730, Time 00:00:31,lr 0.002\n",
      "epoch 22, loss 0.43494, train_acc 0.8515, valid_acc 0.8760, Time 00:00:31,lr 0.002\n",
      "epoch 23, loss 0.42184, train_acc 0.8574, valid_acc 0.8801, Time 00:00:31,lr 0.002\n",
      "epoch 24, loss 0.41728, train_acc 0.8585, valid_acc 0.8719, Time 00:00:31,lr 0.002\n",
      "epoch 25, loss 0.41803, train_acc 0.8566, valid_acc 0.8713, Time 00:00:31,lr 0.002\n",
      "epoch 26, loss 0.41157, train_acc 0.8586, valid_acc 0.8734, Time 00:00:31,lr 0.002\n",
      "epoch 27, loss 0.40844, train_acc 0.8605, valid_acc 0.8761, Time 00:00:31,lr 0.002\n",
      "epoch 28, loss 0.40713, train_acc 0.8602, valid_acc 0.8781, Time 00:00:31,lr 0.002\n",
      "epoch 29, loss 0.40274, train_acc 0.8636, valid_acc 0.8810, Time 00:00:31,lr 0.002\n",
      "epoch 30, loss 0.34908, train_acc 0.8807, valid_acc 0.8912, Time 00:00:31,lr 0.0004\n",
      "epoch 31, loss 0.32949, train_acc 0.8883, valid_acc 0.8973, Time 00:00:31,lr 0.0004\n",
      "epoch 32, loss 0.31813, train_acc 0.8929, valid_acc 0.8957, Time 00:00:31,lr 0.0004\n",
      "epoch 33, loss 0.31530, train_acc 0.8933, valid_acc 0.8949, Time 00:00:31,lr 0.0004\n",
      "epoch 34, loss 0.30938, train_acc 0.8967, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 35, loss 0.30692, train_acc 0.8977, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 36, loss 0.30172, train_acc 0.8962, valid_acc 0.8979, Time 00:00:31,lr 0.0004\n",
      "epoch 37, loss 0.29754, train_acc 0.9001, valid_acc 0.8978, Time 00:00:31,lr 0.0004\n",
      "epoch 38, loss 0.29589, train_acc 0.8995, valid_acc 0.8993, Time 00:00:31,lr 0.0004\n",
      "epoch 39, loss 0.29325, train_acc 0.9014, valid_acc 0.9010, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = 10\n",
    "lr_decay=0.2\n",
    "log_file=None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug2\", ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:34.023597Z",
     "start_time": "2018-03-03T18:38:34.000689Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.save_params('../../models/resnet18_me_9_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
