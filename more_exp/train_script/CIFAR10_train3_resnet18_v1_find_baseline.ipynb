{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T06:32:47.406027Z",
     "start_time": "2018-03-01T06:32:42.929058Z"
    },
    "collapsed": true
   },
   "source": [
    "# 0. baseline re-preduce\n",
    "\n",
    "1. 延后第一次（越靠近后面的延缓带来的效果增益越小）的down sample确实会很有效，对于resnet18的效果甚至超过模型宽度(通道数)翻倍的效果; 同时使用实验证明这个性能提升是延缓down sample带来的，而不是卷积核的padding或kernel_size带来的。\n",
    "2. 一组合适的参数保证网络的充分训练和收敛（num_epoch, lr_period, lr_decay), 同时发现过大的weight_decay也会影响（降低）网络的收敛速度和结果\n",
    "```\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. import needed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:29:23.277953Z",
     "start_time": "2018-03-15T14:29:22.816678Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.insert(0, '../../utils')\n",
    "from netlib import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data loader, data argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:29:23.988950Z",
     "start_time": "2018-03-15T14:29:23.939144Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data loader\n",
    "\"\"\"\n",
    "def _transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "def _transform_test_size(size):\n",
    "    def _transform_test2(data, label):\n",
    "        im = data.astype('float32') / 255\n",
    "        auglist = [image.ResizeAug(size, interp=3),\n",
    "                   image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                           std=np.array([0.2023, 0.1994, 0.2010]))]\n",
    "        for aug in auglist:\n",
    "            im = aug(im)\n",
    "        im = nd.transpose(im, (2, 0, 1))\n",
    "        return im, nd.array([label]).astype('float32')\n",
    "    return _transform_test2\n",
    "\n",
    "\n",
    "def data_loader(batch_size, transform_train, transform_test=None, num_workers=0):\n",
    "    if transform_train is None:\n",
    "        transform_train = _transform_train\n",
    "    if transform_test is None:\n",
    "        transform_test = _transform_test\n",
    "        \n",
    "    # flag=1 mean 3 channel image\n",
    "    train_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=True, transform=transform_train)\n",
    "    test_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=False, transform=transform_test)\n",
    "\n",
    "    loader = gluon.data.DataLoader\n",
    "    train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep', num_workers=num_workers)\n",
    "    test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep', num_workers=num_workers)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:29:24.612150Z",
     "start_time": "2018-03-15T14:29:24.542765Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data argument\n",
    "\"\"\"\n",
    "def transform_train_DA1(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "def transform_train2_DA1(data, label):\n",
    "    im = nd.array(data, dtype='float32') \n",
    "    im = image.imresize(im, 64, 64)\n",
    "    im = im.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 64, 64), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def transform_train_DA2(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "    \n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "    \n",
    "\n",
    "random_clip_rate = 0.3\n",
    "def transform_train_DA3(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "#                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "#                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "        \n",
    "    if random.random() > random_clip_rate:\n",
    "        im = im.clip(0, 1)\n",
    "    _aug = image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                   std=np.array([0.2023, 0.1994, 0.2010]),)\n",
    "    im = _aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 data aurgument: mixup\n",
    "1. mixup define\n",
    "2. mixup visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 mixup: define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:29:26.322877Z",
     "start_time": "2018-03-15T14:29:26.318101Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mixup(x1, y1, x2, y2, alpha, num_class):\n",
    "    y1 = nd.one_hot(y1, num_class)\n",
    "    y2 = nd.one_hot(y2, num_class)\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    x = lam * x1 + (1 - lam) * x2\n",
    "    y = lam * y1 + (1 - lam) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 mixup: visulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:44:07.847731Z",
     "start_time": "2018-03-09T14:44:07.001267Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from time import time\n",
    "batch_size = 32\n",
    "transform_train = _transform_test#transform_train_DA1\n",
    "train_data, test_data = data_loader(batch_size, transform_train)\n",
    "mixup_alpha = 1\n",
    "\n",
    "# for x1, y1 in train_data:\n",
    "#     for x2, y2 in mixup_train_data:\n",
    "#         data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "for x, y in train_data:\n",
    "    l = x.shape[0] / 2\n",
    "    data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:44:09.870996Z",
     "start_time": "2018-03-09T14:44:09.362270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAADYCAYAAACwTgnaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbKwY/w+khsxhHWyQtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd520040990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'airplane'), (1, 'automobile'), (2, 'bird'), (3, 'cat'), (4, 'deer'), (5, 'dog'), (6, 'frog'), (7, 'horse'), (8, 'ship'), (9, 'truck')]\n",
      "\n",
      "[[[ 0.26917797  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.73082203  0.        ]]\n",
      "\n",
      " [[ 0.26917797  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.73082203]]\n",
      "\n",
      " [[ 0.          0.          0.          0.73082203  0.          0.          0.\n",
      "    0.26917797  0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.26917797  0.          0.73082203  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.26917797\n",
      "    0.          0.          0.          0.73082203]]\n",
      "\n",
      " [[ 0.          0.          0.26917797  0.          0.          0.73082203\n",
      "    0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.73082203\n",
      "    0.          0.26917797  0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.26917797  0.          0.          0.          0.\n",
      "    0.73082203  0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.73082203\n",
      "    0.          0.          0.          0.26917797]]\n",
      "\n",
      " [[ 0.          0.          0.26917797  0.          0.73082203  0.          0.\n",
      "    0.          0.          0.        ]]]\n",
      "<NDArray 10x1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from cifar10_utils import show_images\n",
    "%matplotlib inline\n",
    "mean=np.array([0.4914, 0.4822, 0.4465])\n",
    "std=np.array([0.2023, 0.1994, 0.2010])\n",
    "images = data[:10].transpose((0, 2, 3, 1)).asnumpy()\n",
    "images = images * std + mean\n",
    "images = images.transpose((0, 3, 1, 2)) * 255\n",
    "show_images(images)\n",
    "#show_images(data[:9], rgb_mean=mean*255, std=std*255)\n",
    "\n",
    "print [(i, l) for i, l in enumerate(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])]\n",
    "print label[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 mixup: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:13:19.845625Z",
     "start_time": "2018-03-15T14:13:19.806185Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "mixup_test = False\n",
    "\n",
    "if mixup_test:\n",
    "    net = ResNet164_v2(10)\n",
    "    net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    loss_f = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "    num_epochs = 1\n",
    "    learning_rate = 0.1\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    cur_time = time()\n",
    "    iters = 0\n",
    "    \"\"\"\n",
    "    data loader first time run will cost about 3x time than after run.\n",
    "    \"\"\"\n",
    "    for x1, y1 in train_data:\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time\n",
    "        iters += 1\n",
    "    print \"cost time:\", time() - cur_time\n",
    "    print\n",
    "    cur_time = time()\n",
    "\n",
    "    iters = 0\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'momentum': 0.9, 'wd': 1e-4})\n",
    "    for x, y in train_data:\n",
    "        l = x.shape[0] / 2\n",
    "        data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label.as_in_context(ctx))\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time, nd.mean(loss).asscalar()\n",
    "        iters += 1\n",
    "    print iters, time() - cur_time\n",
    "\n",
    "    # load data one by one batch\n",
    "    # iters = 0\n",
    "    # for x1, y1 in train_data:\n",
    "    #     for x2, y2 in mixup_train_data:\n",
    "    #         data, label = mixup(x1, y1, x2[:x1.shape[0]], y2[:y1.shape[0]], mixup_alpha, 10)\n",
    "    #         break\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time\n",
    "    #     iters += 1\n",
    "    # print \"cost time:\", time() - cur_time\n",
    "    # print\n",
    "    # cur_time = time()\n",
    "\n",
    "    # zip will load all datas and then iterate them, too cost memory, will drop speed when memory over.\n",
    "    # iters = 0\n",
    "    # for (x1, y1), (x2, y2) in zip(train_data, mixup_train_data):\n",
    "    #     data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time#, nd.mean(loss).asscalar()\n",
    "    #     iters += 1\n",
    "    # print time() - cur_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:29:38.620183Z",
     "start_time": "2018-03-15T14:29:38.530574Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def abs_mean(W):\n",
    "    return nd.mean(nd.abs(W)).asscalar()\n",
    "\n",
    "def in_list(e, l):\n",
    "    for i in l:\n",
    "        if i == e:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, \n",
    "          lr_decay, wd, ctx, w_key, output_file=None, verbose=False, loss_f=gluon.loss.SoftmaxCrossEntropyLoss(), \n",
    "          use_mixup=False, mixup_alpha=0.2):\n",
    "    def train_batch(data, label, i):\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        _loss = nd.mean(loss).asscalar()\n",
    "        if not use_mixup:\n",
    "            _acc = utils.accuracy(output, label)\n",
    "        else:\n",
    "            _acc = None\n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            print \" # iter\", i,\n",
    "            print \"loss %.5f\" % _loss, \n",
    "            if not use_mixup: print \"acc %.5f\" % _acc,\n",
    "            print \"w (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.data()),\n",
    "            print \") g (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.grad()),\n",
    "            print \")\"\n",
    "        return _loss, _acc\n",
    "            \n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        stdout = sys.stdout\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = output_file\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    if verbose:\n",
    "        print \" #\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "    \n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if in_list(epoch, lr_period):\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            \n",
    "        if not use_mixup:\n",
    "            for data, label in train_data:\n",
    "                _loss, _acc = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                train_acc += _acc\n",
    "                i += 1\n",
    "        else:\n",
    "            for x, y in train_data:\n",
    "                l = x.shape[0] / 2\n",
    "                data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "                _loss, _ = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                i += 1\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        if train_acc < 1e-6:\n",
    "            train_acc = utils.evaluate_accuracy(train_data, net, ctx)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "    if output_file != stdout:\n",
    "        sys.stdout = stdout\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. get net and do EXP\n",
    "```\n",
    "I want to find out the reason that resnet18_v2 get fail baseline in CIFAR10_train2_* script, design follow exp.\n",
    "\n",
    "1. 5.1 train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs.\n",
    "2. 5.2 train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18.\n",
    " there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 re-exp resnet18_v1\n",
    "train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T16:30:21.462308Z",
     "start_time": "2018-03-03T16:30:13.299911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932408146965\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(10)\n",
    "net.load_params('../../models/res18_9', ctx=ctx)\n",
    "valid_acc = utils.evaluate_accuracy(test_data, net, ctx)\n",
    "print valid_acc\n",
    "def get_net(ctx):\n",
    "    num_outputs = 10\n",
    "    net = ResNet(num_outputs)\n",
    "    net.collect_params().initialize(init=init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    return net  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T10:46:50.521728Z",
     "start_time": "2018-03-03T10:05:27.156785Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.80346, train_acc 0.3266, valid_acc 0.3598, Time 00:00:28,lr 0.1\n",
      "epoch 1, loss 1.28281, train_acc 0.5411, valid_acc 0.5778, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.01865, train_acc 0.6431, valid_acc 0.6533, Time 00:00:30,lr 0.1\n",
      "epoch 3, loss 0.90869, train_acc 0.6870, valid_acc 0.5823, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 0.85972, train_acc 0.7049, valid_acc 0.6255, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 0.82922, train_acc 0.7158, valid_acc 0.6503, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 0.81416, train_acc 0.7211, valid_acc 0.6621, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 0.79129, train_acc 0.7286, valid_acc 0.6731, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.79221, train_acc 0.7293, valid_acc 0.6960, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 0.77941, train_acc 0.7357, valid_acc 0.7085, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.77304, train_acc 0.7350, valid_acc 0.6773, Time 00:00:33,lr 0.1\n",
      "epoch 11, loss 0.77300, train_acc 0.7362, valid_acc 0.6708, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 0.77379, train_acc 0.7359, valid_acc 0.7297, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.76346, train_acc 0.7400, valid_acc 0.6792, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.75981, train_acc 0.7400, valid_acc 0.7052, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 0.76189, train_acc 0.7409, valid_acc 0.7065, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 0.75300, train_acc 0.7431, valid_acc 0.7030, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 0.74843, train_acc 0.7451, valid_acc 0.7334, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 0.75400, train_acc 0.7439, valid_acc 0.6873, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 0.74822, train_acc 0.7453, valid_acc 0.6994, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 0.74226, train_acc 0.7492, valid_acc 0.6627, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 0.74857, train_acc 0.7449, valid_acc 0.6691, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 0.73817, train_acc 0.7513, valid_acc 0.6690, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.74331, train_acc 0.7469, valid_acc 0.7008, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 0.74029, train_acc 0.7473, valid_acc 0.6862, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 0.73600, train_acc 0.7491, valid_acc 0.7335, Time 00:00:30,lr 0.1\n",
      "epoch 26, loss 0.73561, train_acc 0.7486, valid_acc 0.7344, Time 00:00:30,lr 0.1\n",
      "epoch 27, loss 0.73582, train_acc 0.7491, valid_acc 0.5638, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.73652, train_acc 0.7464, valid_acc 0.5980, Time 00:00:30,lr 0.1\n",
      "epoch 29, loss 0.73622, train_acc 0.7506, valid_acc 0.7386, Time 00:00:30,lr 0.1\n",
      "epoch 30, loss 0.73189, train_acc 0.7487, valid_acc 0.6339, Time 00:00:30,lr 0.1\n",
      "epoch 31, loss 0.73360, train_acc 0.7494, valid_acc 0.6679, Time 00:00:30,lr 0.1\n",
      "epoch 32, loss 0.73711, train_acc 0.7486, valid_acc 0.6363, Time 00:00:30,lr 0.1\n",
      "epoch 33, loss 0.73315, train_acc 0.7495, valid_acc 0.6735, Time 00:00:30,lr 0.1\n",
      "epoch 34, loss 0.73185, train_acc 0.7496, valid_acc 0.7033, Time 00:00:30,lr 0.1\n",
      "epoch 35, loss 0.73066, train_acc 0.7499, valid_acc 0.7409, Time 00:00:30,lr 0.1\n",
      "epoch 36, loss 0.73286, train_acc 0.7492, valid_acc 0.6494, Time 00:00:30,lr 0.1\n",
      "epoch 37, loss 0.72791, train_acc 0.7502, valid_acc 0.6779, Time 00:00:30,lr 0.1\n",
      "epoch 38, loss 0.73055, train_acc 0.7519, valid_acc 0.6367, Time 00:00:30,lr 0.1\n",
      "epoch 39, loss 0.73307, train_acc 0.7487, valid_acc 0.7078, Time 00:00:30,lr 0.1\n",
      "epoch 40, loss 0.72583, train_acc 0.7522, valid_acc 0.6997, Time 00:00:30,lr 0.1\n",
      "epoch 41, loss 0.73065, train_acc 0.7513, valid_acc 0.6822, Time 00:00:30,lr 0.1\n",
      "epoch 42, loss 0.72384, train_acc 0.7533, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 43, loss 0.73071, train_acc 0.7507, valid_acc 0.6380, Time 00:00:30,lr 0.1\n",
      "epoch 44, loss 0.73206, train_acc 0.7511, valid_acc 0.6253, Time 00:00:30,lr 0.1\n",
      "epoch 45, loss 0.72313, train_acc 0.7533, valid_acc 0.6761, Time 00:00:30,lr 0.1\n",
      "epoch 46, loss 0.72663, train_acc 0.7531, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.72931, train_acc 0.7500, valid_acc 0.6451, Time 00:00:30,lr 0.1\n",
      "epoch 48, loss 0.72035, train_acc 0.7536, valid_acc 0.6857, Time 00:00:30,lr 0.1\n",
      "epoch 49, loss 0.73334, train_acc 0.7483, valid_acc 0.6765, Time 00:00:30,lr 0.1\n",
      "epoch 50, loss 0.72158, train_acc 0.7515, valid_acc 0.7363, Time 00:00:30,lr 0.1\n",
      "epoch 51, loss 0.72392, train_acc 0.7522, valid_acc 0.6832, Time 00:00:30,lr 0.1\n",
      "epoch 52, loss 0.72609, train_acc 0.7512, valid_acc 0.7577, Time 00:00:30,lr 0.1\n",
      "epoch 53, loss 0.72894, train_acc 0.7522, valid_acc 0.7357, Time 00:00:30,lr 0.1\n",
      "epoch 54, loss 0.72575, train_acc 0.7526, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 55, loss 0.72233, train_acc 0.7526, valid_acc 0.6178, Time 00:00:30,lr 0.1\n",
      "epoch 56, loss 0.72252, train_acc 0.7532, valid_acc 0.7112, Time 00:00:30,lr 0.1\n",
      "epoch 57, loss 0.72007, train_acc 0.7541, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 58, loss 0.72001, train_acc 0.7533, valid_acc 0.6893, Time 00:00:30,lr 0.1\n",
      "epoch 59, loss 0.72442, train_acc 0.7541, valid_acc 0.7461, Time 00:00:30,lr 0.1\n",
      "epoch 60, loss 0.72184, train_acc 0.7506, valid_acc 0.6913, Time 00:00:30,lr 0.1\n",
      "epoch 61, loss 0.72317, train_acc 0.7547, valid_acc 0.6189, Time 00:00:30,lr 0.1\n",
      "epoch 62, loss 0.72607, train_acc 0.7537, valid_acc 0.7175, Time 00:00:30,lr 0.1\n",
      "epoch 63, loss 0.71946, train_acc 0.7564, valid_acc 0.5596, Time 00:00:30,lr 0.1\n",
      "epoch 64, loss 0.72751, train_acc 0.7509, valid_acc 0.7269, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.72550, train_acc 0.7516, valid_acc 0.6568, Time 00:00:30,lr 0.1\n",
      "epoch 66, loss 0.72351, train_acc 0.7542, valid_acc 0.6292, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.72099, train_acc 0.7540, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 68, loss 0.72052, train_acc 0.7541, valid_acc 0.6509, Time 00:00:30,lr 0.1\n",
      "epoch 69, loss 0.71841, train_acc 0.7530, valid_acc 0.6510, Time 00:00:30,lr 0.1\n",
      "epoch 70, loss 0.72424, train_acc 0.7510, valid_acc 0.6668, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.71802, train_acc 0.7542, valid_acc 0.6872, Time 00:00:30,lr 0.1\n",
      "epoch 72, loss 0.72042, train_acc 0.7525, valid_acc 0.7391, Time 00:00:30,lr 0.1\n",
      "epoch 73, loss 0.72243, train_acc 0.7534, valid_acc 0.6784, Time 00:00:30,lr 0.1\n",
      "epoch 74, loss 0.72777, train_acc 0.7511, valid_acc 0.6956, Time 00:00:30,lr 0.1\n",
      "epoch 75, loss 0.72433, train_acc 0.7536, valid_acc 0.7030, Time 00:00:30,lr 0.1\n",
      "epoch 76, loss 0.72253, train_acc 0.7537, valid_acc 0.6999, Time 00:00:30,lr 0.1\n",
      "epoch 77, loss 0.72566, train_acc 0.7514, valid_acc 0.6808, Time 00:00:30,lr 0.1\n",
      "epoch 78, loss 0.72288, train_acc 0.7538, valid_acc 0.7191, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 0.72042, train_acc 0.7558, valid_acc 0.6245, Time 00:00:30,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.748897Z",
     "start_time": "2018-03-03T16:30:22.024115Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.61285, train_acc 0.7915, valid_acc 0.7278, Time 00:00:31,lr 0.05\n",
      "epoch 1, loss 0.68711, train_acc 0.7663, valid_acc 0.6986, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.70354, train_acc 0.7601, valid_acc 0.7469, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.70402, train_acc 0.7604, valid_acc 0.6044, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.70819, train_acc 0.7573, valid_acc 0.7099, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.71752, train_acc 0.7569, valid_acc 0.7387, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.71241, train_acc 0.7581, valid_acc 0.7282, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.71346, train_acc 0.7582, valid_acc 0.7717, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.71035, train_acc 0.7555, valid_acc 0.7430, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.71255, train_acc 0.7570, valid_acc 0.7069, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.71683, train_acc 0.7539, valid_acc 0.5970, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.71341, train_acc 0.7572, valid_acc 0.7481, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.71346, train_acc 0.7568, valid_acc 0.7049, Time 00:00:38,lr 0.05\n",
      "epoch 13, loss 0.70515, train_acc 0.7603, valid_acc 0.7366, Time 00:00:40,lr 0.05\n",
      "epoch 14, loss 0.71341, train_acc 0.7570, valid_acc 0.6632, Time 00:00:35,lr 0.05\n",
      "epoch 15, loss 0.70901, train_acc 0.7593, valid_acc 0.6943, Time 00:00:39,lr 0.05\n",
      "epoch 16, loss 0.71456, train_acc 0.7563, valid_acc 0.6658, Time 00:00:41,lr 0.05\n",
      "epoch 17, loss 0.71572, train_acc 0.7575, valid_acc 0.6613, Time 00:00:34,lr 0.05\n",
      "epoch 18, loss 0.71535, train_acc 0.7565, valid_acc 0.6999, Time 00:00:37,lr 0.05\n",
      "epoch 19, loss 0.70597, train_acc 0.7611, valid_acc 0.7358, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.71559, train_acc 0.7546, valid_acc 0.6635, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.71545, train_acc 0.7573, valid_acc 0.6657, Time 00:00:32,lr 0.05\n",
      "epoch 22, loss 0.71076, train_acc 0.7595, valid_acc 0.7046, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.71619, train_acc 0.7562, valid_acc 0.6314, Time 00:00:33,lr 0.05\n",
      "epoch 24, loss 0.70736, train_acc 0.7596, valid_acc 0.7478, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.70959, train_acc 0.7582, valid_acc 0.6643, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.71528, train_acc 0.7543, valid_acc 0.6441, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.72031, train_acc 0.7560, valid_acc 0.7425, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.71745, train_acc 0.7578, valid_acc 0.7464, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.70926, train_acc 0.7598, valid_acc 0.6095, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.46802, train_acc 0.8419, valid_acc 0.8559, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.42038, train_acc 0.8549, valid_acc 0.8552, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.41659, train_acc 0.8567, valid_acc 0.8418, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.41601, train_acc 0.8581, valid_acc 0.8321, Time 00:00:33,lr 0.01\n",
      "epoch 34, loss 0.41853, train_acc 0.8580, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.42103, train_acc 0.8563, valid_acc 0.8627, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.41386, train_acc 0.8588, valid_acc 0.8500, Time 00:00:31,lr 0.01\n",
      "epoch 37, loss 0.41497, train_acc 0.8594, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.40632, train_acc 0.8621, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.40439, train_acc 0.8627, valid_acc 0.8521, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.40516, train_acc 0.8633, valid_acc 0.8540, Time 00:00:31,lr 0.01\n",
      "epoch 41, loss 0.40254, train_acc 0.8627, valid_acc 0.8626, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.39611, train_acc 0.8632, valid_acc 0.8397, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.39748, train_acc 0.8639, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.39310, train_acc 0.8676, valid_acc 0.8517, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.38917, train_acc 0.8680, valid_acc 0.8554, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.38661, train_acc 0.8681, valid_acc 0.8591, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.38413, train_acc 0.8689, valid_acc 0.8555, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.37997, train_acc 0.8711, valid_acc 0.8538, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.38270, train_acc 0.8692, valid_acc 0.8484, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.38268, train_acc 0.8702, valid_acc 0.8478, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.38004, train_acc 0.8712, valid_acc 0.8528, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.37946, train_acc 0.8712, valid_acc 0.8603, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.37720, train_acc 0.8728, valid_acc 0.8533, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.37159, train_acc 0.8733, valid_acc 0.8092, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.37223, train_acc 0.8734, valid_acc 0.8453, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.37370, train_acc 0.8742, valid_acc 0.8661, Time 00:00:31,lr 0.01\n",
      "epoch 57, loss 0.37211, train_acc 0.8736, valid_acc 0.8377, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.37039, train_acc 0.8741, valid_acc 0.8310, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.37287, train_acc 0.8743, valid_acc 0.8420, Time 00:00:31,lr 0.01\n",
      "epoch 60, loss 0.23431, train_acc 0.9229, valid_acc 0.9057, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.19372, train_acc 0.9352, valid_acc 0.9090, Time 00:00:31,lr 0.002\n",
      "epoch 62, loss 0.17911, train_acc 0.9395, valid_acc 0.9086, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.17003, train_acc 0.9431, valid_acc 0.9157, Time 00:00:31,lr 0.002\n",
      "epoch 64, loss 0.16376, train_acc 0.9459, valid_acc 0.9081, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.15820, train_acc 0.9475, valid_acc 0.9153, Time 00:00:32,lr 0.002\n",
      "epoch 66, loss 0.15374, train_acc 0.9485, valid_acc 0.9127, Time 00:00:32,lr 0.002\n",
      "epoch 67, loss 0.15401, train_acc 0.9480, valid_acc 0.9136, Time 00:00:31,lr 0.002\n",
      "epoch 68, loss 0.15006, train_acc 0.9496, valid_acc 0.9137, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.15038, train_acc 0.9497, valid_acc 0.9058, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.15246, train_acc 0.9476, valid_acc 0.9106, Time 00:00:31,lr 0.002\n",
      "epoch 71, loss 0.15303, train_acc 0.9474, valid_acc 0.9042, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.15150, train_acc 0.9477, valid_acc 0.9030, Time 00:00:31,lr 0.002\n",
      "epoch 73, loss 0.15295, train_acc 0.9472, valid_acc 0.9058, Time 00:00:32,lr 0.002\n",
      "epoch 74, loss 0.15067, train_acc 0.9496, valid_acc 0.9097, Time 00:00:31,lr 0.002\n",
      "epoch 75, loss 0.15562, train_acc 0.9473, valid_acc 0.9034, Time 00:00:31,lr 0.002\n",
      "epoch 76, loss 0.15555, train_acc 0.9475, valid_acc 0.9029, Time 00:00:31,lr 0.002\n",
      "epoch 77, loss 0.15436, train_acc 0.9484, valid_acc 0.9031, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.15387, train_acc 0.9485, valid_acc 0.9032, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.15457, train_acc 0.9481, valid_acc 0.9065, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.15198, train_acc 0.9484, valid_acc 0.9091, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.15804, train_acc 0.9467, valid_acc 0.9018, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.15374, train_acc 0.9477, valid_acc 0.9049, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.15341, train_acc 0.9491, valid_acc 0.9067, Time 00:00:32,lr 0.002\n",
      "epoch 84, loss 0.15361, train_acc 0.9499, valid_acc 0.9019, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.15058, train_acc 0.9491, valid_acc 0.8981, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.15706, train_acc 0.9464, valid_acc 0.9073, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.15415, train_acc 0.9483, valid_acc 0.9021, Time 00:00:31,lr 0.002\n",
      "epoch 88, loss 0.14994, train_acc 0.9494, valid_acc 0.8976, Time 00:00:32,lr 0.002\n",
      "epoch 89, loss 0.15133, train_acc 0.9490, valid_acc 0.8961, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.08904, train_acc 0.9723, valid_acc 0.9240, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.06755, train_acc 0.9796, valid_acc 0.9256, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.05880, train_acc 0.9830, valid_acc 0.9272, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.05345, train_acc 0.9850, valid_acc 0.9282, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.05022, train_acc 0.9862, valid_acc 0.9306, Time 00:00:31,lr 0.0004\n",
      "epoch 95, loss 0.04724, train_acc 0.9864, valid_acc 0.9297, Time 00:00:31,lr 0.0004\n",
      "epoch 96, loss 0.04417, train_acc 0.9878, valid_acc 0.9303, Time 00:00:31,lr 0.0004\n",
      "epoch 97, loss 0.04230, train_acc 0.9886, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.04097, train_acc 0.9887, valid_acc 0.9272, Time 00:00:32,lr 0.0004\n",
      "epoch 99, loss 0.03953, train_acc 0.9899, valid_acc 0.9291, Time 00:00:31,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.03685, train_acc 0.9899, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.03604, train_acc 0.9903, valid_acc 0.9277, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.03277, train_acc 0.9917, valid_acc 0.9261, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.03316, train_acc 0.9913, valid_acc 0.9253, Time 00:00:32,lr 0.0004\n",
      "epoch 104, loss 0.03211, train_acc 0.9920, valid_acc 0.9288, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.03214, train_acc 0.9913, valid_acc 0.9277, Time 00:00:32,lr 0.0004\n",
      "epoch 106, loss 0.03095, train_acc 0.9922, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 107, loss 0.03007, train_acc 0.9922, valid_acc 0.9275, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.02834, train_acc 0.9926, valid_acc 0.9271, Time 00:00:31,lr 0.0004\n",
      "epoch 109, loss 0.03008, train_acc 0.9920, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.02855, train_acc 0.9928, valid_acc 0.9279, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.02935, train_acc 0.9926, valid_acc 0.9269, Time 00:00:32,lr 0.0004\n",
      "epoch 112, loss 0.02706, train_acc 0.9931, valid_acc 0.9291, Time 00:00:32,lr 0.0004\n",
      "epoch 113, loss 0.02690, train_acc 0.9933, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.02585, train_acc 0.9932, valid_acc 0.9263, Time 00:00:31,lr 0.0004\n",
      "epoch 115, loss 0.02478, train_acc 0.9938, valid_acc 0.9280, Time 00:00:31,lr 0.0004\n",
      "epoch 116, loss 0.02669, train_acc 0.9930, valid_acc 0.9264, Time 00:00:31,lr 0.0004\n",
      "epoch 117, loss 0.02483, train_acc 0.9937, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 118, loss 0.02543, train_acc 0.9936, valid_acc 0.9281, Time 00:00:31,lr 0.0004\n",
      "epoch 119, loss 0.02483, train_acc 0.9939, valid_acc 0.9300, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_me_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 use gluon renset18_v1\n",
    "train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:09.871178Z",
     "start_time": "2018-03-04T08:31:09.864612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and add must in name_scope, or may got name error\n",
    "def get_resnet18_v1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features)\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T01:41:50.461216Z",
     "start_time": "2018-03-04T01:08:08.533086Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.51019, train_acc 0.2129, valid_acc 0.3261, Time 00:00:27,lr 0.1\n",
      "epoch 1, loss 1.75673, train_acc 0.3417, valid_acc 0.4151, Time 00:00:25,lr 0.1\n",
      "epoch 2, loss 1.60616, train_acc 0.4121, valid_acc 0.4824, Time 00:00:24,lr 0.1\n",
      "epoch 3, loss 1.49376, train_acc 0.4615, valid_acc 0.4921, Time 00:00:24,lr 0.1\n",
      "epoch 4, loss 1.42743, train_acc 0.4928, valid_acc 0.4964, Time 00:00:25,lr 0.1\n",
      "epoch 5, loss 1.37983, train_acc 0.5144, valid_acc 0.5435, Time 00:00:24,lr 0.1\n",
      "epoch 6, loss 1.36243, train_acc 0.5234, valid_acc 0.5876, Time 00:00:25,lr 0.1\n",
      "epoch 7, loss 1.33671, train_acc 0.5313, valid_acc 0.5255, Time 00:00:25,lr 0.1\n",
      "epoch 8, loss 1.31173, train_acc 0.5461, valid_acc 0.5368, Time 00:00:24,lr 0.1\n",
      "epoch 9, loss 1.27490, train_acc 0.5587, valid_acc 0.5911, Time 00:00:24,lr 0.1\n",
      "epoch 10, loss 1.27032, train_acc 0.5597, valid_acc 0.6087, Time 00:00:28,lr 0.1\n",
      "epoch 11, loss 1.25436, train_acc 0.5670, valid_acc 0.5886, Time 00:00:24,lr 0.1\n",
      "epoch 12, loss 1.24745, train_acc 0.5690, valid_acc 0.5414, Time 00:00:27,lr 0.1\n",
      "epoch 13, loss 1.23173, train_acc 0.5756, valid_acc 0.5944, Time 00:00:25,lr 0.1\n",
      "epoch 14, loss 1.22901, train_acc 0.5782, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 15, loss 1.22526, train_acc 0.5812, valid_acc 0.5576, Time 00:00:24,lr 0.1\n",
      "epoch 16, loss 1.22470, train_acc 0.5828, valid_acc 0.5957, Time 00:00:24,lr 0.1\n",
      "epoch 17, loss 1.21181, train_acc 0.5865, valid_acc 0.6237, Time 00:00:25,lr 0.1\n",
      "epoch 18, loss 1.21252, train_acc 0.5851, valid_acc 0.5971, Time 00:00:25,lr 0.1\n",
      "epoch 19, loss 1.20350, train_acc 0.5899, valid_acc 0.6076, Time 00:00:25,lr 0.1\n",
      "epoch 20, loss 1.20077, train_acc 0.5904, valid_acc 0.6148, Time 00:00:25,lr 0.1\n",
      "epoch 21, loss 1.19665, train_acc 0.5925, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 22, loss 1.19718, train_acc 0.5942, valid_acc 0.5745, Time 00:00:25,lr 0.1\n",
      "epoch 23, loss 1.19424, train_acc 0.5912, valid_acc 0.5711, Time 00:00:25,lr 0.1\n",
      "epoch 24, loss 1.19114, train_acc 0.5953, valid_acc 0.6069, Time 00:00:25,lr 0.1\n",
      "epoch 25, loss 1.19080, train_acc 0.5965, valid_acc 0.6262, Time 00:00:25,lr 0.1\n",
      "epoch 26, loss 1.19219, train_acc 0.5964, valid_acc 0.6112, Time 00:00:25,lr 0.1\n",
      "epoch 27, loss 1.18217, train_acc 0.5991, valid_acc 0.5783, Time 00:00:25,lr 0.1\n",
      "epoch 28, loss 1.19244, train_acc 0.5922, valid_acc 0.6090, Time 00:00:25,lr 0.1\n",
      "epoch 29, loss 1.18442, train_acc 0.5964, valid_acc 0.6002, Time 00:00:27,lr 0.1\n",
      "epoch 30, loss 1.18418, train_acc 0.5984, valid_acc 0.5866, Time 00:00:25,lr 0.1\n",
      "epoch 31, loss 1.18398, train_acc 0.5969, valid_acc 0.5779, Time 00:00:25,lr 0.1\n",
      "epoch 32, loss 1.18966, train_acc 0.5938, valid_acc 0.6059, Time 00:00:25,lr 0.1\n",
      "epoch 33, loss 1.19080, train_acc 0.5951, valid_acc 0.6048, Time 00:00:25,lr 0.1\n",
      "epoch 34, loss 1.18026, train_acc 0.5991, valid_acc 0.6034, Time 00:00:25,lr 0.1\n",
      "epoch 35, loss 1.18109, train_acc 0.5999, valid_acc 0.6194, Time 00:00:25,lr 0.1\n",
      "epoch 36, loss 1.18156, train_acc 0.6008, valid_acc 0.5972, Time 00:00:25,lr 0.1\n",
      "epoch 37, loss 1.19249, train_acc 0.5947, valid_acc 0.5651, Time 00:00:25,lr 0.1\n",
      "epoch 38, loss 1.18308, train_acc 0.5992, valid_acc 0.6097, Time 00:00:25,lr 0.1\n",
      "epoch 39, loss 1.18060, train_acc 0.6001, valid_acc 0.6363, Time 00:00:25,lr 0.1\n",
      "epoch 40, loss 1.17950, train_acc 0.6004, valid_acc 0.6394, Time 00:00:25,lr 0.1\n",
      "epoch 41, loss 1.18485, train_acc 0.5964, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 42, loss 1.18305, train_acc 0.5999, valid_acc 0.6338, Time 00:00:25,lr 0.1\n",
      "epoch 43, loss 1.18409, train_acc 0.5985, valid_acc 0.6278, Time 00:00:24,lr 0.1\n",
      "epoch 44, loss 1.17604, train_acc 0.5990, valid_acc 0.6171, Time 00:00:25,lr 0.1\n",
      "epoch 45, loss 1.18259, train_acc 0.5979, valid_acc 0.6266, Time 00:00:25,lr 0.1\n",
      "epoch 46, loss 1.17980, train_acc 0.6023, valid_acc 0.5782, Time 00:00:25,lr 0.1\n",
      "epoch 47, loss 1.18290, train_acc 0.5980, valid_acc 0.6226, Time 00:00:25,lr 0.1\n",
      "epoch 48, loss 1.17870, train_acc 0.5994, valid_acc 0.5915, Time 00:00:25,lr 0.1\n",
      "epoch 49, loss 1.17969, train_acc 0.5983, valid_acc 0.5877, Time 00:00:24,lr 0.1\n",
      "epoch 50, loss 1.18743, train_acc 0.5986, valid_acc 0.6399, Time 00:00:25,lr 0.1\n",
      "epoch 51, loss 1.17115, train_acc 0.6031, valid_acc 0.5693, Time 00:00:25,lr 0.1\n",
      "epoch 52, loss 1.16321, train_acc 0.6042, valid_acc 0.6509, Time 00:00:24,lr 0.1\n",
      "epoch 53, loss 1.17745, train_acc 0.6004, valid_acc 0.6119, Time 00:00:25,lr 0.1\n",
      "epoch 54, loss 1.18013, train_acc 0.5981, valid_acc 0.5628, Time 00:00:25,lr 0.1\n",
      "epoch 55, loss 1.17807, train_acc 0.5997, valid_acc 0.5863, Time 00:00:26,lr 0.1\n",
      "epoch 56, loss 1.18230, train_acc 0.6013, valid_acc 0.6077, Time 00:00:26,lr 0.1\n",
      "epoch 57, loss 1.17381, train_acc 0.6016, valid_acc 0.6339, Time 00:00:24,lr 0.1\n",
      "epoch 58, loss 1.17931, train_acc 0.5979, valid_acc 0.6215, Time 00:00:24,lr 0.1\n",
      "epoch 59, loss 1.17566, train_acc 0.6000, valid_acc 0.5958, Time 00:00:24,lr 0.1\n",
      "epoch 60, loss 1.16982, train_acc 0.6028, valid_acc 0.6310, Time 00:00:25,lr 0.1\n",
      "epoch 61, loss 1.17248, train_acc 0.6016, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 62, loss 1.17132, train_acc 0.6028, valid_acc 0.6392, Time 00:00:24,lr 0.1\n",
      "epoch 63, loss 1.16930, train_acc 0.6059, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 64, loss 1.17535, train_acc 0.6018, valid_acc 0.6257, Time 00:00:24,lr 0.1\n",
      "epoch 65, loss 1.16613, train_acc 0.6031, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 66, loss 1.17103, train_acc 0.6037, valid_acc 0.5942, Time 00:00:24,lr 0.1\n",
      "epoch 67, loss 1.17368, train_acc 0.6013, valid_acc 0.6163, Time 00:00:25,lr 0.1\n",
      "epoch 68, loss 1.17817, train_acc 0.5984, valid_acc 0.5712, Time 00:00:27,lr 0.1\n",
      "epoch 69, loss 1.18750, train_acc 0.5986, valid_acc 0.6246, Time 00:00:24,lr 0.1\n",
      "epoch 70, loss 1.17631, train_acc 0.6010, valid_acc 0.6244, Time 00:00:24,lr 0.1\n",
      "epoch 71, loss 1.18138, train_acc 0.5989, valid_acc 0.5719, Time 00:00:24,lr 0.1\n",
      "epoch 72, loss 1.17125, train_acc 0.6019, valid_acc 0.6373, Time 00:00:24,lr 0.1\n",
      "epoch 73, loss 1.17880, train_acc 0.6007, valid_acc 0.6064, Time 00:00:24,lr 0.1\n",
      "epoch 74, loss 1.17069, train_acc 0.6043, valid_acc 0.6049, Time 00:00:24,lr 0.1\n",
      "epoch 75, loss 1.17566, train_acc 0.6009, valid_acc 0.5390, Time 00:00:24,lr 0.1\n",
      "epoch 76, loss 1.17658, train_acc 0.5984, valid_acc 0.6027, Time 00:00:24,lr 0.1\n",
      "epoch 77, loss 1.17818, train_acc 0.5994, valid_acc 0.6432, Time 00:00:24,lr 0.1\n",
      "epoch 78, loss 1.18056, train_acc 0.6011, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 79, loss 1.18418, train_acc 0.5995, valid_acc 0.6297, Time 00:00:24,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T02:39:40.682812Z",
     "start_time": "2018-03-04T01:49:21.279182Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.03273, train_acc 0.6475, valid_acc 0.6599, Time 00:00:24,lr 0.05\n",
      "epoch 1, loss 1.10147, train_acc 0.6258, valid_acc 0.6101, Time 00:00:24,lr 0.05\n",
      "epoch 2, loss 1.10971, train_acc 0.6221, valid_acc 0.6303, Time 00:00:24,lr 0.05\n",
      "epoch 3, loss 1.11335, train_acc 0.6201, valid_acc 0.6158, Time 00:00:24,lr 0.05\n",
      "epoch 4, loss 1.11669, train_acc 0.6186, valid_acc 0.6162, Time 00:00:24,lr 0.05\n",
      "epoch 5, loss 1.10626, train_acc 0.6214, valid_acc 0.6486, Time 00:00:24,lr 0.05\n",
      "epoch 6, loss 1.11353, train_acc 0.6192, valid_acc 0.6361, Time 00:00:24,lr 0.05\n",
      "epoch 7, loss 1.10059, train_acc 0.6249, valid_acc 0.6364, Time 00:00:24,lr 0.05\n",
      "epoch 8, loss 1.11230, train_acc 0.6210, valid_acc 0.6317, Time 00:00:24,lr 0.05\n",
      "epoch 9, loss 1.10704, train_acc 0.6200, valid_acc 0.5400, Time 00:00:24,lr 0.05\n",
      "epoch 10, loss 1.10685, train_acc 0.6223, valid_acc 0.6251, Time 00:00:24,lr 0.05\n",
      "epoch 11, loss 1.11229, train_acc 0.6209, valid_acc 0.6055, Time 00:00:24,lr 0.05\n",
      "epoch 12, loss 1.11007, train_acc 0.6232, valid_acc 0.6257, Time 00:00:25,lr 0.05\n",
      "epoch 13, loss 1.10097, train_acc 0.6244, valid_acc 0.6220, Time 00:00:25,lr 0.05\n",
      "epoch 14, loss 1.10608, train_acc 0.6263, valid_acc 0.5887, Time 00:00:24,lr 0.05\n",
      "epoch 15, loss 1.10504, train_acc 0.6224, valid_acc 0.6416, Time 00:00:24,lr 0.05\n",
      "epoch 16, loss 1.10434, train_acc 0.6223, valid_acc 0.6064, Time 00:00:24,lr 0.05\n",
      "epoch 17, loss 1.10511, train_acc 0.6221, valid_acc 0.6365, Time 00:00:24,lr 0.05\n",
      "epoch 18, loss 1.10933, train_acc 0.6206, valid_acc 0.6311, Time 00:00:24,lr 0.05\n",
      "epoch 19, loss 1.10533, train_acc 0.6211, valid_acc 0.6559, Time 00:00:25,lr 0.05\n",
      "epoch 20, loss 1.10336, train_acc 0.6231, valid_acc 0.6529, Time 00:00:24,lr 0.05\n",
      "epoch 21, loss 1.11074, train_acc 0.6202, valid_acc 0.6060, Time 00:00:25,lr 0.05\n",
      "epoch 22, loss 1.10799, train_acc 0.6211, valid_acc 0.6211, Time 00:00:24,lr 0.05\n",
      "epoch 23, loss 1.11143, train_acc 0.6213, valid_acc 0.6449, Time 00:00:24,lr 0.05\n",
      "epoch 24, loss 1.10708, train_acc 0.6227, valid_acc 0.6420, Time 00:00:24,lr 0.05\n",
      "epoch 25, loss 1.11583, train_acc 0.6182, valid_acc 0.6106, Time 00:00:24,lr 0.05\n",
      "epoch 26, loss 1.10713, train_acc 0.6237, valid_acc 0.6260, Time 00:00:25,lr 0.05\n",
      "epoch 27, loss 1.10516, train_acc 0.6226, valid_acc 0.6302, Time 00:00:24,lr 0.05\n",
      "epoch 28, loss 1.10940, train_acc 0.6245, valid_acc 0.6110, Time 00:00:24,lr 0.05\n",
      "epoch 29, loss 1.10750, train_acc 0.6217, valid_acc 0.5529, Time 00:00:24,lr 0.05\n",
      "epoch 30, loss 0.82673, train_acc 0.7164, valid_acc 0.7511, Time 00:00:25,lr 0.01\n",
      "epoch 31, loss 0.76458, train_acc 0.7374, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 32, loss 0.75552, train_acc 0.7419, valid_acc 0.7292, Time 00:00:25,lr 0.01\n",
      "epoch 33, loss 0.75799, train_acc 0.7418, valid_acc 0.7546, Time 00:00:25,lr 0.01\n",
      "epoch 34, loss 0.75636, train_acc 0.7434, valid_acc 0.7697, Time 00:00:24,lr 0.01\n",
      "epoch 35, loss 0.74936, train_acc 0.7466, valid_acc 0.7569, Time 00:00:24,lr 0.01\n",
      "epoch 36, loss 0.74819, train_acc 0.7459, valid_acc 0.7532, Time 00:00:25,lr 0.01\n",
      "epoch 37, loss 0.74411, train_acc 0.7466, valid_acc 0.7663, Time 00:00:24,lr 0.01\n",
      "epoch 38, loss 0.74887, train_acc 0.7453, valid_acc 0.7722, Time 00:00:24,lr 0.01\n",
      "epoch 39, loss 0.73962, train_acc 0.7479, valid_acc 0.7629, Time 00:00:24,lr 0.01\n",
      "epoch 40, loss 0.73451, train_acc 0.7482, valid_acc 0.7582, Time 00:00:24,lr 0.01\n",
      "epoch 41, loss 0.72975, train_acc 0.7514, valid_acc 0.7482, Time 00:00:24,lr 0.01\n",
      "epoch 42, loss 0.72403, train_acc 0.7536, valid_acc 0.7622, Time 00:00:24,lr 0.01\n",
      "epoch 43, loss 0.72834, train_acc 0.7540, valid_acc 0.7566, Time 00:00:24,lr 0.01\n",
      "epoch 44, loss 0.72512, train_acc 0.7522, valid_acc 0.7666, Time 00:00:24,lr 0.01\n",
      "epoch 45, loss 0.71810, train_acc 0.7576, valid_acc 0.7462, Time 00:00:25,lr 0.01\n",
      "epoch 46, loss 0.72037, train_acc 0.7547, valid_acc 0.7660, Time 00:00:25,lr 0.01\n",
      "epoch 47, loss 0.71576, train_acc 0.7564, valid_acc 0.7687, Time 00:00:24,lr 0.01\n",
      "epoch 48, loss 0.71955, train_acc 0.7534, valid_acc 0.7430, Time 00:00:24,lr 0.01\n",
      "epoch 49, loss 0.71184, train_acc 0.7587, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 50, loss 0.71714, train_acc 0.7561, valid_acc 0.7544, Time 00:00:24,lr 0.01\n",
      "epoch 51, loss 0.71158, train_acc 0.7576, valid_acc 0.7566, Time 00:00:25,lr 0.01\n",
      "epoch 52, loss 0.70936, train_acc 0.7582, valid_acc 0.7613, Time 00:00:24,lr 0.01\n",
      "epoch 53, loss 0.70673, train_acc 0.7591, valid_acc 0.7554, Time 00:00:24,lr 0.01\n",
      "epoch 54, loss 0.70670, train_acc 0.7597, valid_acc 0.7607, Time 00:00:24,lr 0.01\n",
      "epoch 55, loss 0.70951, train_acc 0.7566, valid_acc 0.7609, Time 00:00:24,lr 0.01\n",
      "epoch 56, loss 0.70382, train_acc 0.7595, valid_acc 0.7573, Time 00:00:24,lr 0.01\n",
      "epoch 57, loss 0.70531, train_acc 0.7601, valid_acc 0.7356, Time 00:00:24,lr 0.01\n",
      "epoch 58, loss 0.70683, train_acc 0.7595, valid_acc 0.7572, Time 00:00:25,lr 0.01\n",
      "epoch 59, loss 0.70069, train_acc 0.7610, valid_acc 0.7556, Time 00:00:24,lr 0.01\n",
      "epoch 60, loss 0.54787, train_acc 0.8137, valid_acc 0.8178, Time 00:00:24,lr 0.002\n",
      "epoch 61, loss 0.49978, train_acc 0.8285, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 62, loss 0.48376, train_acc 0.8345, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 63, loss 0.47104, train_acc 0.8396, valid_acc 0.8270, Time 00:00:24,lr 0.002\n",
      "epoch 64, loss 0.46549, train_acc 0.8415, valid_acc 0.8376, Time 00:00:25,lr 0.002\n",
      "epoch 65, loss 0.45923, train_acc 0.8430, valid_acc 0.8264, Time 00:00:25,lr 0.002\n",
      "epoch 66, loss 0.45508, train_acc 0.8428, valid_acc 0.8332, Time 00:00:24,lr 0.002\n",
      "epoch 67, loss 0.45490, train_acc 0.8441, valid_acc 0.8351, Time 00:00:24,lr 0.002\n",
      "epoch 68, loss 0.45410, train_acc 0.8435, valid_acc 0.8346, Time 00:00:24,lr 0.002\n",
      "epoch 69, loss 0.44802, train_acc 0.8458, valid_acc 0.8378, Time 00:00:24,lr 0.002\n",
      "epoch 70, loss 0.44962, train_acc 0.8464, valid_acc 0.8339, Time 00:00:24,lr 0.002\n",
      "epoch 71, loss 0.45422, train_acc 0.8447, valid_acc 0.8261, Time 00:00:24,lr 0.002\n",
      "epoch 72, loss 0.44599, train_acc 0.8474, valid_acc 0.8287, Time 00:00:25,lr 0.002\n",
      "epoch 73, loss 0.44833, train_acc 0.8458, valid_acc 0.8317, Time 00:00:24,lr 0.002\n",
      "epoch 74, loss 0.44645, train_acc 0.8466, valid_acc 0.8322, Time 00:00:24,lr 0.002\n",
      "epoch 75, loss 0.45153, train_acc 0.8453, valid_acc 0.8240, Time 00:00:24,lr 0.002\n",
      "epoch 76, loss 0.44996, train_acc 0.8456, valid_acc 0.8233, Time 00:00:24,lr 0.002\n",
      "epoch 77, loss 0.44440, train_acc 0.8473, valid_acc 0.8235, Time 00:00:24,lr 0.002\n",
      "epoch 78, loss 0.44680, train_acc 0.8452, valid_acc 0.8229, Time 00:00:24,lr 0.002\n",
      "epoch 79, loss 0.44781, train_acc 0.8466, valid_acc 0.8210, Time 00:00:24,lr 0.002\n",
      "epoch 80, loss 0.44728, train_acc 0.8452, valid_acc 0.8299, Time 00:00:24,lr 0.002\n",
      "epoch 81, loss 0.44830, train_acc 0.8462, valid_acc 0.8278, Time 00:00:24,lr 0.002\n",
      "epoch 82, loss 0.44525, train_acc 0.8456, valid_acc 0.8325, Time 00:00:24,lr 0.002\n",
      "epoch 83, loss 0.44259, train_acc 0.8477, valid_acc 0.8257, Time 00:00:24,lr 0.002\n",
      "epoch 84, loss 0.44439, train_acc 0.8464, valid_acc 0.8292, Time 00:00:24,lr 0.002\n",
      "epoch 85, loss 0.44217, train_acc 0.8476, valid_acc 0.8280, Time 00:00:24,lr 0.002\n",
      "epoch 86, loss 0.44183, train_acc 0.8487, valid_acc 0.8294, Time 00:00:24,lr 0.002\n",
      "epoch 87, loss 0.43943, train_acc 0.8472, valid_acc 0.8248, Time 00:00:24,lr 0.002\n",
      "epoch 88, loss 0.43877, train_acc 0.8485, valid_acc 0.8164, Time 00:00:26,lr 0.002\n",
      "epoch 89, loss 0.43657, train_acc 0.8498, valid_acc 0.8276, Time 00:00:25,lr 0.002\n",
      "epoch 90, loss 0.35472, train_acc 0.8789, valid_acc 0.8585, Time 00:00:25,lr 0.0004\n",
      "epoch 91, loss 0.32633, train_acc 0.8876, valid_acc 0.8528, Time 00:00:25,lr 0.0004\n",
      "epoch 92, loss 0.31385, train_acc 0.8927, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 93, loss 0.30717, train_acc 0.8945, valid_acc 0.8557, Time 00:00:25,lr 0.0004\n",
      "epoch 94, loss 0.30039, train_acc 0.8969, valid_acc 0.8598, Time 00:00:27,lr 0.0004\n",
      "epoch 95, loss 0.29545, train_acc 0.8995, valid_acc 0.8572, Time 00:00:25,lr 0.0004\n",
      "epoch 96, loss 0.29173, train_acc 0.8990, valid_acc 0.8566, Time 00:00:26,lr 0.0004\n",
      "epoch 97, loss 0.28834, train_acc 0.8994, valid_acc 0.8571, Time 00:00:28,lr 0.0004\n",
      "epoch 98, loss 0.28529, train_acc 0.9006, valid_acc 0.8560, Time 00:00:27,lr 0.0004\n",
      "epoch 99, loss 0.28147, train_acc 0.9024, valid_acc 0.8564, Time 00:00:25,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.27405, train_acc 0.9057, valid_acc 0.8599, Time 00:00:26,lr 0.0004\n",
      "epoch 101, loss 0.27207, train_acc 0.9066, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 102, loss 0.27203, train_acc 0.9067, valid_acc 0.8584, Time 00:00:25,lr 0.0004\n",
      "epoch 103, loss 0.26894, train_acc 0.9070, valid_acc 0.8580, Time 00:00:27,lr 0.0004\n",
      "epoch 104, loss 0.26925, train_acc 0.9077, valid_acc 0.8566, Time 00:00:25,lr 0.0004\n",
      "epoch 105, loss 0.26636, train_acc 0.9088, valid_acc 0.8558, Time 00:00:24,lr 0.0004\n",
      "epoch 106, loss 0.26520, train_acc 0.9085, valid_acc 0.8559, Time 00:00:26,lr 0.0004\n",
      "epoch 107, loss 0.26192, train_acc 0.9094, valid_acc 0.8562, Time 00:00:27,lr 0.0004\n",
      "epoch 108, loss 0.26339, train_acc 0.9091, valid_acc 0.8547, Time 00:00:25,lr 0.0004\n",
      "epoch 109, loss 0.26113, train_acc 0.9096, valid_acc 0.8568, Time 00:00:24,lr 0.0004\n",
      "epoch 110, loss 0.25569, train_acc 0.9102, valid_acc 0.8575, Time 00:00:24,lr 0.0004\n",
      "epoch 111, loss 0.25539, train_acc 0.9117, valid_acc 0.8565, Time 00:00:25,lr 0.0004\n",
      "epoch 112, loss 0.25576, train_acc 0.9120, valid_acc 0.8555, Time 00:00:24,lr 0.0004\n",
      "epoch 113, loss 0.25426, train_acc 0.9129, valid_acc 0.8522, Time 00:00:24,lr 0.0004\n",
      "epoch 114, loss 0.25399, train_acc 0.9125, valid_acc 0.8544, Time 00:00:24,lr 0.0004\n",
      "epoch 115, loss 0.25090, train_acc 0.9127, valid_acc 0.8521, Time 00:00:24,lr 0.0004\n",
      "epoch 116, loss 0.24849, train_acc 0.9130, valid_acc 0.8567, Time 00:00:24,lr 0.0004\n",
      "epoch 117, loss 0.25278, train_acc 0.9111, valid_acc 0.8523, Time 00:00:24,lr 0.0004\n",
      "epoch 118, loss 0.25027, train_acc 0.9130, valid_acc 0.8512, Time 00:00:24,lr 0.0004\n",
      "epoch 119, loss 0.24702, train_acc 0.9142, valid_acc 0.8548, Time 00:00:24,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 gluon resnet18 vs my resnet18\n",
    "```\n",
    "there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:40:01.865661Z",
     "start_time": "2018-03-04T08:40:01.754397Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (net): HybridSequential(\n",
      "    (0): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "    (2): Activation(relu)\n",
      "    (3): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (6): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (7): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (8): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (9): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (10): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (11): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (12): AvgPool2D(size=(8, 8), stride=(8, 8), padding=(0, 0), ceil_mode=False)\n",
      "    (13): Flatten\n",
      "    (14): Dense(None -> 10, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net1 = ResNet(10)\n",
    "'''\n",
    "block1: conv(32, 3, 1, 1) + BN + relu\n",
    "block2: Residual_block(32) * 3\n",
    "block3: Resisual_block(64) * 3\n",
    "block4: Residual_block(128) * 3\n",
    "block5: AvgPool(8) + Flatten + Dense(10)\n",
    "\n",
    "actually it has 19 conv + 1 fc, some code name it as resnet20 (each Residual_block has 2 conv or 2 + 1 conv(two line))\n",
    "for cifar10 dataset, most code use this arch (https://github.com/yinglang/pytorch-cifar-models).\n",
    "'''\n",
    "net2 = get_resnet18_v1()\n",
    "'''\n",
    "block1: conv(64, 7, 3, 2) + BN + relu + MaxPool(3, 1, 2)\n",
    "block2: Residual_block(64) * 2\n",
    "block3: Resisual_block(128) * 2\n",
    "block4: Residual_block(256) * 2\n",
    "block5: Residual_block(256) * 2\n",
    "block6: GlobalAvgPool() + Dense(10) \n",
    "\n",
    "has 1 + (2 + 2 + 2 + 2) * 2 = 15 conv + 1 fc, name it as resnet18\n",
    "'''\n",
    "print net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:21.510639Z",
     "start_time": "2018-03-04T08:31:21.405189Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): HybridSequential(\n",
      "    (0): Conv2D(3 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n",
      "    (4): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  )\n",
      "  (1): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 gluon resnet18 vs my resnet 18: first conv is conv(k=7,p=3,s=2) vs conv(k=3,p=1,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T03:26:13.432236Z",
     "start_time": "2018-03-04T03:26:13.422231Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T04:08:38.921689Z",
     "start_time": "2018-03-04T03:26:16.924298Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.20239, train_acc 0.2669, valid_acc 0.3902, Time 00:00:32,lr 0.1\n",
      "epoch 1, loss 1.65517, train_acc 0.3912, valid_acc 0.4528, Time 00:00:33,lr 0.1\n",
      "epoch 2, loss 1.47576, train_acc 0.4642, valid_acc 0.4337, Time 00:00:33,lr 0.1\n",
      "epoch 3, loss 1.34809, train_acc 0.5176, valid_acc 0.5469, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 1.25687, train_acc 0.5574, valid_acc 0.5480, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 1.20031, train_acc 0.5783, valid_acc 0.5777, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 1.15576, train_acc 0.5958, valid_acc 0.6329, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 1.11209, train_acc 0.6100, valid_acc 0.6637, Time 00:00:30,lr 0.1\n",
      "epoch 8, loss 1.06140, train_acc 0.6298, valid_acc 0.5927, Time 00:00:30,lr 0.1\n",
      "epoch 9, loss 1.02432, train_acc 0.6445, valid_acc 0.6795, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 1.00051, train_acc 0.6551, valid_acc 0.6433, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 0.98188, train_acc 0.6637, valid_acc 0.6974, Time 00:00:30,lr 0.1\n",
      "epoch 12, loss 0.96989, train_acc 0.6664, valid_acc 0.6623, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.95365, train_acc 0.6740, valid_acc 0.6860, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.95321, train_acc 0.6733, valid_acc 0.6337, Time 00:00:30,lr 0.1\n",
      "epoch 15, loss 0.94458, train_acc 0.6777, valid_acc 0.6930, Time 00:00:30,lr 0.1\n",
      "epoch 16, loss 0.93070, train_acc 0.6824, valid_acc 0.7069, Time 00:00:30,lr 0.1\n",
      "epoch 17, loss 0.93221, train_acc 0.6827, valid_acc 0.6382, Time 00:00:30,lr 0.1\n",
      "epoch 18, loss 0.92776, train_acc 0.6840, valid_acc 0.7080, Time 00:00:30,lr 0.1\n",
      "epoch 19, loss 0.92143, train_acc 0.6869, valid_acc 0.6862, Time 00:00:30,lr 0.1\n",
      "epoch 20, loss 0.92236, train_acc 0.6877, valid_acc 0.6391, Time 00:00:30,lr 0.1\n",
      "epoch 21, loss 0.92049, train_acc 0.6863, valid_acc 0.6934, Time 00:00:30,lr 0.1\n",
      "epoch 22, loss 0.91650, train_acc 0.6887, valid_acc 0.6944, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.91073, train_acc 0.6888, valid_acc 0.6668, Time 00:00:30,lr 0.1\n",
      "epoch 24, loss 0.91163, train_acc 0.6908, valid_acc 0.6598, Time 00:00:30,lr 0.1\n",
      "epoch 25, loss 0.90582, train_acc 0.6902, valid_acc 0.6914, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 0.90684, train_acc 0.6901, valid_acc 0.6054, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 0.90768, train_acc 0.6901, valid_acc 0.6733, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.90371, train_acc 0.6923, valid_acc 0.6340, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 0.90607, train_acc 0.6906, valid_acc 0.6989, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 0.90220, train_acc 0.6922, valid_acc 0.5081, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.90443, train_acc 0.6917, valid_acc 0.6913, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 0.89974, train_acc 0.6936, valid_acc 0.6244, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 0.89704, train_acc 0.6960, valid_acc 0.7255, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 0.90115, train_acc 0.6939, valid_acc 0.6980, Time 00:00:32,lr 0.1\n",
      "epoch 35, loss 0.89469, train_acc 0.6961, valid_acc 0.6947, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 0.89557, train_acc 0.6974, valid_acc 0.6494, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 0.89949, train_acc 0.6951, valid_acc 0.7107, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 0.89622, train_acc 0.6963, valid_acc 0.5968, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 0.89347, train_acc 0.6955, valid_acc 0.6612, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 0.89192, train_acc 0.6984, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 0.89004, train_acc 0.6967, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 0.88865, train_acc 0.6992, valid_acc 0.6024, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 0.90129, train_acc 0.6922, valid_acc 0.6781, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 0.89063, train_acc 0.6980, valid_acc 0.6661, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 0.89031, train_acc 0.6983, valid_acc 0.6445, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 0.88648, train_acc 0.6996, valid_acc 0.7091, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.88954, train_acc 0.6960, valid_acc 0.7163, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 0.89009, train_acc 0.6982, valid_acc 0.7027, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 0.88359, train_acc 0.7001, valid_acc 0.6971, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 0.89414, train_acc 0.6958, valid_acc 0.6732, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 0.88717, train_acc 0.6997, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 0.89172, train_acc 0.6956, valid_acc 0.6487, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.89213, train_acc 0.6993, valid_acc 0.6922, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 0.88663, train_acc 0.6969, valid_acc 0.7236, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 0.88679, train_acc 0.6988, valid_acc 0.6824, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 0.88335, train_acc 0.6991, valid_acc 0.6743, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 0.88368, train_acc 0.7007, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 0.88596, train_acc 0.6983, valid_acc 0.7060, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 0.88443, train_acc 0.6993, valid_acc 0.6722, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 0.88799, train_acc 0.6990, valid_acc 0.6508, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 0.88132, train_acc 0.7016, valid_acc 0.6703, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 0.87931, train_acc 0.7012, valid_acc 0.6996, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 0.88957, train_acc 0.6987, valid_acc 0.7010, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 0.88078, train_acc 0.7020, valid_acc 0.6454, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.88306, train_acc 0.7007, valid_acc 0.6412, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 0.88245, train_acc 0.7001, valid_acc 0.6534, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 0.87989, train_acc 0.7020, valid_acc 0.6466, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 0.89072, train_acc 0.6977, valid_acc 0.7111, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 0.88862, train_acc 0.6984, valid_acc 0.6285, Time 00:00:35,lr 0.1\n",
      "epoch 70, loss 0.87356, train_acc 0.7045, valid_acc 0.5976, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.88543, train_acc 0.6981, valid_acc 0.7188, Time 00:00:38,lr 0.1\n",
      "epoch 72, loss 0.88926, train_acc 0.6963, valid_acc 0.7050, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 0.87890, train_acc 0.7006, valid_acc 0.6988, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 0.88213, train_acc 0.6988, valid_acc 0.6710, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 0.89143, train_acc 0.6980, valid_acc 0.6485, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 0.88201, train_acc 0.6990, valid_acc 0.6410, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.87847, train_acc 0.7010, valid_acc 0.6491, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.88841, train_acc 0.6974, valid_acc 0.6903, Time 00:00:35,lr 0.1\n",
      "epoch 79, loss 0.88179, train_acc 0.7004, valid_acc 0.6880, Time 00:00:34,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T05:14:50.043433Z",
     "start_time": "2018-03-04T04:10:53.460734Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.77403, train_acc 0.7361, valid_acc 0.6271, Time 00:00:28,lr 0.05\n",
      "epoch 1, loss 0.84400, train_acc 0.7124, valid_acc 0.6732, Time 00:00:30,lr 0.05\n",
      "epoch 2, loss 0.86126, train_acc 0.7060, valid_acc 0.7009, Time 00:00:30,lr 0.05\n",
      "epoch 3, loss 0.86573, train_acc 0.7063, valid_acc 0.6787, Time 00:00:30,lr 0.05\n",
      "epoch 4, loss 0.85972, train_acc 0.7096, valid_acc 0.6623, Time 00:00:30,lr 0.05\n",
      "epoch 5, loss 0.86412, train_acc 0.7051, valid_acc 0.6821, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.86789, train_acc 0.7018, valid_acc 0.6822, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.86329, train_acc 0.7054, valid_acc 0.6411, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.87379, train_acc 0.7028, valid_acc 0.7026, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.86685, train_acc 0.7050, valid_acc 0.6352, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.86982, train_acc 0.7020, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.86871, train_acc 0.7042, valid_acc 0.5869, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.86923, train_acc 0.7037, valid_acc 0.6458, Time 00:00:31,lr 0.05\n",
      "epoch 13, loss 0.86655, train_acc 0.7048, valid_acc 0.6600, Time 00:00:31,lr 0.05\n",
      "epoch 14, loss 0.86976, train_acc 0.7038, valid_acc 0.7020, Time 00:00:31,lr 0.05\n",
      "epoch 15, loss 0.85832, train_acc 0.7083, valid_acc 0.6998, Time 00:00:31,lr 0.05\n",
      "epoch 16, loss 0.86706, train_acc 0.7046, valid_acc 0.6969, Time 00:00:31,lr 0.05\n",
      "epoch 17, loss 0.86458, train_acc 0.7044, valid_acc 0.6914, Time 00:00:31,lr 0.05\n",
      "epoch 18, loss 0.86462, train_acc 0.7072, valid_acc 0.7017, Time 00:00:31,lr 0.05\n",
      "epoch 19, loss 0.86589, train_acc 0.7067, valid_acc 0.7007, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.87160, train_acc 0.7042, valid_acc 0.7153, Time 00:00:31,lr 0.05\n",
      "epoch 21, loss 0.86976, train_acc 0.7027, valid_acc 0.6829, Time 00:00:31,lr 0.05\n",
      "epoch 22, loss 0.85921, train_acc 0.7075, valid_acc 0.7036, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.86768, train_acc 0.7058, valid_acc 0.7057, Time 00:00:31,lr 0.05\n",
      "epoch 24, loss 0.86215, train_acc 0.7067, valid_acc 0.7118, Time 00:00:31,lr 0.05\n",
      "epoch 25, loss 0.86632, train_acc 0.7029, valid_acc 0.7084, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.87295, train_acc 0.7031, valid_acc 0.6990, Time 00:00:31,lr 0.05\n",
      "epoch 27, loss 0.87456, train_acc 0.7033, valid_acc 0.6865, Time 00:00:31,lr 0.05\n",
      "epoch 28, loss 0.86871, train_acc 0.7044, valid_acc 0.6970, Time 00:00:31,lr 0.05\n",
      "epoch 29, loss 0.86998, train_acc 0.7056, valid_acc 0.6560, Time 00:00:31,lr 0.05\n",
      "epoch 30, loss 0.59346, train_acc 0.7979, valid_acc 0.8265, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.54326, train_acc 0.8155, valid_acc 0.8227, Time 00:00:31,lr 0.01\n",
      "epoch 32, loss 0.53949, train_acc 0.8170, valid_acc 0.8028, Time 00:00:31,lr 0.01\n",
      "epoch 33, loss 0.54204, train_acc 0.8157, valid_acc 0.8154, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.53898, train_acc 0.8156, valid_acc 0.8274, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.54012, train_acc 0.8164, valid_acc 0.8242, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.54208, train_acc 0.8150, valid_acc 0.8189, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.53263, train_acc 0.8171, valid_acc 0.8306, Time 00:00:35,lr 0.01\n",
      "epoch 38, loss 0.53156, train_acc 0.8190, valid_acc 0.8076, Time 00:00:33,lr 0.01\n",
      "epoch 39, loss 0.52680, train_acc 0.8204, valid_acc 0.8144, Time 00:00:33,lr 0.01\n",
      "epoch 40, loss 0.52358, train_acc 0.8228, valid_acc 0.7965, Time 00:00:33,lr 0.01\n",
      "epoch 41, loss 0.52257, train_acc 0.8211, valid_acc 0.8152, Time 00:00:34,lr 0.01\n",
      "epoch 42, loss 0.52220, train_acc 0.8232, valid_acc 0.8375, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.51315, train_acc 0.8264, valid_acc 0.8182, Time 00:00:31,lr 0.01\n",
      "epoch 44, loss 0.51323, train_acc 0.8268, valid_acc 0.8144, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.51435, train_acc 0.8240, valid_acc 0.8080, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.51004, train_acc 0.8262, valid_acc 0.8189, Time 00:00:31,lr 0.01\n",
      "epoch 47, loss 0.50912, train_acc 0.8285, valid_acc 0.8285, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.51028, train_acc 0.8259, valid_acc 0.8171, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.50687, train_acc 0.8291, valid_acc 0.8301, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.50086, train_acc 0.8297, valid_acc 0.8241, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.49983, train_acc 0.8290, valid_acc 0.8161, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.50052, train_acc 0.8278, valid_acc 0.8267, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.49692, train_acc 0.8309, valid_acc 0.8120, Time 00:00:31,lr 0.01\n",
      "epoch 54, loss 0.50006, train_acc 0.8279, valid_acc 0.8300, Time 00:00:31,lr 0.01\n",
      "epoch 55, loss 0.49913, train_acc 0.8295, valid_acc 0.7949, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.49605, train_acc 0.8320, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.49692, train_acc 0.8300, valid_acc 0.8328, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.49206, train_acc 0.8337, valid_acc 0.8387, Time 00:00:31,lr 0.01\n",
      "epoch 59, loss 0.49343, train_acc 0.8311, valid_acc 0.8150, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.34705, train_acc 0.8820, valid_acc 0.8809, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.30127, train_acc 0.8983, valid_acc 0.8846, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.28316, train_acc 0.9037, valid_acc 0.8870, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.27436, train_acc 0.9062, valid_acc 0.8866, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.26887, train_acc 0.9072, valid_acc 0.8916, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.26500, train_acc 0.9096, valid_acc 0.8894, Time 00:00:31,lr 0.002\n",
      "epoch 66, loss 0.26247, train_acc 0.9101, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 67, loss 0.26025, train_acc 0.9112, valid_acc 0.8920, Time 00:00:32,lr 0.002\n",
      "epoch 68, loss 0.25301, train_acc 0.9135, valid_acc 0.8888, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.25118, train_acc 0.9139, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.25089, train_acc 0.9137, valid_acc 0.8884, Time 00:00:32,lr 0.002\n",
      "epoch 71, loss 0.25031, train_acc 0.9147, valid_acc 0.8795, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.25265, train_acc 0.9130, valid_acc 0.8856, Time 00:00:33,lr 0.002\n",
      "epoch 73, loss 0.25271, train_acc 0.9129, valid_acc 0.8883, Time 00:00:31,lr 0.002\n",
      "epoch 74, loss 0.25271, train_acc 0.9128, valid_acc 0.8770, Time 00:00:32,lr 0.002\n",
      "epoch 75, loss 0.25109, train_acc 0.9137, valid_acc 0.8796, Time 00:00:32,lr 0.002\n",
      "epoch 76, loss 0.25106, train_acc 0.9132, valid_acc 0.8824, Time 00:00:32,lr 0.002\n",
      "epoch 77, loss 0.25249, train_acc 0.9136, valid_acc 0.8863, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.25310, train_acc 0.9126, valid_acc 0.8882, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.25188, train_acc 0.9132, valid_acc 0.8809, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.25087, train_acc 0.9143, valid_acc 0.8808, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.25043, train_acc 0.9133, valid_acc 0.8659, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.25028, train_acc 0.9126, valid_acc 0.8830, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.24958, train_acc 0.9140, valid_acc 0.8806, Time 00:00:31,lr 0.002\n",
      "epoch 84, loss 0.25462, train_acc 0.9128, valid_acc 0.8817, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.25416, train_acc 0.9109, valid_acc 0.8797, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.24963, train_acc 0.9134, valid_acc 0.8856, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.25088, train_acc 0.9143, valid_acc 0.8799, Time 00:00:32,lr 0.002\n",
      "epoch 88, loss 0.24692, train_acc 0.9147, valid_acc 0.8874, Time 00:00:31,lr 0.002\n",
      "epoch 89, loss 0.24720, train_acc 0.9146, valid_acc 0.8826, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.17336, train_acc 0.9407, valid_acc 0.9051, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.14650, train_acc 0.9515, valid_acc 0.9075, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.13297, train_acc 0.9564, valid_acc 0.9078, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.12877, train_acc 0.9568, valid_acc 0.9069, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.12118, train_acc 0.9599, valid_acc 0.9086, Time 00:00:32,lr 0.0004\n",
      "epoch 95, loss 0.11531, train_acc 0.9619, valid_acc 0.9090, Time 00:00:32,lr 0.0004\n",
      "epoch 96, loss 0.11811, train_acc 0.9601, valid_acc 0.9075, Time 00:00:35,lr 0.0004\n",
      "epoch 97, loss 0.11135, train_acc 0.9634, valid_acc 0.9066, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.10657, train_acc 0.9653, valid_acc 0.9044, Time 00:00:31,lr 0.0004\n",
      "epoch 99, loss 0.10713, train_acc 0.9637, valid_acc 0.9063, Time 00:00:31,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.10466, train_acc 0.9656, valid_acc 0.9050, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.09925, train_acc 0.9674, valid_acc 0.9043, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.09841, train_acc 0.9671, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.09499, train_acc 0.9692, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 104, loss 0.09518, train_acc 0.9685, valid_acc 0.9089, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.09390, train_acc 0.9696, valid_acc 0.9105, Time 00:00:31,lr 0.0004\n",
      "epoch 106, loss 0.09006, train_acc 0.9702, valid_acc 0.9070, Time 00:00:32,lr 0.0004\n",
      "epoch 107, loss 0.09035, train_acc 0.9709, valid_acc 0.9081, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.08887, train_acc 0.9702, valid_acc 0.9073, Time 00:00:33,lr 0.0004\n",
      "epoch 109, loss 0.08896, train_acc 0.9715, valid_acc 0.9060, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.08627, train_acc 0.9710, valid_acc 0.9103, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.08818, train_acc 0.9706, valid_acc 0.9070, Time 00:00:31,lr 0.0004\n",
      "epoch 112, loss 0.08570, train_acc 0.9718, valid_acc 0.9063, Time 00:00:33,lr 0.0004\n",
      "epoch 113, loss 0.08473, train_acc 0.9719, valid_acc 0.9086, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.08530, train_acc 0.9715, valid_acc 0.9090, Time 00:00:33,lr 0.0004\n",
      "epoch 115, loss 0.08070, train_acc 0.9733, valid_acc 0.9075, Time 00:00:33,lr 0.0004\n",
      "epoch 116, loss 0.07957, train_acc 0.9737, valid_acc 0.9041, Time 00:00:32,lr 0.0004\n",
      "epoch 117, loss 0.07966, train_acc 0.9733, valid_acc 0.9027, Time 00:00:32,lr 0.0004\n",
      "epoch 118, loss 0.08058, train_acc 0.9734, valid_acc 0.9058, Time 00:00:32,lr 0.0004\n",
      "epoch 119, loss 0.08208, train_acc 0.9728, valid_acc 0.9074, Time 00:00:35,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 I wanto find out acc impove may from cancel downsample, so use k=7 but not downsmaple and k=3 but downsample to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:41.147882Z",
     "start_time": "2018-03-04T09:17:41.140250Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k7p3s1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=7, strides=1, padding=3, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T11:07:06.585909Z",
     "start_time": "2018-03-04T09:17:41.978248Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.30376, train_acc 0.2260, valid_acc 0.3361, Time 00:00:31,lr 0.1\n",
      "epoch 1, loss 1.68781, train_acc 0.3729, valid_acc 0.4189, Time 00:00:35,lr 0.1\n",
      "epoch 2, loss 1.52445, train_acc 0.4432, valid_acc 0.5236, Time 00:00:32,lr 0.1\n",
      "epoch 3, loss 1.37861, train_acc 0.5034, valid_acc 0.5116, Time 00:00:32,lr 0.1\n",
      "epoch 4, loss 1.29522, train_acc 0.5379, valid_acc 0.5644, Time 00:00:32,lr 0.1\n",
      "epoch 5, loss 1.23774, train_acc 0.5634, valid_acc 0.5271, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.18491, train_acc 0.5845, valid_acc 0.6008, Time 00:00:32,lr 0.1\n",
      "epoch 7, loss 1.13172, train_acc 0.6043, valid_acc 0.6084, Time 00:00:32,lr 0.1\n",
      "epoch 8, loss 1.10778, train_acc 0.6163, valid_acc 0.6573, Time 00:00:33,lr 0.1\n",
      "epoch 9, loss 1.06816, train_acc 0.6311, valid_acc 0.6118, Time 00:00:32,lr 0.1\n",
      "epoch 10, loss 1.03542, train_acc 0.6430, valid_acc 0.6466, Time 00:00:32,lr 0.1\n",
      "epoch 11, loss 1.02805, train_acc 0.6471, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 1.01376, train_acc 0.6525, valid_acc 0.6565, Time 00:00:33,lr 0.1\n",
      "epoch 13, loss 1.00736, train_acc 0.6535, valid_acc 0.6435, Time 00:00:32,lr 0.1\n",
      "epoch 14, loss 0.99958, train_acc 0.6599, valid_acc 0.6482, Time 00:00:32,lr 0.1\n",
      "epoch 15, loss 0.98714, train_acc 0.6617, valid_acc 0.6799, Time 00:00:32,lr 0.1\n",
      "epoch 16, loss 0.97678, train_acc 0.6662, valid_acc 0.6960, Time 00:00:32,lr 0.1\n",
      "epoch 17, loss 0.97524, train_acc 0.6663, valid_acc 0.6830, Time 00:00:32,lr 0.1\n",
      "epoch 18, loss 0.97347, train_acc 0.6669, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 19, loss 0.97192, train_acc 0.6669, valid_acc 0.6242, Time 00:00:32,lr 0.1\n",
      "epoch 20, loss 0.97201, train_acc 0.6679, valid_acc 0.6656, Time 00:00:32,lr 0.1\n",
      "epoch 21, loss 0.96170, train_acc 0.6740, valid_acc 0.6367, Time 00:00:33,lr 0.1\n",
      "epoch 22, loss 0.96810, train_acc 0.6708, valid_acc 0.6185, Time 00:00:32,lr 0.1\n",
      "epoch 23, loss 0.95803, train_acc 0.6731, valid_acc 0.6805, Time 00:00:33,lr 0.1\n",
      "epoch 24, loss 0.95777, train_acc 0.6729, valid_acc 0.6490, Time 00:00:32,lr 0.1\n",
      "epoch 25, loss 0.96024, train_acc 0.6753, valid_acc 0.6834, Time 00:00:32,lr 0.1\n",
      "epoch 26, loss 0.95364, train_acc 0.6740, valid_acc 0.6864, Time 00:00:32,lr 0.1\n",
      "epoch 27, loss 0.95158, train_acc 0.6774, valid_acc 0.6646, Time 00:00:32,lr 0.1\n",
      "epoch 28, loss 0.94868, train_acc 0.6751, valid_acc 0.6987, Time 00:00:32,lr 0.1\n",
      "epoch 29, loss 0.95404, train_acc 0.6749, valid_acc 0.6318, Time 00:00:32,lr 0.1\n",
      "epoch 30, loss 0.95379, train_acc 0.6741, valid_acc 0.6917, Time 00:00:32,lr 0.1\n",
      "epoch 31, loss 0.94801, train_acc 0.6744, valid_acc 0.6283, Time 00:00:32,lr 0.1\n",
      "epoch 32, loss 0.95098, train_acc 0.6751, valid_acc 0.6732, Time 00:00:33,lr 0.1\n",
      "epoch 33, loss 0.94359, train_acc 0.6791, valid_acc 0.6295, Time 00:00:34,lr 0.1\n",
      "epoch 34, loss 0.94709, train_acc 0.6774, valid_acc 0.6200, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.94835, train_acc 0.6774, valid_acc 0.6354, Time 00:00:32,lr 0.1\n",
      "epoch 36, loss 0.94460, train_acc 0.6784, valid_acc 0.6478, Time 00:00:32,lr 0.1\n",
      "epoch 37, loss 0.94238, train_acc 0.6794, valid_acc 0.6423, Time 00:00:32,lr 0.1\n",
      "epoch 38, loss 0.94366, train_acc 0.6784, valid_acc 0.5573, Time 00:00:32,lr 0.1\n",
      "epoch 39, loss 0.93595, train_acc 0.6804, valid_acc 0.6669, Time 00:00:32,lr 0.1\n",
      "epoch 40, loss 0.94138, train_acc 0.6788, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 41, loss 0.94039, train_acc 0.6797, valid_acc 0.6339, Time 00:00:34,lr 0.1\n",
      "epoch 42, loss 0.93921, train_acc 0.6797, valid_acc 0.6167, Time 00:00:32,lr 0.1\n",
      "epoch 43, loss 0.93964, train_acc 0.6790, valid_acc 0.6928, Time 00:00:32,lr 0.1\n",
      "epoch 44, loss 0.93891, train_acc 0.6801, valid_acc 0.6485, Time 00:00:32,lr 0.1\n",
      "epoch 45, loss 0.93846, train_acc 0.6814, valid_acc 0.5983, Time 00:00:32,lr 0.1\n",
      "epoch 46, loss 0.93314, train_acc 0.6822, valid_acc 0.6216, Time 00:00:32,lr 0.1\n",
      "epoch 47, loss 0.93256, train_acc 0.6823, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 48, loss 0.94286, train_acc 0.6781, valid_acc 0.6123, Time 00:00:32,lr 0.1\n",
      "epoch 49, loss 0.93333, train_acc 0.6848, valid_acc 0.6390, Time 00:00:32,lr 0.1\n",
      "epoch 50, loss 0.93153, train_acc 0.6829, valid_acc 0.6682, Time 00:00:32,lr 0.1\n",
      "epoch 51, loss 0.93787, train_acc 0.6817, valid_acc 0.7057, Time 00:00:32,lr 0.1\n",
      "epoch 52, loss 0.93450, train_acc 0.6823, valid_acc 0.6080, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.93795, train_acc 0.6822, valid_acc 0.6733, Time 00:00:32,lr 0.1\n",
      "epoch 54, loss 0.92976, train_acc 0.6836, valid_acc 0.6498, Time 00:00:32,lr 0.1\n",
      "epoch 55, loss 0.93671, train_acc 0.6794, valid_acc 0.6334, Time 00:00:32,lr 0.1\n",
      "epoch 56, loss 0.93371, train_acc 0.6807, valid_acc 0.6849, Time 00:00:32,lr 0.1\n",
      "epoch 57, loss 0.93283, train_acc 0.6830, valid_acc 0.6737, Time 00:00:32,lr 0.1\n",
      "epoch 58, loss 0.94008, train_acc 0.6813, valid_acc 0.6681, Time 00:00:32,lr 0.1\n",
      "epoch 59, loss 0.92572, train_acc 0.6860, valid_acc 0.6837, Time 00:00:32,lr 0.1\n",
      "epoch 60, loss 0.93572, train_acc 0.6810, valid_acc 0.6855, Time 00:00:32,lr 0.1\n",
      "epoch 61, loss 0.92939, train_acc 0.6856, valid_acc 0.6800, Time 00:00:32,lr 0.1\n",
      "epoch 62, loss 0.92851, train_acc 0.6848, valid_acc 0.6933, Time 00:00:32,lr 0.1\n",
      "epoch 63, loss 0.93826, train_acc 0.6798, valid_acc 0.5912, Time 00:00:32,lr 0.1\n",
      "epoch 64, loss 0.92919, train_acc 0.6859, valid_acc 0.6371, Time 00:00:32,lr 0.1\n",
      "epoch 65, loss 0.92681, train_acc 0.6829, valid_acc 0.6720, Time 00:00:32,lr 0.1\n",
      "epoch 66, loss 0.92689, train_acc 0.6861, valid_acc 0.6765, Time 00:00:32,lr 0.1\n",
      "epoch 67, loss 0.92941, train_acc 0.6836, valid_acc 0.6244, Time 00:00:32,lr 0.1\n",
      "epoch 68, loss 0.92887, train_acc 0.6831, valid_acc 0.6602, Time 00:00:32,lr 0.1\n",
      "epoch 69, loss 0.93088, train_acc 0.6826, valid_acc 0.6989, Time 00:00:32,lr 0.1\n",
      "epoch 70, loss 0.92295, train_acc 0.6847, valid_acc 0.7020, Time 00:00:32,lr 0.1\n",
      "epoch 71, loss 0.92583, train_acc 0.6863, valid_acc 0.6810, Time 00:00:32,lr 0.1\n",
      "epoch 72, loss 0.92959, train_acc 0.6839, valid_acc 0.6843, Time 00:00:32,lr 0.1\n",
      "epoch 73, loss 0.93152, train_acc 0.6827, valid_acc 0.6619, Time 00:00:32,lr 0.1\n",
      "epoch 74, loss 0.92909, train_acc 0.6839, valid_acc 0.6328, Time 00:00:32,lr 0.1\n",
      "epoch 75, loss 0.92891, train_acc 0.6839, valid_acc 0.6598, Time 00:00:32,lr 0.1\n",
      "epoch 76, loss 0.93157, train_acc 0.6843, valid_acc 0.6716, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.93235, train_acc 0.6823, valid_acc 0.6870, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.92986, train_acc 0.6865, valid_acc 0.6388, Time 00:00:32,lr 0.1\n",
      "epoch 79, loss 0.92761, train_acc 0.6849, valid_acc 0.6662, Time 00:00:32,lr 0.1\n",
      "epoch 0, loss 0.80896, train_acc 0.7271, valid_acc 0.6203, Time 00:00:30,lr 0.05\n",
      "epoch 1, loss 0.88513, train_acc 0.6974, valid_acc 0.6184, Time 00:00:32,lr 0.05\n",
      "epoch 2, loss 0.90256, train_acc 0.6931, valid_acc 0.7099, Time 00:00:32,lr 0.05\n",
      "epoch 3, loss 0.90922, train_acc 0.6893, valid_acc 0.6401, Time 00:00:32,lr 0.05\n",
      "epoch 4, loss 0.91353, train_acc 0.6878, valid_acc 0.6681, Time 00:00:32,lr 0.05\n",
      "epoch 5, loss 0.90272, train_acc 0.6925, valid_acc 0.6439, Time 00:00:32,lr 0.05\n",
      "epoch 6, loss 0.91182, train_acc 0.6895, valid_acc 0.7033, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.91077, train_acc 0.6885, valid_acc 0.6915, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.91324, train_acc 0.6878, valid_acc 0.6665, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.91369, train_acc 0.6868, valid_acc 0.6593, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.91246, train_acc 0.6879, valid_acc 0.6637, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.91111, train_acc 0.6916, valid_acc 0.7026, Time 00:00:32,lr 0.05\n",
      "epoch 12, loss 0.91954, train_acc 0.6874, valid_acc 0.6634, Time 00:00:32,lr 0.05\n",
      "epoch 13, loss 0.91728, train_acc 0.6863, valid_acc 0.6654, Time 00:00:32,lr 0.05\n",
      "epoch 14, loss 0.91430, train_acc 0.6878, valid_acc 0.6514, Time 00:00:32,lr 0.05\n",
      "epoch 15, loss 0.91699, train_acc 0.6876, valid_acc 0.6709, Time 00:00:32,lr 0.05\n",
      "epoch 16, loss 0.91586, train_acc 0.6865, valid_acc 0.6684, Time 00:00:32,lr 0.05\n",
      "epoch 17, loss 0.90909, train_acc 0.6914, valid_acc 0.6667, Time 00:00:32,lr 0.05\n",
      "epoch 18, loss 0.90777, train_acc 0.6873, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 19, loss 0.91364, train_acc 0.6880, valid_acc 0.6613, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.91116, train_acc 0.6876, valid_acc 0.6495, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.90711, train_acc 0.6921, valid_acc 0.6621, Time 00:00:32,lr 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss 0.91377, train_acc 0.6874, valid_acc 0.7030, Time 00:00:32,lr 0.05\n",
      "epoch 23, loss 0.91145, train_acc 0.6884, valid_acc 0.6387, Time 00:00:32,lr 0.05\n",
      "epoch 24, loss 0.91180, train_acc 0.6891, valid_acc 0.7214, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.91215, train_acc 0.6880, valid_acc 0.6815, Time 00:00:32,lr 0.05\n",
      "epoch 26, loss 0.90945, train_acc 0.6892, valid_acc 0.6991, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.91134, train_acc 0.6903, valid_acc 0.6897, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.91023, train_acc 0.6889, valid_acc 0.6460, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.90890, train_acc 0.6892, valid_acc 0.6105, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.62314, train_acc 0.7879, valid_acc 0.8119, Time 00:00:32,lr 0.01\n",
      "epoch 31, loss 0.57048, train_acc 0.8056, valid_acc 0.8057, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.56539, train_acc 0.8086, valid_acc 0.8094, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.56050, train_acc 0.8097, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.56481, train_acc 0.8090, valid_acc 0.7986, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.56655, train_acc 0.8061, valid_acc 0.8173, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.55780, train_acc 0.8100, valid_acc 0.7797, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.55701, train_acc 0.8100, valid_acc 0.8271, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.55273, train_acc 0.8132, valid_acc 0.8075, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.55390, train_acc 0.8106, valid_acc 0.8073, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.54441, train_acc 0.8154, valid_acc 0.8010, Time 00:00:32,lr 0.01\n",
      "epoch 41, loss 0.54516, train_acc 0.8139, valid_acc 0.8111, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.53969, train_acc 0.8148, valid_acc 0.8155, Time 00:00:32,lr 0.01\n",
      "epoch 43, loss 0.53975, train_acc 0.8157, valid_acc 0.8236, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.53627, train_acc 0.8176, valid_acc 0.8247, Time 00:00:32,lr 0.01\n",
      "epoch 45, loss 0.53035, train_acc 0.8196, valid_acc 0.8118, Time 00:00:32,lr 0.01\n",
      "epoch 46, loss 0.53219, train_acc 0.8190, valid_acc 0.8203, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.53024, train_acc 0.8185, valid_acc 0.8109, Time 00:00:32,lr 0.01\n",
      "epoch 48, loss 0.52706, train_acc 0.8210, valid_acc 0.8323, Time 00:00:32,lr 0.01\n",
      "epoch 49, loss 0.52530, train_acc 0.8220, valid_acc 0.7924, Time 00:00:32,lr 0.01\n",
      "epoch 50, loss 0.52793, train_acc 0.8215, valid_acc 0.8195, Time 00:00:32,lr 0.01\n",
      "epoch 51, loss 0.52560, train_acc 0.8216, valid_acc 0.8287, Time 00:00:32,lr 0.01\n",
      "epoch 52, loss 0.51781, train_acc 0.8240, valid_acc 0.8058, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.51830, train_acc 0.8228, valid_acc 0.8306, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.51420, train_acc 0.8254, valid_acc 0.8258, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.51919, train_acc 0.8242, valid_acc 0.8067, Time 00:00:32,lr 0.01\n",
      "epoch 56, loss 0.51772, train_acc 0.8231, valid_acc 0.8133, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.50818, train_acc 0.8279, valid_acc 0.8285, Time 00:00:32,lr 0.01\n",
      "epoch 58, loss 0.51014, train_acc 0.8252, valid_acc 0.8101, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.51187, train_acc 0.8270, valid_acc 0.8182, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.35304, train_acc 0.8818, valid_acc 0.8777, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.31041, train_acc 0.8950, valid_acc 0.8848, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.29029, train_acc 0.9032, valid_acc 0.8857, Time 00:00:32,lr 0.002\n",
      "epoch 63, loss 0.28051, train_acc 0.9045, valid_acc 0.8852, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.27056, train_acc 0.9079, valid_acc 0.8854, Time 00:00:32,lr 0.002\n",
      "epoch 65, loss 0.26793, train_acc 0.9099, valid_acc 0.8821, Time 00:00:33,lr 0.002\n",
      "epoch 66, loss 0.26292, train_acc 0.9110, valid_acc 0.8788, Time 00:00:35,lr 0.002\n",
      "epoch 67, loss 0.25597, train_acc 0.9123, valid_acc 0.8886, Time 00:00:33,lr 0.002\n",
      "epoch 68, loss 0.25621, train_acc 0.9120, valid_acc 0.8809, Time 00:00:34,lr 0.002\n",
      "epoch 69, loss 0.25495, train_acc 0.9139, valid_acc 0.8817, Time 00:00:34,lr 0.002\n",
      "epoch 70, loss 0.25461, train_acc 0.9128, valid_acc 0.8820, Time 00:00:35,lr 0.002\n",
      "epoch 71, loss 0.25337, train_acc 0.9143, valid_acc 0.8872, Time 00:00:33,lr 0.002\n",
      "epoch 72, loss 0.25769, train_acc 0.9113, valid_acc 0.8756, Time 00:00:32,lr 0.002\n",
      "epoch 73, loss 0.25496, train_acc 0.9130, valid_acc 0.8843, Time 00:00:33,lr 0.002\n",
      "epoch 74, loss 0.25236, train_acc 0.9132, valid_acc 0.8803, Time 00:00:32,lr 0.002\n",
      "epoch 75, loss 0.25699, train_acc 0.9118, valid_acc 0.8812, Time 00:00:33,lr 0.002\n",
      "epoch 76, loss 0.25216, train_acc 0.9150, valid_acc 0.8855, Time 00:00:34,lr 0.002\n",
      "epoch 77, loss 0.25850, train_acc 0.9112, valid_acc 0.8700, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.24882, train_acc 0.9136, valid_acc 0.8773, Time 00:00:33,lr 0.002\n",
      "epoch 79, loss 0.25507, train_acc 0.9135, valid_acc 0.8743, Time 00:00:33,lr 0.002\n",
      "epoch 80, loss 0.25569, train_acc 0.9138, valid_acc 0.8738, Time 00:00:32,lr 0.002\n",
      "epoch 81, loss 0.25226, train_acc 0.9132, valid_acc 0.8836, Time 00:00:32,lr 0.002\n",
      "epoch 82, loss 0.24877, train_acc 0.9143, valid_acc 0.8696, Time 00:00:32,lr 0.002\n",
      "epoch 83, loss 0.24857, train_acc 0.9146, valid_acc 0.8788, Time 00:00:33,lr 0.002\n",
      "epoch 84, loss 0.25128, train_acc 0.9141, valid_acc 0.8837, Time 00:00:33,lr 0.002\n",
      "epoch 85, loss 0.24834, train_acc 0.9156, valid_acc 0.8790, Time 00:00:32,lr 0.002\n",
      "epoch 86, loss 0.24991, train_acc 0.9156, valid_acc 0.8798, Time 00:00:33,lr 0.002\n",
      "epoch 87, loss 0.24662, train_acc 0.9153, valid_acc 0.8713, Time 00:00:36,lr 0.002\n",
      "epoch 88, loss 0.25288, train_acc 0.9130, valid_acc 0.8833, Time 00:00:34,lr 0.002\n",
      "epoch 89, loss 0.24991, train_acc 0.9138, valid_acc 0.8753, Time 00:00:32,lr 0.002\n",
      "epoch 90, loss 0.16786, train_acc 0.9432, valid_acc 0.9054, Time 00:00:33,lr 0.0004\n",
      "epoch 91, loss 0.14234, train_acc 0.9528, valid_acc 0.9077, Time 00:00:33,lr 0.0004\n",
      "epoch 92, loss 0.13349, train_acc 0.9558, valid_acc 0.9064, Time 00:00:33,lr 0.0004\n",
      "epoch 93, loss 0.12081, train_acc 0.9609, valid_acc 0.9081, Time 00:00:32,lr 0.0004\n",
      "epoch 94, loss 0.11819, train_acc 0.9613, valid_acc 0.9066, Time 00:00:33,lr 0.0004\n",
      "epoch 95, loss 0.11401, train_acc 0.9624, valid_acc 0.9075, Time 00:00:33,lr 0.0004\n",
      "epoch 96, loss 0.10900, train_acc 0.9646, valid_acc 0.9096, Time 00:00:33,lr 0.0004\n",
      "epoch 97, loss 0.10655, train_acc 0.9661, valid_acc 0.9090, Time 00:00:32,lr 0.0004\n",
      "epoch 98, loss 0.10042, train_acc 0.9670, valid_acc 0.9089, Time 00:00:33,lr 0.0004\n",
      "epoch 99, loss 0.09990, train_acc 0.9671, valid_acc 0.9062, Time 00:00:33,lr 0.0004\n",
      "epoch 100, loss 0.09451, train_acc 0.9689, valid_acc 0.9094, Time 00:00:33,lr 0.0004\n",
      "epoch 101, loss 0.09150, train_acc 0.9703, valid_acc 0.9071, Time 00:00:33,lr 0.0004\n",
      "epoch 102, loss 0.09121, train_acc 0.9704, valid_acc 0.9092, Time 00:00:33,lr 0.0004\n",
      "epoch 103, loss 0.08833, train_acc 0.9716, valid_acc 0.9052, Time 00:00:32,lr 0.0004\n",
      "epoch 104, loss 0.08499, train_acc 0.9719, valid_acc 0.9048, Time 00:00:35,lr 0.0004\n",
      "epoch 105, loss 0.08679, train_acc 0.9713, valid_acc 0.9080, Time 00:00:33,lr 0.0004\n",
      "epoch 106, loss 0.08194, train_acc 0.9739, valid_acc 0.9084, Time 00:00:32,lr 0.0004\n",
      "epoch 107, loss 0.08463, train_acc 0.9727, valid_acc 0.9057, Time 00:00:32,lr 0.0004\n",
      "epoch 108, loss 0.08137, train_acc 0.9731, valid_acc 0.9066, Time 00:00:32,lr 0.0004\n",
      "epoch 109, loss 0.07945, train_acc 0.9746, valid_acc 0.9056, Time 00:00:33,lr 0.0004\n",
      "epoch 110, loss 0.07836, train_acc 0.9753, valid_acc 0.9054, Time 00:00:33,lr 0.0004\n",
      "epoch 111, loss 0.07749, train_acc 0.9753, valid_acc 0.9080, Time 00:00:34,lr 0.0004\n",
      "epoch 112, loss 0.07876, train_acc 0.9742, valid_acc 0.9073, Time 00:00:34,lr 0.0004\n",
      "epoch 113, loss 0.07657, train_acc 0.9749, valid_acc 0.9067, Time 00:00:33,lr 0.0004\n",
      "epoch 114, loss 0.07602, train_acc 0.9756, valid_acc 0.9083, Time 00:00:32,lr 0.0004\n",
      "epoch 115, loss 0.07467, train_acc 0.9759, valid_acc 0.9054, Time 00:00:33,lr 0.0004\n",
      "epoch 116, loss 0.07413, train_acc 0.9758, valid_acc 0.9057, Time 00:00:34,lr 0.0004\n",
      "epoch 117, loss 0.07005, train_acc 0.9773, valid_acc 0.9093, Time 00:00:32,lr 0.0004\n",
      "epoch 118, loss 0.07078, train_acc 0.9771, valid_acc 0.9078, Time 00:00:32,lr 0.0004\n",
      "epoch 119, loss 0.07212, train_acc 0.9771, valid_acc 0.9074, Time 00:00:32,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k7p3s1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k7p3s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T11:07:06.593926Z",
     "start_time": "2018-03-04T11:07:06.587399Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k3p1s2(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=2, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T12:30:35.597723Z",
     "start_time": "2018-03-04T11:07:06.595085Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.50262, train_acc 0.2350, valid_acc 0.3242, Time 00:00:23,lr 0.1\n",
      "epoch 1, loss 1.78462, train_acc 0.3364, valid_acc 0.4290, Time 00:00:24,lr 0.1\n",
      "epoch 2, loss 1.62260, train_acc 0.4035, valid_acc 0.4589, Time 00:00:24,lr 0.1\n",
      "epoch 3, loss 1.54967, train_acc 0.4401, valid_acc 0.4553, Time 00:00:25,lr 0.1\n",
      "epoch 4, loss 1.47895, train_acc 0.4708, valid_acc 0.4980, Time 00:00:28,lr 0.1\n",
      "epoch 5, loss 1.42545, train_acc 0.4961, valid_acc 0.5139, Time 00:00:25,lr 0.1\n",
      "epoch 6, loss 1.38317, train_acc 0.5145, valid_acc 0.5491, Time 00:00:25,lr 0.1\n",
      "epoch 7, loss 1.33647, train_acc 0.5331, valid_acc 0.5664, Time 00:00:26,lr 0.1\n",
      "epoch 8, loss 1.30906, train_acc 0.5418, valid_acc 0.5851, Time 00:00:26,lr 0.1\n",
      "epoch 9, loss 1.29921, train_acc 0.5507, valid_acc 0.5837, Time 00:00:25,lr 0.1\n",
      "epoch 10, loss 1.28389, train_acc 0.5545, valid_acc 0.5244, Time 00:00:26,lr 0.1\n",
      "epoch 11, loss 1.25926, train_acc 0.5661, valid_acc 0.5465, Time 00:00:25,lr 0.1\n",
      "epoch 12, loss 1.25738, train_acc 0.5654, valid_acc 0.5883, Time 00:00:27,lr 0.1\n",
      "epoch 13, loss 1.24461, train_acc 0.5716, valid_acc 0.5379, Time 00:00:28,lr 0.1\n",
      "epoch 14, loss 1.24978, train_acc 0.5737, valid_acc 0.6021, Time 00:00:26,lr 0.1\n",
      "epoch 15, loss 1.22359, train_acc 0.5794, valid_acc 0.5540, Time 00:00:28,lr 0.1\n",
      "epoch 16, loss 1.23121, train_acc 0.5796, valid_acc 0.5539, Time 00:00:25,lr 0.1\n",
      "epoch 17, loss 1.21983, train_acc 0.5814, valid_acc 0.5995, Time 00:00:26,lr 0.1\n",
      "epoch 18, loss 1.21139, train_acc 0.5877, valid_acc 0.5875, Time 00:00:24,lr 0.1\n",
      "epoch 19, loss 1.22130, train_acc 0.5828, valid_acc 0.5786, Time 00:00:24,lr 0.1\n",
      "epoch 20, loss 1.21126, train_acc 0.5862, valid_acc 0.6172, Time 00:00:24,lr 0.1\n",
      "epoch 21, loss 1.20354, train_acc 0.5890, valid_acc 0.6017, Time 00:00:24,lr 0.1\n",
      "epoch 22, loss 1.20505, train_acc 0.5921, valid_acc 0.5667, Time 00:00:24,lr 0.1\n",
      "epoch 23, loss 1.20091, train_acc 0.5939, valid_acc 0.6174, Time 00:00:24,lr 0.1\n",
      "epoch 24, loss 1.20286, train_acc 0.5919, valid_acc 0.5933, Time 00:00:24,lr 0.1\n",
      "epoch 25, loss 1.19615, train_acc 0.5933, valid_acc 0.6055, Time 00:00:24,lr 0.1\n",
      "epoch 26, loss 1.18975, train_acc 0.5981, valid_acc 0.5990, Time 00:00:24,lr 0.1\n",
      "epoch 27, loss 1.19463, train_acc 0.5954, valid_acc 0.6119, Time 00:00:24,lr 0.1\n",
      "epoch 28, loss 1.19693, train_acc 0.5941, valid_acc 0.5994, Time 00:00:24,lr 0.1\n",
      "epoch 29, loss 1.19191, train_acc 0.5949, valid_acc 0.5786, Time 00:00:25,lr 0.1\n",
      "epoch 30, loss 1.19812, train_acc 0.5941, valid_acc 0.6081, Time 00:00:26,lr 0.1\n",
      "epoch 31, loss 1.19063, train_acc 0.5957, valid_acc 0.6407, Time 00:00:24,lr 0.1\n",
      "epoch 32, loss 1.19470, train_acc 0.5954, valid_acc 0.5912, Time 00:00:24,lr 0.1\n",
      "epoch 33, loss 1.18433, train_acc 0.5977, valid_acc 0.5361, Time 00:00:25,lr 0.1\n",
      "epoch 34, loss 1.18371, train_acc 0.5981, valid_acc 0.6132, Time 00:00:26,lr 0.1\n",
      "epoch 35, loss 1.19304, train_acc 0.5930, valid_acc 0.5312, Time 00:00:24,lr 0.1\n",
      "epoch 36, loss 1.18548, train_acc 0.5992, valid_acc 0.6146, Time 00:00:24,lr 0.1\n",
      "epoch 37, loss 1.19769, train_acc 0.5952, valid_acc 0.5817, Time 00:00:24,lr 0.1\n",
      "epoch 38, loss 1.17540, train_acc 0.6027, valid_acc 0.5742, Time 00:00:24,lr 0.1\n",
      "epoch 39, loss 1.18462, train_acc 0.5991, valid_acc 0.5887, Time 00:00:24,lr 0.1\n",
      "epoch 40, loss 1.18646, train_acc 0.5946, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 41, loss 1.19338, train_acc 0.5940, valid_acc 0.5486, Time 00:00:25,lr 0.1\n",
      "epoch 42, loss 1.18848, train_acc 0.5971, valid_acc 0.6175, Time 00:00:26,lr 0.1\n",
      "epoch 43, loss 1.19524, train_acc 0.5988, valid_acc 0.6187, Time 00:00:29,lr 0.1\n",
      "epoch 44, loss 1.18320, train_acc 0.5983, valid_acc 0.6223, Time 00:00:26,lr 0.1\n",
      "epoch 45, loss 1.17820, train_acc 0.6003, valid_acc 0.5944, Time 00:00:24,lr 0.1\n",
      "epoch 46, loss 1.17758, train_acc 0.6003, valid_acc 0.5844, Time 00:00:24,lr 0.1\n",
      "epoch 47, loss 1.18451, train_acc 0.5996, valid_acc 0.5785, Time 00:00:24,lr 0.1\n",
      "epoch 48, loss 1.18422, train_acc 0.6030, valid_acc 0.6022, Time 00:00:24,lr 0.1\n",
      "epoch 49, loss 1.18552, train_acc 0.5979, valid_acc 0.5716, Time 00:00:24,lr 0.1\n",
      "epoch 50, loss 1.18247, train_acc 0.5989, valid_acc 0.6341, Time 00:00:25,lr 0.1\n",
      "epoch 51, loss 1.18669, train_acc 0.5968, valid_acc 0.5807, Time 00:00:26,lr 0.1\n",
      "epoch 52, loss 1.18293, train_acc 0.5982, valid_acc 0.5857, Time 00:00:25,lr 0.1\n",
      "epoch 53, loss 1.18378, train_acc 0.6022, valid_acc 0.6234, Time 00:00:25,lr 0.1\n",
      "epoch 54, loss 1.17740, train_acc 0.6008, valid_acc 0.5988, Time 00:00:24,lr 0.1\n",
      "epoch 55, loss 1.17670, train_acc 0.5992, valid_acc 0.6202, Time 00:00:24,lr 0.1\n",
      "epoch 56, loss 1.18444, train_acc 0.6006, valid_acc 0.6178, Time 00:00:24,lr 0.1\n",
      "epoch 57, loss 1.18786, train_acc 0.5994, valid_acc 0.5740, Time 00:00:25,lr 0.1\n",
      "epoch 58, loss 1.18102, train_acc 0.6033, valid_acc 0.5980, Time 00:00:27,lr 0.1\n",
      "epoch 59, loss 1.17959, train_acc 0.6037, valid_acc 0.6219, Time 00:00:25,lr 0.1\n",
      "epoch 60, loss 1.18183, train_acc 0.5997, valid_acc 0.5931, Time 00:00:24,lr 0.1\n",
      "epoch 61, loss 1.18130, train_acc 0.5995, valid_acc 0.6252, Time 00:00:24,lr 0.1\n",
      "epoch 62, loss 1.18737, train_acc 0.5990, valid_acc 0.5961, Time 00:00:24,lr 0.1\n",
      "epoch 63, loss 1.18772, train_acc 0.5994, valid_acc 0.6248, Time 00:00:24,lr 0.1\n",
      "epoch 64, loss 1.18769, train_acc 0.5959, valid_acc 0.5748, Time 00:00:25,lr 0.1\n",
      "epoch 65, loss 1.18745, train_acc 0.5981, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 66, loss 1.17686, train_acc 0.6023, valid_acc 0.6277, Time 00:00:25,lr 0.1\n",
      "epoch 67, loss 1.18466, train_acc 0.5998, valid_acc 0.6262, Time 00:00:26,lr 0.1\n",
      "epoch 68, loss 1.18047, train_acc 0.6003, valid_acc 0.6232, Time 00:00:25,lr 0.1\n",
      "epoch 69, loss 1.18025, train_acc 0.5982, valid_acc 0.5672, Time 00:00:25,lr 0.1\n",
      "epoch 70, loss 1.18145, train_acc 0.5999, valid_acc 0.5342, Time 00:00:25,lr 0.1\n",
      "epoch 71, loss 1.18055, train_acc 0.6016, valid_acc 0.6221, Time 00:00:25,lr 0.1\n",
      "epoch 72, loss 1.18384, train_acc 0.5982, valid_acc 0.6094, Time 00:00:24,lr 0.1\n",
      "epoch 73, loss 1.17971, train_acc 0.6009, valid_acc 0.6354, Time 00:00:24,lr 0.1\n",
      "epoch 74, loss 1.18438, train_acc 0.6002, valid_acc 0.6417, Time 00:00:24,lr 0.1\n",
      "epoch 75, loss 1.17807, train_acc 0.5993, valid_acc 0.6025, Time 00:00:24,lr 0.1\n",
      "epoch 76, loss 1.18557, train_acc 0.5996, valid_acc 0.6152, Time 00:00:24,lr 0.1\n",
      "epoch 77, loss 1.18545, train_acc 0.5993, valid_acc 0.5532, Time 00:00:24,lr 0.1\n",
      "epoch 78, loss 1.17480, train_acc 0.5997, valid_acc 0.5950, Time 00:00:24,lr 0.1\n",
      "epoch 79, loss 1.18309, train_acc 0.5995, valid_acc 0.6056, Time 00:00:24,lr 0.1\n",
      "epoch 0, loss 1.03273, train_acc 0.6483, valid_acc 0.6740, Time 00:00:22,lr 0.05\n",
      "epoch 1, loss 1.09674, train_acc 0.6251, valid_acc 0.6415, Time 00:00:24,lr 0.05\n",
      "epoch 2, loss 1.10671, train_acc 0.6231, valid_acc 0.6117, Time 00:00:24,lr 0.05\n",
      "epoch 3, loss 1.11485, train_acc 0.6204, valid_acc 0.6554, Time 00:00:24,lr 0.05\n",
      "epoch 4, loss 1.11149, train_acc 0.6221, valid_acc 0.6662, Time 00:00:26,lr 0.05\n",
      "epoch 5, loss 1.11029, train_acc 0.6228, valid_acc 0.6449, Time 00:00:25,lr 0.05\n",
      "epoch 6, loss 1.10534, train_acc 0.6226, valid_acc 0.6285, Time 00:00:24,lr 0.05\n",
      "epoch 7, loss 1.11431, train_acc 0.6230, valid_acc 0.5960, Time 00:00:24,lr 0.05\n",
      "epoch 8, loss 1.11183, train_acc 0.6227, valid_acc 0.6311, Time 00:00:24,lr 0.05\n",
      "epoch 9, loss 1.11037, train_acc 0.6222, valid_acc 0.6452, Time 00:00:24,lr 0.05\n",
      "epoch 10, loss 1.11295, train_acc 0.6200, valid_acc 0.5944, Time 00:00:24,lr 0.05\n",
      "epoch 11, loss 1.12221, train_acc 0.6177, valid_acc 0.6159, Time 00:00:25,lr 0.05\n",
      "epoch 12, loss 1.11013, train_acc 0.6216, valid_acc 0.6719, Time 00:00:25,lr 0.05\n",
      "epoch 13, loss 1.10863, train_acc 0.6211, valid_acc 0.6305, Time 00:00:29,lr 0.05\n",
      "epoch 14, loss 1.10922, train_acc 0.6231, valid_acc 0.6379, Time 00:00:25,lr 0.05\n",
      "epoch 15, loss 1.11353, train_acc 0.6212, valid_acc 0.6409, Time 00:00:28,lr 0.05\n",
      "epoch 16, loss 1.11767, train_acc 0.6190, valid_acc 0.6056, Time 00:00:24,lr 0.05\n",
      "epoch 17, loss 1.10862, train_acc 0.6219, valid_acc 0.6377, Time 00:00:26,lr 0.05\n",
      "epoch 18, loss 1.10578, train_acc 0.6234, valid_acc 0.6432, Time 00:00:27,lr 0.05\n",
      "epoch 19, loss 1.11750, train_acc 0.6168, valid_acc 0.5950, Time 00:00:25,lr 0.05\n",
      "epoch 20, loss 1.11371, train_acc 0.6195, valid_acc 0.6746, Time 00:00:25,lr 0.05\n",
      "epoch 21, loss 1.10867, train_acc 0.6208, valid_acc 0.5958, Time 00:00:25,lr 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss 1.11496, train_acc 0.6212, valid_acc 0.6180, Time 00:00:24,lr 0.05\n",
      "epoch 23, loss 1.11273, train_acc 0.6213, valid_acc 0.6098, Time 00:00:24,lr 0.05\n",
      "epoch 24, loss 1.11474, train_acc 0.6186, valid_acc 0.6466, Time 00:00:24,lr 0.05\n",
      "epoch 25, loss 1.11465, train_acc 0.6209, valid_acc 0.6061, Time 00:00:24,lr 0.05\n",
      "epoch 26, loss 1.11365, train_acc 0.6206, valid_acc 0.6113, Time 00:00:25,lr 0.05\n",
      "epoch 27, loss 1.11495, train_acc 0.6179, valid_acc 0.5848, Time 00:00:26,lr 0.05\n",
      "epoch 28, loss 1.10766, train_acc 0.6230, valid_acc 0.6258, Time 00:00:25,lr 0.05\n",
      "epoch 29, loss 1.11466, train_acc 0.6197, valid_acc 0.6257, Time 00:00:24,lr 0.05\n",
      "epoch 30, loss 0.83832, train_acc 0.7133, valid_acc 0.7441, Time 00:00:24,lr 0.01\n",
      "epoch 31, loss 0.77775, train_acc 0.7343, valid_acc 0.7425, Time 00:00:24,lr 0.01\n",
      "epoch 32, loss 0.78149, train_acc 0.7322, valid_acc 0.7436, Time 00:00:25,lr 0.01\n",
      "epoch 33, loss 0.77605, train_acc 0.7342, valid_acc 0.7492, Time 00:00:24,lr 0.01\n",
      "epoch 34, loss 0.77268, train_acc 0.7366, valid_acc 0.7493, Time 00:00:24,lr 0.01\n",
      "epoch 35, loss 0.77276, train_acc 0.7349, valid_acc 0.7450, Time 00:00:24,lr 0.01\n",
      "epoch 36, loss 0.77670, train_acc 0.7335, valid_acc 0.7512, Time 00:00:24,lr 0.01\n",
      "epoch 37, loss 0.76556, train_acc 0.7388, valid_acc 0.7374, Time 00:00:24,lr 0.01\n",
      "epoch 38, loss 0.76203, train_acc 0.7419, valid_acc 0.7737, Time 00:00:24,lr 0.01\n",
      "epoch 39, loss 0.76107, train_acc 0.7383, valid_acc 0.7531, Time 00:00:24,lr 0.01\n",
      "epoch 40, loss 0.75353, train_acc 0.7417, valid_acc 0.7496, Time 00:00:24,lr 0.01\n",
      "epoch 41, loss 0.75731, train_acc 0.7428, valid_acc 0.7497, Time 00:00:24,lr 0.01\n",
      "epoch 42, loss 0.75287, train_acc 0.7437, valid_acc 0.7447, Time 00:00:24,lr 0.01\n",
      "epoch 43, loss 0.74855, train_acc 0.7449, valid_acc 0.7481, Time 00:00:24,lr 0.01\n",
      "epoch 44, loss 0.74202, train_acc 0.7472, valid_acc 0.7389, Time 00:00:24,lr 0.01\n",
      "epoch 45, loss 0.74359, train_acc 0.7445, valid_acc 0.7470, Time 00:00:24,lr 0.01\n",
      "epoch 46, loss 0.73820, train_acc 0.7492, valid_acc 0.7462, Time 00:00:24,lr 0.01\n",
      "epoch 47, loss 0.73883, train_acc 0.7470, valid_acc 0.7622, Time 00:00:24,lr 0.01\n",
      "epoch 48, loss 0.73880, train_acc 0.7491, valid_acc 0.7629, Time 00:00:24,lr 0.01\n",
      "epoch 49, loss 0.73663, train_acc 0.7489, valid_acc 0.7739, Time 00:00:25,lr 0.01\n",
      "epoch 50, loss 0.73340, train_acc 0.7486, valid_acc 0.7617, Time 00:00:24,lr 0.01\n",
      "epoch 51, loss 0.73506, train_acc 0.7482, valid_acc 0.7459, Time 00:00:26,lr 0.01\n",
      "epoch 52, loss 0.73561, train_acc 0.7481, valid_acc 0.7577, Time 00:00:25,lr 0.01\n",
      "epoch 53, loss 0.73704, train_acc 0.7482, valid_acc 0.7637, Time 00:00:24,lr 0.01\n",
      "epoch 54, loss 0.72151, train_acc 0.7545, valid_acc 0.7602, Time 00:00:24,lr 0.01\n",
      "epoch 55, loss 0.72973, train_acc 0.7516, valid_acc 0.7717, Time 00:00:24,lr 0.01\n",
      "epoch 56, loss 0.72597, train_acc 0.7522, valid_acc 0.7561, Time 00:00:24,lr 0.01\n",
      "epoch 57, loss 0.72498, train_acc 0.7512, valid_acc 0.7513, Time 00:00:25,lr 0.01\n",
      "epoch 58, loss 0.72307, train_acc 0.7534, valid_acc 0.7575, Time 00:00:24,lr 0.01\n",
      "epoch 59, loss 0.72512, train_acc 0.7530, valid_acc 0.7345, Time 00:00:24,lr 0.01\n",
      "epoch 60, loss 0.57424, train_acc 0.8031, valid_acc 0.8110, Time 00:00:25,lr 0.002\n",
      "epoch 61, loss 0.53185, train_acc 0.8193, valid_acc 0.8204, Time 00:00:24,lr 0.002\n",
      "epoch 62, loss 0.52125, train_acc 0.8227, valid_acc 0.8243, Time 00:00:24,lr 0.002\n",
      "epoch 63, loss 0.50715, train_acc 0.8265, valid_acc 0.8171, Time 00:00:26,lr 0.002\n",
      "epoch 64, loss 0.50566, train_acc 0.8266, valid_acc 0.8231, Time 00:00:24,lr 0.002\n",
      "epoch 65, loss 0.49758, train_acc 0.8306, valid_acc 0.8185, Time 00:00:24,lr 0.002\n",
      "epoch 66, loss 0.49151, train_acc 0.8314, valid_acc 0.8244, Time 00:00:24,lr 0.002\n",
      "epoch 67, loss 0.49002, train_acc 0.8309, valid_acc 0.8193, Time 00:00:24,lr 0.002\n",
      "epoch 68, loss 0.49131, train_acc 0.8323, valid_acc 0.8247, Time 00:00:24,lr 0.002\n",
      "epoch 69, loss 0.48864, train_acc 0.8319, valid_acc 0.8245, Time 00:00:26,lr 0.002\n",
      "epoch 70, loss 0.48861, train_acc 0.8329, valid_acc 0.8152, Time 00:00:26,lr 0.002\n",
      "epoch 71, loss 0.49203, train_acc 0.8310, valid_acc 0.8241, Time 00:00:24,lr 0.002\n",
      "epoch 72, loss 0.48694, train_acc 0.8326, valid_acc 0.8207, Time 00:00:25,lr 0.002\n",
      "epoch 73, loss 0.48480, train_acc 0.8329, valid_acc 0.8162, Time 00:00:26,lr 0.002\n",
      "epoch 74, loss 0.49028, train_acc 0.8328, valid_acc 0.8217, Time 00:00:24,lr 0.002\n",
      "epoch 75, loss 0.48933, train_acc 0.8325, valid_acc 0.8256, Time 00:00:24,lr 0.002\n",
      "epoch 76, loss 0.48938, train_acc 0.8311, valid_acc 0.8240, Time 00:00:24,lr 0.002\n",
      "epoch 77, loss 0.48646, train_acc 0.8330, valid_acc 0.8162, Time 00:00:24,lr 0.002\n",
      "epoch 78, loss 0.48905, train_acc 0.8328, valid_acc 0.8183, Time 00:00:24,lr 0.002\n",
      "epoch 79, loss 0.48690, train_acc 0.8318, valid_acc 0.8164, Time 00:00:24,lr 0.002\n",
      "epoch 80, loss 0.48914, train_acc 0.8336, valid_acc 0.8162, Time 00:00:24,lr 0.002\n",
      "epoch 81, loss 0.49061, train_acc 0.8331, valid_acc 0.8200, Time 00:00:24,lr 0.002\n",
      "epoch 82, loss 0.48518, train_acc 0.8342, valid_acc 0.8108, Time 00:00:24,lr 0.002\n",
      "epoch 83, loss 0.48351, train_acc 0.8340, valid_acc 0.8190, Time 00:00:24,lr 0.002\n",
      "epoch 84, loss 0.48935, train_acc 0.8316, valid_acc 0.8246, Time 00:00:24,lr 0.002\n",
      "epoch 85, loss 0.48496, train_acc 0.8341, valid_acc 0.8153, Time 00:00:24,lr 0.002\n",
      "epoch 86, loss 0.48636, train_acc 0.8330, valid_acc 0.8215, Time 00:00:24,lr 0.002\n",
      "epoch 87, loss 0.48367, train_acc 0.8343, valid_acc 0.8203, Time 00:00:24,lr 0.002\n",
      "epoch 88, loss 0.48439, train_acc 0.8336, valid_acc 0.8209, Time 00:00:24,lr 0.002\n",
      "epoch 89, loss 0.48604, train_acc 0.8307, valid_acc 0.8233, Time 00:00:24,lr 0.002\n",
      "epoch 90, loss 0.40677, train_acc 0.8612, valid_acc 0.8445, Time 00:00:24,lr 0.0004\n",
      "epoch 91, loss 0.37624, train_acc 0.8704, valid_acc 0.8463, Time 00:00:24,lr 0.0004\n",
      "epoch 92, loss 0.36739, train_acc 0.8739, valid_acc 0.8504, Time 00:00:24,lr 0.0004\n",
      "epoch 93, loss 0.35553, train_acc 0.8765, valid_acc 0.8514, Time 00:00:27,lr 0.0004\n",
      "epoch 94, loss 0.35360, train_acc 0.8777, valid_acc 0.8497, Time 00:00:24,lr 0.0004\n",
      "epoch 95, loss 0.34861, train_acc 0.8797, valid_acc 0.8496, Time 00:00:24,lr 0.0004\n",
      "epoch 96, loss 0.34631, train_acc 0.8805, valid_acc 0.8536, Time 00:00:24,lr 0.0004\n",
      "epoch 97, loss 0.34272, train_acc 0.8825, valid_acc 0.8508, Time 00:00:24,lr 0.0004\n",
      "epoch 98, loss 0.33808, train_acc 0.8834, valid_acc 0.8506, Time 00:00:24,lr 0.0004\n",
      "epoch 99, loss 0.33606, train_acc 0.8850, valid_acc 0.8530, Time 00:00:24,lr 0.0004\n",
      "epoch 100, loss 0.33146, train_acc 0.8853, valid_acc 0.8521, Time 00:00:25,lr 0.0004\n",
      "epoch 101, loss 0.32776, train_acc 0.8870, valid_acc 0.8508, Time 00:00:26,lr 0.0004\n",
      "epoch 102, loss 0.32878, train_acc 0.8876, valid_acc 0.8494, Time 00:00:24,lr 0.0004\n",
      "epoch 103, loss 0.32975, train_acc 0.8859, valid_acc 0.8497, Time 00:00:25,lr 0.0004\n",
      "epoch 104, loss 0.32299, train_acc 0.8885, valid_acc 0.8508, Time 00:00:24,lr 0.0004\n",
      "epoch 105, loss 0.32167, train_acc 0.8903, valid_acc 0.8495, Time 00:00:24,lr 0.0004\n",
      "epoch 106, loss 0.32123, train_acc 0.8899, valid_acc 0.8451, Time 00:00:24,lr 0.0004\n",
      "epoch 107, loss 0.31717, train_acc 0.8901, valid_acc 0.8499, Time 00:00:24,lr 0.0004\n",
      "epoch 108, loss 0.32094, train_acc 0.8888, valid_acc 0.8480, Time 00:00:24,lr 0.0004\n",
      "epoch 109, loss 0.31830, train_acc 0.8905, valid_acc 0.8485, Time 00:00:24,lr 0.0004\n",
      "epoch 110, loss 0.31645, train_acc 0.8901, valid_acc 0.8507, Time 00:00:24,lr 0.0004\n",
      "epoch 111, loss 0.31573, train_acc 0.8911, valid_acc 0.8499, Time 00:00:24,lr 0.0004\n",
      "epoch 112, loss 0.31620, train_acc 0.8903, valid_acc 0.8511, Time 00:00:24,lr 0.0004\n",
      "epoch 113, loss 0.31348, train_acc 0.8922, valid_acc 0.8513, Time 00:00:24,lr 0.0004\n",
      "epoch 114, loss 0.31418, train_acc 0.8923, valid_acc 0.8472, Time 00:00:24,lr 0.0004\n",
      "epoch 115, loss 0.31038, train_acc 0.8922, valid_acc 0.8499, Time 00:00:24,lr 0.0004\n",
      "epoch 116, loss 0.31282, train_acc 0.8913, valid_acc 0.8484, Time 00:00:24,lr 0.0004\n",
      "epoch 117, loss 0.30687, train_acc 0.8954, valid_acc 0.8487, Time 00:00:24,lr 0.0004\n",
      "epoch 118, loss 0.31341, train_acc 0.8917, valid_acc 0.8544, Time 00:00:25,lr 0.0004\n",
      "epoch 119, loss 0.30818, train_acc 0.8922, valid_acc 0.8467, Time 00:00:24,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k3p1s2()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k3p1s2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 gluon resnet18 vs my resnet 18: use max pooling vs no max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T15:04:54.477541Z",
     "start_time": "2018-03-04T15:04:54.449458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:3])\n",
    "        net.add(*resnet.features[4:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:31:03.467325Z",
     "start_time": "2018-03-04T05:14:50.124830Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.38056, train_acc 0.1657, valid_acc 0.2737, Time 00:00:56,lr 0.1\n",
      "epoch 1, loss 1.72326, train_acc 0.3529, valid_acc 0.3568, Time 00:00:57,lr 0.1\n",
      "epoch 2, loss 1.41782, train_acc 0.4814, valid_acc 0.5539, Time 00:00:56,lr 0.1\n",
      "epoch 3, loss 1.18895, train_acc 0.5769, valid_acc 0.4499, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.06284, train_acc 0.6269, valid_acc 0.6079, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 0.94214, train_acc 0.6724, valid_acc 0.6066, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.88145, train_acc 0.6946, valid_acc 0.5705, Time 00:00:57,lr 0.1\n",
      "epoch 7, loss 0.84403, train_acc 0.7099, valid_acc 0.5912, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.81196, train_acc 0.7204, valid_acc 0.7287, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.79473, train_acc 0.7280, valid_acc 0.6951, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.78191, train_acc 0.7316, valid_acc 0.6971, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.77675, train_acc 0.7329, valid_acc 0.7301, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.75665, train_acc 0.7406, valid_acc 0.6933, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.75395, train_acc 0.7421, valid_acc 0.6948, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.74696, train_acc 0.7435, valid_acc 0.7115, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.73717, train_acc 0.7483, valid_acc 0.6977, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.73225, train_acc 0.7511, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 17, loss 0.73271, train_acc 0.7495, valid_acc 0.7008, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.72847, train_acc 0.7509, valid_acc 0.6897, Time 00:00:56,lr 0.1\n",
      "epoch 19, loss 0.72694, train_acc 0.7517, valid_acc 0.5876, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.72430, train_acc 0.7523, valid_acc 0.7296, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.71836, train_acc 0.7536, valid_acc 0.6899, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.71967, train_acc 0.7531, valid_acc 0.6885, Time 00:00:56,lr 0.1\n",
      "epoch 23, loss 0.71953, train_acc 0.7555, valid_acc 0.7130, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.70939, train_acc 0.7559, valid_acc 0.6943, Time 00:00:56,lr 0.1\n",
      "epoch 25, loss 0.71292, train_acc 0.7573, valid_acc 0.7349, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.71025, train_acc 0.7568, valid_acc 0.7351, Time 00:00:56,lr 0.1\n",
      "epoch 27, loss 0.71876, train_acc 0.7551, valid_acc 0.7187, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.70902, train_acc 0.7583, valid_acc 0.7208, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.70804, train_acc 0.7554, valid_acc 0.7308, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.69986, train_acc 0.7599, valid_acc 0.7021, Time 00:00:56,lr 0.1\n",
      "epoch 31, loss 0.70725, train_acc 0.7590, valid_acc 0.7294, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.70756, train_acc 0.7607, valid_acc 0.7474, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.70504, train_acc 0.7586, valid_acc 0.7125, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.70649, train_acc 0.7594, valid_acc 0.6955, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.70067, train_acc 0.7614, valid_acc 0.7556, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.70107, train_acc 0.7616, valid_acc 0.5762, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.69916, train_acc 0.7612, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.70580, train_acc 0.7588, valid_acc 0.6723, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.69736, train_acc 0.7607, valid_acc 0.7138, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.70295, train_acc 0.7603, valid_acc 0.7250, Time 00:00:56,lr 0.1\n",
      "epoch 41, loss 0.69599, train_acc 0.7622, valid_acc 0.6537, Time 00:00:57,lr 0.1\n",
      "epoch 42, loss 0.70267, train_acc 0.7622, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 43, loss 0.70145, train_acc 0.7609, valid_acc 0.6835, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.69758, train_acc 0.7642, valid_acc 0.6921, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.69759, train_acc 0.7621, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.69861, train_acc 0.7599, valid_acc 0.6791, Time 00:00:56,lr 0.1\n",
      "epoch 47, loss 0.69245, train_acc 0.7646, valid_acc 0.7376, Time 00:00:58,lr 0.1\n",
      "epoch 48, loss 0.70034, train_acc 0.7613, valid_acc 0.6330, Time 00:01:02,lr 0.1\n",
      "epoch 49, loss 0.69684, train_acc 0.7638, valid_acc 0.7106, Time 00:00:59,lr 0.1\n",
      "epoch 50, loss 0.69907, train_acc 0.7642, valid_acc 0.7437, Time 00:01:02,lr 0.1\n",
      "epoch 51, loss 0.69313, train_acc 0.7632, valid_acc 0.7042, Time 00:01:03,lr 0.1\n",
      "epoch 52, loss 0.69796, train_acc 0.7624, valid_acc 0.6378, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.69387, train_acc 0.7634, valid_acc 0.7631, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.69179, train_acc 0.7639, valid_acc 0.6815, Time 00:00:56,lr 0.1\n",
      "epoch 55, loss 0.70082, train_acc 0.7620, valid_acc 0.7479, Time 00:00:56,lr 0.1\n",
      "epoch 56, loss 0.69588, train_acc 0.7631, valid_acc 0.7191, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.69733, train_acc 0.7639, valid_acc 0.7269, Time 00:00:57,lr 0.1\n",
      "epoch 58, loss 0.69333, train_acc 0.7659, valid_acc 0.6627, Time 00:00:56,lr 0.1\n",
      "epoch 59, loss 0.69576, train_acc 0.7629, valid_acc 0.7450, Time 00:00:56,lr 0.1\n",
      "epoch 60, loss 0.69494, train_acc 0.7634, valid_acc 0.7199, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.69445, train_acc 0.7622, valid_acc 0.7363, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.69379, train_acc 0.7620, valid_acc 0.7078, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.69461, train_acc 0.7638, valid_acc 0.7163, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.69229, train_acc 0.7644, valid_acc 0.7119, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.69231, train_acc 0.7630, valid_acc 0.7010, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.69897, train_acc 0.7628, valid_acc 0.6015, Time 00:01:00,lr 0.1\n",
      "epoch 67, loss 0.69971, train_acc 0.7618, valid_acc 0.7291, Time 00:01:00,lr 0.1\n",
      "epoch 68, loss 0.69992, train_acc 0.7627, valid_acc 0.6869, Time 00:00:59,lr 0.1\n",
      "epoch 69, loss 0.69482, train_acc 0.7612, valid_acc 0.7649, Time 00:01:03,lr 0.1\n",
      "epoch 70, loss 0.69304, train_acc 0.7629, valid_acc 0.6619, Time 00:00:57,lr 0.1\n",
      "epoch 71, loss 0.69240, train_acc 0.7654, valid_acc 0.7104, Time 00:00:56,lr 0.1\n",
      "epoch 72, loss 0.69880, train_acc 0.7608, valid_acc 0.7013, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.69195, train_acc 0.7630, valid_acc 0.6779, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.69141, train_acc 0.7646, valid_acc 0.6973, Time 00:00:59,lr 0.1\n",
      "epoch 75, loss 0.68878, train_acc 0.7666, valid_acc 0.6534, Time 00:00:59,lr 0.1\n",
      "epoch 76, loss 0.69063, train_acc 0.7644, valid_acc 0.6998, Time 00:01:00,lr 0.1\n",
      "epoch 77, loss 0.69243, train_acc 0.7637, valid_acc 0.7247, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.70374, train_acc 0.7610, valid_acc 0.6934, Time 00:00:57,lr 0.1\n",
      "epoch 79, loss 0.69154, train_acc 0.7635, valid_acc 0.7347, Time 00:01:01,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:28:52.313297Z",
     "start_time": "2018-03-04T06:33:29.684980Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.58195, train_acc 0.8029, valid_acc 0.7700, Time 00:00:52,lr 0.05\n",
      "epoch 1, loss 0.65821, train_acc 0.7764, valid_acc 0.7590, Time 00:00:55,lr 0.05\n",
      "epoch 2, loss 0.68045, train_acc 0.7693, valid_acc 0.4827, Time 00:00:56,lr 0.05\n",
      "epoch 3, loss 0.68091, train_acc 0.7667, valid_acc 0.7464, Time 00:00:56,lr 0.05\n",
      "epoch 4, loss 0.68455, train_acc 0.7671, valid_acc 0.7237, Time 00:00:56,lr 0.05\n",
      "epoch 5, loss 0.68415, train_acc 0.7694, valid_acc 0.7485, Time 00:00:57,lr 0.05\n",
      "epoch 6, loss 0.68346, train_acc 0.7678, valid_acc 0.5743, Time 00:00:58,lr 0.05\n",
      "epoch 7, loss 0.68119, train_acc 0.7682, valid_acc 0.6967, Time 00:00:56,lr 0.05\n",
      "epoch 8, loss 0.68399, train_acc 0.7670, valid_acc 0.6504, Time 00:00:58,lr 0.05\n",
      "epoch 9, loss 0.68451, train_acc 0.7676, valid_acc 0.6430, Time 00:00:57,lr 0.05\n",
      "epoch 10, loss 0.68556, train_acc 0.7673, valid_acc 0.6783, Time 00:00:59,lr 0.05\n",
      "epoch 11, loss 0.68058, train_acc 0.7669, valid_acc 0.6660, Time 00:00:57,lr 0.05\n",
      "epoch 12, loss 0.69183, train_acc 0.7643, valid_acc 0.6486, Time 00:00:57,lr 0.05\n",
      "epoch 13, loss 0.68374, train_acc 0.7672, valid_acc 0.7040, Time 00:00:57,lr 0.05\n",
      "epoch 14, loss 0.68587, train_acc 0.7647, valid_acc 0.7557, Time 00:00:58,lr 0.05\n",
      "epoch 15, loss 0.68559, train_acc 0.7704, valid_acc 0.6823, Time 00:00:57,lr 0.05\n",
      "epoch 16, loss 0.68928, train_acc 0.7640, valid_acc 0.6444, Time 00:00:59,lr 0.05\n",
      "epoch 17, loss 0.68680, train_acc 0.7662, valid_acc 0.5039, Time 00:00:58,lr 0.05\n",
      "epoch 18, loss 0.68851, train_acc 0.7660, valid_acc 0.6354, Time 00:01:01,lr 0.05\n",
      "epoch 19, loss 0.68628, train_acc 0.7667, valid_acc 0.6202, Time 00:00:59,lr 0.05\n",
      "epoch 20, loss 0.69103, train_acc 0.7665, valid_acc 0.7290, Time 00:01:01,lr 0.05\n",
      "epoch 21, loss 0.68566, train_acc 0.7666, valid_acc 0.6471, Time 00:00:59,lr 0.05\n",
      "epoch 22, loss 0.69282, train_acc 0.7654, valid_acc 0.7272, Time 00:01:01,lr 0.05\n",
      "epoch 23, loss 0.68336, train_acc 0.7706, valid_acc 0.6115, Time 00:01:02,lr 0.05\n",
      "epoch 24, loss 0.68217, train_acc 0.7695, valid_acc 0.7322, Time 00:01:00,lr 0.05\n",
      "epoch 25, loss 0.68338, train_acc 0.7686, valid_acc 0.7349, Time 00:01:01,lr 0.05\n",
      "epoch 26, loss 0.68748, train_acc 0.7667, valid_acc 0.7034, Time 00:01:04,lr 0.05\n",
      "epoch 27, loss 0.69171, train_acc 0.7651, valid_acc 0.7070, Time 00:00:59,lr 0.05\n",
      "epoch 28, loss 0.68284, train_acc 0.7680, valid_acc 0.7419, Time 00:00:57,lr 0.05\n",
      "epoch 29, loss 0.68182, train_acc 0.7672, valid_acc 0.6750, Time 00:00:57,lr 0.05\n",
      "epoch 30, loss 0.43710, train_acc 0.8514, valid_acc 0.8458, Time 00:00:56,lr 0.01\n",
      "epoch 31, loss 0.38423, train_acc 0.8686, valid_acc 0.8636, Time 00:00:56,lr 0.01\n",
      "epoch 32, loss 0.38351, train_acc 0.8686, valid_acc 0.8521, Time 00:01:01,lr 0.01\n",
      "epoch 33, loss 0.38735, train_acc 0.8679, valid_acc 0.8384, Time 00:01:00,lr 0.01\n",
      "epoch 34, loss 0.38082, train_acc 0.8711, valid_acc 0.8641, Time 00:00:56,lr 0.01\n",
      "epoch 35, loss 0.38738, train_acc 0.8672, valid_acc 0.8248, Time 00:00:55,lr 0.01\n",
      "epoch 36, loss 0.38062, train_acc 0.8709, valid_acc 0.8618, Time 00:00:55,lr 0.01\n",
      "epoch 37, loss 0.38049, train_acc 0.8718, valid_acc 0.8291, Time 00:00:57,lr 0.01\n",
      "epoch 38, loss 0.37551, train_acc 0.8724, valid_acc 0.8629, Time 00:00:58,lr 0.01\n",
      "epoch 39, loss 0.37463, train_acc 0.8717, valid_acc 0.8566, Time 00:01:00,lr 0.01\n",
      "epoch 40, loss 0.37176, train_acc 0.8730, valid_acc 0.8559, Time 00:01:00,lr 0.01\n",
      "epoch 41, loss 0.36386, train_acc 0.8775, valid_acc 0.8362, Time 00:00:56,lr 0.01\n",
      "epoch 42, loss 0.36582, train_acc 0.8751, valid_acc 0.8740, Time 00:00:56,lr 0.01\n",
      "epoch 43, loss 0.36425, train_acc 0.8759, valid_acc 0.8561, Time 00:00:56,lr 0.01\n",
      "epoch 44, loss 0.35750, train_acc 0.8785, valid_acc 0.8442, Time 00:00:56,lr 0.01\n",
      "epoch 45, loss 0.35708, train_acc 0.8781, valid_acc 0.8600, Time 00:00:57,lr 0.01\n",
      "epoch 46, loss 0.35498, train_acc 0.8784, valid_acc 0.8639, Time 00:00:56,lr 0.01\n",
      "epoch 47, loss 0.35540, train_acc 0.8789, valid_acc 0.8602, Time 00:00:56,lr 0.01\n",
      "epoch 48, loss 0.35173, train_acc 0.8803, valid_acc 0.8466, Time 00:00:56,lr 0.01\n",
      "epoch 49, loss 0.35126, train_acc 0.8813, valid_acc 0.8660, Time 00:00:56,lr 0.01\n",
      "epoch 50, loss 0.34843, train_acc 0.8821, valid_acc 0.8473, Time 00:00:56,lr 0.01\n",
      "epoch 51, loss 0.34383, train_acc 0.8816, valid_acc 0.8726, Time 00:00:57,lr 0.01\n",
      "epoch 52, loss 0.34999, train_acc 0.8809, valid_acc 0.8602, Time 00:01:01,lr 0.01\n",
      "epoch 53, loss 0.34417, train_acc 0.8835, valid_acc 0.8419, Time 00:00:56,lr 0.01\n",
      "epoch 54, loss 0.34483, train_acc 0.8828, valid_acc 0.8654, Time 00:00:56,lr 0.01\n",
      "epoch 55, loss 0.34106, train_acc 0.8856, valid_acc 0.8673, Time 00:00:56,lr 0.01\n",
      "epoch 56, loss 0.34146, train_acc 0.8839, valid_acc 0.8758, Time 00:01:01,lr 0.01\n",
      "epoch 57, loss 0.34200, train_acc 0.8854, valid_acc 0.8456, Time 00:00:58,lr 0.01\n",
      "epoch 58, loss 0.34399, train_acc 0.8828, valid_acc 0.8639, Time 00:00:58,lr 0.01\n",
      "epoch 59, loss 0.34053, train_acc 0.8847, valid_acc 0.8597, Time 00:00:58,lr 0.01\n",
      "epoch 60, loss 0.20173, train_acc 0.9333, valid_acc 0.9198, Time 00:00:56,lr 0.002\n",
      "epoch 61, loss 0.15869, train_acc 0.9455, valid_acc 0.9215, Time 00:00:56,lr 0.002\n",
      "epoch 62, loss 0.14265, train_acc 0.9519, valid_acc 0.9222, Time 00:00:56,lr 0.002\n",
      "epoch 63, loss 0.13336, train_acc 0.9559, valid_acc 0.9245, Time 00:00:58,lr 0.002\n",
      "epoch 64, loss 0.12715, train_acc 0.9568, valid_acc 0.9178, Time 00:00:56,lr 0.002\n",
      "epoch 65, loss 0.12213, train_acc 0.9591, valid_acc 0.9210, Time 00:00:58,lr 0.002\n",
      "epoch 66, loss 0.11907, train_acc 0.9592, valid_acc 0.9224, Time 00:00:57,lr 0.002\n",
      "epoch 67, loss 0.11596, train_acc 0.9613, valid_acc 0.9201, Time 00:00:56,lr 0.002\n",
      "epoch 68, loss 0.11954, train_acc 0.9596, valid_acc 0.9202, Time 00:00:56,lr 0.002\n",
      "epoch 69, loss 0.11487, train_acc 0.9614, valid_acc 0.9212, Time 00:00:56,lr 0.002\n",
      "epoch 70, loss 0.11803, train_acc 0.9598, valid_acc 0.9105, Time 00:00:56,lr 0.002\n",
      "epoch 71, loss 0.11419, train_acc 0.9616, valid_acc 0.9124, Time 00:00:56,lr 0.002\n",
      "epoch 72, loss 0.11577, train_acc 0.9608, valid_acc 0.9108, Time 00:00:56,lr 0.002\n",
      "epoch 73, loss 0.11624, train_acc 0.9604, valid_acc 0.9160, Time 00:00:56,lr 0.002\n",
      "epoch 74, loss 0.11776, train_acc 0.9603, valid_acc 0.9185, Time 00:00:56,lr 0.002\n",
      "epoch 75, loss 0.11450, train_acc 0.9622, valid_acc 0.9117, Time 00:00:56,lr 0.002\n",
      "epoch 76, loss 0.11900, train_acc 0.9613, valid_acc 0.9097, Time 00:01:00,lr 0.002\n",
      "epoch 77, loss 0.12027, train_acc 0.9592, valid_acc 0.9142, Time 00:00:57,lr 0.002\n",
      "epoch 78, loss 0.12142, train_acc 0.9593, valid_acc 0.9121, Time 00:00:56,lr 0.002\n",
      "epoch 79, loss 0.12121, train_acc 0.9592, valid_acc 0.9147, Time 00:00:57,lr 0.002\n",
      "epoch 80, loss 0.12328, train_acc 0.9580, valid_acc 0.9057, Time 00:00:57,lr 0.002\n",
      "epoch 81, loss 0.12675, train_acc 0.9566, valid_acc 0.9083, Time 00:00:56,lr 0.002\n",
      "epoch 82, loss 0.11973, train_acc 0.9583, valid_acc 0.9099, Time 00:00:58,lr 0.002\n",
      "epoch 83, loss 0.12241, train_acc 0.9586, valid_acc 0.9161, Time 00:00:55,lr 0.002\n",
      "epoch 84, loss 0.11883, train_acc 0.9593, valid_acc 0.9097, Time 00:00:56,lr 0.002\n",
      "epoch 85, loss 0.12814, train_acc 0.9568, valid_acc 0.9161, Time 00:00:56,lr 0.002\n",
      "epoch 86, loss 0.11971, train_acc 0.9590, valid_acc 0.9075, Time 00:00:55,lr 0.002\n",
      "epoch 87, loss 0.12493, train_acc 0.9573, valid_acc 0.9143, Time 00:00:56,lr 0.002\n",
      "epoch 88, loss 0.11748, train_acc 0.9599, valid_acc 0.9079, Time 00:01:00,lr 0.002\n",
      "epoch 89, loss 0.12178, train_acc 0.9585, valid_acc 0.9043, Time 00:00:55,lr 0.002\n",
      "epoch 90, loss 0.06359, train_acc 0.9809, valid_acc 0.9297, Time 00:00:56,lr 0.0004\n",
      "epoch 91, loss 0.04226, train_acc 0.9878, valid_acc 0.9347, Time 00:00:55,lr 0.0004\n",
      "epoch 92, loss 0.03643, train_acc 0.9901, valid_acc 0.9339, Time 00:00:56,lr 0.0004\n",
      "epoch 93, loss 0.03193, train_acc 0.9917, valid_acc 0.9344, Time 00:00:55,lr 0.0004\n",
      "epoch 94, loss 0.03020, train_acc 0.9919, valid_acc 0.9354, Time 00:00:56,lr 0.0004\n",
      "epoch 95, loss 0.02928, train_acc 0.9922, valid_acc 0.9342, Time 00:00:55,lr 0.0004\n",
      "epoch 96, loss 0.02602, train_acc 0.9928, valid_acc 0.9355, Time 00:00:55,lr 0.0004\n",
      "epoch 97, loss 0.02409, train_acc 0.9935, valid_acc 0.9350, Time 00:00:55,lr 0.0004\n",
      "epoch 98, loss 0.02386, train_acc 0.9939, valid_acc 0.9329, Time 00:00:56,lr 0.0004\n",
      "epoch 99, loss 0.01987, train_acc 0.9951, valid_acc 0.9358, Time 00:00:56,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.02084, train_acc 0.9946, valid_acc 0.9337, Time 00:00:57,lr 0.0004\n",
      "epoch 101, loss 0.01851, train_acc 0.9954, valid_acc 0.9360, Time 00:00:58,lr 0.0004\n",
      "epoch 102, loss 0.01870, train_acc 0.9954, valid_acc 0.9357, Time 00:00:56,lr 0.0004\n",
      "epoch 103, loss 0.01807, train_acc 0.9956, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 104, loss 0.01849, train_acc 0.9954, valid_acc 0.9336, Time 00:00:56,lr 0.0004\n",
      "epoch 105, loss 0.01738, train_acc 0.9961, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 106, loss 0.01575, train_acc 0.9961, valid_acc 0.9359, Time 00:01:01,lr 0.0004\n",
      "epoch 107, loss 0.01618, train_acc 0.9963, valid_acc 0.9350, Time 00:00:56,lr 0.0004\n",
      "epoch 108, loss 0.01708, train_acc 0.9954, valid_acc 0.9334, Time 00:00:56,lr 0.0004\n",
      "epoch 109, loss 0.01544, train_acc 0.9964, valid_acc 0.9364, Time 00:00:56,lr 0.0004\n",
      "epoch 110, loss 0.01496, train_acc 0.9965, valid_acc 0.9346, Time 00:00:57,lr 0.0004\n",
      "epoch 111, loss 0.01420, train_acc 0.9966, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 112, loss 0.01472, train_acc 0.9962, valid_acc 0.9342, Time 00:00:56,lr 0.0004\n",
      "epoch 113, loss 0.01399, train_acc 0.9968, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 114, loss 0.01392, train_acc 0.9969, valid_acc 0.9344, Time 00:00:57,lr 0.0004\n",
      "epoch 115, loss 0.01281, train_acc 0.9975, valid_acc 0.9342, Time 00:00:58,lr 0.0004\n",
      "epoch 116, loss 0.01356, train_acc 0.9968, valid_acc 0.9359, Time 00:00:56,lr 0.0004\n",
      "epoch 117, loss 0.01356, train_acc 0.9971, valid_acc 0.9362, Time 00:00:58,lr 0.0004\n",
      "epoch 118, loss 0.01320, train_acc 0.9970, valid_acc 0.9353, Time 00:00:59,lr 0.0004\n",
      "epoch 119, loss 0.01289, train_acc 0.9971, valid_acc 0.9349, Time 00:01:00,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\", ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3_no_max_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 general lr policy train resnet18_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T18:12:46.252720Z",
     "start_time": "2018-03-04T15:04:58.881087Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.15846, train_acc 0.2636, valid_acc 0.3974, Time 00:00:50,lr 0.1\n",
      "epoch 1, loss 1.62803, train_acc 0.3977, valid_acc 0.4551, Time 00:00:54,lr 0.1\n",
      "epoch 2, loss 1.41918, train_acc 0.4863, valid_acc 0.5159, Time 00:00:55,lr 0.1\n",
      "epoch 3, loss 1.19825, train_acc 0.5719, valid_acc 0.6059, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.01131, train_acc 0.6444, valid_acc 0.6491, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 0.88996, train_acc 0.6878, valid_acc 0.7103, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.78305, train_acc 0.7267, valid_acc 0.7235, Time 00:00:57,lr 0.1\n",
      "epoch 7, loss 0.69410, train_acc 0.7577, valid_acc 0.7858, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.63683, train_acc 0.7805, valid_acc 0.7856, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.60257, train_acc 0.7929, valid_acc 0.7577, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.57003, train_acc 0.8039, valid_acc 0.7868, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.54821, train_acc 0.8109, valid_acc 0.8210, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.52209, train_acc 0.8195, valid_acc 0.8023, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.50669, train_acc 0.8247, valid_acc 0.8284, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.49168, train_acc 0.8307, valid_acc 0.7976, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.48094, train_acc 0.8356, valid_acc 0.8335, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.46743, train_acc 0.8393, valid_acc 0.8340, Time 00:00:55,lr 0.1\n",
      "epoch 17, loss 0.45708, train_acc 0.8422, valid_acc 0.8399, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.44915, train_acc 0.8447, valid_acc 0.8338, Time 00:00:55,lr 0.1\n",
      "epoch 19, loss 0.44107, train_acc 0.8492, valid_acc 0.8440, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.42885, train_acc 0.8522, valid_acc 0.8271, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.41722, train_acc 0.8576, valid_acc 0.8403, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.41695, train_acc 0.8535, valid_acc 0.8352, Time 00:00:55,lr 0.1\n",
      "epoch 23, loss 0.40497, train_acc 0.8610, valid_acc 0.8293, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.40128, train_acc 0.8627, valid_acc 0.8544, Time 00:00:55,lr 0.1\n",
      "epoch 25, loss 0.39778, train_acc 0.8637, valid_acc 0.8255, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.38924, train_acc 0.8654, valid_acc 0.8475, Time 00:00:55,lr 0.1\n",
      "epoch 27, loss 0.38375, train_acc 0.8672, valid_acc 0.8315, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.38154, train_acc 0.8695, valid_acc 0.8524, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.37514, train_acc 0.8702, valid_acc 0.8548, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.37547, train_acc 0.8701, valid_acc 0.8264, Time 00:00:55,lr 0.1\n",
      "epoch 31, loss 0.36880, train_acc 0.8730, valid_acc 0.8621, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.36823, train_acc 0.8750, valid_acc 0.8452, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.36444, train_acc 0.8735, valid_acc 0.8324, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.35993, train_acc 0.8753, valid_acc 0.8566, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.35871, train_acc 0.8765, valid_acc 0.8386, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.35150, train_acc 0.8794, valid_acc 0.8577, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.35466, train_acc 0.8782, valid_acc 0.8438, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.34956, train_acc 0.8793, valid_acc 0.8389, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.34803, train_acc 0.8814, valid_acc 0.8594, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.34794, train_acc 0.8803, valid_acc 0.8586, Time 00:00:55,lr 0.1\n",
      "epoch 41, loss 0.33894, train_acc 0.8836, valid_acc 0.8612, Time 00:00:55,lr 0.1\n",
      "epoch 42, loss 0.33981, train_acc 0.8850, valid_acc 0.8690, Time 00:00:55,lr 0.1\n",
      "epoch 43, loss 0.34106, train_acc 0.8833, valid_acc 0.8577, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.33637, train_acc 0.8857, valid_acc 0.8618, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.33335, train_acc 0.8851, valid_acc 0.8739, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.33010, train_acc 0.8878, valid_acc 0.8711, Time 00:00:55,lr 0.1\n",
      "epoch 47, loss 0.33107, train_acc 0.8860, valid_acc 0.8427, Time 00:00:56,lr 0.1\n",
      "epoch 48, loss 0.33052, train_acc 0.8873, valid_acc 0.8533, Time 00:00:56,lr 0.1\n",
      "epoch 49, loss 0.32679, train_acc 0.8881, valid_acc 0.8696, Time 00:00:56,lr 0.1\n",
      "epoch 50, loss 0.33123, train_acc 0.8852, valid_acc 0.8655, Time 00:00:56,lr 0.1\n",
      "epoch 51, loss 0.33064, train_acc 0.8860, valid_acc 0.8566, Time 00:00:56,lr 0.1\n",
      "epoch 52, loss 0.32772, train_acc 0.8872, valid_acc 0.8538, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.32102, train_acc 0.8909, valid_acc 0.8782, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.32483, train_acc 0.8884, valid_acc 0.8752, Time 00:00:55,lr 0.1\n",
      "epoch 55, loss 0.32031, train_acc 0.8922, valid_acc 0.8608, Time 00:00:58,lr 0.1\n",
      "epoch 56, loss 0.32091, train_acc 0.8904, valid_acc 0.8685, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.31825, train_acc 0.8908, valid_acc 0.8731, Time 00:00:56,lr 0.1\n",
      "epoch 58, loss 0.32040, train_acc 0.8905, valid_acc 0.8439, Time 00:00:59,lr 0.1\n",
      "epoch 59, loss 0.31838, train_acc 0.8906, valid_acc 0.8543, Time 00:01:01,lr 0.1\n",
      "epoch 60, loss 0.31636, train_acc 0.8924, valid_acc 0.8622, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.31622, train_acc 0.8912, valid_acc 0.8482, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.31287, train_acc 0.8932, valid_acc 0.8822, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.30961, train_acc 0.8930, valid_acc 0.8839, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.31417, train_acc 0.8945, valid_acc 0.8728, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.31620, train_acc 0.8919, valid_acc 0.8675, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.30825, train_acc 0.8954, valid_acc 0.8811, Time 00:00:56,lr 0.1\n",
      "epoch 67, loss 0.30862, train_acc 0.8947, valid_acc 0.8865, Time 00:00:56,lr 0.1\n",
      "epoch 68, loss 0.30983, train_acc 0.8933, valid_acc 0.8414, Time 00:00:56,lr 0.1\n",
      "epoch 69, loss 0.30705, train_acc 0.8954, valid_acc 0.8583, Time 00:00:56,lr 0.1\n",
      "epoch 70, loss 0.30614, train_acc 0.8944, valid_acc 0.8689, Time 00:00:59,lr 0.1\n",
      "epoch 71, loss 0.30487, train_acc 0.8951, valid_acc 0.8686, Time 00:00:57,lr 0.1\n",
      "epoch 72, loss 0.30635, train_acc 0.8959, valid_acc 0.8605, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.30478, train_acc 0.8955, valid_acc 0.8763, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.30377, train_acc 0.8953, valid_acc 0.8621, Time 00:01:01,lr 0.1\n",
      "epoch 75, loss 0.30006, train_acc 0.8972, valid_acc 0.8836, Time 00:00:58,lr 0.1\n",
      "epoch 76, loss 0.30860, train_acc 0.8948, valid_acc 0.8890, Time 00:00:56,lr 0.1\n",
      "epoch 77, loss 0.30585, train_acc 0.8953, valid_acc 0.8825, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.30411, train_acc 0.8961, valid_acc 0.8783, Time 00:00:56,lr 0.1\n",
      "epoch 79, loss 0.30075, train_acc 0.8967, valid_acc 0.8733, Time 00:00:55,lr 0.1\n",
      "epoch 80, loss 0.16171, train_acc 0.9455, valid_acc 0.9280, Time 00:00:58,lr 0.01\n",
      "epoch 81, loss 0.12098, train_acc 0.9590, valid_acc 0.9312, Time 00:00:57,lr 0.01\n",
      "epoch 82, loss 0.10316, train_acc 0.9647, valid_acc 0.9346, Time 00:00:56,lr 0.01\n",
      "epoch 83, loss 0.08788, train_acc 0.9699, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 84, loss 0.08168, train_acc 0.9721, valid_acc 0.9355, Time 00:00:56,lr 0.01\n",
      "epoch 85, loss 0.07426, train_acc 0.9743, valid_acc 0.9345, Time 00:00:56,lr 0.01\n",
      "epoch 86, loss 0.06671, train_acc 0.9770, valid_acc 0.9364, Time 00:00:57,lr 0.01\n",
      "epoch 87, loss 0.06325, train_acc 0.9785, valid_acc 0.9351, Time 00:00:56,lr 0.01\n",
      "epoch 88, loss 0.05709, train_acc 0.9806, valid_acc 0.9365, Time 00:00:56,lr 0.01\n",
      "epoch 89, loss 0.05294, train_acc 0.9820, valid_acc 0.9345, Time 00:00:56,lr 0.01\n",
      "epoch 90, loss 0.05128, train_acc 0.9830, valid_acc 0.9374, Time 00:00:56,lr 0.01\n",
      "epoch 91, loss 0.04591, train_acc 0.9843, valid_acc 0.9338, Time 00:00:56,lr 0.01\n",
      "epoch 92, loss 0.04429, train_acc 0.9850, valid_acc 0.9394, Time 00:00:56,lr 0.01\n",
      "epoch 93, loss 0.04115, train_acc 0.9867, valid_acc 0.9374, Time 00:00:56,lr 0.01\n",
      "epoch 94, loss 0.04127, train_acc 0.9853, valid_acc 0.9354, Time 00:00:56,lr 0.01\n",
      "epoch 95, loss 0.03846, train_acc 0.9870, valid_acc 0.9360, Time 00:00:56,lr 0.01\n",
      "epoch 96, loss 0.03880, train_acc 0.9865, valid_acc 0.9355, Time 00:00:56,lr 0.01\n",
      "epoch 97, loss 0.03761, train_acc 0.9873, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 98, loss 0.03572, train_acc 0.9885, valid_acc 0.9355, Time 00:00:56,lr 0.01\n",
      "epoch 99, loss 0.03503, train_acc 0.9886, valid_acc 0.9352, Time 00:00:56,lr 0.01\n",
      "epoch 100, loss 0.03311, train_acc 0.9891, valid_acc 0.9379, Time 00:00:56,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.03337, train_acc 0.9887, valid_acc 0.9390, Time 00:00:56,lr 0.01\n",
      "epoch 102, loss 0.03445, train_acc 0.9890, valid_acc 0.9373, Time 00:00:56,lr 0.01\n",
      "epoch 103, loss 0.03368, train_acc 0.9888, valid_acc 0.9336, Time 00:00:56,lr 0.01\n",
      "epoch 104, loss 0.03256, train_acc 0.9898, valid_acc 0.9330, Time 00:00:56,lr 0.01\n",
      "epoch 105, loss 0.03219, train_acc 0.9896, valid_acc 0.9352, Time 00:00:56,lr 0.01\n",
      "epoch 106, loss 0.03368, train_acc 0.9877, valid_acc 0.9321, Time 00:00:56,lr 0.01\n",
      "epoch 107, loss 0.03187, train_acc 0.9894, valid_acc 0.9285, Time 00:00:56,lr 0.01\n",
      "epoch 108, loss 0.03688, train_acc 0.9875, valid_acc 0.9326, Time 00:00:56,lr 0.01\n",
      "epoch 109, loss 0.03780, train_acc 0.9873, valid_acc 0.9341, Time 00:00:56,lr 0.01\n",
      "epoch 110, loss 0.03586, train_acc 0.9875, valid_acc 0.9356, Time 00:00:56,lr 0.01\n",
      "epoch 111, loss 0.03375, train_acc 0.9887, valid_acc 0.9339, Time 00:00:56,lr 0.01\n",
      "epoch 112, loss 0.03716, train_acc 0.9871, valid_acc 0.9310, Time 00:00:56,lr 0.01\n",
      "epoch 113, loss 0.03485, train_acc 0.9883, valid_acc 0.9332, Time 00:00:56,lr 0.01\n",
      "epoch 114, loss 0.03535, train_acc 0.9884, valid_acc 0.9346, Time 00:00:56,lr 0.01\n",
      "epoch 115, loss 0.03670, train_acc 0.9874, valid_acc 0.9343, Time 00:00:56,lr 0.01\n",
      "epoch 116, loss 0.03895, train_acc 0.9865, valid_acc 0.9309, Time 00:00:56,lr 0.01\n",
      "epoch 117, loss 0.03772, train_acc 0.9875, valid_acc 0.9293, Time 00:00:56,lr 0.01\n",
      "epoch 118, loss 0.03680, train_acc 0.9874, valid_acc 0.9319, Time 00:00:56,lr 0.01\n",
      "epoch 119, loss 0.03773, train_acc 0.9870, valid_acc 0.9334, Time 00:00:56,lr 0.01\n",
      "epoch 120, loss 0.04006, train_acc 0.9861, valid_acc 0.9331, Time 00:00:56,lr 0.01\n",
      "epoch 121, loss 0.04120, train_acc 0.9861, valid_acc 0.9315, Time 00:00:56,lr 0.01\n",
      "epoch 122, loss 0.04192, train_acc 0.9860, valid_acc 0.9294, Time 00:00:56,lr 0.01\n",
      "epoch 123, loss 0.04074, train_acc 0.9864, valid_acc 0.9280, Time 00:00:56,lr 0.01\n",
      "epoch 124, loss 0.04274, train_acc 0.9852, valid_acc 0.9279, Time 00:00:56,lr 0.01\n",
      "epoch 125, loss 0.04207, train_acc 0.9860, valid_acc 0.9332, Time 00:00:56,lr 0.01\n",
      "epoch 126, loss 0.04055, train_acc 0.9862, valid_acc 0.9292, Time 00:00:56,lr 0.01\n",
      "epoch 127, loss 0.04287, train_acc 0.9857, valid_acc 0.9241, Time 00:00:56,lr 0.01\n",
      "epoch 128, loss 0.04564, train_acc 0.9842, valid_acc 0.9270, Time 00:00:56,lr 0.01\n",
      "epoch 129, loss 0.04434, train_acc 0.9847, valid_acc 0.9281, Time 00:00:56,lr 0.01\n",
      "epoch 130, loss 0.04602, train_acc 0.9845, valid_acc 0.9340, Time 00:00:56,lr 0.01\n",
      "epoch 131, loss 0.04299, train_acc 0.9857, valid_acc 0.9290, Time 00:00:56,lr 0.01\n",
      "epoch 132, loss 0.04678, train_acc 0.9846, valid_acc 0.9289, Time 00:00:56,lr 0.01\n",
      "epoch 133, loss 0.04226, train_acc 0.9860, valid_acc 0.9291, Time 00:00:55,lr 0.01\n",
      "epoch 134, loss 0.04568, train_acc 0.9842, valid_acc 0.9266, Time 00:00:56,lr 0.01\n",
      "epoch 135, loss 0.04841, train_acc 0.9839, valid_acc 0.9294, Time 00:00:55,lr 0.01\n",
      "epoch 136, loss 0.04353, train_acc 0.9854, valid_acc 0.9302, Time 00:00:55,lr 0.01\n",
      "epoch 137, loss 0.04479, train_acc 0.9853, valid_acc 0.9284, Time 00:00:55,lr 0.01\n",
      "epoch 138, loss 0.04460, train_acc 0.9849, valid_acc 0.9290, Time 00:00:56,lr 0.01\n",
      "epoch 139, loss 0.05203, train_acc 0.9821, valid_acc 0.9315, Time 00:00:55,lr 0.01\n",
      "epoch 140, loss 0.04833, train_acc 0.9832, valid_acc 0.9216, Time 00:00:55,lr 0.01\n",
      "epoch 141, loss 0.04433, train_acc 0.9851, valid_acc 0.9331, Time 00:00:55,lr 0.01\n",
      "epoch 142, loss 0.04645, train_acc 0.9843, valid_acc 0.9234, Time 00:00:56,lr 0.01\n",
      "epoch 143, loss 0.04340, train_acc 0.9854, valid_acc 0.9286, Time 00:00:55,lr 0.01\n",
      "epoch 144, loss 0.04866, train_acc 0.9828, valid_acc 0.9302, Time 00:00:56,lr 0.01\n",
      "epoch 145, loss 0.05015, train_acc 0.9834, valid_acc 0.9280, Time 00:00:55,lr 0.01\n",
      "epoch 146, loss 0.04580, train_acc 0.9843, valid_acc 0.9310, Time 00:00:56,lr 0.01\n",
      "epoch 147, loss 0.04804, train_acc 0.9835, valid_acc 0.9258, Time 00:00:56,lr 0.01\n",
      "epoch 148, loss 0.04823, train_acc 0.9837, valid_acc 0.9259, Time 00:00:56,lr 0.01\n",
      "epoch 149, loss 0.04857, train_acc 0.9837, valid_acc 0.9285, Time 00:00:56,lr 0.01\n",
      "epoch 150, loss 0.02528, train_acc 0.9924, valid_acc 0.9435, Time 00:00:55,lr 0.001\n",
      "epoch 151, loss 0.01440, train_acc 0.9963, valid_acc 0.9431, Time 00:00:56,lr 0.001\n",
      "epoch 152, loss 0.01215, train_acc 0.9965, valid_acc 0.9442, Time 00:00:55,lr 0.001\n",
      "epoch 153, loss 0.00935, train_acc 0.9976, valid_acc 0.9453, Time 00:00:56,lr 0.001\n",
      "epoch 154, loss 0.00826, train_acc 0.9983, valid_acc 0.9451, Time 00:00:56,lr 0.001\n",
      "epoch 155, loss 0.00747, train_acc 0.9985, valid_acc 0.9453, Time 00:00:55,lr 0.001\n",
      "epoch 156, loss 0.00651, train_acc 0.9986, valid_acc 0.9444, Time 00:00:55,lr 0.001\n",
      "epoch 157, loss 0.00666, train_acc 0.9987, valid_acc 0.9455, Time 00:00:55,lr 0.001\n",
      "epoch 158, loss 0.00589, train_acc 0.9987, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 159, loss 0.00533, train_acc 0.9989, valid_acc 0.9463, Time 00:00:56,lr 0.001\n",
      "epoch 160, loss 0.00520, train_acc 0.9987, valid_acc 0.9466, Time 00:00:56,lr 0.001\n",
      "epoch 161, loss 0.00459, train_acc 0.9991, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 162, loss 0.00516, train_acc 0.9989, valid_acc 0.9456, Time 00:00:55,lr 0.001\n",
      "epoch 163, loss 0.00442, train_acc 0.9991, valid_acc 0.9462, Time 00:00:56,lr 0.001\n",
      "epoch 164, loss 0.00472, train_acc 0.9990, valid_acc 0.9467, Time 00:00:55,lr 0.001\n",
      "epoch 165, loss 0.00432, train_acc 0.9992, valid_acc 0.9462, Time 00:00:56,lr 0.001\n",
      "epoch 166, loss 0.00408, train_acc 0.9993, valid_acc 0.9473, Time 00:00:56,lr 0.001\n",
      "epoch 167, loss 0.00390, train_acc 0.9991, valid_acc 0.9471, Time 00:00:55,lr 0.001\n",
      "epoch 168, loss 0.00405, train_acc 0.9991, valid_acc 0.9457, Time 00:00:56,lr 0.001\n",
      "epoch 169, loss 0.00342, train_acc 0.9994, valid_acc 0.9453, Time 00:00:55,lr 0.001\n",
      "epoch 170, loss 0.00392, train_acc 0.9991, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 171, loss 0.00307, train_acc 0.9994, valid_acc 0.9472, Time 00:00:56,lr 0.001\n",
      "epoch 172, loss 0.00323, train_acc 0.9995, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 173, loss 0.00349, train_acc 0.9992, valid_acc 0.9480, Time 00:00:55,lr 0.001\n",
      "epoch 174, loss 0.00285, train_acc 0.9996, valid_acc 0.9486, Time 00:00:56,lr 0.001\n",
      "epoch 175, loss 0.00360, train_acc 0.9992, valid_acc 0.9475, Time 00:00:56,lr 0.001\n",
      "epoch 176, loss 0.00316, train_acc 0.9996, valid_acc 0.9482, Time 00:00:56,lr 0.001\n",
      "epoch 177, loss 0.00330, train_acc 0.9993, valid_acc 0.9486, Time 00:00:56,lr 0.001\n",
      "epoch 178, loss 0.00291, train_acc 0.9994, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 179, loss 0.00328, train_acc 0.9993, valid_acc 0.9477, Time 00:00:55,lr 0.001\n",
      "epoch 180, loss 0.00271, train_acc 0.9995, valid_acc 0.9472, Time 00:00:56,lr 0.001\n",
      "epoch 181, loss 0.00288, train_acc 0.9994, valid_acc 0.9485, Time 00:00:56,lr 0.001\n",
      "epoch 182, loss 0.00273, train_acc 0.9995, valid_acc 0.9481, Time 00:00:56,lr 0.001\n",
      "epoch 183, loss 0.00283, train_acc 0.9993, valid_acc 0.9480, Time 00:00:55,lr 0.001\n",
      "epoch 184, loss 0.00248, train_acc 0.9996, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 185, loss 0.00256, train_acc 0.9995, valid_acc 0.9480, Time 00:00:56,lr 0.001\n",
      "epoch 186, loss 0.00240, train_acc 0.9995, valid_acc 0.9469, Time 00:00:55,lr 0.001\n",
      "epoch 187, loss 0.00271, train_acc 0.9996, valid_acc 0.9491, Time 00:00:55,lr 0.001\n",
      "epoch 188, loss 0.00239, train_acc 0.9996, valid_acc 0.9476, Time 00:00:56,lr 0.001\n",
      "epoch 189, loss 0.00219, train_acc 0.9997, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 190, loss 0.00211, train_acc 0.9997, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 191, loss 0.00261, train_acc 0.9995, valid_acc 0.9468, Time 00:00:56,lr 0.001\n",
      "epoch 192, loss 0.00210, train_acc 0.9997, valid_acc 0.9472, Time 00:00:55,lr 0.001\n",
      "epoch 193, loss 0.00229, train_acc 0.9996, valid_acc 0.9477, Time 00:00:56,lr 0.001\n",
      "epoch 194, loss 0.00222, train_acc 0.9996, valid_acc 0.9486, Time 00:00:56,lr 0.001\n",
      "epoch 195, loss 0.00208, train_acc 0.9996, valid_acc 0.9490, Time 00:00:55,lr 0.001\n",
      "epoch 196, loss 0.00211, train_acc 0.9997, valid_acc 0.9479, Time 00:00:56,lr 0.001\n",
      "epoch 197, loss 0.00225, train_acc 0.9996, valid_acc 0.9476, Time 00:00:56,lr 0.001\n",
      "epoch 198, loss 0.00184, train_acc 0.9997, valid_acc 0.9473, Time 00:00:56,lr 0.001\n",
      "epoch 199, loss 0.00202, train_acc 0.9997, valid_acc 0.9479, Time 00:00:56,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 general lr policy train resnet18_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T18:12:46.262239Z",
     "start_time": "2018-03-04T18:12:46.254255Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v2_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v2(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features[0])\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[2:4])\n",
    "        net.add(*resnet.features[5:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T21:20:09.948413Z",
     "start_time": "2018-03-04T18:12:46.263723Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.44951, train_acc 0.1700, valid_acc 0.2141, Time 00:00:52,lr 0.1\n",
      "epoch 1, loss 1.86926, train_acc 0.2854, valid_acc 0.3249, Time 00:00:56,lr 0.1\n",
      "epoch 2, loss 1.68936, train_acc 0.3693, valid_acc 0.4108, Time 00:00:56,lr 0.1\n",
      "epoch 3, loss 1.49349, train_acc 0.4522, valid_acc 0.4833, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.28756, train_acc 0.5355, valid_acc 0.4992, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 1.08968, train_acc 0.6136, valid_acc 0.6383, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.91630, train_acc 0.6771, valid_acc 0.6946, Time 00:00:56,lr 0.1\n",
      "epoch 7, loss 0.79485, train_acc 0.7227, valid_acc 0.7397, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.71892, train_acc 0.7512, valid_acc 0.7302, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.66048, train_acc 0.7693, valid_acc 0.7255, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.62220, train_acc 0.7848, valid_acc 0.7608, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.59447, train_acc 0.7961, valid_acc 0.7898, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.56015, train_acc 0.8065, valid_acc 0.8082, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.53974, train_acc 0.8164, valid_acc 0.8009, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.51667, train_acc 0.8223, valid_acc 0.8149, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.50768, train_acc 0.8256, valid_acc 0.7942, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.49144, train_acc 0.8322, valid_acc 0.8099, Time 00:00:56,lr 0.1\n",
      "epoch 17, loss 0.47786, train_acc 0.8355, valid_acc 0.8245, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.49227, train_acc 0.8324, valid_acc 0.8198, Time 00:00:56,lr 0.1\n",
      "epoch 19, loss 0.45751, train_acc 0.8445, valid_acc 0.8412, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.45357, train_acc 0.8442, valid_acc 0.8123, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.44426, train_acc 0.8465, valid_acc 0.8173, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.42997, train_acc 0.8532, valid_acc 0.8631, Time 00:00:56,lr 0.1\n",
      "epoch 23, loss 0.42412, train_acc 0.8549, valid_acc 0.8350, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.42291, train_acc 0.8548, valid_acc 0.8455, Time 00:00:56,lr 0.1\n",
      "epoch 25, loss 0.41066, train_acc 0.8580, valid_acc 0.8565, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.39995, train_acc 0.8625, valid_acc 0.8302, Time 00:00:56,lr 0.1\n",
      "epoch 27, loss 0.39947, train_acc 0.8624, valid_acc 0.8342, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.39504, train_acc 0.8641, valid_acc 0.8619, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.38650, train_acc 0.8688, valid_acc 0.8529, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.38075, train_acc 0.8688, valid_acc 0.8649, Time 00:00:56,lr 0.1\n",
      "epoch 31, loss 0.38200, train_acc 0.8699, valid_acc 0.8489, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.37619, train_acc 0.8716, valid_acc 0.8620, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.36805, train_acc 0.8738, valid_acc 0.8418, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.36680, train_acc 0.8751, valid_acc 0.8586, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.36398, train_acc 0.8745, valid_acc 0.8426, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.36144, train_acc 0.8759, valid_acc 0.8554, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.36382, train_acc 0.8748, valid_acc 0.8499, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.35524, train_acc 0.8783, valid_acc 0.8435, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.35648, train_acc 0.8791, valid_acc 0.8483, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.35342, train_acc 0.8771, valid_acc 0.8595, Time 00:00:56,lr 0.1\n",
      "epoch 41, loss 0.34319, train_acc 0.8819, valid_acc 0.8546, Time 00:00:56,lr 0.1\n",
      "epoch 42, loss 0.34849, train_acc 0.8813, valid_acc 0.8763, Time 00:00:56,lr 0.1\n",
      "epoch 43, loss 0.34387, train_acc 0.8830, valid_acc 0.8559, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.34304, train_acc 0.8825, valid_acc 0.8425, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.34640, train_acc 0.8812, valid_acc 0.8577, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.34386, train_acc 0.8830, valid_acc 0.8669, Time 00:00:56,lr 0.1\n",
      "epoch 47, loss 0.34541, train_acc 0.8797, valid_acc 0.8602, Time 00:00:56,lr 0.1\n",
      "epoch 48, loss 0.33837, train_acc 0.8826, valid_acc 0.8406, Time 00:00:56,lr 0.1\n",
      "epoch 49, loss 0.33817, train_acc 0.8843, valid_acc 0.8596, Time 00:00:56,lr 0.1\n",
      "epoch 50, loss 0.33068, train_acc 0.8861, valid_acc 0.8688, Time 00:00:56,lr 0.1\n",
      "epoch 51, loss 0.33042, train_acc 0.8865, valid_acc 0.8732, Time 00:00:56,lr 0.1\n",
      "epoch 52, loss 0.32935, train_acc 0.8885, valid_acc 0.8560, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.33289, train_acc 0.8855, valid_acc 0.8453, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.32762, train_acc 0.8890, valid_acc 0.8629, Time 00:00:56,lr 0.1\n",
      "epoch 55, loss 0.32562, train_acc 0.8886, valid_acc 0.8745, Time 00:00:56,lr 0.1\n",
      "epoch 56, loss 0.32622, train_acc 0.8895, valid_acc 0.8809, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.32513, train_acc 0.8890, valid_acc 0.8741, Time 00:00:56,lr 0.1\n",
      "epoch 58, loss 0.32801, train_acc 0.8882, valid_acc 0.8769, Time 00:00:56,lr 0.1\n",
      "epoch 59, loss 0.32038, train_acc 0.8905, valid_acc 0.8665, Time 00:00:56,lr 0.1\n",
      "epoch 60, loss 0.32088, train_acc 0.8897, valid_acc 0.8747, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.32201, train_acc 0.8899, valid_acc 0.8699, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.32008, train_acc 0.8901, valid_acc 0.8592, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.31851, train_acc 0.8914, valid_acc 0.8809, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.31505, train_acc 0.8915, valid_acc 0.8750, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.31955, train_acc 0.8915, valid_acc 0.8677, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.31765, train_acc 0.8909, valid_acc 0.8683, Time 00:00:56,lr 0.1\n",
      "epoch 67, loss 0.32005, train_acc 0.8906, valid_acc 0.8666, Time 00:00:56,lr 0.1\n",
      "epoch 68, loss 0.31254, train_acc 0.8927, valid_acc 0.8861, Time 00:00:56,lr 0.1\n",
      "epoch 69, loss 0.30988, train_acc 0.8934, valid_acc 0.8846, Time 00:00:56,lr 0.1\n",
      "epoch 70, loss 0.31350, train_acc 0.8937, valid_acc 0.8795, Time 00:00:56,lr 0.1\n",
      "epoch 71, loss 0.31071, train_acc 0.8928, valid_acc 0.8512, Time 00:00:56,lr 0.1\n",
      "epoch 72, loss 0.31702, train_acc 0.8909, valid_acc 0.8794, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.30821, train_acc 0.8951, valid_acc 0.8532, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.30979, train_acc 0.8936, valid_acc 0.7775, Time 00:00:56,lr 0.1\n",
      "epoch 75, loss 0.30905, train_acc 0.8950, valid_acc 0.8663, Time 00:00:56,lr 0.1\n",
      "epoch 76, loss 0.30563, train_acc 0.8947, valid_acc 0.8680, Time 00:00:56,lr 0.1\n",
      "epoch 77, loss 0.30650, train_acc 0.8947, valid_acc 0.8465, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.30130, train_acc 0.8974, valid_acc 0.8824, Time 00:00:56,lr 0.1\n",
      "epoch 79, loss 0.30596, train_acc 0.8963, valid_acc 0.8652, Time 00:00:56,lr 0.1\n",
      "epoch 80, loss 0.17533, train_acc 0.9401, valid_acc 0.9288, Time 00:00:56,lr 0.01\n",
      "epoch 81, loss 0.12621, train_acc 0.9566, valid_acc 0.9311, Time 00:00:56,lr 0.01\n",
      "epoch 82, loss 0.10747, train_acc 0.9627, valid_acc 0.9352, Time 00:00:56,lr 0.01\n",
      "epoch 83, loss 0.09768, train_acc 0.9671, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 84, loss 0.08784, train_acc 0.9699, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 85, loss 0.07791, train_acc 0.9734, valid_acc 0.9379, Time 00:00:56,lr 0.01\n",
      "epoch 86, loss 0.07193, train_acc 0.9752, valid_acc 0.9375, Time 00:00:56,lr 0.01\n",
      "epoch 87, loss 0.06827, train_acc 0.9765, valid_acc 0.9379, Time 00:00:56,lr 0.01\n",
      "epoch 88, loss 0.06251, train_acc 0.9786, valid_acc 0.9407, Time 00:00:56,lr 0.01\n",
      "epoch 89, loss 0.05615, train_acc 0.9811, valid_acc 0.9362, Time 00:00:56,lr 0.01\n",
      "epoch 90, loss 0.05426, train_acc 0.9811, valid_acc 0.9380, Time 00:00:56,lr 0.01\n",
      "epoch 91, loss 0.05197, train_acc 0.9825, valid_acc 0.9376, Time 00:00:56,lr 0.01\n",
      "epoch 92, loss 0.04575, train_acc 0.9843, valid_acc 0.9390, Time 00:00:56,lr 0.01\n",
      "epoch 93, loss 0.04560, train_acc 0.9843, valid_acc 0.9365, Time 00:00:56,lr 0.01\n",
      "epoch 94, loss 0.04354, train_acc 0.9850, valid_acc 0.9388, Time 00:00:56,lr 0.01\n",
      "epoch 95, loss 0.04189, train_acc 0.9855, valid_acc 0.9404, Time 00:00:56,lr 0.01\n",
      "epoch 96, loss 0.04202, train_acc 0.9863, valid_acc 0.9385, Time 00:00:56,lr 0.01\n",
      "epoch 97, loss 0.03752, train_acc 0.9877, valid_acc 0.9356, Time 00:00:56,lr 0.01\n",
      "epoch 98, loss 0.03980, train_acc 0.9863, valid_acc 0.9395, Time 00:00:56,lr 0.01\n",
      "epoch 99, loss 0.03526, train_acc 0.9883, valid_acc 0.9376, Time 00:00:56,lr 0.01\n",
      "epoch 100, loss 0.03696, train_acc 0.9875, valid_acc 0.9352, Time 00:00:56,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.03855, train_acc 0.9868, valid_acc 0.9370, Time 00:00:56,lr 0.01\n",
      "epoch 102, loss 0.03788, train_acc 0.9867, valid_acc 0.9364, Time 00:00:56,lr 0.01\n",
      "epoch 103, loss 0.03558, train_acc 0.9879, valid_acc 0.9353, Time 00:00:56,lr 0.01\n",
      "epoch 104, loss 0.03517, train_acc 0.9886, valid_acc 0.9349, Time 00:00:56,lr 0.01\n",
      "epoch 105, loss 0.03818, train_acc 0.9870, valid_acc 0.9386, Time 00:00:56,lr 0.01\n",
      "epoch 106, loss 0.03449, train_acc 0.9884, valid_acc 0.9323, Time 00:00:56,lr 0.01\n",
      "epoch 107, loss 0.03584, train_acc 0.9881, valid_acc 0.9361, Time 00:00:56,lr 0.01\n",
      "epoch 108, loss 0.03502, train_acc 0.9877, valid_acc 0.9363, Time 00:00:56,lr 0.01\n",
      "epoch 109, loss 0.03759, train_acc 0.9875, valid_acc 0.9258, Time 00:00:56,lr 0.01\n",
      "epoch 110, loss 0.03876, train_acc 0.9870, valid_acc 0.9316, Time 00:00:56,lr 0.01\n",
      "epoch 111, loss 0.03911, train_acc 0.9871, valid_acc 0.9317, Time 00:00:56,lr 0.01\n",
      "epoch 112, loss 0.03818, train_acc 0.9868, valid_acc 0.9322, Time 00:00:56,lr 0.01\n",
      "epoch 113, loss 0.03854, train_acc 0.9872, valid_acc 0.9324, Time 00:00:56,lr 0.01\n",
      "epoch 114, loss 0.03957, train_acc 0.9870, valid_acc 0.9312, Time 00:00:56,lr 0.01\n",
      "epoch 115, loss 0.03727, train_acc 0.9877, valid_acc 0.9328, Time 00:00:56,lr 0.01\n",
      "epoch 116, loss 0.03950, train_acc 0.9867, valid_acc 0.9341, Time 00:00:56,lr 0.01\n",
      "epoch 117, loss 0.03904, train_acc 0.9873, valid_acc 0.9369, Time 00:00:56,lr 0.01\n",
      "epoch 118, loss 0.04037, train_acc 0.9864, valid_acc 0.9323, Time 00:00:56,lr 0.01\n",
      "epoch 119, loss 0.04153, train_acc 0.9859, valid_acc 0.9333, Time 00:00:56,lr 0.01\n",
      "epoch 120, loss 0.04340, train_acc 0.9856, valid_acc 0.9303, Time 00:00:56,lr 0.01\n",
      "epoch 121, loss 0.03868, train_acc 0.9870, valid_acc 0.9308, Time 00:00:56,lr 0.01\n",
      "epoch 122, loss 0.04523, train_acc 0.9845, valid_acc 0.9321, Time 00:00:56,lr 0.01\n",
      "epoch 123, loss 0.04460, train_acc 0.9852, valid_acc 0.9260, Time 00:00:56,lr 0.01\n",
      "epoch 124, loss 0.04559, train_acc 0.9850, valid_acc 0.9319, Time 00:00:56,lr 0.01\n",
      "epoch 125, loss 0.04768, train_acc 0.9842, valid_acc 0.9292, Time 00:00:56,lr 0.01\n",
      "epoch 126, loss 0.04399, train_acc 0.9851, valid_acc 0.9295, Time 00:00:56,lr 0.01\n",
      "epoch 127, loss 0.04340, train_acc 0.9852, valid_acc 0.9296, Time 00:00:56,lr 0.01\n",
      "epoch 128, loss 0.04687, train_acc 0.9844, valid_acc 0.9247, Time 00:00:56,lr 0.01\n",
      "epoch 129, loss 0.04552, train_acc 0.9849, valid_acc 0.9326, Time 00:00:56,lr 0.01\n",
      "epoch 130, loss 0.04505, train_acc 0.9845, valid_acc 0.9259, Time 00:00:56,lr 0.01\n",
      "epoch 131, loss 0.04696, train_acc 0.9838, valid_acc 0.9226, Time 00:00:56,lr 0.01\n",
      "epoch 132, loss 0.04853, train_acc 0.9839, valid_acc 0.9269, Time 00:00:56,lr 0.01\n",
      "epoch 133, loss 0.05106, train_acc 0.9828, valid_acc 0.9255, Time 00:00:56,lr 0.01\n",
      "epoch 134, loss 0.04962, train_acc 0.9829, valid_acc 0.9282, Time 00:00:56,lr 0.01\n",
      "epoch 135, loss 0.04603, train_acc 0.9844, valid_acc 0.9287, Time 00:00:56,lr 0.01\n",
      "epoch 136, loss 0.05080, train_acc 0.9827, valid_acc 0.9321, Time 00:00:56,lr 0.01\n",
      "epoch 137, loss 0.04662, train_acc 0.9839, valid_acc 0.9287, Time 00:00:56,lr 0.01\n",
      "epoch 138, loss 0.04839, train_acc 0.9832, valid_acc 0.9236, Time 00:00:56,lr 0.01\n",
      "epoch 139, loss 0.05286, train_acc 0.9814, valid_acc 0.9283, Time 00:00:56,lr 0.01\n",
      "epoch 140, loss 0.04797, train_acc 0.9834, valid_acc 0.9238, Time 00:00:56,lr 0.01\n",
      "epoch 141, loss 0.04590, train_acc 0.9841, valid_acc 0.9296, Time 00:00:56,lr 0.01\n",
      "epoch 142, loss 0.05083, train_acc 0.9826, valid_acc 0.9292, Time 00:00:56,lr 0.01\n",
      "epoch 143, loss 0.04744, train_acc 0.9837, valid_acc 0.9221, Time 00:00:56,lr 0.01\n",
      "epoch 144, loss 0.04808, train_acc 0.9835, valid_acc 0.9270, Time 00:00:56,lr 0.01\n",
      "epoch 145, loss 0.05711, train_acc 0.9806, valid_acc 0.9244, Time 00:00:56,lr 0.01\n",
      "epoch 146, loss 0.05352, train_acc 0.9817, valid_acc 0.9268, Time 00:00:56,lr 0.01\n",
      "epoch 147, loss 0.05069, train_acc 0.9826, valid_acc 0.9252, Time 00:00:56,lr 0.01\n",
      "epoch 148, loss 0.05457, train_acc 0.9812, valid_acc 0.9238, Time 00:00:56,lr 0.01\n",
      "epoch 149, loss 0.04820, train_acc 0.9835, valid_acc 0.9207, Time 00:00:56,lr 0.01\n",
      "epoch 150, loss 0.02426, train_acc 0.9927, valid_acc 0.9386, Time 00:00:56,lr 0.001\n",
      "epoch 151, loss 0.01617, train_acc 0.9954, valid_acc 0.9411, Time 00:00:56,lr 0.001\n",
      "epoch 152, loss 0.01236, train_acc 0.9969, valid_acc 0.9421, Time 00:00:56,lr 0.001\n",
      "epoch 153, loss 0.01000, train_acc 0.9978, valid_acc 0.9432, Time 00:00:56,lr 0.001\n",
      "epoch 154, loss 0.00899, train_acc 0.9979, valid_acc 0.9432, Time 00:00:56,lr 0.001\n",
      "epoch 155, loss 0.00864, train_acc 0.9978, valid_acc 0.9441, Time 00:00:56,lr 0.001\n",
      "epoch 156, loss 0.00746, train_acc 0.9982, valid_acc 0.9453, Time 00:00:56,lr 0.001\n",
      "epoch 157, loss 0.00767, train_acc 0.9983, valid_acc 0.9448, Time 00:00:56,lr 0.001\n",
      "epoch 158, loss 0.00699, train_acc 0.9985, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 159, loss 0.00615, train_acc 0.9988, valid_acc 0.9464, Time 00:00:56,lr 0.001\n",
      "epoch 160, loss 0.00609, train_acc 0.9987, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 161, loss 0.00539, train_acc 0.9989, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 162, loss 0.00604, train_acc 0.9984, valid_acc 0.9447, Time 00:00:56,lr 0.001\n",
      "epoch 163, loss 0.00496, train_acc 0.9990, valid_acc 0.9453, Time 00:00:56,lr 0.001\n",
      "epoch 164, loss 0.00543, train_acc 0.9988, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 165, loss 0.00462, train_acc 0.9989, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 166, loss 0.00484, train_acc 0.9989, valid_acc 0.9454, Time 00:00:56,lr 0.001\n",
      "epoch 167, loss 0.00405, train_acc 0.9992, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 168, loss 0.00447, train_acc 0.9990, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 169, loss 0.00409, train_acc 0.9991, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 170, loss 0.00435, train_acc 0.9990, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 171, loss 0.00335, train_acc 0.9993, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 172, loss 0.00361, train_acc 0.9993, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 173, loss 0.00369, train_acc 0.9994, valid_acc 0.9470, Time 00:00:56,lr 0.001\n",
      "epoch 174, loss 0.00371, train_acc 0.9992, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 175, loss 0.00349, train_acc 0.9993, valid_acc 0.9463, Time 00:00:56,lr 0.001\n",
      "epoch 176, loss 0.00300, train_acc 0.9995, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 177, loss 0.00323, train_acc 0.9994, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 178, loss 0.00298, train_acc 0.9994, valid_acc 0.9462, Time 00:00:55,lr 0.001\n",
      "epoch 179, loss 0.00293, train_acc 0.9995, valid_acc 0.9467, Time 00:00:56,lr 0.001\n",
      "epoch 180, loss 0.00275, train_acc 0.9996, valid_acc 0.9463, Time 00:00:56,lr 0.001\n",
      "epoch 181, loss 0.00334, train_acc 0.9992, valid_acc 0.9469, Time 00:00:56,lr 0.001\n",
      "epoch 182, loss 0.00327, train_acc 0.9994, valid_acc 0.9464, Time 00:00:56,lr 0.001\n",
      "epoch 183, loss 0.00269, train_acc 0.9995, valid_acc 0.9459, Time 00:00:55,lr 0.001\n",
      "epoch 184, loss 0.00283, train_acc 0.9995, valid_acc 0.9472, Time 00:00:56,lr 0.001\n",
      "epoch 185, loss 0.00267, train_acc 0.9996, valid_acc 0.9475, Time 00:00:56,lr 0.001\n",
      "epoch 186, loss 0.00273, train_acc 0.9996, valid_acc 0.9460, Time 00:00:56,lr 0.001\n",
      "epoch 187, loss 0.00273, train_acc 0.9995, valid_acc 0.9479, Time 00:00:56,lr 0.001\n",
      "epoch 188, loss 0.00232, train_acc 0.9997, valid_acc 0.9450, Time 00:00:56,lr 0.001\n",
      "epoch 189, loss 0.00264, train_acc 0.9995, valid_acc 0.9464, Time 00:00:56,lr 0.001\n",
      "epoch 190, loss 0.00256, train_acc 0.9994, valid_acc 0.9470, Time 00:00:56,lr 0.001\n",
      "epoch 191, loss 0.00225, train_acc 0.9997, valid_acc 0.9462, Time 00:00:56,lr 0.001\n",
      "epoch 192, loss 0.00242, train_acc 0.9997, valid_acc 0.9455, Time 00:00:56,lr 0.001\n",
      "epoch 193, loss 0.00261, train_acc 0.9996, valid_acc 0.9456, Time 00:00:56,lr 0.001\n",
      "epoch 194, loss 0.00224, train_acc 0.9997, valid_acc 0.9459, Time 00:00:56,lr 0.001\n",
      "epoch 195, loss 0.00238, train_acc 0.9995, valid_acc 0.9461, Time 00:00:56,lr 0.001\n",
      "epoch 196, loss 0.00232, train_acc 0.9996, valid_acc 0.9474, Time 00:00:56,lr 0.001\n",
      "epoch 197, loss 0.00220, train_acc 0.9996, valid_acc 0.9468, Time 00:00:56,lr 0.001\n",
      "epoch 198, loss 0.00265, train_acc 0.9995, valid_acc 0.9454, Time 00:00:56,lr 0.001\n",
      "epoch 199, loss 0.00222, train_acc 0.9996, valid_acc 0.9473, Time 00:00:56,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v2_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 general lr policy train my resnet18v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T03:45:19.543171Z",
     "start_time": "2018-03-05T02:01:38.930989Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.75249, train_acc 0.3416, valid_acc 0.4678, Time 00:00:31,lr 0.1\n",
      "epoch 1, loss 1.23604, train_acc 0.5568, valid_acc 0.5724, Time 00:00:32,lr 0.1\n",
      "epoch 2, loss 0.93259, train_acc 0.6715, valid_acc 0.7011, Time 00:00:32,lr 0.1\n",
      "epoch 3, loss 0.78639, train_acc 0.7277, valid_acc 0.6987, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 0.68671, train_acc 0.7642, valid_acc 0.7434, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 0.62936, train_acc 0.7856, valid_acc 0.7396, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 0.59543, train_acc 0.7951, valid_acc 0.7949, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 0.56563, train_acc 0.8073, valid_acc 0.7511, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.54190, train_acc 0.8125, valid_acc 0.7714, Time 00:00:30,lr 0.1\n",
      "epoch 9, loss 0.51894, train_acc 0.8220, valid_acc 0.8141, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.50352, train_acc 0.8259, valid_acc 0.7803, Time 00:00:30,lr 0.1\n",
      "epoch 11, loss 0.49073, train_acc 0.8327, valid_acc 0.8127, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 0.47917, train_acc 0.8354, valid_acc 0.8078, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.47001, train_acc 0.8364, valid_acc 0.8317, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.45984, train_acc 0.8415, valid_acc 0.8142, Time 00:00:30,lr 0.1\n",
      "epoch 15, loss 0.45590, train_acc 0.8441, valid_acc 0.7748, Time 00:00:30,lr 0.1\n",
      "epoch 16, loss 0.44058, train_acc 0.8479, valid_acc 0.8230, Time 00:00:30,lr 0.1\n",
      "epoch 17, loss 0.43728, train_acc 0.8481, valid_acc 0.8120, Time 00:00:30,lr 0.1\n",
      "epoch 18, loss 0.43228, train_acc 0.8511, valid_acc 0.7777, Time 00:00:30,lr 0.1\n",
      "epoch 19, loss 0.42770, train_acc 0.8548, valid_acc 0.8265, Time 00:00:30,lr 0.1\n",
      "epoch 20, loss 0.42149, train_acc 0.8558, valid_acc 0.8383, Time 00:00:30,lr 0.1\n",
      "epoch 21, loss 0.41593, train_acc 0.8559, valid_acc 0.8471, Time 00:00:30,lr 0.1\n",
      "epoch 22, loss 0.41718, train_acc 0.8565, valid_acc 0.8438, Time 00:00:30,lr 0.1\n",
      "epoch 23, loss 0.41097, train_acc 0.8591, valid_acc 0.8104, Time 00:00:30,lr 0.1\n",
      "epoch 24, loss 0.40569, train_acc 0.8603, valid_acc 0.8265, Time 00:00:30,lr 0.1\n",
      "epoch 25, loss 0.39741, train_acc 0.8625, valid_acc 0.8375, Time 00:00:30,lr 0.1\n",
      "epoch 26, loss 0.39868, train_acc 0.8648, valid_acc 0.8310, Time 00:00:30,lr 0.1\n",
      "epoch 27, loss 0.39217, train_acc 0.8654, valid_acc 0.8222, Time 00:00:30,lr 0.1\n",
      "epoch 28, loss 0.39285, train_acc 0.8661, valid_acc 0.8436, Time 00:00:30,lr 0.1\n",
      "epoch 29, loss 0.38964, train_acc 0.8661, valid_acc 0.8419, Time 00:00:30,lr 0.1\n",
      "epoch 30, loss 0.38233, train_acc 0.8675, valid_acc 0.8302, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.38502, train_acc 0.8687, valid_acc 0.8512, Time 00:00:30,lr 0.1\n",
      "epoch 32, loss 0.37983, train_acc 0.8693, valid_acc 0.8574, Time 00:00:30,lr 0.1\n",
      "epoch 33, loss 0.37806, train_acc 0.8686, valid_acc 0.8553, Time 00:00:30,lr 0.1\n",
      "epoch 34, loss 0.37425, train_acc 0.8709, valid_acc 0.8625, Time 00:00:30,lr 0.1\n",
      "epoch 35, loss 0.37156, train_acc 0.8724, valid_acc 0.8599, Time 00:00:30,lr 0.1\n",
      "epoch 36, loss 0.37042, train_acc 0.8747, valid_acc 0.8524, Time 00:00:30,lr 0.1\n",
      "epoch 37, loss 0.37193, train_acc 0.8734, valid_acc 0.8277, Time 00:00:30,lr 0.1\n",
      "epoch 38, loss 0.37142, train_acc 0.8734, valid_acc 0.8223, Time 00:00:30,lr 0.1\n",
      "epoch 39, loss 0.36700, train_acc 0.8739, valid_acc 0.8126, Time 00:00:30,lr 0.1\n",
      "epoch 40, loss 0.36533, train_acc 0.8738, valid_acc 0.8582, Time 00:00:30,lr 0.1\n",
      "epoch 41, loss 0.36709, train_acc 0.8759, valid_acc 0.8450, Time 00:00:30,lr 0.1\n",
      "epoch 42, loss 0.36452, train_acc 0.8751, valid_acc 0.8551, Time 00:00:30,lr 0.1\n",
      "epoch 43, loss 0.36363, train_acc 0.8752, valid_acc 0.8559, Time 00:00:30,lr 0.1\n",
      "epoch 44, loss 0.36318, train_acc 0.8768, valid_acc 0.8331, Time 00:00:30,lr 0.1\n",
      "epoch 45, loss 0.36180, train_acc 0.8765, valid_acc 0.8542, Time 00:00:30,lr 0.1\n",
      "epoch 46, loss 0.35892, train_acc 0.8770, valid_acc 0.8296, Time 00:00:30,lr 0.1\n",
      "epoch 47, loss 0.36113, train_acc 0.8765, valid_acc 0.8493, Time 00:00:30,lr 0.1\n",
      "epoch 48, loss 0.35691, train_acc 0.8780, valid_acc 0.8250, Time 00:00:30,lr 0.1\n",
      "epoch 49, loss 0.35613, train_acc 0.8781, valid_acc 0.8409, Time 00:00:30,lr 0.1\n",
      "epoch 50, loss 0.35971, train_acc 0.8755, valid_acc 0.8462, Time 00:00:30,lr 0.1\n",
      "epoch 51, loss 0.35123, train_acc 0.8807, valid_acc 0.8555, Time 00:00:30,lr 0.1\n",
      "epoch 52, loss 0.35600, train_acc 0.8809, valid_acc 0.8634, Time 00:00:30,lr 0.1\n",
      "epoch 53, loss 0.34899, train_acc 0.8804, valid_acc 0.8415, Time 00:00:30,lr 0.1\n",
      "epoch 54, loss 0.35340, train_acc 0.8780, valid_acc 0.8486, Time 00:00:30,lr 0.1\n",
      "epoch 55, loss 0.35170, train_acc 0.8798, valid_acc 0.8252, Time 00:00:30,lr 0.1\n",
      "epoch 56, loss 0.35164, train_acc 0.8809, valid_acc 0.8500, Time 00:00:30,lr 0.1\n",
      "epoch 57, loss 0.34906, train_acc 0.8793, valid_acc 0.8432, Time 00:00:30,lr 0.1\n",
      "epoch 58, loss 0.35115, train_acc 0.8790, valid_acc 0.8634, Time 00:00:30,lr 0.1\n",
      "epoch 59, loss 0.35323, train_acc 0.8779, valid_acc 0.8571, Time 00:00:30,lr 0.1\n",
      "epoch 60, loss 0.34844, train_acc 0.8792, valid_acc 0.8580, Time 00:00:30,lr 0.1\n",
      "epoch 61, loss 0.34465, train_acc 0.8826, valid_acc 0.8559, Time 00:00:30,lr 0.1\n",
      "epoch 62, loss 0.34941, train_acc 0.8816, valid_acc 0.8703, Time 00:00:30,lr 0.1\n",
      "epoch 63, loss 0.35071, train_acc 0.8794, valid_acc 0.8159, Time 00:00:30,lr 0.1\n",
      "epoch 64, loss 0.34600, train_acc 0.8824, valid_acc 0.8438, Time 00:00:30,lr 0.1\n",
      "epoch 65, loss 0.34546, train_acc 0.8836, valid_acc 0.8596, Time 00:00:30,lr 0.1\n",
      "epoch 66, loss 0.34340, train_acc 0.8827, valid_acc 0.8552, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.34738, train_acc 0.8812, valid_acc 0.8569, Time 00:00:30,lr 0.1\n",
      "epoch 68, loss 0.34896, train_acc 0.8806, valid_acc 0.8501, Time 00:00:30,lr 0.1\n",
      "epoch 69, loss 0.34608, train_acc 0.8815, valid_acc 0.8504, Time 00:00:30,lr 0.1\n",
      "epoch 70, loss 0.34501, train_acc 0.8821, valid_acc 0.8615, Time 00:00:30,lr 0.1\n",
      "epoch 71, loss 0.34624, train_acc 0.8812, valid_acc 0.8541, Time 00:00:30,lr 0.1\n",
      "epoch 72, loss 0.34812, train_acc 0.8805, valid_acc 0.8727, Time 00:00:30,lr 0.1\n",
      "epoch 73, loss 0.34509, train_acc 0.8817, valid_acc 0.8514, Time 00:00:30,lr 0.1\n",
      "epoch 74, loss 0.34422, train_acc 0.8802, valid_acc 0.8745, Time 00:00:30,lr 0.1\n",
      "epoch 75, loss 0.34185, train_acc 0.8834, valid_acc 0.8565, Time 00:00:30,lr 0.1\n",
      "epoch 76, loss 0.34527, train_acc 0.8807, valid_acc 0.8577, Time 00:00:30,lr 0.1\n",
      "epoch 77, loss 0.33983, train_acc 0.8838, valid_acc 0.8720, Time 00:00:30,lr 0.1\n",
      "epoch 78, loss 0.33928, train_acc 0.8850, valid_acc 0.8603, Time 00:00:30,lr 0.1\n",
      "epoch 79, loss 0.34337, train_acc 0.8827, valid_acc 0.8280, Time 00:00:30,lr 0.1\n",
      "epoch 80, loss 0.19619, train_acc 0.9336, valid_acc 0.9207, Time 00:00:30,lr 0.01\n",
      "epoch 81, loss 0.14760, train_acc 0.9501, valid_acc 0.9249, Time 00:00:30,lr 0.01\n",
      "epoch 82, loss 0.12989, train_acc 0.9549, valid_acc 0.9265, Time 00:00:30,lr 0.01\n",
      "epoch 83, loss 0.11951, train_acc 0.9593, valid_acc 0.9292, Time 00:00:30,lr 0.01\n",
      "epoch 84, loss 0.10738, train_acc 0.9632, valid_acc 0.9288, Time 00:00:30,lr 0.01\n",
      "epoch 85, loss 0.10071, train_acc 0.9657, valid_acc 0.9295, Time 00:00:30,lr 0.01\n",
      "epoch 86, loss 0.09417, train_acc 0.9681, valid_acc 0.9312, Time 00:00:30,lr 0.01\n",
      "epoch 87, loss 0.08605, train_acc 0.9708, valid_acc 0.9300, Time 00:00:30,lr 0.01\n",
      "epoch 88, loss 0.08259, train_acc 0.9722, valid_acc 0.9313, Time 00:00:30,lr 0.01\n",
      "epoch 89, loss 0.07684, train_acc 0.9739, valid_acc 0.9288, Time 00:00:31,lr 0.01\n",
      "epoch 90, loss 0.07300, train_acc 0.9756, valid_acc 0.9294, Time 00:00:30,lr 0.01\n",
      "epoch 91, loss 0.07099, train_acc 0.9757, valid_acc 0.9294, Time 00:00:30,lr 0.01\n",
      "epoch 92, loss 0.06592, train_acc 0.9776, valid_acc 0.9317, Time 00:00:30,lr 0.01\n",
      "epoch 93, loss 0.06440, train_acc 0.9783, valid_acc 0.9258, Time 00:00:30,lr 0.01\n",
      "epoch 94, loss 0.06085, train_acc 0.9799, valid_acc 0.9287, Time 00:00:30,lr 0.01\n",
      "epoch 95, loss 0.06026, train_acc 0.9803, valid_acc 0.9313, Time 00:00:30,lr 0.01\n",
      "epoch 96, loss 0.05759, train_acc 0.9805, valid_acc 0.9257, Time 00:00:30,lr 0.01\n",
      "epoch 97, loss 0.05517, train_acc 0.9812, valid_acc 0.9270, Time 00:00:30,lr 0.01\n",
      "epoch 98, loss 0.05672, train_acc 0.9807, valid_acc 0.9301, Time 00:00:30,lr 0.01\n",
      "epoch 99, loss 0.05231, train_acc 0.9819, valid_acc 0.9292, Time 00:00:30,lr 0.01\n",
      "epoch 100, loss 0.05057, train_acc 0.9828, valid_acc 0.9245, Time 00:00:30,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.05123, train_acc 0.9828, valid_acc 0.9259, Time 00:00:30,lr 0.01\n",
      "epoch 102, loss 0.04977, train_acc 0.9830, valid_acc 0.9254, Time 00:00:30,lr 0.01\n",
      "epoch 103, loss 0.05176, train_acc 0.9827, valid_acc 0.9254, Time 00:00:30,lr 0.01\n",
      "epoch 104, loss 0.05073, train_acc 0.9826, valid_acc 0.9237, Time 00:00:30,lr 0.01\n",
      "epoch 105, loss 0.05045, train_acc 0.9829, valid_acc 0.9311, Time 00:00:30,lr 0.01\n",
      "epoch 106, loss 0.04769, train_acc 0.9841, valid_acc 0.9267, Time 00:00:35,lr 0.01\n",
      "epoch 107, loss 0.05069, train_acc 0.9823, valid_acc 0.9261, Time 00:00:34,lr 0.01\n",
      "epoch 108, loss 0.04908, train_acc 0.9833, valid_acc 0.9308, Time 00:00:31,lr 0.01\n",
      "epoch 109, loss 0.05262, train_acc 0.9825, valid_acc 0.9267, Time 00:00:30,lr 0.01\n",
      "epoch 110, loss 0.05038, train_acc 0.9829, valid_acc 0.9237, Time 00:00:30,lr 0.01\n",
      "epoch 111, loss 0.04573, train_acc 0.9848, valid_acc 0.9262, Time 00:00:30,lr 0.01\n",
      "epoch 112, loss 0.05370, train_acc 0.9811, valid_acc 0.9257, Time 00:00:30,lr 0.01\n",
      "epoch 113, loss 0.05240, train_acc 0.9824, valid_acc 0.9216, Time 00:00:30,lr 0.01\n",
      "epoch 114, loss 0.05037, train_acc 0.9833, valid_acc 0.9265, Time 00:00:30,lr 0.01\n",
      "epoch 115, loss 0.04898, train_acc 0.9839, valid_acc 0.9200, Time 00:00:30,lr 0.01\n",
      "epoch 116, loss 0.05494, train_acc 0.9815, valid_acc 0.9220, Time 00:00:30,lr 0.01\n",
      "epoch 117, loss 0.05175, train_acc 0.9825, valid_acc 0.9261, Time 00:00:30,lr 0.01\n",
      "epoch 118, loss 0.05293, train_acc 0.9816, valid_acc 0.9246, Time 00:00:31,lr 0.01\n",
      "epoch 119, loss 0.05280, train_acc 0.9816, valid_acc 0.9216, Time 00:00:30,lr 0.01\n",
      "epoch 120, loss 0.05557, train_acc 0.9807, valid_acc 0.9241, Time 00:00:30,lr 0.01\n",
      "epoch 121, loss 0.05531, train_acc 0.9812, valid_acc 0.9272, Time 00:00:30,lr 0.01\n",
      "epoch 122, loss 0.05505, train_acc 0.9816, valid_acc 0.9233, Time 00:00:30,lr 0.01\n",
      "epoch 123, loss 0.05293, train_acc 0.9821, valid_acc 0.9178, Time 00:00:30,lr 0.01\n",
      "epoch 124, loss 0.05691, train_acc 0.9807, valid_acc 0.9250, Time 00:00:30,lr 0.01\n",
      "epoch 125, loss 0.05509, train_acc 0.9815, valid_acc 0.9178, Time 00:00:30,lr 0.01\n",
      "epoch 126, loss 0.05544, train_acc 0.9806, valid_acc 0.9197, Time 00:00:30,lr 0.01\n",
      "epoch 127, loss 0.05728, train_acc 0.9800, valid_acc 0.9217, Time 00:00:30,lr 0.01\n",
      "epoch 128, loss 0.05760, train_acc 0.9804, valid_acc 0.9254, Time 00:00:30,lr 0.01\n",
      "epoch 129, loss 0.05628, train_acc 0.9812, valid_acc 0.9191, Time 00:00:30,lr 0.01\n",
      "epoch 130, loss 0.05874, train_acc 0.9795, valid_acc 0.9190, Time 00:00:30,lr 0.01\n",
      "epoch 131, loss 0.05753, train_acc 0.9803, valid_acc 0.9250, Time 00:00:30,lr 0.01\n",
      "epoch 132, loss 0.05882, train_acc 0.9794, valid_acc 0.9200, Time 00:00:30,lr 0.01\n",
      "epoch 133, loss 0.06110, train_acc 0.9796, valid_acc 0.9173, Time 00:00:30,lr 0.01\n",
      "epoch 134, loss 0.06017, train_acc 0.9798, valid_acc 0.9207, Time 00:00:30,lr 0.01\n",
      "epoch 135, loss 0.05784, train_acc 0.9803, valid_acc 0.9088, Time 00:00:30,lr 0.01\n",
      "epoch 136, loss 0.06184, train_acc 0.9790, valid_acc 0.9193, Time 00:00:30,lr 0.01\n",
      "epoch 137, loss 0.05722, train_acc 0.9803, valid_acc 0.9279, Time 00:00:30,lr 0.01\n",
      "epoch 138, loss 0.05880, train_acc 0.9800, valid_acc 0.9203, Time 00:00:30,lr 0.01\n",
      "epoch 139, loss 0.05727, train_acc 0.9806, valid_acc 0.9189, Time 00:00:30,lr 0.01\n",
      "epoch 140, loss 0.06348, train_acc 0.9784, valid_acc 0.9242, Time 00:00:30,lr 0.01\n",
      "epoch 141, loss 0.05603, train_acc 0.9808, valid_acc 0.9251, Time 00:00:30,lr 0.01\n",
      "epoch 142, loss 0.05919, train_acc 0.9797, valid_acc 0.9173, Time 00:00:30,lr 0.01\n",
      "epoch 143, loss 0.06109, train_acc 0.9793, valid_acc 0.9140, Time 00:00:30,lr 0.01\n",
      "epoch 144, loss 0.05916, train_acc 0.9798, valid_acc 0.9181, Time 00:00:30,lr 0.01\n",
      "epoch 145, loss 0.06621, train_acc 0.9776, valid_acc 0.9185, Time 00:00:30,lr 0.01\n",
      "epoch 146, loss 0.06132, train_acc 0.9795, valid_acc 0.9203, Time 00:00:30,lr 0.01\n",
      "epoch 147, loss 0.06416, train_acc 0.9784, valid_acc 0.9221, Time 00:00:30,lr 0.01\n",
      "epoch 148, loss 0.06045, train_acc 0.9793, valid_acc 0.9162, Time 00:00:30,lr 0.01\n",
      "epoch 149, loss 0.06016, train_acc 0.9800, valid_acc 0.9142, Time 00:00:30,lr 0.01\n",
      "epoch 150, loss 0.03468, train_acc 0.9895, valid_acc 0.9345, Time 00:00:30,lr 0.001\n",
      "epoch 151, loss 0.02295, train_acc 0.9933, valid_acc 0.9377, Time 00:00:30,lr 0.001\n",
      "epoch 152, loss 0.01791, train_acc 0.9955, valid_acc 0.9364, Time 00:00:30,lr 0.001\n",
      "epoch 153, loss 0.01725, train_acc 0.9955, valid_acc 0.9369, Time 00:00:30,lr 0.001\n",
      "epoch 154, loss 0.01460, train_acc 0.9961, valid_acc 0.9373, Time 00:00:30,lr 0.001\n",
      "epoch 155, loss 0.01362, train_acc 0.9967, valid_acc 0.9381, Time 00:00:30,lr 0.001\n",
      "epoch 156, loss 0.01294, train_acc 0.9970, valid_acc 0.9391, Time 00:00:30,lr 0.001\n",
      "epoch 157, loss 0.01247, train_acc 0.9970, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 158, loss 0.01062, train_acc 0.9974, valid_acc 0.9394, Time 00:00:30,lr 0.001\n",
      "epoch 159, loss 0.01077, train_acc 0.9977, valid_acc 0.9365, Time 00:00:30,lr 0.001\n",
      "epoch 160, loss 0.01017, train_acc 0.9978, valid_acc 0.9392, Time 00:00:30,lr 0.001\n",
      "epoch 161, loss 0.00928, train_acc 0.9981, valid_acc 0.9389, Time 00:00:30,lr 0.001\n",
      "epoch 162, loss 0.00964, train_acc 0.9978, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 163, loss 0.00897, train_acc 0.9982, valid_acc 0.9390, Time 00:00:30,lr 0.001\n",
      "epoch 164, loss 0.00817, train_acc 0.9985, valid_acc 0.9395, Time 00:00:30,lr 0.001\n",
      "epoch 165, loss 0.00879, train_acc 0.9981, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 166, loss 0.00774, train_acc 0.9985, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 167, loss 0.00806, train_acc 0.9982, valid_acc 0.9403, Time 00:00:30,lr 0.001\n",
      "epoch 168, loss 0.00736, train_acc 0.9985, valid_acc 0.9400, Time 00:00:30,lr 0.001\n",
      "epoch 169, loss 0.00760, train_acc 0.9986, valid_acc 0.9401, Time 00:00:30,lr 0.001\n",
      "epoch 170, loss 0.00736, train_acc 0.9986, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 171, loss 0.00706, train_acc 0.9985, valid_acc 0.9388, Time 00:00:30,lr 0.001\n",
      "epoch 172, loss 0.00672, train_acc 0.9988, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 173, loss 0.00711, train_acc 0.9985, valid_acc 0.9409, Time 00:00:30,lr 0.001\n",
      "epoch 174, loss 0.00627, train_acc 0.9988, valid_acc 0.9410, Time 00:00:30,lr 0.001\n",
      "epoch 175, loss 0.00647, train_acc 0.9987, valid_acc 0.9391, Time 00:00:33,lr 0.001\n",
      "epoch 176, loss 0.00614, train_acc 0.9990, valid_acc 0.9401, Time 00:00:32,lr 0.001\n",
      "epoch 177, loss 0.00592, train_acc 0.9990, valid_acc 0.9412, Time 00:00:30,lr 0.001\n",
      "epoch 178, loss 0.00610, train_acc 0.9986, valid_acc 0.9389, Time 00:00:30,lr 0.001\n",
      "epoch 179, loss 0.00530, train_acc 0.9992, valid_acc 0.9393, Time 00:00:30,lr 0.001\n",
      "epoch 180, loss 0.00577, train_acc 0.9989, valid_acc 0.9415, Time 00:00:33,lr 0.001\n",
      "epoch 181, loss 0.00612, train_acc 0.9986, valid_acc 0.9392, Time 00:00:35,lr 0.001\n",
      "epoch 182, loss 0.00583, train_acc 0.9987, valid_acc 0.9409, Time 00:00:31,lr 0.001\n",
      "epoch 183, loss 0.00538, train_acc 0.9990, valid_acc 0.9394, Time 00:00:30,lr 0.001\n",
      "epoch 184, loss 0.00512, train_acc 0.9992, valid_acc 0.9414, Time 00:00:33,lr 0.001\n",
      "epoch 185, loss 0.00559, train_acc 0.9990, valid_acc 0.9401, Time 00:00:31,lr 0.001\n",
      "epoch 186, loss 0.00473, train_acc 0.9994, valid_acc 0.9397, Time 00:00:31,lr 0.001\n",
      "epoch 187, loss 0.00465, train_acc 0.9992, valid_acc 0.9387, Time 00:00:35,lr 0.001\n",
      "epoch 188, loss 0.00554, train_acc 0.9988, valid_acc 0.9415, Time 00:00:31,lr 0.001\n",
      "epoch 189, loss 0.00482, train_acc 0.9994, valid_acc 0.9401, Time 00:00:33,lr 0.001\n",
      "epoch 190, loss 0.00493, train_acc 0.9992, valid_acc 0.9410, Time 00:00:36,lr 0.001\n",
      "epoch 191, loss 0.00506, train_acc 0.9990, valid_acc 0.9414, Time 00:00:30,lr 0.001\n",
      "epoch 192, loss 0.00492, train_acc 0.9990, valid_acc 0.9404, Time 00:00:30,lr 0.001\n",
      "epoch 193, loss 0.00497, train_acc 0.9991, valid_acc 0.9399, Time 00:00:30,lr 0.001\n",
      "epoch 194, loss 0.00468, train_acc 0.9992, valid_acc 0.9416, Time 00:00:30,lr 0.001\n",
      "epoch 195, loss 0.00445, train_acc 0.9993, valid_acc 0.9425, Time 00:00:30,lr 0.001\n",
      "epoch 196, loss 0.00440, train_acc 0.9994, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 197, loss 0.00445, train_acc 0.9991, valid_acc 0.9427, Time 00:00:30,lr 0.001\n",
      "epoch 198, loss 0.00456, train_acc 0.9992, valid_acc 0.9424, Time 00:00:30,lr 0.001\n",
      "epoch 199, loss 0.00424, train_acc 0.9993, valid_acc 0.9420, Time 00:00:31,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = ResNet(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 try delay pooling resnet arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T03:45:19.563155Z",
     "start_time": "2018-03-05T03:45:19.545584Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_delay_downsample(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_delay_downsample, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(5):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T05:37:46.139687Z",
     "start_time": "2018-03-05T03:45:19.566107Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.74871, train_acc 0.3432, valid_acc 0.4233, Time 00:00:32,lr 0.1\n",
      "epoch 1, loss 1.22673, train_acc 0.5586, valid_acc 0.6011, Time 00:00:33,lr 0.1\n",
      "epoch 2, loss 0.91422, train_acc 0.6805, valid_acc 0.7050, Time 00:00:33,lr 0.1\n",
      "epoch 3, loss 0.77360, train_acc 0.7336, valid_acc 0.6949, Time 00:00:33,lr 0.1\n",
      "epoch 4, loss 0.69274, train_acc 0.7617, valid_acc 0.7435, Time 00:00:33,lr 0.1\n",
      "epoch 5, loss 0.64398, train_acc 0.7797, valid_acc 0.7752, Time 00:00:33,lr 0.1\n",
      "epoch 6, loss 0.60268, train_acc 0.7923, valid_acc 0.7813, Time 00:00:33,lr 0.1\n",
      "epoch 7, loss 0.57284, train_acc 0.8026, valid_acc 0.7886, Time 00:00:33,lr 0.1\n",
      "epoch 8, loss 0.54954, train_acc 0.8118, valid_acc 0.7681, Time 00:00:33,lr 0.1\n",
      "epoch 9, loss 0.52985, train_acc 0.8182, valid_acc 0.8093, Time 00:00:33,lr 0.1\n",
      "epoch 10, loss 0.51509, train_acc 0.8221, valid_acc 0.8017, Time 00:00:33,lr 0.1\n",
      "epoch 11, loss 0.50326, train_acc 0.8282, valid_acc 0.7937, Time 00:00:33,lr 0.1\n",
      "epoch 12, loss 0.48711, train_acc 0.8338, valid_acc 0.8119, Time 00:00:33,lr 0.1\n",
      "epoch 13, loss 0.47811, train_acc 0.8351, valid_acc 0.8344, Time 00:00:33,lr 0.1\n",
      "epoch 14, loss 0.46951, train_acc 0.8389, valid_acc 0.7853, Time 00:00:33,lr 0.1\n",
      "epoch 15, loss 0.46085, train_acc 0.8430, valid_acc 0.8162, Time 00:00:33,lr 0.1\n",
      "epoch 16, loss 0.46285, train_acc 0.8429, valid_acc 0.8195, Time 00:00:33,lr 0.1\n",
      "epoch 17, loss 0.44921, train_acc 0.8452, valid_acc 0.8399, Time 00:00:33,lr 0.1\n",
      "epoch 18, loss 0.44708, train_acc 0.8461, valid_acc 0.8273, Time 00:00:33,lr 0.1\n",
      "epoch 19, loss 0.44015, train_acc 0.8488, valid_acc 0.8203, Time 00:00:33,lr 0.1\n",
      "epoch 20, loss 0.43671, train_acc 0.8511, valid_acc 0.8263, Time 00:00:33,lr 0.1\n",
      "epoch 21, loss 0.43053, train_acc 0.8506, valid_acc 0.8176, Time 00:00:33,lr 0.1\n",
      "epoch 22, loss 0.42708, train_acc 0.8541, valid_acc 0.7993, Time 00:00:33,lr 0.1\n",
      "epoch 23, loss 0.42621, train_acc 0.8544, valid_acc 0.8363, Time 00:00:33,lr 0.1\n",
      "epoch 24, loss 0.41793, train_acc 0.8560, valid_acc 0.8246, Time 00:00:33,lr 0.1\n",
      "epoch 25, loss 0.41721, train_acc 0.8564, valid_acc 0.8430, Time 00:00:33,lr 0.1\n",
      "epoch 26, loss 0.41290, train_acc 0.8587, valid_acc 0.8058, Time 00:00:33,lr 0.1\n",
      "epoch 27, loss 0.41461, train_acc 0.8587, valid_acc 0.8185, Time 00:00:33,lr 0.1\n",
      "epoch 28, loss 0.41093, train_acc 0.8602, valid_acc 0.8380, Time 00:00:33,lr 0.1\n",
      "epoch 29, loss 0.40087, train_acc 0.8614, valid_acc 0.8093, Time 00:00:33,lr 0.1\n",
      "epoch 30, loss 0.40541, train_acc 0.8600, valid_acc 0.8230, Time 00:00:33,lr 0.1\n",
      "epoch 31, loss 0.40255, train_acc 0.8610, valid_acc 0.8079, Time 00:00:33,lr 0.1\n",
      "epoch 32, loss 0.40050, train_acc 0.8628, valid_acc 0.7882, Time 00:00:33,lr 0.1\n",
      "epoch 33, loss 0.39481, train_acc 0.8645, valid_acc 0.8295, Time 00:00:33,lr 0.1\n",
      "epoch 34, loss 0.39867, train_acc 0.8636, valid_acc 0.8184, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.39564, train_acc 0.8642, valid_acc 0.8178, Time 00:00:33,lr 0.1\n",
      "epoch 36, loss 0.39489, train_acc 0.8651, valid_acc 0.8219, Time 00:00:33,lr 0.1\n",
      "epoch 37, loss 0.39592, train_acc 0.8636, valid_acc 0.8539, Time 00:00:33,lr 0.1\n",
      "epoch 38, loss 0.38821, train_acc 0.8660, valid_acc 0.8453, Time 00:00:33,lr 0.1\n",
      "epoch 39, loss 0.39041, train_acc 0.8658, valid_acc 0.7982, Time 00:00:33,lr 0.1\n",
      "epoch 40, loss 0.38574, train_acc 0.8685, valid_acc 0.7816, Time 00:00:33,lr 0.1\n",
      "epoch 41, loss 0.38974, train_acc 0.8665, valid_acc 0.8515, Time 00:00:33,lr 0.1\n",
      "epoch 42, loss 0.38603, train_acc 0.8678, valid_acc 0.8475, Time 00:00:33,lr 0.1\n",
      "epoch 43, loss 0.38327, train_acc 0.8695, valid_acc 0.8285, Time 00:00:33,lr 0.1\n",
      "epoch 44, loss 0.38648, train_acc 0.8657, valid_acc 0.8297, Time 00:00:33,lr 0.1\n",
      "epoch 45, loss 0.38244, train_acc 0.8685, valid_acc 0.8268, Time 00:00:33,lr 0.1\n",
      "epoch 46, loss 0.38622, train_acc 0.8668, valid_acc 0.8441, Time 00:00:33,lr 0.1\n",
      "epoch 47, loss 0.37983, train_acc 0.8692, valid_acc 0.8544, Time 00:00:33,lr 0.1\n",
      "epoch 48, loss 0.38122, train_acc 0.8699, valid_acc 0.8278, Time 00:00:33,lr 0.1\n",
      "epoch 49, loss 0.37919, train_acc 0.8693, valid_acc 0.8447, Time 00:00:33,lr 0.1\n",
      "epoch 50, loss 0.38096, train_acc 0.8695, valid_acc 0.8395, Time 00:00:33,lr 0.1\n",
      "epoch 51, loss 0.38445, train_acc 0.8679, valid_acc 0.8314, Time 00:00:33,lr 0.1\n",
      "epoch 52, loss 0.37595, train_acc 0.8723, valid_acc 0.8600, Time 00:00:33,lr 0.1\n",
      "epoch 53, loss 0.37780, train_acc 0.8703, valid_acc 0.8573, Time 00:00:33,lr 0.1\n",
      "epoch 54, loss 0.37309, train_acc 0.8719, valid_acc 0.8509, Time 00:00:33,lr 0.1\n",
      "epoch 55, loss 0.37771, train_acc 0.8722, valid_acc 0.8346, Time 00:00:33,lr 0.1\n",
      "epoch 56, loss 0.37425, train_acc 0.8728, valid_acc 0.7848, Time 00:00:33,lr 0.1\n",
      "epoch 57, loss 0.37199, train_acc 0.8713, valid_acc 0.8535, Time 00:00:33,lr 0.1\n",
      "epoch 58, loss 0.37070, train_acc 0.8729, valid_acc 0.8559, Time 00:00:33,lr 0.1\n",
      "epoch 59, loss 0.36944, train_acc 0.8733, valid_acc 0.8423, Time 00:00:33,lr 0.1\n",
      "epoch 60, loss 0.37046, train_acc 0.8737, valid_acc 0.8131, Time 00:00:33,lr 0.1\n",
      "epoch 61, loss 0.37506, train_acc 0.8725, valid_acc 0.8269, Time 00:00:33,lr 0.1\n",
      "epoch 62, loss 0.37128, train_acc 0.8726, valid_acc 0.8304, Time 00:00:33,lr 0.1\n",
      "epoch 63, loss 0.37199, train_acc 0.8734, valid_acc 0.8322, Time 00:00:33,lr 0.1\n",
      "epoch 64, loss 0.37266, train_acc 0.8732, valid_acc 0.8573, Time 00:00:33,lr 0.1\n",
      "epoch 65, loss 0.36817, train_acc 0.8733, valid_acc 0.8577, Time 00:00:33,lr 0.1\n",
      "epoch 66, loss 0.36947, train_acc 0.8754, valid_acc 0.8173, Time 00:00:33,lr 0.1\n",
      "epoch 67, loss 0.37241, train_acc 0.8726, valid_acc 0.8525, Time 00:00:33,lr 0.1\n",
      "epoch 68, loss 0.36946, train_acc 0.8737, valid_acc 0.8479, Time 00:00:33,lr 0.1\n",
      "epoch 69, loss 0.36792, train_acc 0.8740, valid_acc 0.8522, Time 00:00:33,lr 0.1\n",
      "epoch 70, loss 0.36331, train_acc 0.8748, valid_acc 0.8248, Time 00:00:33,lr 0.1\n",
      "epoch 71, loss 0.37371, train_acc 0.8709, valid_acc 0.8572, Time 00:00:33,lr 0.1\n",
      "epoch 72, loss 0.36035, train_acc 0.8774, valid_acc 0.8383, Time 00:00:33,lr 0.1\n",
      "epoch 73, loss 0.36753, train_acc 0.8745, valid_acc 0.8642, Time 00:00:33,lr 0.1\n",
      "epoch 74, loss 0.36495, train_acc 0.8748, valid_acc 0.8362, Time 00:00:33,lr 0.1\n",
      "epoch 75, loss 0.36169, train_acc 0.8762, valid_acc 0.8545, Time 00:00:33,lr 0.1\n",
      "epoch 76, loss 0.36737, train_acc 0.8742, valid_acc 0.8660, Time 00:00:33,lr 0.1\n",
      "epoch 77, loss 0.36069, train_acc 0.8762, valid_acc 0.8477, Time 00:00:33,lr 0.1\n",
      "epoch 78, loss 0.35918, train_acc 0.8762, valid_acc 0.7496, Time 00:00:33,lr 0.1\n",
      "epoch 79, loss 0.36596, train_acc 0.8754, valid_acc 0.8702, Time 00:00:33,lr 0.1\n",
      "epoch 80, loss 0.22051, train_acc 0.9255, valid_acc 0.9165, Time 00:00:33,lr 0.01\n",
      "epoch 81, loss 0.16942, train_acc 0.9411, valid_acc 0.9217, Time 00:00:33,lr 0.01\n",
      "epoch 82, loss 0.15287, train_acc 0.9484, valid_acc 0.9220, Time 00:00:33,lr 0.01\n",
      "epoch 83, loss 0.14203, train_acc 0.9515, valid_acc 0.9235, Time 00:00:33,lr 0.01\n",
      "epoch 84, loss 0.13170, train_acc 0.9557, valid_acc 0.9235, Time 00:00:33,lr 0.01\n",
      "epoch 85, loss 0.12316, train_acc 0.9581, valid_acc 0.9272, Time 00:00:33,lr 0.01\n",
      "epoch 86, loss 0.11667, train_acc 0.9605, valid_acc 0.9251, Time 00:00:33,lr 0.01\n",
      "epoch 87, loss 0.11166, train_acc 0.9626, valid_acc 0.9276, Time 00:00:33,lr 0.01\n",
      "epoch 88, loss 0.10688, train_acc 0.9640, valid_acc 0.9276, Time 00:00:33,lr 0.01\n",
      "epoch 89, loss 0.09929, train_acc 0.9659, valid_acc 0.9283, Time 00:00:33,lr 0.01\n",
      "epoch 90, loss 0.09834, train_acc 0.9661, valid_acc 0.9203, Time 00:00:33,lr 0.01\n",
      "epoch 91, loss 0.09401, train_acc 0.9680, valid_acc 0.9287, Time 00:00:33,lr 0.01\n",
      "epoch 92, loss 0.08950, train_acc 0.9696, valid_acc 0.9278, Time 00:00:33,lr 0.01\n",
      "epoch 93, loss 0.08283, train_acc 0.9722, valid_acc 0.9257, Time 00:00:33,lr 0.01\n",
      "epoch 94, loss 0.08354, train_acc 0.9716, valid_acc 0.9247, Time 00:00:33,lr 0.01\n",
      "epoch 95, loss 0.07745, train_acc 0.9740, valid_acc 0.9229, Time 00:00:33,lr 0.01\n",
      "epoch 96, loss 0.07554, train_acc 0.9741, valid_acc 0.9275, Time 00:00:33,lr 0.01\n",
      "epoch 97, loss 0.07464, train_acc 0.9750, valid_acc 0.9286, Time 00:00:33,lr 0.01\n",
      "epoch 98, loss 0.07502, train_acc 0.9737, valid_acc 0.9251, Time 00:00:33,lr 0.01\n",
      "epoch 99, loss 0.07237, train_acc 0.9750, valid_acc 0.9226, Time 00:00:33,lr 0.01\n",
      "epoch 100, loss 0.07316, train_acc 0.9758, valid_acc 0.9264, Time 00:00:33,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.06964, train_acc 0.9760, valid_acc 0.9242, Time 00:00:33,lr 0.01\n",
      "epoch 102, loss 0.06682, train_acc 0.9774, valid_acc 0.9238, Time 00:00:33,lr 0.01\n",
      "epoch 103, loss 0.06689, train_acc 0.9768, valid_acc 0.9242, Time 00:00:33,lr 0.01\n",
      "epoch 104, loss 0.06593, train_acc 0.9776, valid_acc 0.9274, Time 00:00:33,lr 0.01\n",
      "epoch 105, loss 0.06734, train_acc 0.9771, valid_acc 0.9260, Time 00:00:33,lr 0.01\n",
      "epoch 106, loss 0.06837, train_acc 0.9763, valid_acc 0.9233, Time 00:00:33,lr 0.01\n",
      "epoch 107, loss 0.06494, train_acc 0.9778, valid_acc 0.9255, Time 00:00:33,lr 0.01\n",
      "epoch 108, loss 0.06547, train_acc 0.9775, valid_acc 0.9225, Time 00:00:33,lr 0.01\n",
      "epoch 109, loss 0.06840, train_acc 0.9763, valid_acc 0.9247, Time 00:00:33,lr 0.01\n",
      "epoch 110, loss 0.06691, train_acc 0.9776, valid_acc 0.9270, Time 00:00:33,lr 0.01\n",
      "epoch 111, loss 0.06680, train_acc 0.9772, valid_acc 0.9212, Time 00:00:33,lr 0.01\n",
      "epoch 112, loss 0.06807, train_acc 0.9770, valid_acc 0.9235, Time 00:00:33,lr 0.01\n",
      "epoch 113, loss 0.06843, train_acc 0.9765, valid_acc 0.9259, Time 00:00:33,lr 0.01\n",
      "epoch 114, loss 0.06646, train_acc 0.9776, valid_acc 0.9260, Time 00:00:33,lr 0.01\n",
      "epoch 115, loss 0.06611, train_acc 0.9778, valid_acc 0.9272, Time 00:00:33,lr 0.01\n",
      "epoch 116, loss 0.06412, train_acc 0.9782, valid_acc 0.9185, Time 00:00:33,lr 0.01\n",
      "epoch 117, loss 0.06882, train_acc 0.9763, valid_acc 0.9218, Time 00:00:33,lr 0.01\n",
      "epoch 118, loss 0.06780, train_acc 0.9770, valid_acc 0.9180, Time 00:00:37,lr 0.01\n",
      "epoch 119, loss 0.06749, train_acc 0.9765, valid_acc 0.9227, Time 00:00:38,lr 0.01\n",
      "epoch 120, loss 0.07060, train_acc 0.9760, valid_acc 0.9226, Time 00:00:34,lr 0.01\n",
      "epoch 121, loss 0.06805, train_acc 0.9769, valid_acc 0.9198, Time 00:00:33,lr 0.01\n",
      "epoch 122, loss 0.07168, train_acc 0.9755, valid_acc 0.9225, Time 00:00:33,lr 0.01\n",
      "epoch 123, loss 0.07041, train_acc 0.9762, valid_acc 0.9240, Time 00:00:33,lr 0.01\n",
      "epoch 124, loss 0.07179, train_acc 0.9752, valid_acc 0.9199, Time 00:00:33,lr 0.01\n",
      "epoch 125, loss 0.06785, train_acc 0.9778, valid_acc 0.9214, Time 00:00:33,lr 0.01\n",
      "epoch 126, loss 0.07291, train_acc 0.9752, valid_acc 0.9160, Time 00:00:33,lr 0.01\n",
      "epoch 127, loss 0.06623, train_acc 0.9773, valid_acc 0.9237, Time 00:00:33,lr 0.01\n",
      "epoch 128, loss 0.07073, train_acc 0.9759, valid_acc 0.9204, Time 00:00:33,lr 0.01\n",
      "epoch 129, loss 0.06996, train_acc 0.9762, valid_acc 0.9191, Time 00:00:34,lr 0.01\n",
      "epoch 130, loss 0.07506, train_acc 0.9734, valid_acc 0.9173, Time 00:00:33,lr 0.01\n",
      "epoch 131, loss 0.07572, train_acc 0.9737, valid_acc 0.9076, Time 00:00:33,lr 0.01\n",
      "epoch 132, loss 0.07105, train_acc 0.9755, valid_acc 0.9201, Time 00:00:33,lr 0.01\n",
      "epoch 133, loss 0.07461, train_acc 0.9743, valid_acc 0.9150, Time 00:00:33,lr 0.01\n",
      "epoch 134, loss 0.06875, train_acc 0.9768, valid_acc 0.9231, Time 00:00:33,lr 0.01\n",
      "epoch 135, loss 0.06996, train_acc 0.9763, valid_acc 0.9169, Time 00:00:33,lr 0.01\n",
      "epoch 136, loss 0.07493, train_acc 0.9741, valid_acc 0.9214, Time 00:00:33,lr 0.01\n",
      "epoch 137, loss 0.07146, train_acc 0.9747, valid_acc 0.9171, Time 00:00:33,lr 0.01\n",
      "epoch 138, loss 0.07328, train_acc 0.9754, valid_acc 0.9215, Time 00:00:33,lr 0.01\n",
      "epoch 139, loss 0.07751, train_acc 0.9734, valid_acc 0.9269, Time 00:00:33,lr 0.01\n",
      "epoch 140, loss 0.07599, train_acc 0.9738, valid_acc 0.9154, Time 00:00:34,lr 0.01\n",
      "epoch 141, loss 0.06780, train_acc 0.9768, valid_acc 0.9156, Time 00:00:35,lr 0.01\n",
      "epoch 142, loss 0.07360, train_acc 0.9751, valid_acc 0.9236, Time 00:00:33,lr 0.01\n",
      "epoch 143, loss 0.07394, train_acc 0.9740, valid_acc 0.9175, Time 00:00:34,lr 0.01\n",
      "epoch 144, loss 0.07369, train_acc 0.9744, valid_acc 0.9217, Time 00:00:37,lr 0.01\n",
      "epoch 145, loss 0.07437, train_acc 0.9742, valid_acc 0.9182, Time 00:00:33,lr 0.01\n",
      "epoch 146, loss 0.07432, train_acc 0.9737, valid_acc 0.9102, Time 00:00:33,lr 0.01\n",
      "epoch 147, loss 0.07380, train_acc 0.9745, valid_acc 0.9145, Time 00:00:33,lr 0.01\n",
      "epoch 148, loss 0.07292, train_acc 0.9748, valid_acc 0.9212, Time 00:00:33,lr 0.01\n",
      "epoch 149, loss 0.07682, train_acc 0.9734, valid_acc 0.9200, Time 00:00:33,lr 0.01\n",
      "epoch 150, loss 0.04344, train_acc 0.9862, valid_acc 0.9350, Time 00:00:33,lr 0.001\n",
      "epoch 151, loss 0.03075, train_acc 0.9912, valid_acc 0.9356, Time 00:00:33,lr 0.001\n",
      "epoch 152, loss 0.02638, train_acc 0.9927, valid_acc 0.9351, Time 00:00:33,lr 0.001\n",
      "epoch 153, loss 0.02355, train_acc 0.9938, valid_acc 0.9350, Time 00:00:33,lr 0.001\n",
      "epoch 154, loss 0.02280, train_acc 0.9940, valid_acc 0.9359, Time 00:00:34,lr 0.001\n",
      "epoch 155, loss 0.02053, train_acc 0.9951, valid_acc 0.9363, Time 00:00:33,lr 0.001\n",
      "epoch 156, loss 0.01953, train_acc 0.9952, valid_acc 0.9352, Time 00:00:33,lr 0.001\n",
      "epoch 157, loss 0.01891, train_acc 0.9957, valid_acc 0.9354, Time 00:00:33,lr 0.001\n",
      "epoch 158, loss 0.01852, train_acc 0.9954, valid_acc 0.9353, Time 00:00:33,lr 0.001\n",
      "epoch 159, loss 0.01763, train_acc 0.9956, valid_acc 0.9363, Time 00:00:33,lr 0.001\n",
      "epoch 160, loss 0.01674, train_acc 0.9960, valid_acc 0.9366, Time 00:00:33,lr 0.001\n",
      "epoch 161, loss 0.01659, train_acc 0.9961, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 162, loss 0.01573, train_acc 0.9962, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 163, loss 0.01550, train_acc 0.9966, valid_acc 0.9362, Time 00:00:33,lr 0.001\n",
      "epoch 164, loss 0.01424, train_acc 0.9968, valid_acc 0.9367, Time 00:00:33,lr 0.001\n",
      "epoch 165, loss 0.01404, train_acc 0.9969, valid_acc 0.9368, Time 00:00:33,lr 0.001\n",
      "epoch 166, loss 0.01377, train_acc 0.9969, valid_acc 0.9372, Time 00:00:33,lr 0.001\n",
      "epoch 167, loss 0.01356, train_acc 0.9970, valid_acc 0.9376, Time 00:00:33,lr 0.001\n",
      "epoch 168, loss 0.01291, train_acc 0.9973, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 169, loss 0.01344, train_acc 0.9973, valid_acc 0.9378, Time 00:00:33,lr 0.001\n",
      "epoch 170, loss 0.01252, train_acc 0.9973, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 171, loss 0.01335, train_acc 0.9972, valid_acc 0.9366, Time 00:00:33,lr 0.001\n",
      "epoch 172, loss 0.01244, train_acc 0.9973, valid_acc 0.9371, Time 00:00:33,lr 0.001\n",
      "epoch 173, loss 0.01190, train_acc 0.9976, valid_acc 0.9378, Time 00:00:33,lr 0.001\n",
      "epoch 174, loss 0.01216, train_acc 0.9977, valid_acc 0.9380, Time 00:00:33,lr 0.001\n",
      "epoch 175, loss 0.01098, train_acc 0.9982, valid_acc 0.9383, Time 00:00:33,lr 0.001\n",
      "epoch 176, loss 0.01147, train_acc 0.9977, valid_acc 0.9368, Time 00:00:33,lr 0.001\n",
      "epoch 177, loss 0.01014, train_acc 0.9983, valid_acc 0.9380, Time 00:00:33,lr 0.001\n",
      "epoch 178, loss 0.01104, train_acc 0.9978, valid_acc 0.9372, Time 00:00:33,lr 0.001\n",
      "epoch 179, loss 0.01147, train_acc 0.9975, valid_acc 0.9379, Time 00:00:33,lr 0.001\n",
      "epoch 180, loss 0.01022, train_acc 0.9982, valid_acc 0.9375, Time 00:00:33,lr 0.001\n",
      "epoch 181, loss 0.01028, train_acc 0.9980, valid_acc 0.9376, Time 00:00:33,lr 0.001\n",
      "epoch 182, loss 0.00943, train_acc 0.9982, valid_acc 0.9375, Time 00:00:33,lr 0.001\n",
      "epoch 183, loss 0.00947, train_acc 0.9981, valid_acc 0.9381, Time 00:00:33,lr 0.001\n",
      "epoch 184, loss 0.00997, train_acc 0.9979, valid_acc 0.9380, Time 00:00:33,lr 0.001\n",
      "epoch 185, loss 0.00923, train_acc 0.9983, valid_acc 0.9382, Time 00:00:33,lr 0.001\n",
      "epoch 186, loss 0.00944, train_acc 0.9983, valid_acc 0.9373, Time 00:00:33,lr 0.001\n",
      "epoch 187, loss 0.00932, train_acc 0.9982, valid_acc 0.9377, Time 00:00:33,lr 0.001\n",
      "epoch 188, loss 0.00963, train_acc 0.9981, valid_acc 0.9372, Time 00:00:33,lr 0.001\n",
      "epoch 189, loss 0.00881, train_acc 0.9984, valid_acc 0.9383, Time 00:00:33,lr 0.001\n",
      "epoch 190, loss 0.00906, train_acc 0.9982, valid_acc 0.9378, Time 00:00:33,lr 0.001\n",
      "epoch 191, loss 0.00878, train_acc 0.9985, valid_acc 0.9385, Time 00:00:33,lr 0.001\n",
      "epoch 192, loss 0.00873, train_acc 0.9985, valid_acc 0.9400, Time 00:00:33,lr 0.001\n",
      "epoch 193, loss 0.00856, train_acc 0.9985, valid_acc 0.9392, Time 00:00:33,lr 0.001\n",
      "epoch 194, loss 0.00905, train_acc 0.9983, valid_acc 0.9387, Time 00:00:33,lr 0.001\n",
      "epoch 195, loss 0.00956, train_acc 0.9983, valid_acc 0.9385, Time 00:00:33,lr 0.001\n",
      "epoch 196, loss 0.00896, train_acc 0.9985, valid_acc 0.9376, Time 00:00:33,lr 0.001\n",
      "epoch 197, loss 0.00819, train_acc 0.9986, valid_acc 0.9386, Time 00:00:33,lr 0.001\n",
      "epoch 198, loss 0.00819, train_acc 0.9986, valid_acc 0.9382, Time 00:00:33,lr 0.001\n",
      "epoch 199, loss 0.00833, train_acc 0.9985, valid_acc 0.9386, Time 00:00:33,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_delay_downsample(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_522_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 to find if width is important, compare to 5.8 double all conv layers' channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:27:01.435710Z",
     "start_time": "2018-03-05T06:27:01.415583Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_522_c64(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_522_c64, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(5):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=256, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=256))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T09:55:35.962114Z",
     "start_time": "2018-03-05T06:27:02.356290Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.80125, train_acc 0.3235, valid_acc 0.4435, Time 00:01:01,lr 0.1\n",
      "epoch 1, loss 1.21529, train_acc 0.5671, valid_acc 0.6560, Time 00:01:02,lr 0.1\n",
      "epoch 2, loss 0.84985, train_acc 0.7038, valid_acc 0.7225, Time 00:01:02,lr 0.1\n",
      "epoch 3, loss 0.70113, train_acc 0.7565, valid_acc 0.7654, Time 00:01:03,lr 0.1\n",
      "epoch 4, loss 0.62073, train_acc 0.7878, valid_acc 0.7823, Time 00:01:03,lr 0.1\n",
      "epoch 5, loss 0.56917, train_acc 0.8041, valid_acc 0.7874, Time 00:01:04,lr 0.1\n",
      "epoch 6, loss 0.52424, train_acc 0.8196, valid_acc 0.7884, Time 00:01:02,lr 0.1\n",
      "epoch 7, loss 0.50291, train_acc 0.8289, valid_acc 0.8291, Time 00:01:03,lr 0.1\n",
      "epoch 8, loss 0.47482, train_acc 0.8385, valid_acc 0.7859, Time 00:01:02,lr 0.1\n",
      "epoch 9, loss 0.44850, train_acc 0.8471, valid_acc 0.8380, Time 00:01:02,lr 0.1\n",
      "epoch 10, loss 0.43947, train_acc 0.8487, valid_acc 0.8139, Time 00:01:02,lr 0.1\n",
      "epoch 11, loss 0.42596, train_acc 0.8529, valid_acc 0.7957, Time 00:01:02,lr 0.1\n",
      "epoch 12, loss 0.41112, train_acc 0.8588, valid_acc 0.8242, Time 00:01:03,lr 0.1\n",
      "epoch 13, loss 0.39968, train_acc 0.8629, valid_acc 0.8341, Time 00:01:02,lr 0.1\n",
      "epoch 14, loss 0.38834, train_acc 0.8670, valid_acc 0.8343, Time 00:01:02,lr 0.1\n",
      "epoch 15, loss 0.38437, train_acc 0.8676, valid_acc 0.8528, Time 00:01:02,lr 0.1\n",
      "epoch 16, loss 0.37885, train_acc 0.8690, valid_acc 0.8522, Time 00:01:02,lr 0.1\n",
      "epoch 17, loss 0.36954, train_acc 0.8750, valid_acc 0.8218, Time 00:01:02,lr 0.1\n",
      "epoch 18, loss 0.36590, train_acc 0.8740, valid_acc 0.8669, Time 00:01:02,lr 0.1\n",
      "epoch 19, loss 0.35680, train_acc 0.8776, valid_acc 0.7886, Time 00:01:02,lr 0.1\n",
      "epoch 20, loss 0.35435, train_acc 0.8782, valid_acc 0.8549, Time 00:01:02,lr 0.1\n",
      "epoch 21, loss 0.35435, train_acc 0.8771, valid_acc 0.8495, Time 00:01:02,lr 0.1\n",
      "epoch 22, loss 0.34705, train_acc 0.8807, valid_acc 0.8506, Time 00:01:02,lr 0.1\n",
      "epoch 23, loss 0.34090, train_acc 0.8825, valid_acc 0.8496, Time 00:01:02,lr 0.1\n",
      "epoch 24, loss 0.34386, train_acc 0.8808, valid_acc 0.8626, Time 00:01:02,lr 0.1\n",
      "epoch 25, loss 0.34245, train_acc 0.8825, valid_acc 0.7985, Time 00:01:02,lr 0.1\n",
      "epoch 26, loss 0.33509, train_acc 0.8857, valid_acc 0.8650, Time 00:01:02,lr 0.1\n",
      "epoch 27, loss 0.33300, train_acc 0.8857, valid_acc 0.8669, Time 00:01:02,lr 0.1\n",
      "epoch 28, loss 0.32566, train_acc 0.8876, valid_acc 0.8621, Time 00:01:02,lr 0.1\n",
      "epoch 29, loss 0.32526, train_acc 0.8898, valid_acc 0.8516, Time 00:01:02,lr 0.1\n",
      "epoch 30, loss 0.32913, train_acc 0.8874, valid_acc 0.8742, Time 00:01:02,lr 0.1\n",
      "epoch 31, loss 0.32649, train_acc 0.8875, valid_acc 0.8690, Time 00:01:02,lr 0.1\n",
      "epoch 32, loss 0.32220, train_acc 0.8900, valid_acc 0.8302, Time 00:01:02,lr 0.1\n",
      "epoch 33, loss 0.32117, train_acc 0.8895, valid_acc 0.8569, Time 00:01:02,lr 0.1\n",
      "epoch 34, loss 0.32370, train_acc 0.8887, valid_acc 0.8769, Time 00:01:02,lr 0.1\n",
      "epoch 35, loss 0.31867, train_acc 0.8903, valid_acc 0.8533, Time 00:01:03,lr 0.1\n",
      "epoch 36, loss 0.31809, train_acc 0.8898, valid_acc 0.8441, Time 00:01:02,lr 0.1\n",
      "epoch 37, loss 0.31201, train_acc 0.8933, valid_acc 0.8406, Time 00:01:02,lr 0.1\n",
      "epoch 38, loss 0.31065, train_acc 0.8922, valid_acc 0.8659, Time 00:01:02,lr 0.1\n",
      "epoch 39, loss 0.30870, train_acc 0.8940, valid_acc 0.8659, Time 00:01:02,lr 0.1\n",
      "epoch 40, loss 0.30990, train_acc 0.8946, valid_acc 0.8653, Time 00:01:02,lr 0.1\n",
      "epoch 41, loss 0.31143, train_acc 0.8916, valid_acc 0.8661, Time 00:01:02,lr 0.1\n",
      "epoch 42, loss 0.30731, train_acc 0.8949, valid_acc 0.8734, Time 00:01:02,lr 0.1\n",
      "epoch 43, loss 0.30617, train_acc 0.8952, valid_acc 0.8598, Time 00:01:02,lr 0.1\n",
      "epoch 44, loss 0.30193, train_acc 0.8966, valid_acc 0.8589, Time 00:01:03,lr 0.1\n",
      "epoch 45, loss 0.30429, train_acc 0.8963, valid_acc 0.8502, Time 00:01:02,lr 0.1\n",
      "epoch 46, loss 0.30656, train_acc 0.8956, valid_acc 0.8829, Time 00:01:02,lr 0.1\n",
      "epoch 47, loss 0.30519, train_acc 0.8958, valid_acc 0.8526, Time 00:01:02,lr 0.1\n",
      "epoch 48, loss 0.30416, train_acc 0.8945, valid_acc 0.8602, Time 00:01:02,lr 0.1\n",
      "epoch 49, loss 0.30313, train_acc 0.8963, valid_acc 0.8058, Time 00:01:02,lr 0.1\n",
      "epoch 50, loss 0.30975, train_acc 0.8951, valid_acc 0.8783, Time 00:01:03,lr 0.1\n",
      "epoch 51, loss 0.29615, train_acc 0.8990, valid_acc 0.8925, Time 00:01:02,lr 0.1\n",
      "epoch 52, loss 0.29919, train_acc 0.8976, valid_acc 0.8604, Time 00:01:02,lr 0.1\n",
      "epoch 53, loss 0.30477, train_acc 0.8962, valid_acc 0.8638, Time 00:01:02,lr 0.1\n",
      "epoch 54, loss 0.29803, train_acc 0.8965, valid_acc 0.8488, Time 00:01:02,lr 0.1\n",
      "epoch 55, loss 0.29738, train_acc 0.8977, valid_acc 0.8526, Time 00:01:02,lr 0.1\n",
      "epoch 56, loss 0.30025, train_acc 0.8967, valid_acc 0.8243, Time 00:01:02,lr 0.1\n",
      "epoch 57, loss 0.29852, train_acc 0.8984, valid_acc 0.8712, Time 00:01:02,lr 0.1\n",
      "epoch 58, loss 0.29340, train_acc 0.8992, valid_acc 0.8735, Time 00:01:02,lr 0.1\n",
      "epoch 59, loss 0.29646, train_acc 0.8973, valid_acc 0.8564, Time 00:01:02,lr 0.1\n",
      "epoch 60, loss 0.29942, train_acc 0.8973, valid_acc 0.8898, Time 00:01:02,lr 0.1\n",
      "epoch 61, loss 0.29024, train_acc 0.8999, valid_acc 0.8471, Time 00:01:02,lr 0.1\n",
      "epoch 62, loss 0.29388, train_acc 0.9000, valid_acc 0.8524, Time 00:01:02,lr 0.1\n",
      "epoch 63, loss 0.29573, train_acc 0.8968, valid_acc 0.8145, Time 00:01:02,lr 0.1\n",
      "epoch 64, loss 0.29297, train_acc 0.9013, valid_acc 0.8845, Time 00:01:02,lr 0.1\n",
      "epoch 65, loss 0.28833, train_acc 0.9022, valid_acc 0.8770, Time 00:01:02,lr 0.1\n",
      "epoch 66, loss 0.29792, train_acc 0.8971, valid_acc 0.8943, Time 00:01:02,lr 0.1\n",
      "epoch 67, loss 0.29277, train_acc 0.9005, valid_acc 0.8739, Time 00:01:02,lr 0.1\n",
      "epoch 68, loss 0.29278, train_acc 0.9000, valid_acc 0.8596, Time 00:01:02,lr 0.1\n",
      "epoch 69, loss 0.29351, train_acc 0.8999, valid_acc 0.8461, Time 00:01:02,lr 0.1\n",
      "epoch 70, loss 0.28908, train_acc 0.9000, valid_acc 0.8510, Time 00:01:02,lr 0.1\n",
      "epoch 71, loss 0.28986, train_acc 0.9004, valid_acc 0.8559, Time 00:01:02,lr 0.1\n",
      "epoch 72, loss 0.29072, train_acc 0.9007, valid_acc 0.8637, Time 00:01:02,lr 0.1\n",
      "epoch 73, loss 0.28822, train_acc 0.9014, valid_acc 0.8428, Time 00:01:02,lr 0.1\n",
      "epoch 74, loss 0.28894, train_acc 0.9002, valid_acc 0.8768, Time 00:01:02,lr 0.1\n",
      "epoch 75, loss 0.28853, train_acc 0.9013, valid_acc 0.8787, Time 00:01:02,lr 0.1\n",
      "epoch 76, loss 0.29077, train_acc 0.9001, valid_acc 0.8747, Time 00:01:02,lr 0.1\n",
      "epoch 77, loss 0.29205, train_acc 0.9006, valid_acc 0.8222, Time 00:01:02,lr 0.1\n",
      "epoch 78, loss 0.28894, train_acc 0.9010, valid_acc 0.8473, Time 00:01:02,lr 0.1\n",
      "epoch 79, loss 0.28275, train_acc 0.9019, valid_acc 0.8289, Time 00:01:02,lr 0.1\n",
      "epoch 80, loss 0.15574, train_acc 0.9475, valid_acc 0.9311, Time 00:01:02,lr 0.01\n",
      "epoch 81, loss 0.10297, train_acc 0.9658, valid_acc 0.9371, Time 00:01:02,lr 0.01\n",
      "epoch 82, loss 0.08857, train_acc 0.9698, valid_acc 0.9369, Time 00:01:02,lr 0.01\n",
      "epoch 83, loss 0.07886, train_acc 0.9731, valid_acc 0.9416, Time 00:01:02,lr 0.01\n",
      "epoch 84, loss 0.07118, train_acc 0.9763, valid_acc 0.9419, Time 00:01:02,lr 0.01\n",
      "epoch 85, loss 0.06399, train_acc 0.9787, valid_acc 0.9402, Time 00:01:02,lr 0.01\n",
      "epoch 86, loss 0.05531, train_acc 0.9816, valid_acc 0.9414, Time 00:01:02,lr 0.01\n",
      "epoch 87, loss 0.04941, train_acc 0.9840, valid_acc 0.9415, Time 00:01:02,lr 0.01\n",
      "epoch 88, loss 0.04808, train_acc 0.9843, valid_acc 0.9430, Time 00:01:02,lr 0.01\n",
      "epoch 89, loss 0.04049, train_acc 0.9871, valid_acc 0.9413, Time 00:01:02,lr 0.01\n",
      "epoch 90, loss 0.04131, train_acc 0.9871, valid_acc 0.9419, Time 00:01:02,lr 0.01\n",
      "epoch 91, loss 0.03723, train_acc 0.9883, valid_acc 0.9424, Time 00:01:02,lr 0.01\n",
      "epoch 92, loss 0.03402, train_acc 0.9892, valid_acc 0.9437, Time 00:01:02,lr 0.01\n",
      "epoch 93, loss 0.03021, train_acc 0.9912, valid_acc 0.9400, Time 00:01:02,lr 0.01\n",
      "epoch 94, loss 0.02831, train_acc 0.9914, valid_acc 0.9430, Time 00:01:02,lr 0.01\n",
      "epoch 95, loss 0.02827, train_acc 0.9916, valid_acc 0.9427, Time 00:01:02,lr 0.01\n",
      "epoch 96, loss 0.02427, train_acc 0.9931, valid_acc 0.9421, Time 00:01:02,lr 0.01\n",
      "epoch 97, loss 0.02600, train_acc 0.9922, valid_acc 0.9424, Time 00:01:02,lr 0.01\n",
      "epoch 98, loss 0.02444, train_acc 0.9927, valid_acc 0.9409, Time 00:01:02,lr 0.01\n",
      "epoch 99, loss 0.02349, train_acc 0.9931, valid_acc 0.9427, Time 00:01:02,lr 0.01\n",
      "epoch 100, loss 0.02360, train_acc 0.9932, valid_acc 0.9429, Time 00:01:02,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.02139, train_acc 0.9936, valid_acc 0.9407, Time 00:01:02,lr 0.01\n",
      "epoch 102, loss 0.02296, train_acc 0.9928, valid_acc 0.9436, Time 00:01:02,lr 0.01\n",
      "epoch 103, loss 0.02055, train_acc 0.9945, valid_acc 0.9414, Time 00:01:02,lr 0.01\n",
      "epoch 104, loss 0.02213, train_acc 0.9933, valid_acc 0.9417, Time 00:01:02,lr 0.01\n",
      "epoch 105, loss 0.01892, train_acc 0.9947, valid_acc 0.9410, Time 00:01:02,lr 0.01\n",
      "epoch 106, loss 0.02145, train_acc 0.9937, valid_acc 0.9418, Time 00:01:02,lr 0.01\n",
      "epoch 107, loss 0.01967, train_acc 0.9943, valid_acc 0.9386, Time 00:01:02,lr 0.01\n",
      "epoch 108, loss 0.01919, train_acc 0.9945, valid_acc 0.9394, Time 00:01:02,lr 0.01\n",
      "epoch 109, loss 0.02001, train_acc 0.9939, valid_acc 0.9409, Time 00:01:02,lr 0.01\n",
      "epoch 110, loss 0.02147, train_acc 0.9938, valid_acc 0.9424, Time 00:01:02,lr 0.01\n",
      "epoch 111, loss 0.02209, train_acc 0.9933, valid_acc 0.9395, Time 00:01:02,lr 0.01\n",
      "epoch 112, loss 0.02179, train_acc 0.9936, valid_acc 0.9397, Time 00:01:02,lr 0.01\n",
      "epoch 113, loss 0.01823, train_acc 0.9951, valid_acc 0.9407, Time 00:01:02,lr 0.01\n",
      "epoch 114, loss 0.02256, train_acc 0.9930, valid_acc 0.9420, Time 00:01:02,lr 0.01\n",
      "epoch 115, loss 0.02210, train_acc 0.9931, valid_acc 0.9397, Time 00:01:02,lr 0.01\n",
      "epoch 116, loss 0.02782, train_acc 0.9908, valid_acc 0.9360, Time 00:01:02,lr 0.01\n",
      "epoch 117, loss 0.02374, train_acc 0.9929, valid_acc 0.9350, Time 00:01:02,lr 0.01\n",
      "epoch 118, loss 0.02712, train_acc 0.9913, valid_acc 0.9391, Time 00:01:02,lr 0.01\n",
      "epoch 119, loss 0.02585, train_acc 0.9920, valid_acc 0.9348, Time 00:01:02,lr 0.01\n",
      "epoch 120, loss 0.02814, train_acc 0.9905, valid_acc 0.9378, Time 00:01:02,lr 0.01\n",
      "epoch 121, loss 0.03003, train_acc 0.9902, valid_acc 0.9358, Time 00:01:02,lr 0.01\n",
      "epoch 122, loss 0.02864, train_acc 0.9912, valid_acc 0.9377, Time 00:01:02,lr 0.01\n",
      "epoch 123, loss 0.02835, train_acc 0.9907, valid_acc 0.9376, Time 00:01:02,lr 0.01\n",
      "epoch 124, loss 0.02621, train_acc 0.9918, valid_acc 0.9327, Time 00:01:02,lr 0.01\n",
      "epoch 125, loss 0.03241, train_acc 0.9895, valid_acc 0.9360, Time 00:01:02,lr 0.01\n",
      "epoch 126, loss 0.03135, train_acc 0.9899, valid_acc 0.9392, Time 00:01:02,lr 0.01\n",
      "epoch 127, loss 0.03428, train_acc 0.9888, valid_acc 0.9312, Time 00:01:02,lr 0.01\n",
      "epoch 128, loss 0.03211, train_acc 0.9897, valid_acc 0.9371, Time 00:01:02,lr 0.01\n",
      "epoch 129, loss 0.03596, train_acc 0.9874, valid_acc 0.9394, Time 00:01:02,lr 0.01\n",
      "epoch 130, loss 0.03371, train_acc 0.9887, valid_acc 0.9314, Time 00:01:02,lr 0.01\n",
      "epoch 131, loss 0.03617, train_acc 0.9882, valid_acc 0.9349, Time 00:01:02,lr 0.01\n",
      "epoch 132, loss 0.03470, train_acc 0.9883, valid_acc 0.9340, Time 00:01:02,lr 0.01\n",
      "epoch 133, loss 0.03657, train_acc 0.9883, valid_acc 0.9276, Time 00:01:02,lr 0.01\n",
      "epoch 134, loss 0.03695, train_acc 0.9878, valid_acc 0.9336, Time 00:01:02,lr 0.01\n",
      "epoch 135, loss 0.03452, train_acc 0.9886, valid_acc 0.9347, Time 00:01:02,lr 0.01\n",
      "epoch 136, loss 0.03210, train_acc 0.9895, valid_acc 0.9336, Time 00:01:02,lr 0.01\n",
      "epoch 137, loss 0.03506, train_acc 0.9886, valid_acc 0.9329, Time 00:01:02,lr 0.01\n",
      "epoch 138, loss 0.03861, train_acc 0.9871, valid_acc 0.9294, Time 00:01:02,lr 0.01\n",
      "epoch 139, loss 0.03809, train_acc 0.9873, valid_acc 0.9319, Time 00:01:02,lr 0.01\n",
      "epoch 140, loss 0.03851, train_acc 0.9873, valid_acc 0.9233, Time 00:01:02,lr 0.01\n",
      "epoch 141, loss 0.04022, train_acc 0.9868, valid_acc 0.9359, Time 00:01:02,lr 0.01\n",
      "epoch 142, loss 0.03846, train_acc 0.9874, valid_acc 0.9306, Time 00:01:02,lr 0.01\n",
      "epoch 143, loss 0.03732, train_acc 0.9877, valid_acc 0.9261, Time 00:01:02,lr 0.01\n",
      "epoch 144, loss 0.03795, train_acc 0.9875, valid_acc 0.9339, Time 00:01:02,lr 0.01\n",
      "epoch 145, loss 0.04298, train_acc 0.9860, valid_acc 0.9302, Time 00:01:02,lr 0.01\n",
      "epoch 146, loss 0.03716, train_acc 0.9884, valid_acc 0.9316, Time 00:01:02,lr 0.01\n",
      "epoch 147, loss 0.04139, train_acc 0.9867, valid_acc 0.9273, Time 00:01:02,lr 0.01\n",
      "epoch 148, loss 0.03711, train_acc 0.9879, valid_acc 0.9309, Time 00:01:02,lr 0.01\n",
      "epoch 149, loss 0.03742, train_acc 0.9880, valid_acc 0.9299, Time 00:01:02,lr 0.01\n",
      "epoch 150, loss 0.02054, train_acc 0.9940, valid_acc 0.9442, Time 00:01:02,lr 0.001\n",
      "epoch 151, loss 0.01223, train_acc 0.9969, valid_acc 0.9461, Time 00:01:02,lr 0.001\n",
      "epoch 152, loss 0.00925, train_acc 0.9981, valid_acc 0.9482, Time 00:01:02,lr 0.001\n",
      "epoch 153, loss 0.00812, train_acc 0.9984, valid_acc 0.9469, Time 00:01:02,lr 0.001\n",
      "epoch 154, loss 0.00758, train_acc 0.9986, valid_acc 0.9478, Time 00:01:02,lr 0.001\n",
      "epoch 155, loss 0.00682, train_acc 0.9988, valid_acc 0.9477, Time 00:01:02,lr 0.001\n",
      "epoch 156, loss 0.00649, train_acc 0.9989, valid_acc 0.9477, Time 00:01:02,lr 0.001\n",
      "epoch 157, loss 0.00602, train_acc 0.9990, valid_acc 0.9478, Time 00:01:02,lr 0.001\n",
      "epoch 158, loss 0.00580, train_acc 0.9990, valid_acc 0.9494, Time 00:01:02,lr 0.001\n",
      "epoch 159, loss 0.00597, train_acc 0.9990, valid_acc 0.9495, Time 00:01:02,lr 0.001\n",
      "epoch 160, loss 0.00537, train_acc 0.9991, valid_acc 0.9493, Time 00:01:02,lr 0.001\n",
      "epoch 161, loss 0.00494, train_acc 0.9993, valid_acc 0.9495, Time 00:01:02,lr 0.001\n",
      "epoch 162, loss 0.00495, train_acc 0.9993, valid_acc 0.9512, Time 00:01:02,lr 0.001\n",
      "epoch 163, loss 0.00436, train_acc 0.9996, valid_acc 0.9504, Time 00:01:02,lr 0.001\n",
      "epoch 164, loss 0.00453, train_acc 0.9993, valid_acc 0.9499, Time 00:01:02,lr 0.001\n",
      "epoch 165, loss 0.00423, train_acc 0.9995, valid_acc 0.9507, Time 00:01:02,lr 0.001\n",
      "epoch 166, loss 0.00424, train_acc 0.9994, valid_acc 0.9496, Time 00:01:02,lr 0.001\n",
      "epoch 167, loss 0.00392, train_acc 0.9995, valid_acc 0.9500, Time 00:01:02,lr 0.001\n",
      "epoch 168, loss 0.00373, train_acc 0.9996, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 169, loss 0.00395, train_acc 0.9993, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 170, loss 0.00376, train_acc 0.9996, valid_acc 0.9512, Time 00:01:02,lr 0.001\n",
      "epoch 171, loss 0.00352, train_acc 0.9996, valid_acc 0.9496, Time 00:01:02,lr 0.001\n",
      "epoch 172, loss 0.00372, train_acc 0.9995, valid_acc 0.9503, Time 00:01:02,lr 0.001\n",
      "epoch 173, loss 0.00341, train_acc 0.9997, valid_acc 0.9516, Time 00:01:02,lr 0.001\n",
      "epoch 174, loss 0.00373, train_acc 0.9994, valid_acc 0.9518, Time 00:01:02,lr 0.001\n",
      "epoch 175, loss 0.00336, train_acc 0.9996, valid_acc 0.9524, Time 00:01:02,lr 0.001\n",
      "epoch 176, loss 0.00351, train_acc 0.9994, valid_acc 0.9515, Time 00:01:02,lr 0.001\n",
      "epoch 177, loss 0.00332, train_acc 0.9995, valid_acc 0.9498, Time 00:01:02,lr 0.001\n",
      "epoch 178, loss 0.00337, train_acc 0.9997, valid_acc 0.9515, Time 00:01:02,lr 0.001\n",
      "epoch 179, loss 0.00315, train_acc 0.9996, valid_acc 0.9511, Time 00:01:02,lr 0.001\n",
      "epoch 180, loss 0.00273, train_acc 0.9998, valid_acc 0.9521, Time 00:01:02,lr 0.001\n",
      "epoch 181, loss 0.00303, train_acc 0.9996, valid_acc 0.9518, Time 00:01:02,lr 0.001\n",
      "epoch 182, loss 0.00319, train_acc 0.9997, valid_acc 0.9507, Time 00:01:02,lr 0.001\n",
      "epoch 183, loss 0.00281, train_acc 0.9997, valid_acc 0.9508, Time 00:01:02,lr 0.001\n",
      "epoch 184, loss 0.00315, train_acc 0.9996, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 185, loss 0.00279, train_acc 0.9998, valid_acc 0.9517, Time 00:01:02,lr 0.001\n",
      "epoch 186, loss 0.00277, train_acc 0.9998, valid_acc 0.9515, Time 00:01:02,lr 0.001\n",
      "epoch 187, loss 0.00271, train_acc 0.9998, valid_acc 0.9520, Time 00:01:02,lr 0.001\n",
      "epoch 188, loss 0.00276, train_acc 0.9998, valid_acc 0.9518, Time 00:01:02,lr 0.001\n",
      "epoch 189, loss 0.00269, train_acc 0.9998, valid_acc 0.9510, Time 00:01:02,lr 0.001\n",
      "epoch 190, loss 0.00292, train_acc 0.9997, valid_acc 0.9506, Time 00:01:02,lr 0.001\n",
      "epoch 191, loss 0.00248, train_acc 0.9998, valid_acc 0.9520, Time 00:01:02,lr 0.001\n",
      "epoch 192, loss 0.00255, train_acc 0.9998, valid_acc 0.9511, Time 00:01:02,lr 0.001\n",
      "epoch 193, loss 0.00247, train_acc 0.9998, valid_acc 0.9516, Time 00:01:02,lr 0.001\n",
      "epoch 194, loss 0.00274, train_acc 0.9997, valid_acc 0.9503, Time 00:01:02,lr 0.001\n",
      "epoch 195, loss 0.00255, train_acc 0.9998, valid_acc 0.9511, Time 00:01:02,lr 0.001\n",
      "epoch 196, loss 0.00253, train_acc 0.9998, valid_acc 0.9502, Time 00:01:02,lr 0.001\n",
      "epoch 197, loss 0.00240, train_acc 0.9998, valid_acc 0.9504, Time 00:01:02,lr 0.001\n",
      "epoch 198, loss 0.00272, train_acc 0.9997, valid_acc 0.9517, Time 00:01:02,lr 0.001\n",
      "epoch 199, loss 0.00250, train_acc 0.9999, valid_acc 0.9516, Time 00:01:02,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_522_c64(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_522_c64_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T06:23:24.183057Z",
     "start_time": "2018-03-05T06:20:05.615Z"
    }
   },
   "source": [
    "## 5.10 to find if width is important, compare to my resnet18 double all conv layers' channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T09:55:35.978942Z",
     "start_time": "2018-03-05T09:55:35.963930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_333_c64(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_333_c64, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=256, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=256))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T13:38:25.724067Z",
     "start_time": "2018-03-05T09:55:35.983801Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.91070, train_acc 0.3031, valid_acc 0.3908, Time 00:01:02,lr 0.1\n",
      "epoch 1, loss 1.49198, train_acc 0.4545, valid_acc 0.5138, Time 00:01:06,lr 0.1\n",
      "epoch 2, loss 1.16327, train_acc 0.5859, valid_acc 0.6698, Time 00:01:06,lr 0.1\n",
      "epoch 3, loss 0.89234, train_acc 0.6859, valid_acc 0.6864, Time 00:01:06,lr 0.1\n",
      "epoch 4, loss 0.74111, train_acc 0.7439, valid_acc 0.6966, Time 00:01:06,lr 0.1\n",
      "epoch 5, loss 0.64906, train_acc 0.7765, valid_acc 0.7626, Time 00:01:06,lr 0.1\n",
      "epoch 6, loss 0.59122, train_acc 0.7965, valid_acc 0.7955, Time 00:01:06,lr 0.1\n",
      "epoch 7, loss 0.54251, train_acc 0.8130, valid_acc 0.7992, Time 00:01:06,lr 0.1\n",
      "epoch 8, loss 0.51203, train_acc 0.8241, valid_acc 0.7816, Time 00:01:06,lr 0.1\n",
      "epoch 9, loss 0.48736, train_acc 0.8330, valid_acc 0.8220, Time 00:01:06,lr 0.1\n",
      "epoch 10, loss 0.46056, train_acc 0.8435, valid_acc 0.8035, Time 00:01:06,lr 0.1\n",
      "epoch 11, loss 0.44692, train_acc 0.8453, valid_acc 0.8289, Time 00:01:06,lr 0.1\n",
      "epoch 12, loss 0.42780, train_acc 0.8531, valid_acc 0.7941, Time 00:01:06,lr 0.1\n",
      "epoch 13, loss 0.41150, train_acc 0.8595, valid_acc 0.8375, Time 00:01:06,lr 0.1\n",
      "epoch 14, loss 0.40133, train_acc 0.8611, valid_acc 0.8405, Time 00:01:06,lr 0.1\n",
      "epoch 15, loss 0.39545, train_acc 0.8640, valid_acc 0.8352, Time 00:01:06,lr 0.1\n",
      "epoch 16, loss 0.37821, train_acc 0.8688, valid_acc 0.8246, Time 00:01:06,lr 0.1\n",
      "epoch 17, loss 0.37287, train_acc 0.8725, valid_acc 0.8544, Time 00:01:06,lr 0.1\n",
      "epoch 18, loss 0.36702, train_acc 0.8742, valid_acc 0.8677, Time 00:01:06,lr 0.1\n",
      "epoch 19, loss 0.36043, train_acc 0.8769, valid_acc 0.8453, Time 00:01:06,lr 0.1\n",
      "epoch 20, loss 0.35116, train_acc 0.8796, valid_acc 0.8630, Time 00:01:06,lr 0.1\n",
      "epoch 21, loss 0.34817, train_acc 0.8812, valid_acc 0.8529, Time 00:01:06,lr 0.1\n",
      "epoch 22, loss 0.34614, train_acc 0.8798, valid_acc 0.8583, Time 00:01:06,lr 0.1\n",
      "epoch 23, loss 0.34516, train_acc 0.8813, valid_acc 0.8291, Time 00:01:06,lr 0.1\n",
      "epoch 24, loss 0.33574, train_acc 0.8840, valid_acc 0.8728, Time 00:01:06,lr 0.1\n",
      "epoch 25, loss 0.32641, train_acc 0.8882, valid_acc 0.8566, Time 00:01:06,lr 0.1\n",
      "epoch 26, loss 0.32412, train_acc 0.8877, valid_acc 0.8509, Time 00:01:06,lr 0.1\n",
      "epoch 27, loss 0.33088, train_acc 0.8869, valid_acc 0.8587, Time 00:01:06,lr 0.1\n",
      "epoch 28, loss 0.32500, train_acc 0.8877, valid_acc 0.8625, Time 00:01:06,lr 0.1\n",
      "epoch 29, loss 0.32184, train_acc 0.8900, valid_acc 0.8426, Time 00:01:06,lr 0.1\n",
      "epoch 30, loss 0.31916, train_acc 0.8906, valid_acc 0.8364, Time 00:01:06,lr 0.1\n",
      "epoch 31, loss 0.31433, train_acc 0.8931, valid_acc 0.8625, Time 00:01:06,lr 0.1\n",
      "epoch 32, loss 0.30779, train_acc 0.8951, valid_acc 0.8342, Time 00:01:06,lr 0.1\n",
      "epoch 33, loss 0.31361, train_acc 0.8931, valid_acc 0.8827, Time 00:01:06,lr 0.1\n",
      "epoch 34, loss 0.30608, train_acc 0.8943, valid_acc 0.8668, Time 00:01:06,lr 0.1\n",
      "epoch 35, loss 0.30962, train_acc 0.8936, valid_acc 0.8702, Time 00:01:06,lr 0.1\n",
      "epoch 36, loss 0.30872, train_acc 0.8944, valid_acc 0.8554, Time 00:01:06,lr 0.1\n",
      "epoch 37, loss 0.30220, train_acc 0.8944, valid_acc 0.8865, Time 00:01:06,lr 0.1\n",
      "epoch 38, loss 0.30488, train_acc 0.8969, valid_acc 0.8845, Time 00:01:06,lr 0.1\n",
      "epoch 39, loss 0.30197, train_acc 0.8975, valid_acc 0.8600, Time 00:01:06,lr 0.1\n",
      "epoch 40, loss 0.30346, train_acc 0.8963, valid_acc 0.8598, Time 00:01:06,lr 0.1\n",
      "epoch 41, loss 0.29583, train_acc 0.8996, valid_acc 0.8866, Time 00:01:06,lr 0.1\n",
      "epoch 42, loss 0.29905, train_acc 0.8973, valid_acc 0.8841, Time 00:01:06,lr 0.1\n",
      "epoch 43, loss 0.29592, train_acc 0.9001, valid_acc 0.8851, Time 00:01:06,lr 0.1\n",
      "epoch 44, loss 0.29321, train_acc 0.9009, valid_acc 0.8920, Time 00:01:07,lr 0.1\n",
      "epoch 45, loss 0.29935, train_acc 0.8979, valid_acc 0.8725, Time 00:01:06,lr 0.1\n",
      "epoch 46, loss 0.29592, train_acc 0.8992, valid_acc 0.8432, Time 00:01:06,lr 0.1\n",
      "epoch 47, loss 0.29559, train_acc 0.8981, valid_acc 0.8579, Time 00:01:06,lr 0.1\n",
      "epoch 48, loss 0.29578, train_acc 0.8989, valid_acc 0.8650, Time 00:01:06,lr 0.1\n",
      "epoch 49, loss 0.29405, train_acc 0.8994, valid_acc 0.8795, Time 00:01:07,lr 0.1\n",
      "epoch 50, loss 0.29210, train_acc 0.8995, valid_acc 0.8885, Time 00:01:06,lr 0.1\n",
      "epoch 51, loss 0.29042, train_acc 0.8995, valid_acc 0.8897, Time 00:01:06,lr 0.1\n",
      "epoch 52, loss 0.28023, train_acc 0.9043, valid_acc 0.8725, Time 00:01:06,lr 0.1\n",
      "epoch 53, loss 0.29078, train_acc 0.9008, valid_acc 0.8650, Time 00:01:06,lr 0.1\n",
      "epoch 54, loss 0.28800, train_acc 0.9012, valid_acc 0.8774, Time 00:01:06,lr 0.1\n",
      "epoch 55, loss 0.28512, train_acc 0.9018, valid_acc 0.8775, Time 00:01:06,lr 0.1\n",
      "epoch 56, loss 0.28262, train_acc 0.9041, valid_acc 0.8773, Time 00:01:06,lr 0.1\n",
      "epoch 57, loss 0.28406, train_acc 0.9025, valid_acc 0.8589, Time 00:01:06,lr 0.1\n",
      "epoch 58, loss 0.28683, train_acc 0.9019, valid_acc 0.8626, Time 00:01:06,lr 0.1\n",
      "epoch 59, loss 0.28206, train_acc 0.9033, valid_acc 0.8879, Time 00:01:06,lr 0.1\n",
      "epoch 60, loss 0.28285, train_acc 0.9028, valid_acc 0.8679, Time 00:01:06,lr 0.1\n",
      "epoch 61, loss 0.28471, train_acc 0.9047, valid_acc 0.8763, Time 00:01:06,lr 0.1\n",
      "epoch 62, loss 0.28389, train_acc 0.9025, valid_acc 0.8808, Time 00:01:06,lr 0.1\n",
      "epoch 63, loss 0.28368, train_acc 0.9015, valid_acc 0.8510, Time 00:01:06,lr 0.1\n",
      "epoch 64, loss 0.28125, train_acc 0.9062, valid_acc 0.8661, Time 00:01:06,lr 0.1\n",
      "epoch 65, loss 0.28093, train_acc 0.9039, valid_acc 0.8646, Time 00:01:06,lr 0.1\n",
      "epoch 66, loss 0.28358, train_acc 0.9046, valid_acc 0.8738, Time 00:01:06,lr 0.1\n",
      "epoch 67, loss 0.27802, train_acc 0.9047, valid_acc 0.8539, Time 00:01:07,lr 0.1\n",
      "epoch 68, loss 0.28430, train_acc 0.9032, valid_acc 0.8884, Time 00:01:08,lr 0.1\n",
      "epoch 69, loss 0.27718, train_acc 0.9048, valid_acc 0.8814, Time 00:01:06,lr 0.1\n",
      "epoch 70, loss 0.28220, train_acc 0.9031, valid_acc 0.8893, Time 00:01:06,lr 0.1\n",
      "epoch 71, loss 0.28048, train_acc 0.9037, valid_acc 0.8666, Time 00:01:06,lr 0.1\n",
      "epoch 72, loss 0.28267, train_acc 0.9032, valid_acc 0.8719, Time 00:01:06,lr 0.1\n",
      "epoch 73, loss 0.27851, train_acc 0.9040, valid_acc 0.8690, Time 00:01:09,lr 0.1\n",
      "epoch 74, loss 0.27998, train_acc 0.9024, valid_acc 0.8920, Time 00:01:06,lr 0.1\n",
      "epoch 75, loss 0.27545, train_acc 0.9062, valid_acc 0.8621, Time 00:01:06,lr 0.1\n",
      "epoch 76, loss 0.28088, train_acc 0.9036, valid_acc 0.8745, Time 00:01:06,lr 0.1\n",
      "epoch 77, loss 0.27670, train_acc 0.9058, valid_acc 0.8773, Time 00:01:06,lr 0.1\n",
      "epoch 78, loss 0.27581, train_acc 0.9069, valid_acc 0.8827, Time 00:01:06,lr 0.1\n",
      "epoch 79, loss 0.27913, train_acc 0.9040, valid_acc 0.8706, Time 00:01:06,lr 0.1\n",
      "epoch 80, loss 0.13957, train_acc 0.9539, valid_acc 0.9370, Time 00:01:06,lr 0.01\n",
      "epoch 81, loss 0.09086, train_acc 0.9695, valid_acc 0.9394, Time 00:01:06,lr 0.01\n",
      "epoch 82, loss 0.07365, train_acc 0.9756, valid_acc 0.9393, Time 00:01:07,lr 0.01\n",
      "epoch 83, loss 0.06506, train_acc 0.9781, valid_acc 0.9417, Time 00:01:06,lr 0.01\n",
      "epoch 84, loss 0.05740, train_acc 0.9813, valid_acc 0.9424, Time 00:01:06,lr 0.01\n",
      "epoch 85, loss 0.05083, train_acc 0.9829, valid_acc 0.9400, Time 00:01:06,lr 0.01\n",
      "epoch 86, loss 0.04590, train_acc 0.9857, valid_acc 0.9419, Time 00:01:06,lr 0.01\n",
      "epoch 87, loss 0.03799, train_acc 0.9875, valid_acc 0.9441, Time 00:01:06,lr 0.01\n",
      "epoch 88, loss 0.03804, train_acc 0.9876, valid_acc 0.9427, Time 00:01:06,lr 0.01\n",
      "epoch 89, loss 0.03164, train_acc 0.9903, valid_acc 0.9441, Time 00:01:06,lr 0.01\n",
      "epoch 90, loss 0.02854, train_acc 0.9905, valid_acc 0.9425, Time 00:01:06,lr 0.01\n",
      "epoch 91, loss 0.02570, train_acc 0.9920, valid_acc 0.9443, Time 00:01:06,lr 0.01\n",
      "epoch 92, loss 0.02691, train_acc 0.9915, valid_acc 0.9447, Time 00:01:06,lr 0.01\n",
      "epoch 93, loss 0.02434, train_acc 0.9921, valid_acc 0.9428, Time 00:01:06,lr 0.01\n",
      "epoch 94, loss 0.02427, train_acc 0.9922, valid_acc 0.9455, Time 00:01:06,lr 0.01\n",
      "epoch 95, loss 0.02132, train_acc 0.9932, valid_acc 0.9446, Time 00:01:06,lr 0.01\n",
      "epoch 96, loss 0.01910, train_acc 0.9945, valid_acc 0.9442, Time 00:01:06,lr 0.01\n",
      "epoch 97, loss 0.01803, train_acc 0.9950, valid_acc 0.9443, Time 00:01:06,lr 0.01\n",
      "epoch 98, loss 0.02039, train_acc 0.9934, valid_acc 0.9434, Time 00:01:06,lr 0.01\n",
      "epoch 99, loss 0.01921, train_acc 0.9939, valid_acc 0.9445, Time 00:01:06,lr 0.01\n",
      "epoch 100, loss 0.01898, train_acc 0.9937, valid_acc 0.9406, Time 00:01:06,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.02047, train_acc 0.9936, valid_acc 0.9447, Time 00:01:06,lr 0.01\n",
      "epoch 102, loss 0.01822, train_acc 0.9943, valid_acc 0.9429, Time 00:01:06,lr 0.01\n",
      "epoch 103, loss 0.01926, train_acc 0.9939, valid_acc 0.9436, Time 00:01:06,lr 0.01\n",
      "epoch 104, loss 0.01883, train_acc 0.9943, valid_acc 0.9438, Time 00:01:06,lr 0.01\n",
      "epoch 105, loss 0.02021, train_acc 0.9935, valid_acc 0.9413, Time 00:01:06,lr 0.01\n",
      "epoch 106, loss 0.01931, train_acc 0.9936, valid_acc 0.9428, Time 00:01:06,lr 0.01\n",
      "epoch 107, loss 0.01908, train_acc 0.9940, valid_acc 0.9422, Time 00:01:06,lr 0.01\n",
      "epoch 108, loss 0.01983, train_acc 0.9935, valid_acc 0.9426, Time 00:01:09,lr 0.01\n",
      "epoch 109, loss 0.01886, train_acc 0.9941, valid_acc 0.9405, Time 00:01:09,lr 0.01\n",
      "epoch 110, loss 0.02233, train_acc 0.9931, valid_acc 0.9411, Time 00:01:08,lr 0.01\n",
      "epoch 111, loss 0.01953, train_acc 0.9939, valid_acc 0.9378, Time 00:01:06,lr 0.01\n",
      "epoch 112, loss 0.02158, train_acc 0.9933, valid_acc 0.9376, Time 00:01:06,lr 0.01\n",
      "epoch 113, loss 0.02251, train_acc 0.9925, valid_acc 0.9409, Time 00:01:06,lr 0.01\n",
      "epoch 114, loss 0.02559, train_acc 0.9916, valid_acc 0.9346, Time 00:01:06,lr 0.01\n",
      "epoch 115, loss 0.02289, train_acc 0.9927, valid_acc 0.9398, Time 00:01:07,lr 0.01\n",
      "epoch 116, loss 0.02405, train_acc 0.9918, valid_acc 0.9409, Time 00:01:06,lr 0.01\n",
      "epoch 117, loss 0.02692, train_acc 0.9910, valid_acc 0.9377, Time 00:01:06,lr 0.01\n",
      "epoch 118, loss 0.02370, train_acc 0.9924, valid_acc 0.9361, Time 00:01:06,lr 0.01\n",
      "epoch 119, loss 0.02839, train_acc 0.9913, valid_acc 0.9372, Time 00:01:06,lr 0.01\n",
      "epoch 120, loss 0.02599, train_acc 0.9913, valid_acc 0.9359, Time 00:01:06,lr 0.01\n",
      "epoch 121, loss 0.02696, train_acc 0.9914, valid_acc 0.9357, Time 00:01:06,lr 0.01\n",
      "epoch 122, loss 0.02899, train_acc 0.9901, valid_acc 0.9317, Time 00:01:06,lr 0.01\n",
      "epoch 123, loss 0.03193, train_acc 0.9896, valid_acc 0.9408, Time 00:01:06,lr 0.01\n",
      "epoch 124, loss 0.02635, train_acc 0.9917, valid_acc 0.9395, Time 00:01:06,lr 0.01\n",
      "epoch 125, loss 0.03074, train_acc 0.9901, valid_acc 0.9381, Time 00:01:06,lr 0.01\n",
      "epoch 126, loss 0.03066, train_acc 0.9894, valid_acc 0.9352, Time 00:01:06,lr 0.01\n",
      "epoch 127, loss 0.03199, train_acc 0.9894, valid_acc 0.9390, Time 00:01:06,lr 0.01\n",
      "epoch 128, loss 0.03443, train_acc 0.9882, valid_acc 0.9352, Time 00:01:06,lr 0.01\n",
      "epoch 129, loss 0.03173, train_acc 0.9892, valid_acc 0.9283, Time 00:01:06,lr 0.01\n",
      "epoch 130, loss 0.03065, train_acc 0.9898, valid_acc 0.9334, Time 00:01:06,lr 0.01\n",
      "epoch 131, loss 0.03048, train_acc 0.9903, valid_acc 0.9392, Time 00:01:06,lr 0.01\n",
      "epoch 132, loss 0.03334, train_acc 0.9890, valid_acc 0.9350, Time 00:01:06,lr 0.01\n",
      "epoch 133, loss 0.03732, train_acc 0.9877, valid_acc 0.9384, Time 00:01:06,lr 0.01\n",
      "epoch 134, loss 0.03484, train_acc 0.9879, valid_acc 0.9261, Time 00:01:06,lr 0.01\n",
      "epoch 135, loss 0.03484, train_acc 0.9882, valid_acc 0.9351, Time 00:01:06,lr 0.01\n",
      "epoch 136, loss 0.03939, train_acc 0.9872, valid_acc 0.9350, Time 00:01:06,lr 0.01\n",
      "epoch 137, loss 0.03319, train_acc 0.9892, valid_acc 0.9326, Time 00:01:06,lr 0.01\n",
      "epoch 138, loss 0.03750, train_acc 0.9875, valid_acc 0.9313, Time 00:01:06,lr 0.01\n",
      "epoch 139, loss 0.03828, train_acc 0.9873, valid_acc 0.9325, Time 00:01:06,lr 0.01\n",
      "epoch 140, loss 0.03764, train_acc 0.9875, valid_acc 0.9328, Time 00:01:06,lr 0.01\n",
      "epoch 141, loss 0.03222, train_acc 0.9899, valid_acc 0.9264, Time 00:01:06,lr 0.01\n",
      "epoch 142, loss 0.03475, train_acc 0.9885, valid_acc 0.9321, Time 00:01:06,lr 0.01\n",
      "epoch 143, loss 0.03488, train_acc 0.9884, valid_acc 0.9293, Time 00:01:06,lr 0.01\n",
      "epoch 144, loss 0.03515, train_acc 0.9885, valid_acc 0.9354, Time 00:01:07,lr 0.01\n",
      "epoch 145, loss 0.03172, train_acc 0.9897, valid_acc 0.9288, Time 00:01:08,lr 0.01\n",
      "epoch 146, loss 0.04010, train_acc 0.9867, valid_acc 0.9282, Time 00:01:13,lr 0.01\n",
      "epoch 147, loss 0.03735, train_acc 0.9873, valid_acc 0.9327, Time 00:01:09,lr 0.01\n",
      "epoch 148, loss 0.03458, train_acc 0.9883, valid_acc 0.9316, Time 00:01:11,lr 0.01\n",
      "epoch 149, loss 0.04376, train_acc 0.9848, valid_acc 0.9363, Time 00:01:06,lr 0.01\n",
      "epoch 150, loss 0.01750, train_acc 0.9948, valid_acc 0.9462, Time 00:01:06,lr 0.001\n",
      "epoch 151, loss 0.01047, train_acc 0.9974, valid_acc 0.9471, Time 00:01:06,lr 0.001\n",
      "epoch 152, loss 0.00795, train_acc 0.9982, valid_acc 0.9488, Time 00:01:06,lr 0.001\n",
      "epoch 153, loss 0.00667, train_acc 0.9984, valid_acc 0.9484, Time 00:01:07,lr 0.001\n",
      "epoch 154, loss 0.00581, train_acc 0.9986, valid_acc 0.9473, Time 00:01:06,lr 0.001\n",
      "epoch 155, loss 0.00485, train_acc 0.9990, valid_acc 0.9505, Time 00:01:06,lr 0.001\n",
      "epoch 156, loss 0.00483, train_acc 0.9987, valid_acc 0.9504, Time 00:01:06,lr 0.001\n",
      "epoch 157, loss 0.00408, train_acc 0.9993, valid_acc 0.9510, Time 00:01:06,lr 0.001\n",
      "epoch 158, loss 0.00388, train_acc 0.9994, valid_acc 0.9500, Time 00:01:06,lr 0.001\n",
      "epoch 159, loss 0.00368, train_acc 0.9993, valid_acc 0.9508, Time 00:01:06,lr 0.001\n",
      "epoch 160, loss 0.00365, train_acc 0.9992, valid_acc 0.9506, Time 00:01:06,lr 0.001\n",
      "epoch 161, loss 0.00294, train_acc 0.9997, valid_acc 0.9511, Time 00:01:06,lr 0.001\n",
      "epoch 162, loss 0.00302, train_acc 0.9996, valid_acc 0.9504, Time 00:01:06,lr 0.001\n",
      "epoch 163, loss 0.00283, train_acc 0.9995, valid_acc 0.9506, Time 00:01:06,lr 0.001\n",
      "epoch 164, loss 0.00246, train_acc 0.9997, valid_acc 0.9514, Time 00:01:06,lr 0.001\n",
      "epoch 165, loss 0.00236, train_acc 0.9996, valid_acc 0.9500, Time 00:01:06,lr 0.001\n",
      "epoch 166, loss 0.00279, train_acc 0.9995, valid_acc 0.9510, Time 00:01:06,lr 0.001\n",
      "epoch 167, loss 0.00224, train_acc 0.9996, valid_acc 0.9521, Time 00:01:06,lr 0.001\n",
      "epoch 168, loss 0.00224, train_acc 0.9998, valid_acc 0.9508, Time 00:01:06,lr 0.001\n",
      "epoch 169, loss 0.00219, train_acc 0.9997, valid_acc 0.9526, Time 00:01:06,lr 0.001\n",
      "epoch 170, loss 0.00219, train_acc 0.9997, valid_acc 0.9520, Time 00:01:06,lr 0.001\n",
      "epoch 171, loss 0.00200, train_acc 0.9997, valid_acc 0.9516, Time 00:01:06,lr 0.001\n",
      "epoch 172, loss 0.00190, train_acc 0.9997, valid_acc 0.9522, Time 00:01:06,lr 0.001\n",
      "epoch 173, loss 0.00208, train_acc 0.9996, valid_acc 0.9507, Time 00:01:06,lr 0.001\n",
      "epoch 174, loss 0.00183, train_acc 0.9998, valid_acc 0.9513, Time 00:01:06,lr 0.001\n",
      "epoch 175, loss 0.00196, train_acc 0.9997, valid_acc 0.9517, Time 00:01:06,lr 0.001\n",
      "epoch 176, loss 0.00178, train_acc 0.9997, valid_acc 0.9512, Time 00:01:06,lr 0.001\n",
      "epoch 177, loss 0.00176, train_acc 0.9997, valid_acc 0.9525, Time 00:01:06,lr 0.001\n",
      "epoch 178, loss 0.00173, train_acc 0.9998, valid_acc 0.9513, Time 00:01:06,lr 0.001\n",
      "epoch 179, loss 0.00203, train_acc 0.9997, valid_acc 0.9531, Time 00:01:06,lr 0.001\n",
      "epoch 180, loss 0.00158, train_acc 0.9998, valid_acc 0.9521, Time 00:01:06,lr 0.001\n",
      "epoch 181, loss 0.00166, train_acc 0.9998, valid_acc 0.9522, Time 00:01:06,lr 0.001\n",
      "epoch 182, loss 0.00158, train_acc 0.9997, valid_acc 0.9518, Time 00:01:06,lr 0.001\n",
      "epoch 183, loss 0.00173, train_acc 0.9997, valid_acc 0.9513, Time 00:01:06,lr 0.001\n",
      "epoch 184, loss 0.00152, train_acc 0.9999, valid_acc 0.9527, Time 00:01:06,lr 0.001\n",
      "epoch 185, loss 0.00155, train_acc 0.9998, valid_acc 0.9529, Time 00:01:06,lr 0.001\n",
      "epoch 186, loss 0.00138, train_acc 0.9998, valid_acc 0.9518, Time 00:01:06,lr 0.001\n",
      "epoch 187, loss 0.00139, train_acc 0.9999, valid_acc 0.9535, Time 00:01:06,lr 0.001\n",
      "epoch 188, loss 0.00149, train_acc 0.9997, valid_acc 0.9529, Time 00:01:06,lr 0.001\n",
      "epoch 189, loss 0.00136, train_acc 0.9999, valid_acc 0.9528, Time 00:01:06,lr 0.001\n",
      "epoch 190, loss 0.00143, train_acc 0.9998, valid_acc 0.9527, Time 00:01:06,lr 0.001\n",
      "epoch 191, loss 0.00152, train_acc 0.9997, valid_acc 0.9536, Time 00:01:06,lr 0.001\n",
      "epoch 192, loss 0.00138, train_acc 0.9998, valid_acc 0.9547, Time 00:01:06,lr 0.001\n",
      "epoch 193, loss 0.00123, train_acc 0.9999, valid_acc 0.9551, Time 00:01:06,lr 0.001\n",
      "epoch 194, loss 0.00131, train_acc 0.9998, valid_acc 0.9532, Time 00:01:07,lr 0.001\n",
      "epoch 195, loss 0.00131, train_acc 0.9998, valid_acc 0.9544, Time 00:01:07,lr 0.001\n",
      "epoch 196, loss 0.00135, train_acc 0.9998, valid_acc 0.9538, Time 00:01:08,lr 0.001\n",
      "epoch 197, loss 0.00130, train_acc 0.9998, valid_acc 0.9536, Time 00:01:06,lr 0.001\n",
      "epoch 198, loss 0.00139, train_acc 0.9998, valid_acc 0.9537, Time 00:01:13,lr 0.001\n",
      "epoch 199, loss 0.00130, train_acc 0.9998, valid_acc 0.9524, Time 00:01:15,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_333_c64(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_333_c64_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12 remove global pooling use flatten to get more infomation\n",
    "\n",
    "more param need smaller lr cause bigger loss<br/>\n",
    "if remove global pooling completely, params will up to 64x, use start lr=0.001 can get loss down(lr=0.1,0.01 can not)<br/>\n",
    "so trade-off, I set global pooling size to 4, and start lr=0.05<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:44:38.277888Z",
     "start_time": "2018-03-09T14:44:38.256242Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_no_avgpooling(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_no_avgpooling, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            #net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.AvgPool2D(pool_size=4))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T16:31:42.369297Z",
     "start_time": "2018-03-09T14:44:38.722549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.86369, train_acc 0.3183, valid_acc 0.4513, Time 00:00:31,lr 0.05\n",
      "epoch 1, loss 1.41296, train_acc 0.4844, valid_acc 0.5312, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 1.09882, train_acc 0.6103, valid_acc 0.6673, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.86951, train_acc 0.6941, valid_acc 0.7282, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.72841, train_acc 0.7466, valid_acc 0.7471, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.64875, train_acc 0.7751, valid_acc 0.7870, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.59037, train_acc 0.7980, valid_acc 0.7901, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.54705, train_acc 0.8113, valid_acc 0.7860, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.51094, train_acc 0.8247, valid_acc 0.8068, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.48787, train_acc 0.8323, valid_acc 0.8220, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.46276, train_acc 0.8392, valid_acc 0.8186, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.44473, train_acc 0.8475, valid_acc 0.8391, Time 00:00:32,lr 0.05\n",
      "epoch 12, loss 0.42401, train_acc 0.8535, valid_acc 0.8411, Time 00:00:31,lr 0.05\n",
      "epoch 13, loss 0.41548, train_acc 0.8551, valid_acc 0.8389, Time 00:00:31,lr 0.05\n",
      "epoch 14, loss 0.39785, train_acc 0.8616, valid_acc 0.8540, Time 00:00:31,lr 0.05\n",
      "epoch 15, loss 0.38835, train_acc 0.8661, valid_acc 0.8544, Time 00:00:31,lr 0.05\n",
      "epoch 16, loss 0.37839, train_acc 0.8692, valid_acc 0.8538, Time 00:00:31,lr 0.05\n",
      "epoch 17, loss 0.36657, train_acc 0.8717, valid_acc 0.8496, Time 00:00:31,lr 0.05\n",
      "epoch 18, loss 0.35731, train_acc 0.8752, valid_acc 0.8531, Time 00:00:31,lr 0.05\n",
      "epoch 19, loss 0.35369, train_acc 0.8787, valid_acc 0.8546, Time 00:00:31,lr 0.05\n",
      "epoch 20, loss 0.34612, train_acc 0.8799, valid_acc 0.8628, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.34034, train_acc 0.8812, valid_acc 0.8433, Time 00:00:31,lr 0.05\n",
      "epoch 22, loss 0.33359, train_acc 0.8841, valid_acc 0.8722, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.33043, train_acc 0.8868, valid_acc 0.8686, Time 00:00:31,lr 0.05\n",
      "epoch 24, loss 0.32236, train_acc 0.8888, valid_acc 0.8731, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.31772, train_acc 0.8895, valid_acc 0.8647, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.31008, train_acc 0.8926, valid_acc 0.8538, Time 00:00:31,lr 0.05\n",
      "epoch 27, loss 0.31079, train_acc 0.8924, valid_acc 0.8575, Time 00:00:31,lr 0.05\n",
      "epoch 28, loss 0.30798, train_acc 0.8940, valid_acc 0.8420, Time 00:00:31,lr 0.05\n",
      "epoch 29, loss 0.29690, train_acc 0.8988, valid_acc 0.8604, Time 00:00:31,lr 0.05\n",
      "epoch 30, loss 0.29969, train_acc 0.8970, valid_acc 0.8636, Time 00:00:32,lr 0.05\n",
      "epoch 31, loss 0.30198, train_acc 0.8961, valid_acc 0.8509, Time 00:00:31,lr 0.05\n",
      "epoch 32, loss 0.29014, train_acc 0.8992, valid_acc 0.8540, Time 00:00:31,lr 0.05\n",
      "epoch 33, loss 0.28759, train_acc 0.8998, valid_acc 0.8803, Time 00:00:31,lr 0.05\n",
      "epoch 34, loss 0.28576, train_acc 0.9018, valid_acc 0.8521, Time 00:00:32,lr 0.05\n",
      "epoch 35, loss 0.28093, train_acc 0.9021, valid_acc 0.8730, Time 00:00:34,lr 0.05\n",
      "epoch 36, loss 0.28297, train_acc 0.9011, valid_acc 0.8515, Time 00:00:33,lr 0.05\n",
      "epoch 37, loss 0.27783, train_acc 0.9031, valid_acc 0.8719, Time 00:00:32,lr 0.05\n",
      "epoch 38, loss 0.28279, train_acc 0.9009, valid_acc 0.8848, Time 00:00:32,lr 0.05\n",
      "epoch 39, loss 0.27545, train_acc 0.9060, valid_acc 0.8570, Time 00:00:31,lr 0.05\n",
      "epoch 40, loss 0.27234, train_acc 0.9038, valid_acc 0.8626, Time 00:00:32,lr 0.05\n",
      "epoch 41, loss 0.27001, train_acc 0.9061, valid_acc 0.8645, Time 00:00:32,lr 0.05\n",
      "epoch 42, loss 0.27082, train_acc 0.9065, valid_acc 0.8616, Time 00:00:32,lr 0.05\n",
      "epoch 43, loss 0.26503, train_acc 0.9087, valid_acc 0.8690, Time 00:00:31,lr 0.05\n",
      "epoch 44, loss 0.26470, train_acc 0.9084, valid_acc 0.8840, Time 00:00:31,lr 0.05\n",
      "epoch 45, loss 0.26786, train_acc 0.9056, valid_acc 0.8561, Time 00:00:32,lr 0.05\n",
      "epoch 46, loss 0.26153, train_acc 0.9100, valid_acc 0.8676, Time 00:00:32,lr 0.05\n",
      "epoch 47, loss 0.26105, train_acc 0.9092, valid_acc 0.8640, Time 00:00:31,lr 0.05\n",
      "epoch 48, loss 0.26102, train_acc 0.9093, valid_acc 0.8787, Time 00:00:31,lr 0.05\n",
      "epoch 49, loss 0.26048, train_acc 0.9102, valid_acc 0.8803, Time 00:00:32,lr 0.05\n",
      "epoch 50, loss 0.26086, train_acc 0.9086, valid_acc 0.8816, Time 00:00:31,lr 0.05\n",
      "epoch 51, loss 0.25517, train_acc 0.9123, valid_acc 0.8743, Time 00:00:32,lr 0.05\n",
      "epoch 52, loss 0.25648, train_acc 0.9114, valid_acc 0.8671, Time 00:00:32,lr 0.05\n",
      "epoch 53, loss 0.25214, train_acc 0.9122, valid_acc 0.8923, Time 00:00:31,lr 0.05\n",
      "epoch 54, loss 0.25421, train_acc 0.9120, valid_acc 0.8528, Time 00:00:31,lr 0.05\n",
      "epoch 55, loss 0.25016, train_acc 0.9134, valid_acc 0.8674, Time 00:00:31,lr 0.05\n",
      "epoch 56, loss 0.24797, train_acc 0.9137, valid_acc 0.8738, Time 00:00:32,lr 0.05\n",
      "epoch 57, loss 0.25224, train_acc 0.9138, valid_acc 0.8915, Time 00:00:31,lr 0.05\n",
      "epoch 58, loss 0.24750, train_acc 0.9144, valid_acc 0.8754, Time 00:00:32,lr 0.05\n",
      "epoch 59, loss 0.25205, train_acc 0.9125, valid_acc 0.8849, Time 00:00:31,lr 0.05\n",
      "epoch 60, loss 0.24709, train_acc 0.9143, valid_acc 0.8655, Time 00:00:32,lr 0.05\n",
      "epoch 61, loss 0.24498, train_acc 0.9162, valid_acc 0.8681, Time 00:00:31,lr 0.05\n",
      "epoch 62, loss 0.24547, train_acc 0.9151, valid_acc 0.8779, Time 00:00:31,lr 0.05\n",
      "epoch 63, loss 0.24523, train_acc 0.9149, valid_acc 0.8876, Time 00:00:32,lr 0.05\n",
      "epoch 64, loss 0.24677, train_acc 0.9144, valid_acc 0.8803, Time 00:00:32,lr 0.05\n",
      "epoch 65, loss 0.24252, train_acc 0.9163, valid_acc 0.8838, Time 00:00:32,lr 0.05\n",
      "epoch 66, loss 0.24278, train_acc 0.9159, valid_acc 0.8731, Time 00:00:34,lr 0.05\n",
      "epoch 67, loss 0.24482, train_acc 0.9152, valid_acc 0.8764, Time 00:00:31,lr 0.05\n",
      "epoch 68, loss 0.24281, train_acc 0.9162, valid_acc 0.8858, Time 00:00:32,lr 0.05\n",
      "epoch 69, loss 0.23871, train_acc 0.9180, valid_acc 0.8865, Time 00:00:32,lr 0.05\n",
      "epoch 70, loss 0.24334, train_acc 0.9157, valid_acc 0.8676, Time 00:00:31,lr 0.05\n",
      "epoch 71, loss 0.24154, train_acc 0.9151, valid_acc 0.8914, Time 00:00:31,lr 0.05\n",
      "epoch 72, loss 0.24128, train_acc 0.9159, valid_acc 0.8880, Time 00:00:32,lr 0.05\n",
      "epoch 73, loss 0.23845, train_acc 0.9174, valid_acc 0.8726, Time 00:00:32,lr 0.05\n",
      "epoch 74, loss 0.23552, train_acc 0.9181, valid_acc 0.8898, Time 00:00:31,lr 0.05\n",
      "epoch 75, loss 0.23641, train_acc 0.9191, valid_acc 0.8718, Time 00:00:32,lr 0.05\n",
      "epoch 76, loss 0.24092, train_acc 0.9162, valid_acc 0.8507, Time 00:00:34,lr 0.05\n",
      "epoch 77, loss 0.23294, train_acc 0.9193, valid_acc 0.8802, Time 00:00:31,lr 0.05\n",
      "epoch 78, loss 0.23625, train_acc 0.9183, valid_acc 0.8936, Time 00:00:32,lr 0.05\n",
      "epoch 79, loss 0.23129, train_acc 0.9189, valid_acc 0.8932, Time 00:00:32,lr 0.05\n",
      "epoch 80, loss 0.12499, train_acc 0.9566, valid_acc 0.9254, Time 00:00:32,lr 0.005\n",
      "epoch 81, loss 0.09201, train_acc 0.9695, valid_acc 0.9301, Time 00:00:32,lr 0.005\n",
      "epoch 82, loss 0.07932, train_acc 0.9734, valid_acc 0.9335, Time 00:00:31,lr 0.005\n",
      "epoch 83, loss 0.07113, train_acc 0.9761, valid_acc 0.9311, Time 00:00:32,lr 0.005\n",
      "epoch 84, loss 0.06533, train_acc 0.9780, valid_acc 0.9320, Time 00:00:32,lr 0.005\n",
      "epoch 85, loss 0.05805, train_acc 0.9810, valid_acc 0.9354, Time 00:00:32,lr 0.005\n",
      "epoch 86, loss 0.05357, train_acc 0.9818, valid_acc 0.9335, Time 00:00:32,lr 0.005\n",
      "epoch 87, loss 0.04943, train_acc 0.9836, valid_acc 0.9341, Time 00:00:32,lr 0.005\n",
      "epoch 88, loss 0.04771, train_acc 0.9843, valid_acc 0.9345, Time 00:00:32,lr 0.005\n",
      "epoch 89, loss 0.04259, train_acc 0.9861, valid_acc 0.9324, Time 00:00:32,lr 0.005\n",
      "epoch 90, loss 0.04138, train_acc 0.9865, valid_acc 0.9352, Time 00:00:32,lr 0.005\n",
      "epoch 91, loss 0.03834, train_acc 0.9870, valid_acc 0.9330, Time 00:00:32,lr 0.005\n",
      "epoch 92, loss 0.03543, train_acc 0.9888, valid_acc 0.9340, Time 00:00:31,lr 0.005\n",
      "epoch 93, loss 0.03302, train_acc 0.9896, valid_acc 0.9353, Time 00:00:32,lr 0.005\n",
      "epoch 94, loss 0.03238, train_acc 0.9895, valid_acc 0.9350, Time 00:00:32,lr 0.005\n",
      "epoch 95, loss 0.02967, train_acc 0.9903, valid_acc 0.9354, Time 00:00:32,lr 0.005\n",
      "epoch 96, loss 0.02814, train_acc 0.9911, valid_acc 0.9355, Time 00:00:31,lr 0.005\n",
      "epoch 97, loss 0.02808, train_acc 0.9911, valid_acc 0.9359, Time 00:00:32,lr 0.005\n",
      "epoch 98, loss 0.02755, train_acc 0.9912, valid_acc 0.9348, Time 00:00:32,lr 0.005\n",
      "epoch 99, loss 0.02720, train_acc 0.9913, valid_acc 0.9355, Time 00:00:32,lr 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.02379, train_acc 0.9925, valid_acc 0.9356, Time 00:00:32,lr 0.005\n",
      "epoch 101, loss 0.02311, train_acc 0.9928, valid_acc 0.9340, Time 00:00:32,lr 0.005\n",
      "epoch 102, loss 0.02330, train_acc 0.9929, valid_acc 0.9349, Time 00:00:32,lr 0.005\n",
      "epoch 103, loss 0.02119, train_acc 0.9933, valid_acc 0.9338, Time 00:00:32,lr 0.005\n",
      "epoch 104, loss 0.02129, train_acc 0.9935, valid_acc 0.9359, Time 00:00:32,lr 0.005\n",
      "epoch 105, loss 0.02065, train_acc 0.9940, valid_acc 0.9342, Time 00:00:32,lr 0.005\n",
      "epoch 106, loss 0.01940, train_acc 0.9942, valid_acc 0.9351, Time 00:00:31,lr 0.005\n",
      "epoch 107, loss 0.02016, train_acc 0.9935, valid_acc 0.9342, Time 00:00:32,lr 0.005\n",
      "epoch 108, loss 0.02009, train_acc 0.9940, valid_acc 0.9345, Time 00:00:32,lr 0.005\n",
      "epoch 109, loss 0.01818, train_acc 0.9946, valid_acc 0.9370, Time 00:00:32,lr 0.005\n",
      "epoch 110, loss 0.01757, train_acc 0.9948, valid_acc 0.9357, Time 00:00:32,lr 0.005\n",
      "epoch 111, loss 0.01770, train_acc 0.9946, valid_acc 0.9345, Time 00:00:31,lr 0.005\n",
      "epoch 112, loss 0.01733, train_acc 0.9949, valid_acc 0.9339, Time 00:00:32,lr 0.005\n",
      "epoch 113, loss 0.01655, train_acc 0.9953, valid_acc 0.9342, Time 00:00:32,lr 0.005\n",
      "epoch 114, loss 0.01659, train_acc 0.9948, valid_acc 0.9340, Time 00:00:32,lr 0.005\n",
      "epoch 115, loss 0.01606, train_acc 0.9956, valid_acc 0.9347, Time 00:00:32,lr 0.005\n",
      "epoch 116, loss 0.01714, train_acc 0.9947, valid_acc 0.9349, Time 00:00:32,lr 0.005\n",
      "epoch 117, loss 0.01472, train_acc 0.9959, valid_acc 0.9351, Time 00:00:32,lr 0.005\n",
      "epoch 118, loss 0.01462, train_acc 0.9959, valid_acc 0.9366, Time 00:00:32,lr 0.005\n",
      "epoch 119, loss 0.01521, train_acc 0.9953, valid_acc 0.9338, Time 00:00:32,lr 0.005\n",
      "epoch 120, loss 0.01750, train_acc 0.9946, valid_acc 0.9371, Time 00:00:32,lr 0.005\n",
      "epoch 121, loss 0.01686, train_acc 0.9951, valid_acc 0.9346, Time 00:00:32,lr 0.005\n",
      "epoch 122, loss 0.01576, train_acc 0.9951, valid_acc 0.9336, Time 00:00:32,lr 0.005\n",
      "epoch 123, loss 0.01641, train_acc 0.9951, valid_acc 0.9358, Time 00:00:32,lr 0.005\n",
      "epoch 124, loss 0.01478, train_acc 0.9957, valid_acc 0.9362, Time 00:00:32,lr 0.005\n",
      "epoch 125, loss 0.01503, train_acc 0.9958, valid_acc 0.9337, Time 00:00:32,lr 0.005\n",
      "epoch 126, loss 0.01427, train_acc 0.9960, valid_acc 0.9355, Time 00:00:32,lr 0.005\n",
      "epoch 127, loss 0.01637, train_acc 0.9951, valid_acc 0.9328, Time 00:00:32,lr 0.005\n",
      "epoch 128, loss 0.01419, train_acc 0.9957, valid_acc 0.9331, Time 00:00:32,lr 0.005\n",
      "epoch 129, loss 0.01513, train_acc 0.9956, valid_acc 0.9337, Time 00:00:32,lr 0.005\n",
      "epoch 130, loss 0.01384, train_acc 0.9958, valid_acc 0.9334, Time 00:00:32,lr 0.005\n",
      "epoch 131, loss 0.01315, train_acc 0.9965, valid_acc 0.9364, Time 00:00:32,lr 0.005\n",
      "epoch 132, loss 0.01436, train_acc 0.9954, valid_acc 0.9354, Time 00:00:32,lr 0.005\n",
      "epoch 133, loss 0.01490, train_acc 0.9956, valid_acc 0.9345, Time 00:00:32,lr 0.005\n",
      "epoch 134, loss 0.01443, train_acc 0.9956, valid_acc 0.9358, Time 00:00:32,lr 0.005\n",
      "epoch 135, loss 0.01281, train_acc 0.9961, valid_acc 0.9341, Time 00:00:32,lr 0.005\n",
      "epoch 136, loss 0.01672, train_acc 0.9947, valid_acc 0.9333, Time 00:00:32,lr 0.005\n",
      "epoch 137, loss 0.01504, train_acc 0.9954, valid_acc 0.9322, Time 00:00:32,lr 0.005\n",
      "epoch 138, loss 0.01695, train_acc 0.9945, valid_acc 0.9328, Time 00:00:33,lr 0.005\n",
      "epoch 139, loss 0.01555, train_acc 0.9955, valid_acc 0.9360, Time 00:00:32,lr 0.005\n",
      "epoch 140, loss 0.01496, train_acc 0.9953, valid_acc 0.9346, Time 00:00:32,lr 0.005\n",
      "epoch 141, loss 0.01565, train_acc 0.9950, valid_acc 0.9342, Time 00:00:32,lr 0.005\n",
      "epoch 142, loss 0.01392, train_acc 0.9958, valid_acc 0.9368, Time 00:00:32,lr 0.005\n",
      "epoch 143, loss 0.01587, train_acc 0.9950, valid_acc 0.9344, Time 00:00:31,lr 0.005\n",
      "epoch 144, loss 0.01565, train_acc 0.9954, valid_acc 0.9346, Time 00:00:32,lr 0.005\n",
      "epoch 145, loss 0.01661, train_acc 0.9946, valid_acc 0.9318, Time 00:00:32,lr 0.005\n",
      "epoch 146, loss 0.01586, train_acc 0.9954, valid_acc 0.9333, Time 00:00:32,lr 0.005\n",
      "epoch 147, loss 0.01866, train_acc 0.9938, valid_acc 0.9348, Time 00:00:32,lr 0.005\n",
      "epoch 148, loss 0.01782, train_acc 0.9941, valid_acc 0.9333, Time 00:00:32,lr 0.005\n",
      "epoch 149, loss 0.01754, train_acc 0.9946, valid_acc 0.9324, Time 00:00:32,lr 0.005\n",
      "epoch 150, loss 0.01265, train_acc 0.9964, valid_acc 0.9367, Time 00:00:32,lr 0.0005\n",
      "epoch 151, loss 0.00918, train_acc 0.9979, valid_acc 0.9380, Time 00:00:32,lr 0.0005\n",
      "epoch 152, loss 0.00856, train_acc 0.9978, valid_acc 0.9392, Time 00:00:33,lr 0.0005\n",
      "epoch 153, loss 0.00722, train_acc 0.9985, valid_acc 0.9394, Time 00:00:32,lr 0.0005\n",
      "epoch 154, loss 0.00695, train_acc 0.9986, valid_acc 0.9408, Time 00:00:31,lr 0.0005\n",
      "epoch 155, loss 0.00634, train_acc 0.9987, valid_acc 0.9401, Time 00:00:32,lr 0.0005\n",
      "epoch 156, loss 0.00589, train_acc 0.9988, valid_acc 0.9390, Time 00:00:31,lr 0.0005\n",
      "epoch 157, loss 0.00578, train_acc 0.9988, valid_acc 0.9407, Time 00:00:32,lr 0.0005\n",
      "epoch 158, loss 0.00610, train_acc 0.9988, valid_acc 0.9400, Time 00:00:31,lr 0.0005\n",
      "epoch 159, loss 0.00541, train_acc 0.9988, valid_acc 0.9399, Time 00:00:31,lr 0.0005\n",
      "epoch 160, loss 0.00581, train_acc 0.9988, valid_acc 0.9420, Time 00:00:31,lr 0.0005\n",
      "epoch 161, loss 0.00533, train_acc 0.9990, valid_acc 0.9408, Time 00:00:32,lr 0.0005\n",
      "epoch 162, loss 0.00517, train_acc 0.9989, valid_acc 0.9405, Time 00:00:32,lr 0.0005\n",
      "epoch 163, loss 0.00501, train_acc 0.9988, valid_acc 0.9406, Time 00:00:31,lr 0.0005\n",
      "epoch 164, loss 0.00445, train_acc 0.9992, valid_acc 0.9397, Time 00:00:31,lr 0.0005\n",
      "epoch 165, loss 0.00476, train_acc 0.9992, valid_acc 0.9412, Time 00:00:32,lr 0.0005\n",
      "epoch 166, loss 0.00492, train_acc 0.9992, valid_acc 0.9414, Time 00:00:31,lr 0.0005\n",
      "epoch 167, loss 0.00465, train_acc 0.9991, valid_acc 0.9410, Time 00:00:31,lr 0.0005\n",
      "epoch 168, loss 0.00429, train_acc 0.9992, valid_acc 0.9408, Time 00:00:32,lr 0.0005\n",
      "epoch 169, loss 0.00439, train_acc 0.9993, valid_acc 0.9404, Time 00:00:32,lr 0.0005\n",
      "epoch 170, loss 0.00433, train_acc 0.9994, valid_acc 0.9411, Time 00:00:31,lr 0.0005\n",
      "epoch 171, loss 0.00413, train_acc 0.9993, valid_acc 0.9422, Time 00:00:31,lr 0.0005\n",
      "epoch 172, loss 0.00436, train_acc 0.9991, valid_acc 0.9417, Time 00:00:32,lr 0.0005\n",
      "epoch 173, loss 0.00403, train_acc 0.9994, valid_acc 0.9407, Time 00:00:31,lr 0.0005\n",
      "epoch 174, loss 0.00434, train_acc 0.9992, valid_acc 0.9408, Time 00:00:31,lr 0.0005\n",
      "epoch 175, loss 0.00427, train_acc 0.9992, valid_acc 0.9409, Time 00:00:32,lr 0.0005\n",
      "epoch 176, loss 0.00408, train_acc 0.9994, valid_acc 0.9398, Time 00:00:32,lr 0.0005\n",
      "epoch 177, loss 0.00393, train_acc 0.9995, valid_acc 0.9415, Time 00:00:31,lr 0.0005\n",
      "epoch 178, loss 0.00401, train_acc 0.9991, valid_acc 0.9417, Time 00:00:31,lr 0.0005\n",
      "epoch 179, loss 0.00382, train_acc 0.9994, valid_acc 0.9403, Time 00:00:31,lr 0.0005\n",
      "epoch 180, loss 0.00371, train_acc 0.9994, valid_acc 0.9416, Time 00:00:32,lr 0.0005\n",
      "epoch 181, loss 0.00402, train_acc 0.9992, valid_acc 0.9420, Time 00:00:31,lr 0.0005\n",
      "epoch 182, loss 0.00379, train_acc 0.9994, valid_acc 0.9415, Time 00:00:31,lr 0.0005\n",
      "epoch 183, loss 0.00376, train_acc 0.9994, valid_acc 0.9417, Time 00:00:31,lr 0.0005\n",
      "epoch 184, loss 0.00328, train_acc 0.9995, valid_acc 0.9412, Time 00:00:31,lr 0.0005\n",
      "epoch 185, loss 0.00321, train_acc 0.9996, valid_acc 0.9420, Time 00:00:31,lr 0.0005\n",
      "epoch 186, loss 0.00375, train_acc 0.9994, valid_acc 0.9412, Time 00:00:32,lr 0.0005\n",
      "epoch 187, loss 0.00350, train_acc 0.9996, valid_acc 0.9421, Time 00:00:32,lr 0.0005\n",
      "epoch 188, loss 0.00338, train_acc 0.9995, valid_acc 0.9416, Time 00:00:31,lr 0.0005\n",
      "epoch 189, loss 0.00361, train_acc 0.9994, valid_acc 0.9419, Time 00:00:31,lr 0.0005\n",
      "epoch 190, loss 0.00312, train_acc 0.9997, valid_acc 0.9419, Time 00:00:31,lr 0.0005\n",
      "epoch 191, loss 0.00351, train_acc 0.9995, valid_acc 0.9414, Time 00:00:31,lr 0.0005\n",
      "epoch 192, loss 0.00369, train_acc 0.9993, valid_acc 0.9410, Time 00:00:31,lr 0.0005\n",
      "epoch 193, loss 0.00352, train_acc 0.9994, valid_acc 0.9422, Time 00:00:31,lr 0.0005\n",
      "epoch 194, loss 0.00325, train_acc 0.9996, valid_acc 0.9405, Time 00:00:31,lr 0.0005\n",
      "epoch 195, loss 0.00320, train_acc 0.9996, valid_acc 0.9405, Time 00:00:31,lr 0.0005\n",
      "epoch 196, loss 0.00296, train_acc 0.9996, valid_acc 0.9396, Time 00:00:31,lr 0.0005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 197, loss 0.00340, train_acc 0.9993, valid_acc 0.9413, Time 00:00:31,lr 0.0005\n",
      "epoch 198, loss 0.00369, train_acc 0.9994, valid_acc 0.9424, Time 00:00:31,lr 0.0005\n",
      "epoch 199, loss 0.00298, train_acc 0.9995, valid_acc 0.9427, Time 00:00:31,lr 0.0005\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_no_avgpooling(10)\n",
    "net.initialize(ctx=ctx)\n",
    "#net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_no_avgpooling_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.13 add a fc layer to preduce more complex feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T16:31:42.385920Z",
     "start_time": "2018-03-09T16:31:42.371030Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_more_fc(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_more_fc, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(256))\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T18:16:07.937867Z",
     "start_time": "2018-03-09T16:31:42.386955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.84084, train_acc 0.3016, valid_acc 0.3551, Time 00:00:28,lr 0.1\n",
      "epoch 1, loss 1.42016, train_acc 0.4837, valid_acc 0.5336, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.07032, train_acc 0.6217, valid_acc 0.6744, Time 00:00:31,lr 0.1\n",
      "epoch 3, loss 0.85827, train_acc 0.7024, valid_acc 0.6448, Time 00:00:31,lr 0.1\n",
      "epoch 4, loss 0.75849, train_acc 0.7396, valid_acc 0.7088, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 0.68497, train_acc 0.7663, valid_acc 0.7673, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 0.65194, train_acc 0.7788, valid_acc 0.7865, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 0.61634, train_acc 0.7906, valid_acc 0.7910, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.59677, train_acc 0.7959, valid_acc 0.8123, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 0.56562, train_acc 0.8072, valid_acc 0.7965, Time 00:00:31,lr 0.1\n",
      "epoch 10, loss 0.54864, train_acc 0.8129, valid_acc 0.7974, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 0.54022, train_acc 0.8156, valid_acc 0.7896, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 0.52684, train_acc 0.8212, valid_acc 0.7972, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.51129, train_acc 0.8251, valid_acc 0.7693, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.49267, train_acc 0.8338, valid_acc 0.7919, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 0.49151, train_acc 0.8330, valid_acc 0.7909, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 0.47818, train_acc 0.8380, valid_acc 0.8131, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 0.47659, train_acc 0.8371, valid_acc 0.8400, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 0.46470, train_acc 0.8435, valid_acc 0.8023, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 0.46479, train_acc 0.8417, valid_acc 0.8185, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 0.45483, train_acc 0.8444, valid_acc 0.8321, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 0.45297, train_acc 0.8476, valid_acc 0.8130, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 0.44549, train_acc 0.8481, valid_acc 0.8214, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.43862, train_acc 0.8501, valid_acc 0.7940, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 0.44332, train_acc 0.8492, valid_acc 0.8516, Time 00:00:34,lr 0.1\n",
      "epoch 25, loss 0.43015, train_acc 0.8543, valid_acc 0.8209, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 0.43308, train_acc 0.8524, valid_acc 0.8355, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 0.42334, train_acc 0.8561, valid_acc 0.8167, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.42647, train_acc 0.8546, valid_acc 0.8329, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 0.42939, train_acc 0.8554, valid_acc 0.8249, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 0.41913, train_acc 0.8591, valid_acc 0.8388, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.41592, train_acc 0.8575, valid_acc 0.8514, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 0.42113, train_acc 0.8583, valid_acc 0.8504, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 0.41423, train_acc 0.8601, valid_acc 0.8166, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 0.41033, train_acc 0.8603, valid_acc 0.8385, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.41056, train_acc 0.8597, valid_acc 0.8245, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 0.40680, train_acc 0.8622, valid_acc 0.8280, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 0.40883, train_acc 0.8613, valid_acc 0.8429, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 0.40213, train_acc 0.8640, valid_acc 0.8499, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 0.40179, train_acc 0.8621, valid_acc 0.8122, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 0.40148, train_acc 0.8641, valid_acc 0.8228, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 0.40658, train_acc 0.8604, valid_acc 0.8372, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 0.39684, train_acc 0.8655, valid_acc 0.8439, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 0.39689, train_acc 0.8653, valid_acc 0.8213, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 0.39331, train_acc 0.8653, valid_acc 0.8270, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 0.38926, train_acc 0.8685, valid_acc 0.8436, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 0.39460, train_acc 0.8660, valid_acc 0.7843, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.39429, train_acc 0.8671, valid_acc 0.8347, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 0.39361, train_acc 0.8659, valid_acc 0.8322, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 0.39343, train_acc 0.8658, valid_acc 0.8452, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 0.39544, train_acc 0.8670, valid_acc 0.8375, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 0.38113, train_acc 0.8705, valid_acc 0.8064, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 0.39157, train_acc 0.8672, valid_acc 0.7971, Time 00:00:31,lr 0.1\n",
      "epoch 53, loss 0.38478, train_acc 0.8679, valid_acc 0.8652, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 0.39092, train_acc 0.8674, valid_acc 0.8151, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 0.39118, train_acc 0.8678, valid_acc 0.8563, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 0.38691, train_acc 0.8691, valid_acc 0.8365, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 0.38456, train_acc 0.8703, valid_acc 0.8668, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 0.38546, train_acc 0.8681, valid_acc 0.8456, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 0.38389, train_acc 0.8697, valid_acc 0.8272, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 0.38263, train_acc 0.8717, valid_acc 0.8393, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 0.38081, train_acc 0.8720, valid_acc 0.8297, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 0.38424, train_acc 0.8699, valid_acc 0.8627, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 0.38171, train_acc 0.8700, valid_acc 0.8329, Time 00:00:32,lr 0.1\n",
      "epoch 64, loss 0.37898, train_acc 0.8714, valid_acc 0.8445, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.37736, train_acc 0.8735, valid_acc 0.8348, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 0.38240, train_acc 0.8694, valid_acc 0.8683, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.37648, train_acc 0.8721, valid_acc 0.8433, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 0.38541, train_acc 0.8685, valid_acc 0.8696, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 0.37955, train_acc 0.8710, valid_acc 0.8425, Time 00:00:31,lr 0.1\n",
      "epoch 70, loss 0.37959, train_acc 0.8707, valid_acc 0.8404, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.38186, train_acc 0.8693, valid_acc 0.8105, Time 00:00:31,lr 0.1\n",
      "epoch 72, loss 0.37423, train_acc 0.8738, valid_acc 0.8484, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 0.37755, train_acc 0.8718, valid_acc 0.8682, Time 00:00:32,lr 0.1\n",
      "epoch 74, loss 0.37839, train_acc 0.8714, valid_acc 0.8089, Time 00:00:32,lr 0.1\n",
      "epoch 75, loss 0.37892, train_acc 0.8716, valid_acc 0.8286, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 0.37683, train_acc 0.8715, valid_acc 0.8524, Time 00:00:31,lr 0.1\n",
      "epoch 77, loss 0.37559, train_acc 0.8720, valid_acc 0.8177, Time 00:00:31,lr 0.1\n",
      "epoch 78, loss 0.37038, train_acc 0.8748, valid_acc 0.8545, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 0.37068, train_acc 0.8739, valid_acc 0.8067, Time 00:00:31,lr 0.1\n",
      "epoch 80, loss 0.20291, train_acc 0.9307, valid_acc 0.9189, Time 00:00:31,lr 0.01\n",
      "epoch 81, loss 0.15385, train_acc 0.9474, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 82, loss 0.13708, train_acc 0.9531, valid_acc 0.9238, Time 00:00:31,lr 0.01\n",
      "epoch 83, loss 0.12160, train_acc 0.9578, valid_acc 0.9270, Time 00:00:31,lr 0.01\n",
      "epoch 84, loss 0.11336, train_acc 0.9614, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 85, loss 0.10344, train_acc 0.9646, valid_acc 0.9318, Time 00:00:31,lr 0.01\n",
      "epoch 86, loss 0.09695, train_acc 0.9665, valid_acc 0.9289, Time 00:00:31,lr 0.01\n",
      "epoch 87, loss 0.09538, train_acc 0.9667, valid_acc 0.9283, Time 00:00:31,lr 0.01\n",
      "epoch 88, loss 0.08728, train_acc 0.9697, valid_acc 0.9312, Time 00:00:31,lr 0.01\n",
      "epoch 89, loss 0.08003, train_acc 0.9726, valid_acc 0.9298, Time 00:00:31,lr 0.01\n",
      "epoch 90, loss 0.07821, train_acc 0.9730, valid_acc 0.9290, Time 00:00:31,lr 0.01\n",
      "epoch 91, loss 0.07061, train_acc 0.9760, valid_acc 0.9322, Time 00:00:31,lr 0.01\n",
      "epoch 92, loss 0.06887, train_acc 0.9760, valid_acc 0.9314, Time 00:00:31,lr 0.01\n",
      "epoch 93, loss 0.06383, train_acc 0.9783, valid_acc 0.9317, Time 00:00:31,lr 0.01\n",
      "epoch 94, loss 0.06293, train_acc 0.9784, valid_acc 0.9330, Time 00:00:31,lr 0.01\n",
      "epoch 95, loss 0.06038, train_acc 0.9789, valid_acc 0.9332, Time 00:00:31,lr 0.01\n",
      "epoch 96, loss 0.05686, train_acc 0.9804, valid_acc 0.9325, Time 00:00:31,lr 0.01\n",
      "epoch 97, loss 0.05618, train_acc 0.9813, valid_acc 0.9272, Time 00:00:31,lr 0.01\n",
      "epoch 98, loss 0.05491, train_acc 0.9804, valid_acc 0.9329, Time 00:00:31,lr 0.01\n",
      "epoch 99, loss 0.05103, train_acc 0.9825, valid_acc 0.9324, Time 00:00:31,lr 0.01\n",
      "epoch 100, loss 0.05291, train_acc 0.9820, valid_acc 0.9289, Time 00:00:31,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.05048, train_acc 0.9827, valid_acc 0.9288, Time 00:00:31,lr 0.01\n",
      "epoch 102, loss 0.04965, train_acc 0.9822, valid_acc 0.9342, Time 00:00:31,lr 0.01\n",
      "epoch 103, loss 0.04977, train_acc 0.9828, valid_acc 0.9318, Time 00:00:31,lr 0.01\n",
      "epoch 104, loss 0.04749, train_acc 0.9835, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 105, loss 0.04683, train_acc 0.9839, valid_acc 0.9288, Time 00:00:31,lr 0.01\n",
      "epoch 106, loss 0.04838, train_acc 0.9836, valid_acc 0.9314, Time 00:00:31,lr 0.01\n",
      "epoch 107, loss 0.04906, train_acc 0.9831, valid_acc 0.9306, Time 00:00:31,lr 0.01\n",
      "epoch 108, loss 0.04616, train_acc 0.9843, valid_acc 0.9316, Time 00:00:31,lr 0.01\n",
      "epoch 109, loss 0.05024, train_acc 0.9823, valid_acc 0.9245, Time 00:00:31,lr 0.01\n",
      "epoch 110, loss 0.04866, train_acc 0.9832, valid_acc 0.9214, Time 00:00:31,lr 0.01\n",
      "epoch 111, loss 0.04861, train_acc 0.9838, valid_acc 0.9251, Time 00:00:31,lr 0.01\n",
      "epoch 112, loss 0.04792, train_acc 0.9832, valid_acc 0.9262, Time 00:00:31,lr 0.01\n",
      "epoch 113, loss 0.04782, train_acc 0.9837, valid_acc 0.9280, Time 00:00:31,lr 0.01\n",
      "epoch 114, loss 0.05068, train_acc 0.9829, valid_acc 0.9261, Time 00:00:31,lr 0.01\n",
      "epoch 115, loss 0.04660, train_acc 0.9840, valid_acc 0.9179, Time 00:00:31,lr 0.01\n",
      "epoch 116, loss 0.04832, train_acc 0.9829, valid_acc 0.9286, Time 00:00:31,lr 0.01\n",
      "epoch 117, loss 0.05213, train_acc 0.9824, valid_acc 0.9272, Time 00:00:31,lr 0.01\n",
      "epoch 118, loss 0.04753, train_acc 0.9839, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 119, loss 0.04884, train_acc 0.9831, valid_acc 0.9263, Time 00:00:31,lr 0.01\n",
      "epoch 120, loss 0.04744, train_acc 0.9838, valid_acc 0.9226, Time 00:00:31,lr 0.01\n",
      "epoch 121, loss 0.05282, train_acc 0.9822, valid_acc 0.9196, Time 00:00:31,lr 0.01\n",
      "epoch 122, loss 0.04887, train_acc 0.9834, valid_acc 0.9208, Time 00:00:31,lr 0.01\n",
      "epoch 123, loss 0.05482, train_acc 0.9812, valid_acc 0.9263, Time 00:00:31,lr 0.01\n",
      "epoch 124, loss 0.05311, train_acc 0.9818, valid_acc 0.9273, Time 00:00:31,lr 0.01\n",
      "epoch 125, loss 0.04994, train_acc 0.9826, valid_acc 0.9252, Time 00:00:31,lr 0.01\n",
      "epoch 126, loss 0.05178, train_acc 0.9823, valid_acc 0.9192, Time 00:00:31,lr 0.01\n",
      "epoch 127, loss 0.05746, train_acc 0.9798, valid_acc 0.9240, Time 00:00:31,lr 0.01\n",
      "epoch 128, loss 0.05325, train_acc 0.9821, valid_acc 0.9217, Time 00:00:31,lr 0.01\n",
      "epoch 129, loss 0.05352, train_acc 0.9822, valid_acc 0.9218, Time 00:00:31,lr 0.01\n",
      "epoch 130, loss 0.05345, train_acc 0.9819, valid_acc 0.9244, Time 00:00:31,lr 0.01\n",
      "epoch 131, loss 0.05321, train_acc 0.9817, valid_acc 0.9235, Time 00:00:31,lr 0.01\n",
      "epoch 132, loss 0.05923, train_acc 0.9796, valid_acc 0.9248, Time 00:00:31,lr 0.01\n",
      "epoch 133, loss 0.05523, train_acc 0.9815, valid_acc 0.9221, Time 00:00:31,lr 0.01\n",
      "epoch 134, loss 0.05554, train_acc 0.9810, valid_acc 0.9274, Time 00:00:31,lr 0.01\n",
      "epoch 135, loss 0.05406, train_acc 0.9812, valid_acc 0.9237, Time 00:00:31,lr 0.01\n",
      "epoch 136, loss 0.05649, train_acc 0.9806, valid_acc 0.9226, Time 00:00:31,lr 0.01\n",
      "epoch 137, loss 0.05934, train_acc 0.9798, valid_acc 0.9132, Time 00:00:31,lr 0.01\n",
      "epoch 138, loss 0.05779, train_acc 0.9800, valid_acc 0.9260, Time 00:00:31,lr 0.01\n",
      "epoch 139, loss 0.05183, train_acc 0.9819, valid_acc 0.9133, Time 00:00:31,lr 0.01\n",
      "epoch 140, loss 0.05781, train_acc 0.9800, valid_acc 0.9199, Time 00:00:31,lr 0.01\n",
      "epoch 141, loss 0.05686, train_acc 0.9805, valid_acc 0.9286, Time 00:00:31,lr 0.01\n",
      "epoch 142, loss 0.05394, train_acc 0.9822, valid_acc 0.9280, Time 00:00:31,lr 0.01\n",
      "epoch 143, loss 0.05580, train_acc 0.9811, valid_acc 0.9157, Time 00:00:31,lr 0.01\n",
      "epoch 144, loss 0.05751, train_acc 0.9805, valid_acc 0.9211, Time 00:00:31,lr 0.01\n",
      "epoch 145, loss 0.05333, train_acc 0.9814, valid_acc 0.9258, Time 00:00:31,lr 0.01\n",
      "epoch 146, loss 0.05588, train_acc 0.9811, valid_acc 0.9233, Time 00:00:31,lr 0.01\n",
      "epoch 147, loss 0.05699, train_acc 0.9810, valid_acc 0.9230, Time 00:00:31,lr 0.01\n",
      "epoch 148, loss 0.05750, train_acc 0.9804, valid_acc 0.9186, Time 00:00:31,lr 0.01\n",
      "epoch 149, loss 0.05856, train_acc 0.9794, valid_acc 0.9221, Time 00:00:31,lr 0.01\n",
      "epoch 150, loss 0.03365, train_acc 0.9893, valid_acc 0.9351, Time 00:00:31,lr 0.001\n",
      "epoch 151, loss 0.02033, train_acc 0.9941, valid_acc 0.9376, Time 00:00:31,lr 0.001\n",
      "epoch 152, loss 0.01665, train_acc 0.9954, valid_acc 0.9393, Time 00:00:31,lr 0.001\n",
      "epoch 153, loss 0.01391, train_acc 0.9964, valid_acc 0.9385, Time 00:00:31,lr 0.001\n",
      "epoch 154, loss 0.01289, train_acc 0.9965, valid_acc 0.9390, Time 00:00:31,lr 0.001\n",
      "epoch 155, loss 0.01123, train_acc 0.9972, valid_acc 0.9394, Time 00:00:31,lr 0.001\n",
      "epoch 156, loss 0.01094, train_acc 0.9972, valid_acc 0.9407, Time 00:00:31,lr 0.001\n",
      "epoch 157, loss 0.00943, train_acc 0.9977, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 158, loss 0.00905, train_acc 0.9979, valid_acc 0.9430, Time 00:00:31,lr 0.001\n",
      "epoch 159, loss 0.00914, train_acc 0.9978, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 160, loss 0.00756, train_acc 0.9985, valid_acc 0.9421, Time 00:00:31,lr 0.001\n",
      "epoch 161, loss 0.00801, train_acc 0.9982, valid_acc 0.9428, Time 00:00:31,lr 0.001\n",
      "epoch 162, loss 0.00760, train_acc 0.9984, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 163, loss 0.00727, train_acc 0.9983, valid_acc 0.9428, Time 00:00:31,lr 0.001\n",
      "epoch 164, loss 0.00701, train_acc 0.9983, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 165, loss 0.00687, train_acc 0.9986, valid_acc 0.9415, Time 00:00:31,lr 0.001\n",
      "epoch 166, loss 0.00622, train_acc 0.9988, valid_acc 0.9420, Time 00:00:31,lr 0.001\n",
      "epoch 167, loss 0.00629, train_acc 0.9987, valid_acc 0.9412, Time 00:00:31,lr 0.001\n",
      "epoch 168, loss 0.00670, train_acc 0.9985, valid_acc 0.9420, Time 00:00:31,lr 0.001\n",
      "epoch 169, loss 0.00619, train_acc 0.9987, valid_acc 0.9425, Time 00:00:31,lr 0.001\n",
      "epoch 170, loss 0.00618, train_acc 0.9986, valid_acc 0.9418, Time 00:00:31,lr 0.001\n",
      "epoch 171, loss 0.00542, train_acc 0.9990, valid_acc 0.9414, Time 00:00:31,lr 0.001\n",
      "epoch 172, loss 0.00555, train_acc 0.9987, valid_acc 0.9423, Time 00:00:31,lr 0.001\n",
      "epoch 173, loss 0.00527, train_acc 0.9989, valid_acc 0.9425, Time 00:00:31,lr 0.001\n",
      "epoch 174, loss 0.00495, train_acc 0.9992, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 175, loss 0.00497, train_acc 0.9991, valid_acc 0.9427, Time 00:00:31,lr 0.001\n",
      "epoch 176, loss 0.00520, train_acc 0.9989, valid_acc 0.9417, Time 00:00:30,lr 0.001\n",
      "epoch 177, loss 0.00522, train_acc 0.9990, valid_acc 0.9428, Time 00:00:31,lr 0.001\n",
      "epoch 178, loss 0.00491, train_acc 0.9989, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 179, loss 0.00445, train_acc 0.9991, valid_acc 0.9423, Time 00:00:30,lr 0.001\n",
      "epoch 180, loss 0.00460, train_acc 0.9990, valid_acc 0.9422, Time 00:00:31,lr 0.001\n",
      "epoch 181, loss 0.00439, train_acc 0.9991, valid_acc 0.9438, Time 00:00:30,lr 0.001\n",
      "epoch 182, loss 0.00478, train_acc 0.9990, valid_acc 0.9439, Time 00:00:31,lr 0.001\n",
      "epoch 183, loss 0.00425, train_acc 0.9991, valid_acc 0.9419, Time 00:00:30,lr 0.001\n",
      "epoch 184, loss 0.00399, train_acc 0.9992, valid_acc 0.9433, Time 00:00:30,lr 0.001\n",
      "epoch 185, loss 0.00447, train_acc 0.9991, valid_acc 0.9418, Time 00:00:31,lr 0.001\n",
      "epoch 186, loss 0.00408, train_acc 0.9992, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 187, loss 0.00414, train_acc 0.9992, valid_acc 0.9428, Time 00:00:30,lr 0.001\n",
      "epoch 188, loss 0.00410, train_acc 0.9992, valid_acc 0.9425, Time 00:00:30,lr 0.001\n",
      "epoch 189, loss 0.00393, train_acc 0.9993, valid_acc 0.9435, Time 00:00:31,lr 0.001\n",
      "epoch 190, loss 0.00365, train_acc 0.9994, valid_acc 0.9430, Time 00:00:31,lr 0.001\n",
      "epoch 191, loss 0.00389, train_acc 0.9994, valid_acc 0.9435, Time 00:00:31,lr 0.001\n",
      "epoch 192, loss 0.00363, train_acc 0.9994, valid_acc 0.9440, Time 00:00:30,lr 0.001\n",
      "epoch 193, loss 0.00337, train_acc 0.9994, valid_acc 0.9425, Time 00:00:30,lr 0.001\n",
      "epoch 194, loss 0.00371, train_acc 0.9995, valid_acc 0.9435, Time 00:00:31,lr 0.001\n",
      "epoch 195, loss 0.00345, train_acc 0.9993, valid_acc 0.9435, Time 00:00:31,lr 0.001\n",
      "epoch 196, loss 0.00354, train_acc 0.9994, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 197, loss 0.00344, train_acc 0.9992, valid_acc 0.9439, Time 00:00:30,lr 0.001\n",
      "epoch 198, loss 0.00367, train_acc 0.9993, valid_acc 0.9437, Time 00:00:30,lr 0.001\n",
      "epoch 199, loss 0.00382, train_acc 0.9992, valid_acc 0.9442, Time 00:00:30,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_more_fc(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_more_fc_e200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T16:18:14.176114Z",
     "start_time": "2018-03-13T16:18:14.153083Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_more_fc_v2(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_more_fc_v2, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(256))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.LeakyReLU(0.1))\n",
    "            net.add(nn.Dropout(0.5))\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:02:18.497294Z",
     "start_time": "2018-03-13T16:18:15.562189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.00073, train_acc 0.2576, valid_acc 0.3418, Time 00:00:29,lr 0.1\n",
      "epoch 1, loss 1.61386, train_acc 0.3917, valid_acc 0.3939, Time 00:00:30,lr 0.1\n",
      "epoch 2, loss 1.35985, train_acc 0.5063, valid_acc 0.5346, Time 00:00:30,lr 0.1\n",
      "epoch 3, loss 1.20042, train_acc 0.5723, valid_acc 0.5073, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 1.09322, train_acc 0.6125, valid_acc 0.6078, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 1.00440, train_acc 0.6488, valid_acc 0.6568, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 0.92570, train_acc 0.6811, valid_acc 0.6783, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 0.85635, train_acc 0.7083, valid_acc 0.6975, Time 00:00:30,lr 0.1\n",
      "epoch 8, loss 0.80856, train_acc 0.7270, valid_acc 0.7123, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 0.75043, train_acc 0.7454, valid_acc 0.7440, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.72465, train_acc 0.7563, valid_acc 0.7775, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 0.69689, train_acc 0.7659, valid_acc 0.7670, Time 00:00:30,lr 0.1\n",
      "epoch 12, loss 0.68061, train_acc 0.7707, valid_acc 0.7782, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.65685, train_acc 0.7784, valid_acc 0.7834, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.64559, train_acc 0.7835, valid_acc 0.7696, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 0.62460, train_acc 0.7912, valid_acc 0.7549, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 0.61036, train_acc 0.7950, valid_acc 0.7727, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 0.59955, train_acc 0.7995, valid_acc 0.7016, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 0.59752, train_acc 0.8002, valid_acc 0.8084, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 0.58358, train_acc 0.8053, valid_acc 0.8234, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 0.57880, train_acc 0.8036, valid_acc 0.8154, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 0.57451, train_acc 0.8094, valid_acc 0.7929, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 0.56095, train_acc 0.8141, valid_acc 0.7753, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.55166, train_acc 0.8163, valid_acc 0.8080, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 0.55101, train_acc 0.8173, valid_acc 0.8184, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 0.54711, train_acc 0.8159, valid_acc 0.7839, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 0.53676, train_acc 0.8203, valid_acc 0.8090, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 0.54151, train_acc 0.8194, valid_acc 0.8228, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.52513, train_acc 0.8252, valid_acc 0.8085, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 0.51619, train_acc 0.8255, valid_acc 0.7979, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 0.52401, train_acc 0.8253, valid_acc 0.8342, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.52383, train_acc 0.8248, valid_acc 0.7965, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 0.51241, train_acc 0.8294, valid_acc 0.8305, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 0.51211, train_acc 0.8288, valid_acc 0.8324, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 0.50378, train_acc 0.8320, valid_acc 0.8289, Time 00:00:31,lr 0.1\n",
      "epoch 35, loss 0.50846, train_acc 0.8300, valid_acc 0.8116, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 0.50149, train_acc 0.8312, valid_acc 0.8295, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 0.49688, train_acc 0.8342, valid_acc 0.7949, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 0.50258, train_acc 0.8324, valid_acc 0.8101, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 0.49238, train_acc 0.8347, valid_acc 0.8354, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 0.49236, train_acc 0.8358, valid_acc 0.8413, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 0.49341, train_acc 0.8353, valid_acc 0.8349, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 0.48956, train_acc 0.8371, valid_acc 0.8297, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 0.48531, train_acc 0.8384, valid_acc 0.7897, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 0.49237, train_acc 0.8363, valid_acc 0.8282, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 0.48191, train_acc 0.8385, valid_acc 0.8315, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 0.48087, train_acc 0.8395, valid_acc 0.8139, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.48068, train_acc 0.8413, valid_acc 0.8455, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 0.47891, train_acc 0.8401, valid_acc 0.8451, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 0.47575, train_acc 0.8424, valid_acc 0.8292, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 0.47623, train_acc 0.8423, valid_acc 0.8520, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 0.46882, train_acc 0.8425, valid_acc 0.8632, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 0.47480, train_acc 0.8412, valid_acc 0.8430, Time 00:00:31,lr 0.1\n",
      "epoch 53, loss 0.46909, train_acc 0.8426, valid_acc 0.8223, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 0.46675, train_acc 0.8451, valid_acc 0.8337, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 0.47103, train_acc 0.8438, valid_acc 0.8455, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 0.46642, train_acc 0.8446, valid_acc 0.8489, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 0.47139, train_acc 0.8429, valid_acc 0.8460, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 0.46614, train_acc 0.8457, valid_acc 0.8272, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 0.46920, train_acc 0.8442, valid_acc 0.8460, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 0.46730, train_acc 0.8458, valid_acc 0.8350, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 0.46690, train_acc 0.8469, valid_acc 0.7937, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 0.46394, train_acc 0.8432, valid_acc 0.8455, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 0.46547, train_acc 0.8449, valid_acc 0.8096, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 0.46452, train_acc 0.8465, valid_acc 0.8268, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.45723, train_acc 0.8478, valid_acc 0.8140, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 0.45878, train_acc 0.8478, valid_acc 0.8431, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 0.45622, train_acc 0.8485, valid_acc 0.8116, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 0.45423, train_acc 0.8465, valid_acc 0.8274, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 0.46137, train_acc 0.8480, valid_acc 0.8255, Time 00:00:31,lr 0.1\n",
      "epoch 70, loss 0.45929, train_acc 0.8473, valid_acc 0.8282, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.45939, train_acc 0.8478, valid_acc 0.8237, Time 00:00:31,lr 0.1\n",
      "epoch 72, loss 0.45660, train_acc 0.8496, valid_acc 0.8362, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 0.45809, train_acc 0.8482, valid_acc 0.8485, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 0.45251, train_acc 0.8507, valid_acc 0.8287, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 0.45656, train_acc 0.8489, valid_acc 0.8270, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 0.45245, train_acc 0.8484, valid_acc 0.8233, Time 00:00:31,lr 0.1\n",
      "epoch 77, loss 0.45207, train_acc 0.8515, valid_acc 0.8037, Time 00:00:31,lr 0.1\n",
      "epoch 78, loss 0.45135, train_acc 0.8504, valid_acc 0.8261, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 0.45028, train_acc 0.8496, valid_acc 0.8059, Time 00:00:31,lr 0.1\n",
      "epoch 80, loss 0.27970, train_acc 0.9058, valid_acc 0.9146, Time 00:00:31,lr 0.01\n",
      "epoch 81, loss 0.21555, train_acc 0.9278, valid_acc 0.9217, Time 00:00:31,lr 0.01\n",
      "epoch 82, loss 0.19625, train_acc 0.9337, valid_acc 0.9219, Time 00:00:31,lr 0.01\n",
      "epoch 83, loss 0.17856, train_acc 0.9396, valid_acc 0.9257, Time 00:00:31,lr 0.01\n",
      "epoch 84, loss 0.16509, train_acc 0.9445, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 85, loss 0.15734, train_acc 0.9467, valid_acc 0.9294, Time 00:00:31,lr 0.01\n",
      "epoch 86, loss 0.14427, train_acc 0.9512, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 87, loss 0.14058, train_acc 0.9528, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 88, loss 0.13016, train_acc 0.9548, valid_acc 0.9272, Time 00:00:31,lr 0.01\n",
      "epoch 89, loss 0.12546, train_acc 0.9560, valid_acc 0.9281, Time 00:00:31,lr 0.01\n",
      "epoch 90, loss 0.12218, train_acc 0.9583, valid_acc 0.9279, Time 00:00:31,lr 0.01\n",
      "epoch 91, loss 0.11370, train_acc 0.9610, valid_acc 0.9269, Time 00:00:31,lr 0.01\n",
      "epoch 92, loss 0.11037, train_acc 0.9613, valid_acc 0.9287, Time 00:00:31,lr 0.01\n",
      "epoch 93, loss 0.10608, train_acc 0.9637, valid_acc 0.9242, Time 00:00:31,lr 0.01\n",
      "epoch 94, loss 0.10247, train_acc 0.9651, valid_acc 0.9293, Time 00:00:31,lr 0.01\n",
      "epoch 95, loss 0.09912, train_acc 0.9664, valid_acc 0.9281, Time 00:00:31,lr 0.01\n",
      "epoch 96, loss 0.09731, train_acc 0.9658, valid_acc 0.9268, Time 00:00:31,lr 0.01\n",
      "epoch 97, loss 0.09295, train_acc 0.9687, valid_acc 0.9278, Time 00:00:31,lr 0.01\n",
      "epoch 98, loss 0.08828, train_acc 0.9695, valid_acc 0.9294, Time 00:00:31,lr 0.01\n",
      "epoch 99, loss 0.09078, train_acc 0.9694, valid_acc 0.9307, Time 00:00:31,lr 0.01\n",
      "epoch 100, loss 0.09008, train_acc 0.9687, valid_acc 0.9214, Time 00:00:31,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.08993, train_acc 0.9693, valid_acc 0.9258, Time 00:00:31,lr 0.01\n",
      "epoch 102, loss 0.08545, train_acc 0.9711, valid_acc 0.9258, Time 00:00:31,lr 0.01\n",
      "epoch 103, loss 0.08577, train_acc 0.9698, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 104, loss 0.08481, train_acc 0.9709, valid_acc 0.9224, Time 00:00:31,lr 0.01\n",
      "epoch 105, loss 0.08369, train_acc 0.9711, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 106, loss 0.07927, train_acc 0.9727, valid_acc 0.9257, Time 00:00:31,lr 0.01\n",
      "epoch 107, loss 0.08274, train_acc 0.9719, valid_acc 0.9215, Time 00:00:31,lr 0.01\n",
      "epoch 108, loss 0.08576, train_acc 0.9704, valid_acc 0.9230, Time 00:00:31,lr 0.01\n",
      "epoch 109, loss 0.07944, train_acc 0.9723, valid_acc 0.9245, Time 00:00:31,lr 0.01\n",
      "epoch 110, loss 0.08079, train_acc 0.9724, valid_acc 0.9221, Time 00:00:31,lr 0.01\n",
      "epoch 111, loss 0.07629, train_acc 0.9734, valid_acc 0.9257, Time 00:00:31,lr 0.01\n",
      "epoch 112, loss 0.07999, train_acc 0.9726, valid_acc 0.9288, Time 00:00:31,lr 0.01\n",
      "epoch 113, loss 0.08064, train_acc 0.9728, valid_acc 0.9265, Time 00:00:31,lr 0.01\n",
      "epoch 114, loss 0.07886, train_acc 0.9739, valid_acc 0.9274, Time 00:00:31,lr 0.01\n",
      "epoch 115, loss 0.07992, train_acc 0.9726, valid_acc 0.9273, Time 00:00:31,lr 0.01\n",
      "epoch 116, loss 0.08113, train_acc 0.9718, valid_acc 0.9136, Time 00:00:31,lr 0.01\n",
      "epoch 117, loss 0.07861, train_acc 0.9733, valid_acc 0.9223, Time 00:00:31,lr 0.01\n",
      "epoch 118, loss 0.08072, train_acc 0.9723, valid_acc 0.9240, Time 00:00:31,lr 0.01\n",
      "epoch 119, loss 0.07984, train_acc 0.9721, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 120, loss 0.07906, train_acc 0.9731, valid_acc 0.9222, Time 00:00:31,lr 0.01\n",
      "epoch 121, loss 0.08023, train_acc 0.9720, valid_acc 0.9231, Time 00:00:31,lr 0.01\n",
      "epoch 122, loss 0.08163, train_acc 0.9726, valid_acc 0.9263, Time 00:00:31,lr 0.01\n",
      "epoch 123, loss 0.07641, train_acc 0.9740, valid_acc 0.9247, Time 00:00:31,lr 0.01\n",
      "epoch 124, loss 0.08009, train_acc 0.9722, valid_acc 0.9242, Time 00:00:31,lr 0.01\n",
      "epoch 125, loss 0.08629, train_acc 0.9698, valid_acc 0.9156, Time 00:00:31,lr 0.01\n",
      "epoch 126, loss 0.07593, train_acc 0.9746, valid_acc 0.9254, Time 00:00:31,lr 0.01\n",
      "epoch 127, loss 0.07903, train_acc 0.9731, valid_acc 0.9239, Time 00:00:31,lr 0.01\n",
      "epoch 128, loss 0.07729, train_acc 0.9727, valid_acc 0.9245, Time 00:00:31,lr 0.01\n",
      "epoch 129, loss 0.08826, train_acc 0.9694, valid_acc 0.9250, Time 00:00:31,lr 0.01\n",
      "epoch 130, loss 0.07843, train_acc 0.9737, valid_acc 0.9174, Time 00:00:31,lr 0.01\n",
      "epoch 131, loss 0.07892, train_acc 0.9734, valid_acc 0.9204, Time 00:00:31,lr 0.01\n",
      "epoch 132, loss 0.07630, train_acc 0.9739, valid_acc 0.9200, Time 00:00:31,lr 0.01\n",
      "epoch 133, loss 0.08327, train_acc 0.9719, valid_acc 0.9202, Time 00:00:31,lr 0.01\n",
      "epoch 134, loss 0.08164, train_acc 0.9722, valid_acc 0.9232, Time 00:00:31,lr 0.01\n",
      "epoch 135, loss 0.07699, train_acc 0.9732, valid_acc 0.9239, Time 00:00:31,lr 0.01\n",
      "epoch 136, loss 0.07931, train_acc 0.9725, valid_acc 0.9196, Time 00:00:31,lr 0.01\n",
      "epoch 137, loss 0.07859, train_acc 0.9732, valid_acc 0.9195, Time 00:00:31,lr 0.01\n",
      "epoch 138, loss 0.08196, train_acc 0.9718, valid_acc 0.9167, Time 00:00:31,lr 0.01\n",
      "epoch 139, loss 0.08276, train_acc 0.9715, valid_acc 0.9167, Time 00:00:31,lr 0.01\n",
      "epoch 140, loss 0.08000, train_acc 0.9726, valid_acc 0.9248, Time 00:00:31,lr 0.01\n",
      "epoch 141, loss 0.08144, train_acc 0.9724, valid_acc 0.9234, Time 00:00:31,lr 0.01\n",
      "epoch 142, loss 0.08052, train_acc 0.9717, valid_acc 0.9225, Time 00:00:31,lr 0.01\n",
      "epoch 143, loss 0.07770, train_acc 0.9731, valid_acc 0.9205, Time 00:00:31,lr 0.01\n",
      "epoch 144, loss 0.08108, train_acc 0.9717, valid_acc 0.9230, Time 00:00:30,lr 0.01\n",
      "epoch 145, loss 0.08003, train_acc 0.9740, valid_acc 0.9187, Time 00:00:31,lr 0.01\n",
      "epoch 146, loss 0.07216, train_acc 0.9759, valid_acc 0.9246, Time 00:00:31,lr 0.01\n",
      "epoch 147, loss 0.08240, train_acc 0.9715, valid_acc 0.9200, Time 00:00:31,lr 0.01\n",
      "epoch 148, loss 0.07850, train_acc 0.9742, valid_acc 0.9179, Time 00:00:31,lr 0.01\n",
      "epoch 149, loss 0.07673, train_acc 0.9732, valid_acc 0.9272, Time 00:00:31,lr 0.01\n",
      "epoch 150, loss 0.04617, train_acc 0.9849, valid_acc 0.9342, Time 00:00:31,lr 0.001\n",
      "epoch 151, loss 0.02932, train_acc 0.9909, valid_acc 0.9374, Time 00:00:31,lr 0.001\n",
      "epoch 152, loss 0.02423, train_acc 0.9928, valid_acc 0.9369, Time 00:00:31,lr 0.001\n",
      "epoch 153, loss 0.02016, train_acc 0.9938, valid_acc 0.9386, Time 00:00:31,lr 0.001\n",
      "epoch 154, loss 0.01895, train_acc 0.9947, valid_acc 0.9387, Time 00:00:31,lr 0.001\n",
      "epoch 155, loss 0.01889, train_acc 0.9947, valid_acc 0.9383, Time 00:00:31,lr 0.001\n",
      "epoch 156, loss 0.01606, train_acc 0.9955, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 157, loss 0.01441, train_acc 0.9960, valid_acc 0.9393, Time 00:00:31,lr 0.001\n",
      "epoch 158, loss 0.01322, train_acc 0.9963, valid_acc 0.9386, Time 00:00:31,lr 0.001\n",
      "epoch 159, loss 0.01366, train_acc 0.9960, valid_acc 0.9395, Time 00:00:31,lr 0.001\n",
      "epoch 160, loss 0.01316, train_acc 0.9965, valid_acc 0.9410, Time 00:00:31,lr 0.001\n",
      "epoch 161, loss 0.01252, train_acc 0.9966, valid_acc 0.9409, Time 00:00:31,lr 0.001\n",
      "epoch 162, loss 0.01185, train_acc 0.9965, valid_acc 0.9412, Time 00:00:31,lr 0.001\n",
      "epoch 163, loss 0.01146, train_acc 0.9969, valid_acc 0.9405, Time 00:00:31,lr 0.001\n",
      "epoch 164, loss 0.01123, train_acc 0.9966, valid_acc 0.9426, Time 00:00:31,lr 0.001\n",
      "epoch 165, loss 0.01011, train_acc 0.9971, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 166, loss 0.01004, train_acc 0.9974, valid_acc 0.9418, Time 00:00:31,lr 0.001\n",
      "epoch 167, loss 0.01025, train_acc 0.9969, valid_acc 0.9421, Time 00:00:31,lr 0.001\n",
      "epoch 168, loss 0.00900, train_acc 0.9974, valid_acc 0.9431, Time 00:00:31,lr 0.001\n",
      "epoch 169, loss 0.00938, train_acc 0.9973, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 170, loss 0.00877, train_acc 0.9976, valid_acc 0.9412, Time 00:00:31,lr 0.001\n",
      "epoch 171, loss 0.00853, train_acc 0.9979, valid_acc 0.9401, Time 00:00:31,lr 0.001\n",
      "epoch 172, loss 0.00879, train_acc 0.9975, valid_acc 0.9405, Time 00:00:31,lr 0.001\n",
      "epoch 173, loss 0.00694, train_acc 0.9984, valid_acc 0.9411, Time 00:00:31,lr 0.001\n",
      "epoch 174, loss 0.00787, train_acc 0.9979, valid_acc 0.9412, Time 00:00:31,lr 0.001\n",
      "epoch 175, loss 0.00749, train_acc 0.9979, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 176, loss 0.00742, train_acc 0.9979, valid_acc 0.9407, Time 00:00:31,lr 0.001\n",
      "epoch 177, loss 0.00709, train_acc 0.9980, valid_acc 0.9404, Time 00:00:31,lr 0.001\n",
      "epoch 178, loss 0.00736, train_acc 0.9980, valid_acc 0.9431, Time 00:00:31,lr 0.001\n",
      "epoch 179, loss 0.00733, train_acc 0.9982, valid_acc 0.9424, Time 00:00:31,lr 0.001\n",
      "epoch 180, loss 0.00715, train_acc 0.9980, valid_acc 0.9410, Time 00:00:31,lr 0.001\n",
      "epoch 181, loss 0.00649, train_acc 0.9984, valid_acc 0.9413, Time 00:00:31,lr 0.001\n",
      "epoch 182, loss 0.00619, train_acc 0.9982, valid_acc 0.9415, Time 00:00:31,lr 0.001\n",
      "epoch 183, loss 0.00693, train_acc 0.9983, valid_acc 0.9419, Time 00:00:31,lr 0.001\n",
      "epoch 184, loss 0.00602, train_acc 0.9983, valid_acc 0.9399, Time 00:00:31,lr 0.001\n",
      "epoch 185, loss 0.00650, train_acc 0.9983, valid_acc 0.9419, Time 00:00:31,lr 0.001\n",
      "epoch 186, loss 0.00580, train_acc 0.9984, valid_acc 0.9431, Time 00:00:31,lr 0.001\n",
      "epoch 187, loss 0.00560, train_acc 0.9987, valid_acc 0.9418, Time 00:00:31,lr 0.001\n",
      "epoch 188, loss 0.00543, train_acc 0.9987, valid_acc 0.9411, Time 00:00:31,lr 0.001\n",
      "epoch 189, loss 0.00639, train_acc 0.9983, valid_acc 0.9423, Time 00:00:31,lr 0.001\n",
      "epoch 190, loss 0.00550, train_acc 0.9987, valid_acc 0.9417, Time 00:00:31,lr 0.001\n",
      "epoch 191, loss 0.00579, train_acc 0.9984, valid_acc 0.9425, Time 00:00:31,lr 0.001\n",
      "epoch 192, loss 0.00533, train_acc 0.9987, valid_acc 0.9437, Time 00:00:31,lr 0.001\n",
      "epoch 193, loss 0.00488, train_acc 0.9989, valid_acc 0.9416, Time 00:00:31,lr 0.001\n",
      "epoch 194, loss 0.00521, train_acc 0.9987, valid_acc 0.9430, Time 00:00:31,lr 0.001\n",
      "epoch 195, loss 0.00488, train_acc 0.9988, valid_acc 0.9405, Time 00:00:31,lr 0.001\n",
      "epoch 196, loss 0.00500, train_acc 0.9988, valid_acc 0.9428, Time 00:00:31,lr 0.001\n",
      "epoch 197, loss 0.00553, train_acc 0.9985, valid_acc 0.9437, Time 00:00:31,lr 0.001\n",
      "epoch 198, loss 0.00487, train_acc 0.9987, valid_acc 0.9427, Time 00:00:31,lr 0.001\n",
      "epoch 199, loss 0.00476, train_acc 0.9987, valid_acc 0.9427, Time 00:00:31,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_more_fc_v2(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_more_fc_v2_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.14 bigger input and flip ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:30:20.636391Z",
     "start_time": "2018-03-15T14:30:20.595165Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyResNet18_333_c64(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet18_333_c64, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=256, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=256))\n",
    "            # block 5\n",
    "            net.add(nn.GlobalAvgPool2D())\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:30:22.947914Z",
     "start_time": "2018-03-15T14:30:21.008889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAABiCAYAAAALHznJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnbeuNM4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8ffb82e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAABiCAYAAAALHznJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvUmsZdmVHbZv9/r29xHxI3602WcyOzJV7IoqVlGFkmyo\ngQVDECRAkkceGh544qnhmWF4ZEBTwS544JJcYJGSWSQri0VmkklmZmQTffMjft+9/r3berDXuXu/\n/MFk/P9CEgycBSTi57n33eZ09+x91l7bybKMLCwsLCwsLE4P9z/3A1hYWFhYWPz/HfZjamFhYWFh\nMSPsx9TCwsLCwmJG2I+phYWFhYXFjLAfUwsLCwsLixlhP6YWFhYWFhYzwn5MLSwsLCwsZoT9mFpY\nWFhYWMwI+zG1sLCwsLCYEf5JTi4XvaxZ9cl1nLzMxZ/O1JnT/0eOqCyZ32a//WxSl1fHjis1mfMc\n/YPs+NlZZo7rC6e/9fz8LHVdoxTl4OjUPb9wjr7eg53RXpZli8dOfgrMz89lq6vnj1fQFx/4i3jS\n+U/87RNOdJ5w4VnudZJrzQDTHh9++OEM9b2Qnb9wYaosTdOp6xMROej0Wcovo9vd9bA+/ZJqdJ5Q\naZn6gblekiREROR7/vHzMv1bRpJKoYN1ckbHTs+vn05dY/qek/FE3snla3l+ID/AKzy4ef3U9U1E\n1Gw2spXlRXK9kn4YID32fFmW4t/j1/pd88CX4YtKcE9qo6cdV+ZKaZZ+8dDUJcwtXbSvp54/SRNc\nQ54rjblNHjx4eOo6LxQLWalayq/PF+bnDArSvubZo0lERESNZk3Ox3P2B+MvFlHg413843Pn1MSO\nsmgSExFRHEtd5aepucj01XAcySUSLixUue+YcTkFPSfH/M5JxP86npxvxljge+r6fF6v93Rz+Ik+\nps2qT//0D89RpSAGbQ1XKLjyEKkZyA4GoScNV0CDZaqjuc70wC8qe9n3zEdPruG6fGaABvNc6QTm\nsgnFeVkUlnF9eUYn6POzJuZ3UhV4HPJceZAwDPE8Ad5DBn+Gi4SR3DNz+F7/4n/59QM6JVZXz9MP\nf/iD6U6Y3+BLpomp800965nTnf5XnZ5ikaEnE0ea6tj5J/+YPruvqZ4AfQzixcWlU9f3+QsX6Ec/\neTdvTyKi0XhERERBoZiXBYUCEckHJ4plgFerFSKankgNMGeR78q1MhTGqn+bftTr9YiIaK7dzo+Z\nSTBO5N0xN1B/EOZlnl/GdbmBwlSeJ0xS3EfKIkwcR50uERHdvXMnP1Ys8fPOL56Rl/HQv//gyqnr\nm4hoZXmR/vf/9X+myvyLeVkWYhw58j5hynU9mfAE/qRu5Ko5KEvd33qeKUtVO8cx39NFh9bXMn3c\ne0LZ1Nca1TlGXY5iWZDEqH9XDZg05b+rjQUiIqqria/b4/lplEjf6u/dJCKi/+Zf/benrvNStURf\n+95XqTvo5WXJiP8+d17adxTxsz+6/4iIiP7O934/P+YUeaz97L3P87LA5fdbWlkmIqLGnMzJUcTt\n6KpFYYr63ry3T0REB7uj/Jg5zQ3k3Uf42K7f2pTrdvj4hbde4meoq8Ue+ngWqTY+4HsdbvH7Fusy\nDufaLSIiWp6bl+v3OkRE9Jf/4enm8BN9TIl4DGUkD5iZHqRmXCfjlzITnKs+YoSP1kQN5Awfwwwf\nr9iTHlrwuNK9RCq7kKJx0hi31h/CAv5VqzwMgjiRzhpF5ln5OYoFtZLCMsjVH3Wf/8fNcK9UfWhT\nfrZxJo3vOFWaFY7jkOd5v9MKftLv1P994V+izCx2zDs4emGDf6cu+IV76UnlhPiPpQWtB+qpr+E6\nVCoERJkMyniCCVE9diHg42ZCSFJZRPn4yDjqB1GIfoFqLpdkIRanfH4ykb7jor6LAbdCoSD1jVuS\nWkCTi7E0dmRcFosObsnXcEL1cUef99XkHWNhNR7x78zkSERUq/DzNmoy+aTO7PXND+ZSUqySU29K\n2cSMb+mzRfwdxTwRHh4eyrPD4sjUgsRxYdVjDjKLYSKiFB87Rw1w44Ewi/0okg9hgom5XKkcu0Yh\nUG2Jn4RYXIXa+sOrRJHyHng8R9Tn2ehJVB+OAr5XTAO5Z6as91Miy7h6a41WXtYf8vy1s7mbl43Q\nZ4ddPlauSIer1hpEROQ7UhaZcYK5fjyS/jwZ8DW8oJCXYZqm2gK/52G3nx/z/ToRETWb0j51LKz2\ntzt52cHhHt97xAus8py0DzlmoMg1lq7wYsHBHDQYSZ+oVdnyLi3KwjXL5PjTwO6ZWlhYWFhYzAj7\nMbWwsLCwsJgRJ/LVOA7/V1Cbyx7cWplyLaaOIemwLT+YyG06Q3YHhJl8x3vjEL/ja9Vq4j6owB1w\nptnIywJvjOvztVK1r5DC1ZOlx104qXLfeT67rHwHv1UuIs+QSBQBwuyfJnApOFPkArigdVkqLrFZ\n8Nvcol/mLn3yMSlLzHMat7ra6y0GaE/17o752xAslPvN7M/+507llz1hj/IUFyFKYvIDtbeTcN0U\nFDkD3lcK4Y71Vd2aY2kodUpwGe7ssWtyYVG4DMWScU1pNyz278xenXKzh5FxPSm3PdyKrtp+cbAt\nUiqyazAcC1mEYr5GuSxbERH2YItwjwau4jmA8+A78k7pk4hqp4JD5LjkKI6DR1xfrpqeMriVWw1+\nZtNPicStql3xxoNr2lLvj5q+qrusmb4MV3E8VvvPaIhCUdyUKbaCtLu7B1d6CZ5IryMu2gJc+/2G\nEHnSJrsUSxWQvybSRgnqN9Hckkz1qVMiiRLq7RzR2efP5WX1i2tERDTuyj5qZ32Ly4ZctyW1r1DC\nnqmvCDwD7Nf72C7Te/r7O7wPn4QyT8+v8Lubb8VQ3TvFFkW9OpeXhQ7352pV5tUu2jhB3w40QQ5t\nFis+Q7nOdb+yyvuiW4/2pQ4a7FrWbRypb87TwFqmFhYWFhYWM+KElqlDxcClgn+c+ZqqzehxzH/v\nHfBqZX1LVmhemVcA/bFa+daZzVZtmpWIrNAmI/7t0ZGsIl5d45V9BuJPwdUrZv5tFMvKcmGeN54b\nrYW8bGubmXFpiBWRMgx8Q7DRLFec4MMqLyqyxwSWse+otUkiBIZZ8CTy0UlgQpFGiuCyd3DEx8BK\n3d7ayY9du3yJiOQ9icTaMkQkb7ZHOoZZ35Gv8WzWhVkaEynL27BntSciJxeZsBl1zNRNqjpPCIvl\n8xvc54KHj/Jjb771NpdpthuuaxwkunYmYBeXCrKCjsdDIhJCHj8H/8p3Qfl3pf1LIPXpsgQEnYDw\nryIgeQTLVxEyfE/uPyu8xCU3kes5sGqcTN58AkvZg0Xo1ISw1N5j4ky8/qmc3+Tj9QFfNwnEUooD\njOGhsgTxummB66uyL3PWBN4rzXBvwarvJ1JPzYzbIUv5ujvXhe1aXuB5r/rN7+VlXR+s8AwkNB0G\nA29AQqqdqU6zIs1iGo4OaRyL1Td/cZWIiJZCmR8DvF+hxM9WLgr5ybCaC5pQ5JmQR64rT30jyg1u\ni0C932KTrcPeAMSjWPruJALJzBFLc5xymaMs3iK8RTHGhK+IkcZKHat5PcSU3TrD7z4YyBxdqrGH\naDSSdm8uLtFJYC1TCwsLCwuLGWE/phYWFhYWFjPiZG7ejMjLiBJFWHGLbB4PQ/kub/fYnt4bIsat\nvZYf68PWnrt4Ni8LSrwxXK7xvwXloqUJm/eP7w7zoh/87BYREV06x66CtVUxx32PzfTAExfW3NmL\nRET05td/Ly/7iz+9TUREowlsfyUHgxAy8lSMUoxnKmAjPlbrEMfj55giJf3HdvM+QU3kyYBwhnKD\nmiD8fQSGb+8d5Mfac1yXrabEbAU1Q9YCcSN5OvLJSeNjT4opxalnQEDKsoySJBWhECIqor2jUFyC\ncczurTQJj93bPNNExRMaLpKJ5ex1pS93hvz3QlXq27hcTRxkpMhD/S676EttcdOlhmSRqjhnxEP7\ncOUmY3Ffmb4eKBfiOOF7eIYIpPpv4E7HvRJNq8fMAoeIfDcjR7mVExCpMuX6jgy5KgG5UbX95BG7\neXsffpKXuSs8Jt0tfv9YeaWTEsiEfalXE3tt9jS8ULZxBgHOU8+TRVy/SgSIRnDzBpd4q8RZuCLn\np1z/Y0cISDEIkQkECZJYCdNkhug4i0LKcVSqFXrr61+hnoqj9UAaylS/ry3zc74IMY35JZljJ0Ou\n+0pN+uzBIeYGfBtGSkDEwX5FUpDPzXbM75eA+LmyJspj/R67bY+wHUVEFGMMjAfSj01tRUZQR9VV\nAQTUVAmkjMf8bHXMZ/UFIRitrfL3aGfrsVzDP1k8vbVMLSwsLCwsZsSJQ2MC188l9YiIXKhV7B6K\nQtHNTV49DIlXLpcuidpGExvbYSwry1aDzwuwcvFU2IxRQ/IbEk6w3+Prf7rDq6uPH36WH1s7wyuq\ntXNyz8LddSIi6hwI0WY4ZGvMBbFIU+dzUpViopsnGuNgN5EVz2Hf6FFKWTMQ62Mm/BbL1Jni9X/J\n73GaMpRokkwrgBjvAhHR+g6rihz0hXBQhlJOo8oSdQtNIUIYEkCqV9VYwX8Zr+iJFqqOuHkKA1Zb\nvlr68bTI0pTCyYBcV6wHQwqJY2nPaAI6/5Ate61Za0KzQmXZQeSIhiCBffSJWFCXL7PXZrlxPi87\nOOR+6oDUt6fq1ij5tNqyqo7T40otGaxmDyQOX3lqDFwl15elJhwlxjuKyWUs08DRBKdnuQ53yXUV\nqRH16WtiF/HYCmFhBooAmF1lqyJo/6GUIYRiACUeX3lTCnXu772eWDkl4wXAuzrbQnj0QXDJXVZE\nNILlm0Wq0+5xOEm8zITHsiMkqWyHFekCFR4yqYa4LGRRlXcsw2BIFcEpUfc/LTzPpWqtRiu1FblX\njSv1oCMeKrfKZWXM9a2GeEKOooF5yBwLCHUxY/jWRzfzY81FyCUuCcEpCtHf0I3KS9Kfyws8/ioL\n5bxscMTxRjt3pc9GCNtJ4JlJIjnWRwhmqS6KRhNICw73+VrVQObr9YfcPo7yRk46orb0NLCWqYWF\nhYWFxYw4mcBmRkQpkad8ySMswNe3u3nZzoAv24bocTQWq3VlgVcK45GUjQ9ZvLgyz6uf5RVZNXX7\nvNS5uy4rtMI8r3BMIPTwSFYQXo2tplQF2T/a4lXmsCPWQruOsAZYpDoo31g8qVoJBggS74PG/tef\nieDyp7d4pfbtr4sY9CSefc/UIYc8cinRcTsmuHwqDYg5djzDSQy6+OOdDfW8d4mIqIeVnaMsqwpW\nyR21T+fiBkttrtu60ij1IQA/Vnsw5vaNkqwsjaBFkp+jVvTuk8xQtIHJ9BBrzwF0WtWqndzjlteJ\nkSVE4y4laj8wNHuNiVgxEfaWRhALL1ZkVb2xs01ERLfv3svLBtiX3z3iPaCiWhGHfbZ4Hz6WvZp9\nnOcgRGagdEtrZp/K0fWN/SqVecNo/TqwnErKkjOi955qs2yEe+B3o46MZ/cMe4WcUIW4Bc9w7zsl\nSpWnKoaFNvDlfVKMOw+WeXJTvFEJPASlkoiUx49ZICNY5j7obEsdFsHNcEvSDpN9niOM7nJ2KOe7\n0JZNJlJffcwXJVJ73Rn/7Rxy+6Xpdn7M+9V7fG+Se7pv8X5kmMCzRYI0NbrPx8f+LBj0hvT+X31A\nL7z0fF726jfeJCKieKKsfYwBF96IUlH6eKVlFDHk/GaVPY6lClvjR3vSV1ziefqFay/nZdevf0RE\nRAc9bqfzL67mx1bW2KvYWBbvYpLw/Wt1qe97v2RrcoAQp6EaJ9TnftJuSp+olJGEAufvH4m+8+MH\nPD9eev21vCxITzanWMvUwsLCwsJiRtiPqYWFhYWFxYw4mZvXIXJ9okS5eTf2kL6nq7RtC2ySj/ow\n9SviRuweGHeK3DqBu6kOAlK7KuSX7S12p4ZKwadeZ3dwAB3Fiib+zPO955bFLdHdfUhERI+ORHlm\n4rJbYhGPERR0aAy73hKlhZmBFDUM+dkOBkJSiQpQz1C09yAPLfkbOi0yB2I8TyB7ZDqr3ReSKbvq\n/Bi+ob5yURWK7PpqBUblRNyxpTLXZbUidWoUfyCGQpOxkHGqEE+OQnHbHxyw+6R5+ao8L57Re0L6\nttSEQuhkwPg3NIm4laKKyV+bkCYcqNCP0yKJyOlvU5bI+/W2mJBVqErbup7JZ8rPfffBrfzYv/3z\nHxAR0cc3hIBRqLLrK0YoSkUpx9y+za6q9oKQVZ67epmIiC6f5+2OSxfEBRZP+D3DrmxtBGbo6Wzf\nyLXqDLn+yhOV+sskWFZuvZZJ1Qb1p6LOH2zCwtTeQvFYktvTIcuIwjSjsXZRI5bIV2njfJOIHW73\nHkiFREQhXNTzV8UFmD0G4ecAoUoqdVcJ6cfiklJdghZvgbhvhcqFOSyBlKTktqsh95F0Q9KWuVXe\nBnF67EYsjOWedbj2h49ku6UG8lmM9HOaBJmCMKX1iQNF/DwtoklM27d3KfCkrl76GqtwtRQR0aQu\ni12Ef6mUljsTJiotXJZ5YwS3bhG51c4siXu1uMDbfTHJHDTBlsHePd7eGB1KGEy9+Qr/e1bmcBeh\nLhffEE3h0SHX8yfv3eD/P5D6XoSaXlHrSWPbzqtz+6dNef7Jfe5P/b7SUz5hqklrmVpYWFhYWMyI\nE1qmDjmFgCJPvuj3YTke9sVybC2ZDA/Y/FeW0iHIQr5a6ZwBUamCwPXNR2JBfvrhh0RE5BUlHMMs\nmsddXkX4mSYKseVYq0koTb3Cv72naOyP9thCvvwaW0+jrqx0jcxwGom1Fcf820qdV0Zf/frX5Z1+\nwuQCvyzP+PxrZoP/39DMUGQdYxvETziemJAiZaCk4J5fvHotL1tjw4cmCI3RIgwmpEJbpsM+9IuR\nQaVVV4nPTYJ2FcYQg/au483Ba5o6T3CczJKYbDQgOKUqXMqQURIVTuH7sydOHk9C+uzWPao1hU5/\n/TNe9VYbstIu13hlu3vEJJ1Pb93Jj21sMOmkUpFrrFx4joiIhhAC0DwSExiuYtzp3iNe+ftIFr24\nINci0P/3Nvfyoioyz6RK7CSGp8BkAkliWWX3oTSwsy+hECbhswnfSRQ56XCXx0o8kesHwbPR5s2I\nKMoy0hEmRtfYUW0+MrdDqE/5ldfzYzWERvTmFKFoHoSYEGSmNaX3DZJi6Mn1zQxSxvmRshKHnsk6\nJf20Cu9EtKBUG5DEPUMdBolcP3yb79lVWUwIGsEmCXqqMl0Z14yvEoY7hWeRiSqjLE1pd1dCf7o9\ntrLLyuNoqsZoA28fiAU+Dtnz1DojlmytyePPPeL2+fZXheD0+Qbfa2tX5tivfOMlIiJCAhq6+eH9\n/NidX/L8/+YfCxkoA0nRK0kdnX+BQ5D213nMnVNCM7VF9iQddKWPn4Mn4PyVi0REdDQSa3jnAb+f\nTiLfWpBQnqeBtUwtLCwsLCxmhP2YWlhYWFhYzIgTuXkz8iiiOt16IBu9dx6yCV9ZkI3hBcSLlkFm\n0W4tD3qU820hXCxCISMxJAN1voklPNgVt1YLuqS1CrsWCsqN7Gfslhj3xRF6dondAdlZcTG+9XfZ\nTXQepv/63V/lx9776Z/iWRUJw7jczrCGpBMIueaFXXb1rF1+Li+rqPimmeB+IbgM/ztRSiq3bnNM\n4/4+3BbKvxpU2NVeb0p9t/B3q4HkxIrcYxJZ6wTwJde4r01Mrpwfw/UbFMUF1Z7j605UnJZH00nE\nHU2WAZlFkxxCtPv7H31MRES37z3Mj/V6Js2eXD/7Yj2dAp3BiL7/i0+oUhGy0dYWu5DK/oO87OoV\n1lx1Cly3pYK4mL/1rW8REdHPf/VxXmbSpnkFvm6meng51zeVOo1Qb5sHTKz55JbagoCyUqziQCsg\n0qSx0g8G4SxCfK7iH9EAZJtDpfASmfzvJgG8ivvchDvYVYSMLJtdjYeI6yLNkqnA6TwGWY1r07oZ\ntFa9JYlFN6ftHmzlZcMe12EB7xUlikzS47opVmS7wqh29SP+3TiS9zOqSHrMmUTpOh1liLjtAohC\nmYoRXYT+uK80mEdQrsrQ3q4i4Jk+UlKxsH6iCEKnBrNID/fEbbu/y4pby+fO5GWeD+U5KGONYtny\nKmHPZvu2uIqXz/G2WtBGX6yJYtJZhIt2e7J10Jjn+njhHSYbdZQa1e46P9vuZ/KM1169iucRElZz\nhbcmnNcwJkbSPlvb/L2oqgTfl17k+blQ4mtEqg8XkA4v9aSflOdOlvLOWqYWFhYWFhYz4kSWaRSn\n9GinT7fWZVXsFGGFKsKFh91rHyoazYYcO4LSCKlV7hK0LI3SztzFS/mxDz9ncsf9rZvqfF6VjhGi\ncemKZGcw+pnaWigjdKXky0qjXgVBqcJhB0FbNqrv4/2eO68sMBATPISVVOtieb7x1leJiOjOHQmR\neOOVi/QskNG0JWM0OycqVOgWCDDr2IgvKYp7oVzAc6tkvWV+h7l5fodzKiPExbPcFokiPfkIQTIq\nRmGqLXZeORfU9T1YbJpqZDKrQGp5Slt4DB1YbV0mDt9zY51DCTYfidVhoihilVA4KM9OQAqTjNaP\nYkoPZMVdg5Z0qyzv/NprrFzzS2QpOTiUFTSBxHHhnFhOB0hCvL7LYQCFsiKSZCbLkS7jf4YI9xkq\ns9Io9Ey02kvMfyeJyoICMWZDYOmqcCZDpAmVglQBJKYJyGOdQ6mDog9lH0faeDjq0bOAQw45+cjH\n86XHSWrFLxjCmoBnwtYKsSbr8N8mG00ayTVNIutYKTr5sCtMOERThfNFINlFivAWYwxEkdRrGQnb\nHSQyjz0V6oIwLjeTaxRgicbo9zpLVQHPU6sI2TMKn0WmnozSLJ5Sd0vwDlGixpPL5qTJfuW5I3WM\n6+jOp6ICF/b5ei++zkRHmU2J6pd4/i92VPYjWKIFhEF+5e1X82O7t3isL5MoIM1BK/zIkXkvbiJ8\naIXH6OfviuZ1BeFor738vCrjujSZajLV/zP0j0D18ZJ/slAka5laWFhYWFjMiBNZpmEU06PNPfLK\nYsk0sFfWbKkgXUT3t1psCQ5U0r8Yq4HuQFZ0I+jANursR79yVfYeL19lX/n7H16Xh8YKbm4Oe63K\nn9+s8Srl0mXZw42hxVgoytphd4+tuA389N/+n/86PzYHTdCGEpaYQCjArHgmBbH+JhPee5qMVehA\nSXz1syJTVqLJYqLFD65e4jqaDPi5V89dzI+1IGIxVlaLB13RECvusdrbjGBBumqdZe5JsBgitZ9k\n8rzq/UtjWZgVOhFRGRaVgz28eCTPE2GvT2sEVxvctr//9W8TEdHX3lb7o7DOB0rfuYgV/P/2P/53\ndFpkWUbjKJ5aYY4hfnDhOcnJGxT4jDt3OGzm6vMv5ccuXeG+WyrIftxN6H4O3/sl/74s1kbRWFDK\n8kpgvXgBj4dhLPWSQSO1tCj9azLhUIXBoVjvJotOpQV6/6bs+e5tsyXdbMuYbbWX8W7oG8pzkEI7\ndjCU/t2LtS05GzzPJW8qNIYxUXrLZq/dw5QV69yisBINV4ML2aoJYcOmev8VsW9eorwvJhzH++32\nRaj0oZNca1rp2ZLhfJg9XxVqgr1V05+IiOKE36/e4L7iK6vVjdgSK5aOW9uzIElTGvT75Jfl2YwY\nS9ETy9TU0QAejpEKE5xf4Hq++oZYfekBe/P693mvsjRS71LlOnVbUtZchM53xlZr85xYpger7BXZ\nvys8iQYEGg482effr/IzFs7yeKotiufxzAJ72Fafu5iXmUgro0GshYAMxyBTbbzknWyP2lqmFhYW\nFhYWM8J+TC0sLCwsLGbECUNjMkqylNrzQiiKMhByauLWcohN8hHccCNFWXZAiEhV+MYW1DjKSO11\n1BWC09mz7K6t1cSEv3CBw1MCuHC8TFwnZy+wu+rc2nJe9vA2pxy79+hGXpa6cDNWmLyx//h2fuzl\nF11cX1xZE6g+9QbsBgjqQhgZDfk9h31xXTve7C4ZIqLMcUnzMYbGtalcVNeusKSRUcJptaR9THhS\nrDKdm9R1xlWVKg1i49ne3hVSza9/zWEeRx0mnej2GcJdO1DuqzFcJgXlPv57f/xdIiK6dJ4TOT+8\nJ27HMORrXLx8OS9zoXvbKHG7B0rXsw9dVE8lrdIEk9MiJYcmrkNFRcTowCW6MvdWXvbgzn1+tjK7\nYb/yoqSWaiJsq6c0PheRyPsciHOHI0VMASV/HMn5uQrLhPtmIZDzHbj3fVfGW5qarRbZ2phHeFIR\nVP9iSQhaEyjtjJWrPUSbhRGXJao+5xd4W6dak7LxxpQG16nhOkRF16GScq/moVeKDGKS25tk2VpJ\ny2wr6O0QB25gEyo3RWrCtaakV/FTSWyvVMfgYi5oVzTqMFH2SAoiUYCxX1RBgRH6+L3bn+Zlhz2e\n9xbPMAlyTiXsbi4gBEglcPfd2eu8UAjowqUztHMg6ccM+aZSl/rego75BOntfF9c6OMez0FnVTq5\nogNVqQ3uq4t12YYI9/n8MFXnQ6muCnWpTldSEG7d5b+DoYz5Sp3n3zXlei1iW6sMXevzf/RteacS\njw9XuckT9Pf9jR3cU8bcxLh5U7nn4PBkJDtrmVpYWFhYWMyIE5lPvufSwlyF/KYQIiYZrzAqKmlr\nBmGG/T1eAYwnsiosIczCU2EQo7FZFYMQE8oKzEe2haVFIT2ZFUABq9lWQ1ZNC8u8yiurcBwPgc8X\noMlIRLSNYHwXSWQLvlhRJrQkcuQ9Hx5APKLHYSjfvfiN/NjZBSZy3L2hVr/p7ASNJM2oOxzTUBE/\nzHOPFKlrAdqtrRY/d5aJhby/hyTJKuME+DNUxAo6KIjOquF1pFXxBBRQD3du3iciotsPxars4jli\ntQo3e/hltbr/5je5fsttJsSsFUQY4QD9pD+Rdr+3gZCoW7xK7Spr2FimGamsH8HsGTXSNKFBv0f7\nh0LsXwxgxakMKzdv8rNdBvErU/31cIffZaIIWRXU74WzbHk4+yrROOrWU+1TBVnGJMx2FZGlCNdB\nHMk9I8QKVZX1OUI4znaHhTy6Pam/KOXrhYpEdP8BC0M0GtwujlpnG6vV84U41W49G1GSNMtoHEYU\nqPcxJBDUzWk/AAAgAElEQVQdCGKsTlPmaaGWgN9HW5PmrzypvLJ8XVjrriYx5WoVx8NPEhNCMSVU\nAWvVPW6PHG5zn71/+/O87NZtDtvo9IVAswQvDZlxmIm3ob3EbekrEYH4CdmjTopytUyvvP0K3X0o\nluDYzFVKb7lQ5nHtxlyW9VVy+QMef3MSPUULNfYEdmFplsvSn4shX2PQlTmrO+BxMkCIjhtIvZ+t\n8m+DlliyIUJiSmosXPNA0NvmY8XzoqU7mGNrVcmO0/4W9HfxvUlVn2vN8/mloowhp3IyLWRrmVpY\nWFhYWMwI+zG1sLCwsLCYESdy8waBR8tLddofSuqaSpVdE42muO1CqKg0W+x+rSiyTAS1m+Vlcdu2\n5zgmrI/Y04lym6X5916++z24/C5cPU9E04o/d+6zW3M8ERfLHPQZ164KUWQdqbK2NthlmbgqB1aB\nTf7DSFwKtw75HQoL/L6OIte0sDnerIkbLPBnJyBFcUxb+wfU6YjazfXr7C4aHsnmeADXnwc93XCi\niCWoy2IgLotWi+vbkMbOLEi6uqU2u4wbLdHy/TaSB7/+Ctffxp64qj5BirKbd+/Jc8N9UlJKPwFc\nkCbFV0kpFhnVEfNuRER9eNaOEEP83ke/Vu/EB4sqLd8oUqmtTok0SWjU7VCsEo1PQGD4v7//w7ys\ngkThz73EKaKOOjIeytDJTZRKVAbZp8UW/65QVpqwRS6LHeVq9wxphtuzo9SOfMTiOiow06hJxZFK\nwYY+4PncBkFRCCE726xcM+wrcguuGyNtmPkdEVEUH7++FzyLdGAsyRunrD5lYMhkjiLGaXLRsWvg\nXx0b7Znk5dnx7RZTdb4Obj12fRn7AVzEiZrHQuj0Hh3IlsCtG58REdHNz35DRER7e6KpXKxwm775\n9t/Ky15+800iIqrU2UXqp9KfU5/7oONrltTsyCijMAuprBTLfHwGeiMh3xQwp9Thfj24sZMfuzbP\nRLf5c+LqN67weMC+396RjIkEWx6J2vpYqvH4dwxBriLzQanC9x6qrQmE5FK3r+LTjzimtYjx4jXl\n+ZNmah4sLzvY4DjsKr5LWle5vdieeg8iolJLpZp8CljL1MLCwsLCYkacjIDkEi2UHfIVXftwyOou\nQahWVaCjV0psqR0cycq622WrxtUZWbACfeF51nUcHImSy/iAV3crbdnQNguKDZBxqmql75d51fH4\nsVz/j/7OP+XrdmTH/BfvcULvVgNknQVZZfU9Xil2UrnuLnQ8315l3eBUqR0ZXc8zKpOF9yUr6adF\nmmXUH02oUBbLe36R71GriqXR77GVeojVYEeRdYz+ZDiRFXR4n5VFApB22g3RwGzD01BvyD0bTW7b\nZovvefa8qAH9/f+CQ172VKLpDrKRpKqNHVgKO+vsCRh0xbod7jAx4N6nonK1A5LA1VfY+vv6NyQ0\n5bOPWKf50SMJ3zk4FKr/aeE5DjWKATUaElZVA1trfVP6ztVFJo4kUMUah7IiNs0+Vlq42/vch49G\n3M/7kVhLqxe5z1eaKgsKyEgTWPjttrR1BcSwRJNhYH3pImOtGgLOeCwHL1zgPhyOlZYvrIbtHR7P\ncSrHXBMWpkLcvODZrcOTbDpULicbKbKJIRcZ4qImG6WpIWrJNc3TZQgJmzJQM6PkpUL2QFkymXE8\nV35g1ItSpY6zs8X19PFHH+Zlt6GINR7yWLh45UJ+7Fu//7eJiGhlVVSDMhB+jPHkasqVCdFResBP\n0iw+KVyXqFhx6d7Nz/Iyo9/cviZZYypVWHsHbP1dbMuxdpst00yFnTy4ze9+0OG5e2VeZVyBdV1S\nyl/zCN8roz8fHKn5Yx9zgyKAbiFM5eObotH+zbfZsn8HFv5HKoG5qatRV3nwoJIWJyYBvHg+jCa1\nErTKPaVPC2uZWlhYWFhYzIgTWaYuEZUopTW1N+hjBX7wWAQRwhKHp0xc3g8ajeQLX8Qqb3FBQleW\nl9gyikNeRfR3Za/hXJlXj/ddsTwmCV93DwIKY5X9gbAyWj27mhf92f/1fxARUVVZc1Xk2/vwV39G\nREQvvfhafuzQYcurF8g+MHkcDvHcpQt4D9njOsLSstEW66Kgwk1OizAM6dGjDSqVxEpcXDKWqZQl\nWGmZ8JFer6+OpVP/EhFF4fT+otFjJSIajbkuE7U/tosVWifk/UsdCH1mlfdbL9bFmgsnXM+eWmlP\noI/cQx7OjZ4KGsde7/Kc9IkMweKFjJ/jT/74j/JjJYSCvPvDf5+X9XUWlVOiGAR06ewZeuPVF/Oy\n5y7zvvxvPr0r94KFFqHeHTWMshihNKGyImDtdHu8+t4+kNVyCX2yrs6PsFdq1s2NeaH8T3B9V+1p\n5laasr4y7O8Zi08LFATIfFTQ7d7n+ltEBiFX5bM97Jqge7FWC6XZ+zeenlw3mM5dij91/lQTvZHl\nv9JXgGWuzM8865FrcovKsQh1o8NrDMfB941FL96GrfX7RET02Scf5WV3bjEno6NywnaRCaU5x3PL\nN77x3fzYteeYbxAqr4QReciMNeyrZ4QnJ9XWuTv7/mmhWKRLV67Q9bbM1/td9miFody/ivCzBjLV\nXLn2Sn6sO+aeeesT4TEEEM756uusU92oydxptLwdZe3vbLHFe9hh72JNif60oSc9DsVMHG7xfFF2\nZW/1a6+yRWq03R8pz2OlMp0jm4hocZW/CaMB9+dBJuOwAI15n1S2IKX9/TSwlqmFhYWFhcWMsB9T\nCwsLCwuLGXEybV7HobhQpoJKPza/wm6q0YFs5sYgFO1ss7u2q1xwLnZ4k4mUnQEteW+XTf93f/z9\n/NgbL1wkIqKLz4mb4fomuxS6ff63MS/P099n2v+mIjQ8htqH74lryrhJK3V2m1Za5/NjpRrf89P3\nJVSjiVCRUg0pzVy5lkkC3F4Qd5zWKj4t4jih/d1DGg5Uai1QyQNNJQdBqQ3Kd1WpPxkCQ60qz1NC\nmIpRgIl0GAcSFgfKo2Ro7nPtJq4lrhCTsCxVoQ1duJkX21IfNSjr1GrsFl4EiYeIKMHm/wtKz7YH\njVijgTrfkmv9wXf/gIiI/t2f/UVe9v7Pf06zolQq0svPXyNP6b4+AmGqWZf6CwogN0zYBa3JMwW4\nwDOV7LuFdGlOieuvuSDbHm2EJRVLsnWyg3CLEdzx4z1FFAI5yS/IlkUZoWEVlfatgL7uwJXpqXSC\nGZKcHR2Ii/LWTXb7eUimXW8KKa1cgc5vSTpF/xkmB/emgloUWUi1g1EmkkPK0YvcWprAY1zfZktD\nCQlRyYjsZiqlXIfJK9vbTCy6c+dWfuzePdbt7iAUg4iIUnNdpR+MrZGLFy8SEdHq+Yv5MROippWu\nfC/3Z/M/+nUNgUy9k/sMSI1RGNPWoz1yijJ/rT0PQuFIxp8Jm3plhd/hgw+FaNVqcj+uq/7QxFbE\nGNs5/UM158OFbjS4iYiK2Gqo17jvahU208iDvpCSzi3xNtI7X31H3gUu/J9/zKFI4ZLMib0N/g4E\nRRkTg01uv+bytG41EdH5NZ7/u/vy3A6djPBlLVMLCwsLC4sZcUICUkZVZ0Se0n6tgBxTOyMr8Z0u\nbwQfYZM5DEQUYGePV4CB0sr82c/Yqlhb4w3i17/6zfxYswSxhKpYW6MJrx4zUKcvXbqSHztAAvBQ\nCT9UK2wV/c3P3svLzp1jy2jtOQ65CKpCY+8MeUUyVOEv165cQyXw8wyVJeaV+DmqSoggjU9Gq34S\nsiSlUX9ETiZ1NRiCDKSyuvSxUZ7BijPWHJEERS8oq7mBlWURm+77e7Jxb0IkLl+Q+jAhTndusKfB\nVSvSYpHf/eySiHAkEVua+121qqYSrs//73mKLWMe11OZUHz+ewyvxo3bYp37IL/81//sX+Vl+0e8\n6r358Q/otHAchxwvoHuPhQAXg4hSrYqlVoflH6D+tI6tod2XlPaw8SZUavzcflnCBkrwGIQqoL0M\nr4PxdIyUhqjRrR5PhBzRgUV/kIrlZEQXKghHaFSkbhOEhExi6d/zizxGNzd5bG3eFMvM8/h5WsrT\nUG+JZTwLMsooS1PKVHC9k4eA6PAf/sfNs+YobV6UaQ3jDO3QgaDGPjxWRETjMVvkW5v387LNLW7z\nw30muoQq3MnL5yqxVHKy01SmGv67gYwppZLMickTtLozxDLlieGnMttwoedLWS5EMQP8IKCF5WU6\n/7wQNJvz3Ed2HojwSv+I+8gIIWFXz0pGIpPgqFxVmbOQaSkccZ9aVn3FeFN0CFmrzuNjAj3ujS2Z\nz4qwUo9UuNuFK6yDffXytbzsz3/EQioHyKrkq/Y52ufnDgIVxrnBmXDqbf5mnanLdynFPBb2ZKxN\nzVFPAWuZWlhYWFhYzAj7MbWwsLCwsJgRJxOQzYjSmKioYnH81KR/EhdLCYmze0tsRvvZpfzYPrQV\ne0qRpTvk2MPGHB9bvSAKO16E5N17ovV4AKLSSy8zKemacvPewCb6zVuS7LsA/dNJLCb8C6+8ymUu\nm/z3t2TzPUkMMUDeaXGOFZIckHUc5ZLxHb5uWaUw6u2LW/LUcDjmbqiUOMIx39eNpemKDr/DGLGk\nk7G4DBNs+o/7oq3pl7m+CwFIL1rX1rjbou28KChwe3YQj6a8yOQjLdeDeXGnFPIYRSEQmDBXQxLx\nlJvfOFN06mMXbAwPrj6d6sokj55fEVfVP/on/4SIiP6n/+H0bt44yeiwP6Kb9zbysgF0elstUZVq\n1Pm95ue5fy8sikvLpHDyXCHFmTg74+rT8ZMR3Ls67aA5v4imqCnCXxOprWIVU2mISoOJuNE60Fnd\nAcmofyh9M4Cubkkpa7Xa/C7tBSZ6HOyJ2+3GDY6pXH90Py87m4nbbxY4xCq4gXLRGraQTj+WKwKh\nl0xGaj5AfOHurvTxm7fZbXv9Y07Gvbcnbeq7XLHlsoyhCgg5Fbi0yyqOPG+jsdRvF2MyUonkB0iV\nuAl1pIk6v2TiLjNtv6CBsffhZJpshHlGywdns7t5g2JAZ66sUtaUd//VT/+aj8Uyr5eqTGZcR0rB\nVy/JHG50p2P1HTCieEnI73eoEnufPcvXqmWy9XEIkp2JnT134WJ+rAvVoqWzEru+tMLbSD/+6U/z\nsk832S39/O99hYiIDvZlm6OCcfjo/qO8zMeW4fY+9+3CQMbQmRfYfVxuy3aO76hthqeAtUwtLCws\nLCxmxIks05A8epi2qejKxrOLrB4tV6kFYVXXQ9aQB+tiWWUlJm882hKSRwOJxa/fZWvu4aasildb\nvPpxHXnUc2fYEr0CC7ZYEOLPpeeZULR1JCujW4/4XpU5Wen88jprPFahifqH3/uT/Njnn3OYQL0u\n72TCSXzDoFE07wBxJCW1mr17SxRzTgvHISoERKHizI9x36OOhCaMYNWkUAdJVLN6IGIkyrqNzPlI\niF5RYRlBhdtCiaFQihVxDaE3QaKVabBxr8RCRlimpqrQJJiOYQWPlC5sEhqVJr0SNJkmkHw8Uucj\nhURGukzbtadDr9el//dHP8rbmkg0Pje2xVIfICNLB8SvRzvihVhAqMv5FQm1MmEArg+CmLqnSYTd\nUVrFn33GlmC5zP1vbk6IEu02t0GpISQmE3qgw5/MbycVbov9bVm1RybcQll+HYQzZVhfe0p168rz\nrGqj28d5BmEaeHpy3BKZ9iYiyhBydNQXAsoGwuw2N/jfx5sP82OP1nms6dCLwRBZXTBOyooc2Koj\nxEuFrxkVJSO1pJOv9/pQgFLJ1PuwQrsd8VicP8Pt1D/kuv7wg1/lx975+reJaDrUxTHqTAmPDa3S\nlDogtynVIHJmz4wUpzFtjfeIijKHX7zI6kxH2zLvNkAgqoOsONRhLQgHqilbrIC+lJYR5hNI/wmQ\ngWc0krCTFFmSmgjB8lXoVggCY7MtY+gnv+K6fPe6hOh85bscJvPgFluoe2oMra4ygbKnwn1qyPZk\nnDoHhzImJjf4udurErJXOqHKl7VMLSwsLCwsZoT9mFpYWFhYWMyIE7l5u0mV/rLzNvk9cVRVcYVy\nJG4Dk+g3yWD618XF2GzwsdqKbGiPBmyed46YSDDoy/nG/fLm65LYu7FymYiI5nANryKbxrUGu27e\n+Y6kVFtFfFGno8TVEcf1j//h3yciorNnxLzfRRqq3/xG3ABdpDmjJRCRFJkpNS6KoriFf31b3IKn\nRpaRk0yooNw7xQAKNWVx0ZlYyA7q8WigXJ5Ixk1K5Sg2riYjgq5IEQFIVFOJvUHOyIwKzVhcT9Cc\np4lKlj4O2fUVDyV+NURCBEPi0HGSyQT9JZJ+5RilrJwEoK4P9Z1JKP2kqtLInRbj8Zhu3LhBKyuy\nHXAG/aKfyP23IW7egr+oUhGX1nCT4xl396WvzcFl1ga5oazON38fHYmL6u69+0QkKch8JFkgIrpw\nkbc2XnhNEjM4cLulqv4iI5yPf7RX1rivFN+FPLigIzDFtKpTYFykqs9rcs0syLKMxmFEbiTu0ghx\ns8Oe1EkAUtulSxf5+VR/u/E5u8U1P8ckunjrTW5L7Y7d2ODx/fLLX8nLLq5xvcZ4r3fffVeu/4Dn\nD50w3Wiwf+c7Imb/1mtMiDSktbFKtt1HIoFyTdzzJqObY9y76vnjDEpaqt9HKi74tEjSlLrjPpVU\nMvqX3+bnfvCp9LOtx/zO83DzZir9nOkasYobNeSrAsiKhUTG5sYWXMRKNe7sKtf3YMBj2fVV4vCz\n7C6/8+BBXnZ7k5/n0lvyHSjUuV8+/ICPrarUkGPEuxo3NRGRjzHcxjZOY03m/I/e5/SPflG2A+Yv\nCcHxaWAtUwsLCwsLixlxIss0dTzqBXP5xjkR0QArp6ovq+0xrMkxtCpLRUVw8YxVJKsa12eLroWk\nzFpjM0t5hfPpjtp8P+LV/94Iya0VxT3EK+nN4yEsmYpKW/Zf/QMmHF06z1aDk8n1v/X1N/ien0nK\nJRPOEKf8b+YpkkrAFsdeIqowNztyr9PC8xxqNcq0r3SM3YxXeVVXrIS5ZX4HyGLSY0XgWh+zBXgw\nkI34EVbfE0NE0mwjhN6QJ21gdF2HUF8aKIswBSnDV/XnwMqIRhKqkEsf5SfJOs4FcStQ2slNeBia\nC0y4iZWlNLzPYRG5xioRLS1x3fdmiEjKiNPZ7e4psg5W5IWGtK1J2j2B1V+pCDnDKA5FKjdyf4u1\noTf32FtRUuSPKpIjZ8p0XLvAK2LD99GhTgVDclOhWS70lPf3xRNw9zaTMox2rOfLmC3gGWsNWbU3\nWuxxcd3j62tTNpmIZXTz5qfHzjsNMiJKnWwqs3elZhLUz+VlKUh4CazjmkqmfuY8hzXocKs5E8qG\ny3aUZToCcey5557LyxogG8ZQPau1JBn227/HWtApaYIQ//vqy6IZfnaRfxPnBDmlmGRez5FxYJ7W\nNe+uU+jBWp3KaT6V4fx0yIgoiTMaOEJg3JnwOG0si4rZGKFRlYDHerUq810dc5+vuopX5T6Vwhu5\ncyihS+U2k04XlPcoBKEw8EBuVFrjXaSBDNU34upV9kamSof9BkieL7/5OhERFctCKHtwh4lqidJf\nXmwj9WARKmIqxVu9wuNj0JHn7h7J+HgaWMvUwsLCwsJiRpxYtMGLHCoEyqrAflvcn6jTQK9HdH/k\nimU6Au3ZSVX4BnRYCSvssTJihjjPV8mTYySs3fqYVybeRFZZjrFulI6lb7Y0lbXagybkf//P/y4R\nEZ2dl/3Oy5fZMvi9r70lDwLL2wHle5TISn+rwyvR33z0cV5282j2dYrv+zQ/36bJWFHKYRVFfRUa\nc8AruYUmrywvvngxP7Y94dXXncdism1usIW0i2wwR5nsXw6wqp6o/RBjfZr2KTbEiliE9XC2LqbY\njQ//hoiIDhOVgQHVkevUqlVkrcW/XVqQEJClBVgWWBF//Pn1/FihgH13pX2aRbNrITPSqRCQI+yP\n+iqhetHo3IIbkKq+ebjHe6XVBVnVtlq8Iq9k/C4TJa5wdMQW05R0K8wpU0dNZaG1EBrjqTANF5uF\nqdrXGppQpARhHROxnkP0Ib2f9PyLvBfl+iZcQwahA0/ReKTCq1SfnAVxnNLe/pj6KvF41ofOayje\nlEnKFoax4kolGX+N1mU8qA5diXB96Lb60rfqTe6DD9bl+lnK7VAuICtTUbTAl8+xVyLTYSroz0dD\nqZMhMgxl8Bpo0Yk8V7kS7DCWtA8TzyfFg8CeZqzmsdEJk1U/GRlllJKvxs4+RC/aReGZXD4Dy3zA\nFt6wKJZ9M+G5sqj2QIfGSwNPS6J1hvFnnEiZ4aA0MZZpIvNlD4IOCzXpnyOc9lOVoL1Q4e9Ks8Zj\nYm8s7TmOIbYi9Ac6f43b9Dw8oOfnr+bHPj/HoTS/viH6xIOBvPPTwFqmFhYWFhYWM8J+TC0sLCws\nLGbEyVKwuT4V6i2KlLvBh9tTb/6XQTYxRaFyXwwGULZR3/ExSAXGFZKoMATjGgg0IQbamm5g1DaE\nHJKBUu4qQkMCN9hQucY++JRJIX/6F+8TEdG//Md/lB+rB/zb88vi6vkpqNOXX/4aERHd2RWXwvd/\nxWSMhx3ln/bFjXlaeJ5HtWaT5ibiSoqjaSIGEdGwy+6IjQ1WdcpU6rjFBSZq/O3zQgP3zrM7+Aik\npF2lRjQCsyJRoTSGHGJSts23pF4uQIXq+mfi4v7g5/+PeVr1NmgPw2/yxOVTxt/BROo0gqzv7ga7\nE7ceiuKNa55N9cNhX95hFjiuNx1HAqSxPG84YBdqiv5Xaco2hlFZOVTuvy7CIoxe6NKcuNOM+pSj\nkl1PQm6/gzG7jLeVepH3gOuhWZVtiSauV6rLOLhwkd1WQ7RtokJZ+mj3QGnz5go0mVGjkjGYor5D\npWBT9p/NOnwSTuje3TuUlsSlliEFYpaplGq4nWmaTMXBGIWuWIWOZHALm/O02zEn8ihCj7muIcT0\n+0JEOURS8ChWW1kI2fLUvFQq8vOasekpl64P97lWjvKRmiwwCd91WjkktR7EMiYavmhdnxaO41Ch\nGJCvSKQxCIX9QNqgD7Lc4sTkTBS38KEDtaxMPh+1Krtkl5Z4vjm6/Xl+rBxAr1olUg9RRya8JlHf\nD7OFkQUyB22j751bk5DKaMDj/+EDDumpL8uYWLvKY2F+WbZI1uZ53ru28CIRETWr4gOuIS2iX5O5\nbaMnRM6ngbVMLSwsLCwsZsSJLNNCsUCXLl+intJA3EVWilSvLBAuY5LZFhxZBXkB/93rKXIKViyG\n+BFoSjmsWk0KcWH5xljNp1rvEis/T2VwMUdTtTqdIDPMn//oZ0RE9NrzogP53Xc4o8ziimTGiImt\nvpsPmMjzi08loHjzkFePTllWNXPIwiE5C04DhxzPE8ILEVWavJIbKt3bCjKUmEwrh5uSCPnBHbYY\ntz+Xjfv5Mq/aVhd5ZXZZhUg0oXNcKsg9MzC4XJ/boqyWYNsPecP+13/9w7wsBCFMewcMKSMB2X/Q\nl/b3YDXFrggdGLrUAVbNqbKUU1gUaVG679mzXPedrlzjpHAcl4KgRKHWSMU7+JkmwHHdRzHXR7Eo\nbeEYS0V5B0oFZHoZsZX4+PFjOQYN3FJJVtVFkGv8Ald0oCyceMLPsduRsKONPb6eDq8xer3VCsgi\nRbFaSxgbRV/qb3TAJBQTQuMo8ozrGYtLheM8IYTmNIijIW1vf0CTRCyIshEzUHqtQW06GXk6lUgb\n/SEVT4jxaBUw3ziqL5p4k0z1qQK8Bi6mxKIv739umcdJmqk5CHObDpfxQBoyYhvaenZcvq4zFfZn\nsgjBw6bCx2KcVi7KOCy5s3tfwklI67fv0fm1C3mZj/45VASze9Anf2eJz7u0ImEzgz5bsH1FMGzV\nkcgeXpWKIg/VIPqShHL++haPcAfEwXPz0v5VeF12FaFo+TJbpM2heEw+/OgXRERUanK7LK+oMVRk\nS3OlKXPy1RUmHDkpW9mfP5TMYg/u8Ty2vaeyN7knC0WylqmFhYWFhcWMsB9TCwsLCwuLGXEiN2+W\npjTp96ip4tM8uCG2d4QkcQD1iyJICp6rXF4gXNSaooYRYzPaxFFlWvYT7hHfU2QWxMAZj5oKL8vJ\nRmNFqqnATeorFZgJtGFHAV/rx7+QdElf/QqnnCorFRQqs9v2p+8z2WijL8+TePxOJukyP69KbHxa\nOESZ45GrXNYVpN6qqdRCHWziexU+b+GSuKfHbXaPbd8Tt/TOOrus79/id1n1RX3kWpHdjiuOlNUQ\nL2wSoqfKnbiecj3euvtr9eBwXyl3aULTLpM0UsmtE+MCljoNoc0rmr963ce/bcwrws01dkfd+Pz0\nqe8c/OerVHoJtixSTVYxx+Am7PaUrixIMImK+Uwn3O8K0AR1VZx2CJWwwwNJXm2yDdYb3K+KytXn\nw5WodlXIhxJUrO4Zw922P2TXWhQJmSJFfKwmwwRIuG60mbUObQkxiWOlgBQ/g5R3REQRxbSd7dLm\nY2m3yhFvEwRqvCYFk4oR+tDqGoZQpF2opr842CbSbl5zDU+N0QLmJRepz7QX2ZyfKret7xvdZO1a\n5zrxA+PulecnxMsHaqwVgjKuhTlObW8ZUbJRInVerc6uh+ykCfmDAd39XAhCxoWqiXedDveb/Sq3\nxX5X4tpNbHm1KuMkRKz63iZvF6wsC+ExQ4y2IXIREX16i93IO/u8XfHcmmyzGQWwrvD6qLPOY6xz\nKPHyzVVu46VVJuCpXQE612K38+qcPMcA20l70ID/65+/nx+7f+s+ERHVG6Lv6/knm8OtZWphYWFh\nYTEjTmSZRmFIG+sPKVKEiKVVVh9pL4pW5gBqGB2sRKpKecjB6jlUahi5sgdWmDWl5RuNebXkKFq1\ngywqRqVFEwNiWAta/9T/grYsEVEBx0sVfpeNQ1l5rePv0UiF0txgy2Ebe+ihJ5vdKajtpLKvaDWa\n08Mh13OndFXNalqvmszfI+jqOmrdXmvwc/rQtiQiOmiypXO4wcSV65sSBnBvl/++Esu7vEFcV+ex\nyiNVapwAAArjSURBVHdVMvHBmM+PRooc4RpCiBR9cSs/1iky8KdOfZyarDH4f21zmGstroiqTaM9\nR7MiTRMaj3pUUYQsF+tNrVBk/jQEkyhSia3Rhws6DAAqRBH6qyF8EBE1EGY0f0HeZXub+1rvkMlU\nA0+8EEb71FXumEJuVcp1DanFGBue8uxMEGplkjATEWUDhCOgwuOJDjMx11CKPs+IgOQFBWqsnqdU\neCL0+GMmy3lH8t7BCOEy5v6qQziQ15rmGBmPAr+3TrztwqwvFlQWmJj7sfGS6bCZNCcsyfXzUBt1\nnpkGAnjTXE/aI5wYspGU+fk8Ck+bGiUxjg1CaYfJmZMlq34SXMehcsklT1nNO7d4Hlh9XrKkLDzH\nnp4PfsMeg0ZdGqiN8XGo9I7nlxA2t8zfga7S5q0jM1KsDL1DZInxr7BFOmqLgtqjIluOUVF5r+Bp\nrJ6V564t8DzWqnHZhfZFuSdCdTYPhKh3cMihRe//7DMiIvrl33ySHyug8yy+IXrN3dHJVNWsZWph\nYWFhYTEjTqjNmxElCT24JZTiDqyh1oJYpssrvNfogz4ej8TqGw7YtNNhE/nKEvsPUSSraO8JtPwQ\nq2YHS0VPBZA3oIPqK9q/CRYOpsIrEJwNK/rehqxgPnvAAcs/+rHso97b4VVYAoGIVAlR5IIVeuma\nftEWOzkcInIdb2oVbkKE9B6QX0AOUuzJxcriCPF+XlVW4fN1Xg3WIErRWZFg8M37vEr9y/v387KP\nulwfL0GI4sWCBDt/bgK+pzaZYM1N72w94e0YYgSra1A2dZa2to2WabGo9/VmF8lI04RGg85Uf6o1\nuI7Gqk9mqPsUe8k6BCLGeU58POwigQU7UNq8JQgnFBfVuyDkJ4U14ykrMDaWssrnaXJlxrG27Rmm\n72tLNoGFrLOQmD9TI6Cir4XXc1Wfd5/ROjyNQhqur9PSc5J9pf2dd4iI6PHHN/Oy3gb3UQ/16iTq\n2WHZxVPWMsJTjBiG3r7Es0+UUIYRTsizQ+n8nThPJ21xzca26pcuQnPM3q32KAVegmuo7EpmDCcm\nlEbty2fBFx+b3KxEs8L3PFpotOlbf/C9vOy9v2It7Z/85Bd52ZUXONfr/k3eo7xx/1Z+rApRm8vL\nws2oYixOYEEOlcCH4bj0lJV95WUOU6nNsUXq+HLsMOE5KFAenMIiW5rForRZK+DfXptni3qxJN6d\nDQjZ7HWEK3DzExZ3+A9/xrlqJyrccs1kzEnlOcqVk9W3tUwtLCwsLCxmhP2YWlhYWFhYzIgTuXmT\nLKXBeEiZcl8UoW5RLsilqkhj1ALF+XBPFHn2dtns1umECggp8WDW+1M6vPy9Lym1G+OyjBFWoHVk\njctTK6SMhkxkGAzE9TCAPqmJgpiriUn/r//NvyMiooePhMrtVJh+bVSXNG1/EvO7FFVaqPnWyRLL\nfhk0saQIV25J6apGqAfDDxmrffM+XE86/VERbqiFKrus55ZlTVUv8bH78+J23HrMbfbjLSYVfDzc\nzo89RJqsYaDXZUbtRaXxOi53myOXYNXtDhUZz7jylQuvBEr+YCzbB88iVCNNUxoNunm/IiKqgmzl\nKW3SSLvz6YtJm0GcUvyzccJ1VChz26VKV3nQN2Q3IduEQ+5PSU6AUKo5CK3QCkTGvZ+qtIBmTBhy\nVKhT6uHhMq1Xmx+Ee12pCRn/pn7NNHkWBDuiJAypu7lB23tCZll74QUiIrr41ut52eN51hLr3mBX\nna+0mAMQ3kJHuaYRQpQ9YYpLzLtN7cpgOyRAZ1Ru8QzKYtrNbeouTOQ5ElzQuOLdUF0jNfdRKlJo\no7yqXalz3+ffJqrdkvhLBtFTwnM9ataa5PmSSH7tKkiKP5M23UUo3Zmzi3gOmVQOApDs+qKE5+9y\nH6zWoDvtyXx6ZMKz1MTkuOy23405xVtpRZH+4DJuNhfyshJIlo2K9PG1BU4K3w74t0cDmZf2xuwq\nfvRAQs7e/fe/ISJx4a+9KFuTAUIBI7VbFJxQf9paphYWFhYWFjPiRJZpHMW0t7NPTaWj2O+wtbI0\nJ2XzCDZvwYIspmKFpqDlHyY6ABk0aVgjC+pahvY/GopVGY1xPWRuCJVO7SF0IyeK2m+sVE3tzwkG\nID9N1Krv/gZbYn5Zh79M0+99klWcybBRVEsTLRpxWmRElFE6lW0hCI6HxgSwxnPikTIDPYQFaWsq\njEzmHr5GqSTXb2V8rSu+rNpWqrxCPFzgut3uyAow6vBv54YSLhMbspiywAxZzFhx2pYDj4dUxFWu\nR2sWh0anlogoKPPq1CvIVaJk9oB2fqqUIpWl52CXiWntsxLMHaLvGMKPFj8wT+RlqgzvPEH4kKtW\nvCaEaqySdw8HbK2OkThcJc+gpIQMO+oaXq77KhDikcnKLm2cGpEMZX2msEhT9OVUk97QjskU0erZ\nWKZu5lApcmgykPa79z6HLCxclXa49PwVIiI6RKjXg5tCThrusZVVzyR0xMV4dkAUqjdkLJtsLtq6\nz9Byvs+/0/NHkhgi2HFSoybZHUIXOsqzYOlQIpCMjlcBeSYERHlmHFjGXqIa4hnowCRpRp3uiI6U\nNnpW5nn6T/7Bf5mX7W3yHGiyJVWVdnQDXryNG5LJ6dFHnFXL1GOgwufmzvD8sflIiKvFJa6/519g\ngZxiW64/xPdiXunqLlX4Gu26WLDGefD4iK3Qje56fuz2Tdba/asffJCXHR1wH1u6wATKpfOSvenT\nX9zne54Ra3tu8WSkRmuZWlhYWFhYzAj7MbWwsLCwsJgRJ3LzOo5Dju/lCh9ERBFi5vpHsqGdQJki\ngc6l74hLqAQ3ik43ZEg1Ds5LUqXMgn+1Wku3y5vXHSjEdPtC3nB8di9UqkLQMW7eqbRRxt0IN0qo\nyALmvHiKaAJ1IRMnpuLwKqiOonLTTEbPJll1lmZTCjtheDxG0KjRmFRZgVJ2SU1MoS/vMoZ7r4s2\niLTQK3H9zameUS3y/atNPq89keu3++x20Sn1hsYNGkp9GD1YkyZLa90a15COnTS/NGQ0XxGcPLh8\nfeVK8oLZyRkOOeQ6fq6aQ0TU7fI2RqEqCi3lBscyT9D3HbUmNV3gSdqxZmdAq2O5Hr9z/0jIVIfQ\nuR7sszvd74qajAfiWVkR0AqGuKcVkEzaPLMvoRI5m4fTmtZGf9YBI89X6j1pAE1mRfRzvGfj5s0o\nozTLyHcVGQqEvr3b4srtd6H5+gLryK69JIpeuzehTbwj9UQ54Y7rN1Iat5E5pFWtIP1k+uJUyke4\na7U7nyITx6rqAXrfRaMfrOaPDH3KVXOnOR7jGZNY93/Uvzw2ufHsfl4/CGjp3DmiRLZxKGV3+pJy\na5bzuG1+lxoJoWhhnt2v5Yr0wUePOB718UMmLu0f3c+PdUf87gsXZN74zt96i4iIrl1+jYiIdhU5\naXfI/X+uIfrt5RJv/R0MhBS6scNktG7MrtnDXfkO/Pz7TDZ6fEu+S3WoLC2c5+f31Pwx7nNFf/b+\n9bzslbdEDelpYC1TCwsLCwuLGeFM0/p/x8mOs0tED37niRYaa1mWLf7u047D1vepYOv7Py1OXd9E\nts5PCdvH/9Piqer7RB9TCwsLCwsLi+Owbl4LCwsLC4sZYT+mFhYWFhYWM8J+TC0sLCwsLGaE/Zha\nWFhYWFjMCPsxtbCwsLCwmBH2Y2phYWFhYTEj7MfUwsLCwsJiRtiPqYWFhYWFxYywH1MLCwsLC4sZ\n8f8BIa1WsqsLg0QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8ffb82dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAABiCAYAAAALHznJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvVmPJUmWHnZ8vfu9se+ZGblvVZVV1bV0V9V0dXM4I444\nJMGBMBBEgCBIvlHQb+CTHgQ96UFPgkQIxEB8YA8lYWYIqnuammbvXUtXV1Vn5Z4ZGRl7xI273+ub\nHs5nfk5UZtdkxM1pQYJ9L3nT3MPd3NzM3M6x73zHybKMLCwsLCwsLE4O9//tClhYWFhYWPx/HfZj\namFhYWFhMSbsx9TCwsLCwmJM2I+phYWFhYXFmLAfUwsLCwsLizFhP6YWFhYWFhZjwn5MLSwsLCws\nxoT9mFpYWFhYWIwJ+zG1sLCwsLAYE/6xTi7VskJthhxK87IkiflH5uRl8hPqSpmcT0ZxSSkvpWmG\ns/lfx3GeOl0VUZqmRw56nqwJPM/jf315NBdlWSr3TJLkyLPl1ySiIAj4X3UNc90E1+gPR/mxKIr4\nPq7UI425XeL29m6WZbN0AhSKhaxSK1MUDfOyDPU8IlyF/zxTzEo3nCki5zee4zh8EVeVmfYNXA/H\n5DnNMx95Z+anOs+0rqnikWvgbx3Vfg6O59fHvbk+3pFj+vzbN2+euL1d38/8IDxS5jyj/ejL7fes\nI0dOyb70j7woo0Cm+2Zqxot51+qYiz4ZFIpyPvqa7sNP1VC/ny/VR//00KZpKuMjjkb05T8wfSjL\nshO3NxFRMQyySrlIw0jV3flSPdX9nuq7X6rXbzqm+89XvL5nTzjP8WdERIlq/y//gRPwe/ML4VMn\nRAMe3xneo66H7n8unqXV7p+4zcNimJVrRer3BnmZ6TeVRknKYi4bDXhuSxN5trDAz1KbrORlns/9\nxnO4nQO/IMc8Pj9L5flizL++i3ZRY3k47BMRUbvbVedzO8SYa4mIfA/zdMhtGoaB1BHjOI6lHyd0\n9P2YeYSIKMUxT81LoxH3+42HW8/V3sf6mBZqM3T9j/8luak8ZLd5wJVJpBKJqaOHykc9qfSIX2Kq\nJogeXqz5MIcFeRExOpivPmy9Ll8vi7nR61V5qY3JSS6bnMnL6o0GERENBvJRah22+BrotMOBdK75\nuXkiIpqbm8vLJnCNZpev8fmdR/mxze1tIiIqq4HSPtgjIqLd7/0PD+mEqNTK9Lf/4e/S5pM7eVk8\n4GdORtIxYkxEcWIWJQLX5Q6mJyHz8XLRkV1Pjvkh/3XZl45Wq1WJiGi+ym1QKcigq4T8u1CQjjw0\nP8NyXtZH3WJ01mIoH4MiOn6hJO/RfCyKZb53qSTXKqMe1WotLwsD7jO///V3TtzefhDS3NmL5KqJ\n1/zWH+7MfPzxfz3JuphMjszFjvnYcRukatI0k8NoJJNEv8/9O8G/cb+fH6tMc99cunA5L2vv7xMR\n0bAr48x9ar0kBTF6SHLkY8p9qF7mdu63mvmx3Y21/C/z66PvjIbDE7c3EVGlXKQ/eP8NevCkJXUN\nTZ2kTTziTuXjX0cv1DBJm4Ug/+bnSTG+C5WqHMvfr0y0Zh5IMy7TE23+IXee8UFWbdjNJ3+0byh1\n9OcmiIho9tLZvMyFY3Dz5n0iIhps78nzoo8Eqh4lj+v2F9/96MRtXq4V6f0/eoM+/eCWqjf3mzf/\n7g0p2+sQEdGTW5tERNQ+aOfHVi/yd+V3/ujNvGxylsdu3eexuTR3Pj82UeG5uD/Yz8v20b9my3xs\ntijj+/a9T4iI6D/+5IO8bLfL7313/XFeNj2xSEREy6fPEBHRyspCfuzU0mk+f+8gL+uQmeP5/dRq\nk/mxYcbvrqLmrCdrPMf/y3/+3z9Xex/rY+q6LhVKReq0OnlZD+OrGMoEm2Vc6GEwaCskxqD1VUfr\nD9EjU9Np1cTvPu2JnpzijlkJ+aWWSnLvMORJNQjkg+zgMZUBS3HGnXQ0RAM70hSDODtyDhGRV+ZO\n4mX8Ma1OTEsdDw6JiChKZfBn8VHL9yTIMqIkyqi5r9q7zZ066su9YnxYY7N2UVaF+emqRVn+EUA7\nZ+r9EBYtvi9l9TLPbgcN7nzzjfn82OQUVrV1ab9amdutQvLBDBweDIeYwIbqg2IsUnckix3P5/ON\nJa77RBjCOgvkA+58+etxAmRpStFwSL5aSES4/xFPB+rmYmXs6nvjt6M7m8ftZ1bfmbJgzFUDtWoP\n8SI9LJyitnxoilNTRERUnZCFRL/DfSJNpD0y0xnSZ1htqJr6NFCWceEQFkOlMZUfG3X5+q3Wbl6W\nZsqKGgcOUeonRKFawBTx7jNp8wzPFmXPsBjxccwc/XHk38a7VC7X5foYAblXjYgStL+HhWVBLYxj\nzAd6QWqsuUSNNY9GuDeuq1YrrU1uu14sZWcuXSIiostvvkVEROtf3M2P7d7hD6yj6uj6X7J8TwAv\n8KgxP0lX33wpL/v0Fx9zdQeyaNtd44/QsMfzzPJFme++/gevcv1PreRlS/VTREQ0Xefz9Gs6aPOz\nb/ek/+wd8sJhDx/VXx7IHPfFR5/zvbfl2VdX+Pr1ivSJqM31bX72gM/fkQVgv8cLhFZHjKTugO/h\nYm6bWRSjsDGHe3ny3dheX6fjwO6ZWlhYWFhYjAn7MbWwsLCwsBgTx3LzOq5DxWKR9sSaphiurkg5\njTK4JnwQKdK+mNOrK+ya3doXk98PDOOAr1EsiqldwV5HQe2jGtdv1Osf+TsiojTlY72euEFbHfbV\nJ8rlFcOtFWMPZqRJPi2ur1uQBx267EqOzN8pt3C1wW7nzXuyD7E0x+4O2QU5PhzHpcArUDISn8nB\nNrsqBl0hQBniQwJ3rd4LIxAHnOQ3kzQyTR4DWcBX+2PTBd5HKOxy2d1QvbsFfvaFZdmfX53l374j\n1/Bq7PINsX4bajJYhPMCvYnHvyOQABLlNo8i/q3ddJ4rLs6TwnUdKobBESKXcRfqfc68KXH/I9uj\nxmWt9rq8wGw9BKjr02QqTw1FcFVyd5TygFKhwu7doiIguXBLa49uscTHR/0BnkORmIx7X9XbkESM\nO12TaaZmmTvQH8qebDQy7rMxtzMyvkQSqf1OuFpj5RI1ZCxs1VKayfg220h6tyLFlosh93R64sIM\n/SLOl2dM8RymbYJAuRPz/ibnmz1mz316CnVc1DuRMdrAvBGtyx7eo8NfEhFR8zLvLy5fWc2P1SZ5\nzD28eTMvGyjyzUnhkEuuW6bGtNR7EWN3e01mq+1HW3zsFLv7v/2fyf7opSvniIjozOSpvGyiwls/\nBxFveT3cvZ0fa2EuHqgxvH/Ic7Iz4rYabB7mx6ZKPHfOnpKthjr2VK9eXc7LTB/vdrjeD3a28mMP\nP+e5+O7aWl7mYT80Dvkd937y4/zYf/WPf5efY0bc2Y26bB8+D6xlamFhYWFhMSaOZZnGcUJ7zUMa\nKivH8XmjPopl1VQEq86LePX67luv5Mf+1re+RkRE/+P/8q/zsgCWaJpgxaiWmMUirw60FRLDWhmA\n5ZjEasWIvw2KmrbNFkGibIgEFoETYPXvi2UTYYm/rUzw/R6vqqbneWXUH0l9hgiTcVQbVBQ77aRw\niMj3fKoUhWzSbvJK+8gz09HQometkbIj3IWvCiXg5yory+R3amx9vl3mleKvW7IC/PwWs+s2NoWp\nN1rlOrZX5T4NWAGGVVnU1nBkGNXiHTCRS6YZg0CsohCFxaJYZ2FwvFXks+GQ6wVHwgAM70lbPQQL\n04SpaOauseyMpUdElOJZh0N5PgPfP0rvJxIGtpcznqUdwzI/Z0kxnytgNQ+U9WX+oljm83KiHQnD\nlehpQot5FE2sCVCPiRkhnnUO2cJqj8bxvRCR45DnF8gL1TOCrFb0dbgVfntgyiorx7Bs/UBM+Bje\nnJy5rKY644nJjrxn/K1rQpWkii5CLhJlGZrjkfLgmOZ08vEnc0psPERqDsp6PG/s/4qtqGhPPD7L\nL4Oc9M47eVnz1qf49RGdFFlKFHVTClS1o30eWw+3tvOyArwd7/0ek41Wzizlx2aKPAc2fGFIbx/e\nIyKiL3Z4PmhGwv4dgmCaqXDCFOz1SY+vVZsTi7Ozz38bxDK+J0Lux72R9OMQlmYNnrC4L8dGW+zB\ne9CWso0hexyvfZut7Ls3ZX5/eIfPn18Uy/SwI2zi54G1TC0sLCwsLMaE/ZhaWFhYWFiMiWO5eckh\njqNTPq8kYXM9VDGWXsKm9dl5ju36p3/8B/mxUondHGeXRBDhwTZvPnd6fGyk3H1GdafXExLTAO4y\nc0yTBRyjtqFcb0btKFBuwVxxBe6jUllcFoasMVBCDs0DdmMa4tLKimy+z07w324XNQNi/DjTNEtp\nMOgecQ9Go6fdRcap56PMVWskQ6yIv8K166pjKX5XVXzvFbhTXnW5/a4W5d29iet/lEod7z7aISKi\njQMhFbQ22VU8ucTunKlZcRkWK+y61LGnRoDCxzaCVgEypB1NFDLB9uMgc1xK3HJOLiEiygzBRofi\nQmTCeAaPxMCCbFQui5vfxC6a07RSUZSLNiiCE1yCmREeUNcfDPm8YV/a24icVGrSh2O40VJsPQSK\nwGcIgrFq78yQBVOjaCXPO8B5hbJsN5htBiNOcmI4GaXOgCISF6AhXDk6Xtr099TH/8WFmmIOclQc\nZubi+eGGPToc4YrXClr46ebvSNrGqPS4rnS4YTzCdZWrGH/soo6OriM6UKb9x9je8DEXte6KIMEB\nxAaWX5YtsolTMuecFGmaUr83oEBtQ6U+12N+UYRu3vjGdSIiunKd3c1TKu64Uud54HF3Jy97dMAC\nBweYpzu7Ml/3u/wuSmr+DfrcNptNnlc3lCiE02GX65VXXs/LalXECfcUUa9oCEg8z+hxcm6FSVJd\nRWzb+OhDIiLaWmdS0o33386PffoFt/39//07eVmYHY/wZS1TCwsLCwuLMXG80BjHYRq+I6srF1Zo\nmAj5Yb7Kl339Mm9az5VlxTAB5ZalSVnl3rzHShNRxBZKUcnVxQibMP8SEfm5Riz/P1P0/FyL0VME\nBawszUY4EVGIcA+jnqTlCs29MqUC5Jd5ZT/s8SroYEdWnfNzvKK7cuVKXtaHUtGHdHIkcUytg33a\n2dx56pi2M03NC2iXUIUp5et9tSD+skale0TZhf8NKrKKrGCDP0V4UFKXdzeH1f27KszgfMZ94rYK\npXj8mFV8dkB26J+Slev8WZYDKypVH2MqGKKJJpwYT0OiCF/aQjgpgrBIC2cukq/IQ0Uo4WgpyiTm\n3xE0a6ORtvBAeEm0ipeRqAM5Tik3VSrcDz0VapUg1CvO+PrDobRVu8Xj7NEjofxXakZ6UamQwUIe\nwbsSq7YqY3z1FWHDWMjPUMnLVaDSTOpdLKl3NQbCoESnTr1EhSmpewmKUaGyjmOs+401PYpkvoli\nPIer3wM/j9EVTmM1vo20pepTJrzGkLO08FoK1bORUggKML+kyj2S5Rrj5rpPkwS1F8P8MteojOSm\n0QHfa+uTz6WOp8Zvc9d1qFQKaRBLf15c5fG9enE1L3v3G0zSmYHXLVRfise7rK631pR5yQyBvcfc\nV2PhI1IDITQNNf/WcfuHICzFD6U/z02z58vX3gS0W31yIi/a3HrCZfACTTZE5aqL0MGVabG2X7/A\nUo4f3GIreuU1kTycu8ges6179/Oy0rPUw74C1jK1sLCwsLAYE8eyTLMsozhJjmQx8bFqOzvfyMve\nvsaWBnVYJLnf3MiPnUaQ/7fe/lpe9uMPWcg9xv7YSGm0GoEGHQYxAsU6zcy/OuMGgv0TtYdrwmA8\nlbHByAHjWXQogxF2L9ZUeA1WuiMTfK0o8ZsbvEKanZI2qNXl90kxGo7o0e1H1NoTbdZ8yaxWuAWE\nUkxhb1OvkLomeF1ZJrlIAhoh1dqyWI1dvPFWXnTq9/8It8Z+mhZLwOp+Qu2/FUFfn0zE8lltsUX/\nGIHVe115pp0vHhARUWlO9mUmF1nEOkCf6HREu7NYYqu/pLJEOC9gz9TzfKpPTNPMjAhQTEELN1VZ\nhrIvWcZdtZ8/NHq6kewBDoctnMfPcEToHkHrjtKcCBAuU8I+Z6MhFsnURIr7yPscQHN0MBBPgNnP\nLdTQJ5SQfhXhZoEv5zebHCZgssXorEFmgR7H8kzuC9BC5oo65PoFqijRcR97jr4yDFxY7n7Az1+q\nPJ0FR2clMeF1RnQjyVTdYROmmc5chKxQeM0FJRxj9l2HKrzI6AHrvWVjmY5g5ftKn9kkBqCv0B2P\nOjImMozRvhbscPZpXPiBTzMLMxSkUo/5V1hj98xZEeFfgbctc/mZ72xKso1NhEVFQ7G8125zWM/g\nkCt84xXZjyyWuT/XtiX0ZxY8ifkSz7XLM8LDyALus/W6zAcmy0ynKd6BU4v8nelDK357X0L2BvBc\nTNfFWp2G4EmjAW6B+m6U4Jk8p7yLaw8lmcnzwFqmFhYWFhYWY8J+TC0sLCwsLMbEMRWQYtrb3SGK\nxT20iASx79wQ8/j8Ers4733GZvLOpqSyuXSRXQlvqvP/7t9ilY8//T4re8QqFMRQ4rUqkpcn/IWO\nqHKXmIgXT4cfwBXpKKKSTyCRmNzmvriAW00urFTEzRuav8VlXZXuyejfbqyJK3K3NL5LJh4ltPnk\n4AhByEdi3lTpu549yxvps0V2X3SUFvI+XItJU8JU2nCLJEZ3NtXELHaFvPbu7+dlO6excf+IiQd7\nh1rdBO9Kh66gbiWlWzo7zRv8q6eZan9KvYuNXXaT7+xu5mWHm8btyO+2nKp8tiG7w6plFe5RFXfO\nyZFRmqZ5omMicRO6Ki2bZ8Je4LLbaUnbGiJPVeXPrE6zqsopbFUUVX8dIL3a4b6EmBx2+f1EA4R0\nKXHe0DOJmSW0wXH52XuKILON6/XgmvSK0r9NesJSUYV6lEBUwgBKlK6sk0IfWW1tRNHxyBm/CWmW\n0iDqUV+p3VR8qN0o4lWM8Wy2b2K1jWNIX5pkZRScTOq8wVAT2DB/KE1vk8x6coLdmwFJe0VGmc2T\ndyrSWGpsot8EBaMVHKpj/FunoxRXNP6vRLwczC9OWbmKB4bw86/opHBcl4rlAi3VZCvj1UvsLp2c\nka2pwy67TG/vsjbwwz1RR9rbAkEzlT446nEbNTAHra8LoaiK0LBTM5JvNEYKyRWQGxtVadsWUjc6\nvvSJ9gHfv9+XOba0wFtBbfTPqrpG3Oc6bkVS7xLSt12/glzAKkdyjJyu5Um5xqkL4vZ+HljL1MLC\nwsLCYkwcyzL1KKNqOqKvv34pL3v1DK+6FxS3p4yVyNlzF4mIaG1LMiWYzAc1lSHkj/8O07CNNuQH\nHwsdfDjgVYfibBCBOJBi9ZwWFGUcZJyCsnxMSILJcENEVDBPnj2dJcOEjvR6sjrvIRzIGC0BqUTW\nqaG9y/VHbZVa54TwwoCml5epf6gC4xHqUJsX0YPli7zSyrp8LJJ9fuq3ecWVJGrdZKzaZ3B2KsjY\n8IOffZaX/Zvv/YSIiHZahnigrRIjFKEIUUjQXilKe0xUedk9O8EEtMUlqf/5ZV4ZX148l5d1QFDa\nO+Q+kURCzjDWxn6gNJwL42shpxlRP04pLMnq1IXmb3ZE2AL9D3253RJLvX3IVurhodQ3RIYXQ0i5\ndvlifuwiCA+pInXtwao0QiG9vly/B6+D1s41WgIlJcxweplJJUbL1BCMiIgOdvl3oMLBQljNbojk\n7T2pT67HciQMZHzCFxFRrVaj97/5barOiTiBj/HtK9JQbMRHYmM5q4wvqIsW8TCEFZPFZ6TIScYy\n1RFiDgg5rglLUo/XH5lrKQ8BtJRT1Q4m1MYklw8UQc7MM72ekJgGIJEZz1BBWbJGqtmpyjsaHIzv\nfUmimPY3D2j9lliOcYfH9ZWrEiqytseEozXocN++/SQ/1u/xmFg+J6Sh02d4PN+/xTrDI+Vl+uZ1\nvFvVfomPZOwgevnqfXp47VqPfZQd9TQQEW1sg+Da5TFXLihCosNjeOAp8mbCf3umwmNjzxVXwEEI\ngqkSn4mHx+vj1jK1sLCwsLAYE/ZjamFhYWFhMSaO5eYteBldmkjpH7z3cl6WtXhT3O2KGymL2Eex\nsclm+IN7kij2lZdfIiKia1fFVbw4xy6Cf/pf/n0iIqoVFNkErsuyUlwZwPUbghBx0Bd3wB0TGzQS\nMkYfyX0HytWTmvg+xK+Vdco0xJxFStVpANWaAEQOX6WH6mLzOtKKJ8H4BA3HcckJynTYkzjdJEby\nYk/a484ddsG0d9gteNgS4kZkXOHaT54r2eBfFWPbidnt8otPJK4sdkzKMb53JRRiltGgTZRCldEc\njVWc3X6Xj3cj7idbbWnbR+vch07NiLrJ8iITE8pTTDIYDsRd04XWpxvINfb2ZCvhpHBcl4JilYpV\nqUcJRCLtzjNP5UBn9SLc7EREHZCRRn2pm4lHNW4rTWZKsW+QKCLL/j6/x13E5BaUu9woMs3MCoHE\ngX+z2xV3sNGv9nDdypyQObpI39ZTdcy1rxHDXVFJkkctvn+zKW2cxC+GgOS6LpVKJarVxbXuwK1c\n9KTORtQrdcy4VRfBq3FUlUwaNB9bO092ZAz96jNOZebqOFZsU8yALHbunGw5+BjzrbaQX27BnWkU\n1IhEgSfL+D3v74tr1Lil52aFhFNb4dha47p0U6V5DndjpGSgOq5qjxPCdT2qVOq0vyPv8j989/tE\nRHRnXeIqizVu8L19HndNtXXUAOm0qjTRDzvcZyOoUF25LmNiF2OzpN7ZmQWkXIMrPFMxq3VsV/SU\n/rRpmn5X3oHRVnYxhoapSvGGMVEeimt8L+M6bm1wX5ibup4fm27wez9oiaqTo0hozwNrmVpYWFhY\nWIyJY1mmRS+lS40ezahN3X1YGo5Su/EKvKoxq/mNDaEn/+BHHP5y7sK1vOzJBofO/Nl3/h0REVUc\nWbkvLfDqbXlpJS+r19ly8GNeTT/ZFat4psTL2tamrLo//JjJNAcdpWIDlY3GBK9I3Fho4R4o2a5K\nPTMNy9UDsSFUhJsKVKC6yjiL0/HXKVGU0vZ2iwYjRZyCylFrb6TORDJ24meoKuUQDyQdryD1NdrH\nRWz+h8oq98vcHsVQVnRGYcmQr5yi0heFORCpxL+D/lHtWiKi1Og5OybDjTYj+H2vdWQl2n7CXo0G\n6OyTNVVHh99PnEj3bbbkfZ8UxUKJLl26Rq5SyjKElyNitXiGAMpac3OL+aH5WSZl6BV00RCDDHkt\nVFl9coFpKRtBaagJElO8L+1oQnV01qJzZ07zPXsSsmZ+Hxzw2Ng/ENPC8LZqVbGqeghtaLb4nq5i\n8yw0+F6Bsqg3N0VtZlw4jnMkC5KxOjOtbGZeA8KtlDAQ9XtskX/w0x9LWYefu4Lk6B988ml+7Ce/\n+DkRERWVZWX0lm+8xJ6z9957Lz/mY5w8ePhQrvETvtfSkiTNrsMy3ULbbG1JGxki1Lfe/3Zedv48\nE9GM1VoOxdtUrvFYDibEC6Tno5PC8zyqT9So0hDvy8aDB0QkbUtENBpyCx8cILTKkTFRAcFw+5Fk\nuXm8xm3z1vsc5rixJv2t/4DH8vlZSXReLvJzdTo8h0eKIFkBGW5XtV8Z/aOmtHl70MseQDGvUJY6\nNuHRHKrEL72E55DmkL1HRZUM/cw0k69GviKIZXqO/ethLVMLCwsLC4sxcSzLtFwI6LUL80RDWXWb\nXJipoiybdC71Gq+0JiZEuX9vn1eRf/7vv5+Xffc//DkREXUPeQXTaYvlW8U1TqmV+Dfe5hVOrcSr\np/1NWSFt3uewmu723bxseYLro8MbWn3opW4in15bVvVzC3yvutJ1vDTBK5aLC7AylH++2YLAgC8r\n/SFWTT+jk6Naq9O77//eEd1gBwHNngrz8QOU5UHOcszoEnvKGjJ5BX3sA4cqpMIN+Rk8lSGEYCm5\n2MyKVEae2OTGVNqvJlNJ1BNL0wgKDGEBuMpqNZT5RKWm8LAfHqV8rXZf7lnEswSqjqEnz3BSOEQU\nOA4lKkMMecjlqbShHRNOBas19cROirCfr8M0ytDWLWDvzXN1iAnybao97etXeS9nCUHp+/uyj2OC\n1icmxJNidHSrZbFiGvg9B8/LUFlQa5scFrG7K9ctwHIq4FqHSuTDZGqZn5f9vgYsmyf3v6Bx4DgO\nBX5IoQojyYj7yhFLzHhFTL5iJRJigvWLJekDH334ayIiunub9zbXNuRZjQU5URFvRwihiAFC2v7y\nu3+RH4tdM3887ZH59NcSxmey8EzgfVfKqn8i1mVjS/ZuXXh8Fpd4vqlMyzutVNkj5wTS79JQ9sRP\nijRLaRgPqTYr3qtVhGdtPJQ93soy78lH4D10VG7iM2d4v/PxYwl/Ob3InsNRm9vg/gcSWvfeSzeI\niGimJs+3/YTfx0SDy3xP+v82RH4mq/LspTJ/Q9Z2lBgOMlVNIuPQSPEaHJfHsErGRKMu9lErPCYe\n35c94k4PWr4XJWSvHtg9UwsLCwsLi98q7MfUwsLCwsJiTBwvNKZQoHMXzlFHEW0GKVxXOnssXCAD\nuPvailLeBsHhww9+mpedO8Puo0LK7pc//7M/y4+9BlWO7R1R7Lj/xQdERHT5Gitr3H0kxID1DXb5\nvvOSqMyEYFzcCIR+/6vP2D31xe17RERUV8mJowFTuc+cVTT2Pm+GTw+hr6rcEjPYtB6lEpLSd7Vk\n08lQLpfp1VffoKsviTrMHtKxjVRC6hHYGQkZ4pdKao5/j0QSgPCQGuLSkSUVHxup0AcTdhLHcL8p\n5ZgGyGA6I1fbEGGU+7M4we/WRWqkWCWmTkYIdfLkIiWQooohv7OCIr8YKrwmeSkv84kRxxHt72zR\nk3XRkjbJuy9fFi3pKaRoM2FBA6WFbHJ4RWpLYR3u1HIZ+qXKHVkpGLUo6X8NaIbOzEyhXqfzY50O\nyBPKNW8Ud7JIKbbADVrE9ku1IMSNAtzkK0pFa3eHCSN7ezuol7jYjHpSRbmR332Ht1q+/+//DxoL\nmUOUBUdSumU+u+j8TNywiXGNm7gZRVI0jKUbr0tax199zoSjvTYTHmfn5PkDuIqLxadJLz7aK1WK\naCYReU/uslf4AAAgAElEQVSR7PoIx/NU+M7qKofTvPQykysvXZCtqVkQ0+oNCTkqlNjFmZlRqu4p\nCkzK/Z3KmDkpsjSjQadPpHSJVy/zHLtxUz0zXkeAetRK8u6TiOtxdlVIoVNIwv3Fj39JRET/+deE\nwPXaSxwG+ei+bL0Vyty3ewHmzp64kV2Q7LQSWYT4pzSS7bghQrtcjIVMp6UEuUxvka3d47plqOug\nK+FBzT6TkaZPSz9ZqMn8/zywlqmFhYWFhcWYOJZl6voB1aYXaGddLM0YlH5X6S5mLq8ed0wmDLUx\nPD+FgN+SrIL2d3kj+OYttjALiswU4LITdVmlNpFB4DZWOo+eyEb41CyvOqaVAMD+Nm+sN1W2k8ka\nrygXp9li6vZl1bkDktTG2n15ziKv2Gt1vv5cXdYhJktEHMnK0aXxaewH+/v0b//kT+hf/DdCFrjx\nGq9644EOO0ECdQS+H1khGc1LZU0mCTLxmAUxSb2NzqwXiOXzZJu9D0PwzNOBvE9jreiMJRSYzX9Z\nVU9DBMBkmWkdykp0d5etokwl4CaXn28IYYRAkbtC0OpLKoFzplb1J4Xn+zQxOU2e0lhuIqTk5uef\n5GUTk/w+FheZ1FOpyQp6ahKB+IFcowVBBCOS0GypECAkXw61znBo/uUXVFIhHAEGRFF5gkwy8YGy\nnEJ4Bcyf6sTeDkIgTIgUEdHpFSaVzMCD0G7Jqr3b5TLfl/aeVGE1Y8FxKHNcSnToEaymzHna25UZ\nhYZMaWmDzKYTei8u8LtZQpL5Rk2FO+F8R2XvMeSiPtopDOVaS3MruKZYmguLq0REVCwKqWZ6mu81\nC4vfUZ6WFNZWrObCocnQA4tTe3ey1LSBynZD4yNNEuodtvI5gIioVudnXQSxiIhokPFY72xwP5g/\nLVZob8hzRH1KCJpNCK9cW+RMK9/65jfzY7cxdrafSKjL/BL32fUmfyOWFkSEpACvyK7KpFTE+5hU\nmaJGqAeFCLdT3imTuaqthElOzbF34AFCqaZmZNwGE9yfP//FL/My5+LxvIvWMrWwsLCwsBgT9mNq\nYWFhYWExJo7l5nUcl1y/TIf7m6oM2rYludQQ5IdHa0wGKqvExA0kux10HuRla3eZULS4xJq/tYa4\nTnb22NRXXmTa2WV3wSH0HJ1M7HuThmqkNpcPQV76+Kbc871v/QERES0jVdWPfiwRoRFco4NI3HGH\nJXYbPexwDNl+V2JhV+DuKPjiiu5lslF+UgwHPbp382P63/7X/ykve/uN/46IiBYvyOb4YxBm3Jjb\nqqZi02pQUPGU28pHLKkRJ01J2m/ngMvqkxIbfPUKP3sXcaOdrpy/j/iz2oS41a+9znGSjooTDFxD\njkLbDsS13OmyGzRT2slGK/agyXFlg760Z4Q0VolyC5cr47sdsyyjYRTR1IykllpZ4Wfv96Q/tQ+Z\nkLO3w66tqUlxwwdoW1/FqM2BRJVNgtylEm/3Rvyc/aE8X2eEGOgBl2VD6YdVxGPWlGpVt8PbLge7\nougyUeXjAZSjopFcw0dauZEiLGVG1Qn1XlkSVaeA2HWtvMjUj14A44s4HjfJXEqUApSJjT4SJAjW\nm5MLUun8adzPBio+uIVY9TweVQX+Onlib7l+DbHwZ89eICJRJyIimp/nPlBTcZKErYDBSLmb0ZyG\nEJgp37Uh+2lRYbM9Q8bdrMSFjdvZUSQ+7ZY+KdI0pV6/T5myo5pwpw5V+rHpKXZVz2F+rMxM5sei\nYYg6yjWmIq7nOzdeIyKiO0qPfR1KRhOTco0q3LXVCv+rtQUOD7lvDYfioh2GPKfpeOxSiesRgmR0\n2JQxFFR4TAbKU/v+a0xQK20+4GtWZYw+uMdbemtfiGbBfF3mwOeBtUwtLCwsLCzGxLEs0zRNqdvp\n0sGu6C7OTfEKoJjKCuqww6sMo6Jy47Iknd3fesA/Igl16bXZ0l18na3FaWUZdGEBbu+qzClYwaUx\nWyj1siJjYDP/wS1ZGb37JoeWXL4m2W4GHltSbWg4nr0gK9FHa7iXUhlq1CfRBmwF9AeyCtob8e/l\nGaUfG4+vFes6LpVKFXpwX4hQf/m9vyQior/zh387L/vhT/+KiIh21nlVtVCWjfUzp3hlWZoW62lp\nhUMtalCCKZTFkm2ATKPDVIylfu8LVjV58EhCR0bIEOP7cg0P1k2pKHT66EuxKzpxskne7qkYnRrq\nsTR/CcdU9gxk6WmrBOyDwfieANdxqVIq0nAgoS4FWPnzsxLSMAtLNIROcqkq7Z3CGklIlsQOCHUm\n05CvvATlKmj9JKv2NOYV8RDWeWtPxptpBa1F3D5kC/n+bVEjmp9iD8ppEDtqFXkXJqwmLCjSIKyk\nR7tspUQleT+vXuOQj1v3ZMx+9sV4ykcaaZZRohOPw6Ohs8AkMEkdWKSuIpyFIGPdvyN1egC92QBE\nOs+X55+C0tD5ixLudBUJrOfmF3G+JixxfUbKKkpAgFIRJuQGUMYy717LYBlLU1mXxnOTn6Ws1iwn\nXAkSGp9klxFRSg71OkIiPURy8MaS9PEIFvf0CjxgKlH3FIhWvQ3pl1+/xm0ZIJP8n37nT/NjS6fY\nsr9+9kxeNhhy3zYhUUajl4iogCTfSyvifcuQvN0Q8IiIWniGjcfskYkcGVeXzvO8d251NS/7+U//\nIxERlWd53hsqheeHXzD5NVPul4UVa5laWFhYWFj8VnFsy3Q0GlHrQPZMl8xXXlHvbxnNw4QpyI1I\nrMRGwCuQX6+LpXnuyjeIiKiLnHh/+Pf/Xn6sgiDnf/U//+u8bH6B7xn3+fyiCppvD5DtIJWV/nqf\nV+kLi0JtjztskW7d52fZ2ZU9sSFWoBNV2SOpVni1VAj4ugVHVro7m1/gmrLnMFEcf50SFEu0dPl6\nHm5BRPTR55xndODL9UPkuNxp8krNGYgVOLfIdHdH7UltIURoA/9m6piLYPiG2reegVW7MMvXSgey\nSj1ssuegpcKO2ttsKT3qyns5gH5xhL22UIWOmPo7yjJt1Pn+k9iLNRrNREQFiCWcWZX3mSTj7+Gl\naUK9XouSSCyAAcK81luyH+kjr+T8HK+4Oy159hDB/44KO4EEKzlYCZtVNpEYI46ySgJYUYnH9ZiZ\nlZAFE4yeqX1XI8IwUPv4VVhJJiytrLSZiz5ft6+sjQzaxuuP2fq8c+tmfuyTD3kMdtT+YGs4vigJ\n7kxZmuShI0RESYwcr7G0SeIZkQT+v6P2bB9CH/iHPxS97yE8FRMz/I4unJf8mi+/wlqxp05JuEeI\ncIwEYz/PFkREKfKTup7YiSarTqzaME+MhNCjfE+Ucg2NIwFzjrE7cTDV1jnsHEeFAKXp+H3ccV0K\nSiXqbausP6ZNB1LfnsNjt3PAnoqFZdlDd/ESVpQAhdH7/umHHxIRUV8JTJSRn7rfl/7Z7/H7bkBj\nemlJBET6yMv7EN4FIqKBCV1UHqrNPT5vBZ62CxfkHWfw6txdE29KCyE02STPH2FXxtA0xGdur/8q\nL7t/V0QmngfWMrWwsLCwsBgT9mNqYWFhYWExJo7l5iUiSrKMNrbFzXvhElPJXRX+svGYCTOzIVwt\nmRBFKgkUcHZF3YKq7L4oeuymfHDn1/kh47FcXhZS0voG/22pxtUfqLCCz+6wWb95IC6R4tQq33tC\nVDYePmHtzqkZ3mROFU0+ddil5ynlGaPw4yFpthsK6SQpsytpqyvEnDhTabxOiCAMaXblDJWUS3Rr\nj0ldBz/8IC976+tvEhHR9VffIiKiqloirV7jMJXpFUnBNYCL7LPPOD3Vf/rhL+QYEnsH6tkvnWNV\nk+tX2Y1y9syF/Fj9KrszU+XuaoF4tn0grvMN6NN2QTQweq9EREOEIIVK1cekhYsQErPdEhdReYJd\nvovz4mZqNOR9nBRxHNHezsYRl3i3CbdfpPR3oQlsXKGpct5NQPfTEFmIiKpQCyqVoP+qdG9NCI3W\nFTUhFXHM76lRVwQnvLtUubX9EshG5y7lZQ0kgzfbDW6iQmMQpqT5MSPcK+pxO89OirrNBtKX7Sn3\nvl+T4+PBIdd1KVVhUbmalUqzliB8q91k9+RncCcSEX34i58QEdGwJ267mRneArh09VUiIrp4+Wp+\nbAIJpjOl7xubMBbMA65KM+jm2w9Snyji+mYq/CtvT7hmtapTCvKQ8xUyRs4zwndcxcLynKf+5Njw\nXJeqlTL5zyAABio5uI+bdds871VUOkoj1RSrMfmjTzgV3RDbD29fl75YRyo1L5A+W0b/6SHc7kCR\nLI0SWqcjrmIPKl+eUlWbajBBqV7m/t/aku9SijCfeweynZjN8/wfI5QyKMrznr3IJFntth/2jkf4\nspaphYWFhYXFmDiWZZplGUVxTIcdoeU3sXKZnBUacQ2b11OTvKqaDGQ5NoKp6aViyZ5dYsp0OmRr\n5eEdSSw7B9LQm197KS8rfsqroC9+zRasDhOo5SsosRJdrAoLyvKpIhvI5r0HRES0tSmrGrPqbLeF\nWFKE/q7JKjHoyyqrN+CVWsEX4YKt7Ts0LsrlEr3x2kuUKbKFQ7zyq4YShlOHWMSZZSaqzE6rbAtY\n8Y9UlpZyhVeKEYQTPvxQdGf7WB2rJC308ae8KV//Hv/dhdNCcf/GW68TEdE3330zLzt1gYkdy6kQ\nPF5Clhuz8muqZMMRrKyS0t10YDW00b92FUGshPrPLwhpoVw+XiLfZyHLMkqiITVUcH6K9B3Dvlh2\nBYRb9GL2pGgd49GQV+t7u9LnoyH3yQQJtUfKYu9CYzlUQhsZyDjDET+7Jue4CK/R2rRlZJkJlHjJ\nqWkmMU2u8GpchxaFWN0XCtKHHjxgy2B1gclmly69kx/7k3/7f/L1VbqgYl0JGLwAJLGeI7hfPror\n5JEnO0wGOdh9QERE22uSyHpygi2Ui29cz8tWL3CoxuSM9BGDGJZvqjIveSAXeUokIQesQ00Sy2Dd\nu0dmUD7uYfDoQxnGVXYkXOYo3CMEJFxRvbfhCzB90iSlQbNLo67058EQIYaT4rkLME6nlhCeUlCW\nLPqeX5d+H4DU04DnLurJnBxBC7mrCGx7EFgwuiFppAQu8A7qVSEdViDaoC3qPubgUYe/G7sdmVPK\ny1yPqcvi0byzw3XyI673VFX6/xOEM/UVsS50bHJwCwsLCwuL3yrsx9TCwsLCwmJMHI+A5DhErnMk\nfuoJiETzS2JOf/0V3swdIN5Uu59aQ7brR7GY67NzTHChHrtmH959kB+bnuTr9lVKqN0tVvrZ2+c4\n00QpX6RYH2zvSFxgDIKLTlsVgmBj0nhp944P4oNxBRMRVRBnaly/KYlbrttiN0ngyHNG++Mr8kw2\n6vQP/97vURqJ66ECPUrt5jXP0B9x3bpdIXc9uMfJz9tI+kxEdBpKJBNVfqaXr4o7doSgSJ18uoLr\nl0GgmVBkhBHe7V5XXOIudJojFavnuiZZL1+jrLyEvmvi0MT11Gzy+15aZjfTjNpGMPF+lZLEcnov\nYFmYJDEdNPeoqpJgF0zi4bY8y9Q0u2tN/O/WtrRt4HL/Pr0s7sV6A9q2IFb09qUvB0YdSRG+OnDB\ndaH0ZJJTM0yKMBk/eYxtJAScDOmpVuaQLm5atiBqVX6+w4P9vOzefd6WuPESlGyU0gxB+3RpVtyA\nQ+/Y3MXfgIyyLKVY6SynGH81FVs86xxV4jl3+kZ+7CUomwWhjMkhHKUDnO8qMpOXB1Yqty3mDZOx\nULtjE8Tq6jjQNL+eij2FupGbGnKScunCVex6OqbbPVLma4IR/tRV/aKo3vmJkaaU9Ya08VjiTM10\nflbdK8SYn4SClsrYRoeY8w+UUpg3j5Rqu+xqXUhV4vWE56wHWzJOImzpLF/kram6K2O5C7LcQCUC\nT7EtFEZy3k6bx9EBGJfDaXn/E6vQP1DEtv46UvX14b4tyHgpglT14Oa9vKy3LeP0eWAtUwsLCwsL\nizFxvOVlllGWxFRWG7elMq8UIpVhZbLCK79BwivqSG2ibzSh+bgs4RVLK2zJBl0kt04VGQPZCNYf\nykrqySP+PRrxirHVlzCL/RavZjodsZSePGGL97233sjLAqwGF6C5+kgpZWRYzZbVythkKOgNYP0p\nLck9rNQKrrRBw3kB6xTHITdwKYqeThDcGsn9I1gr9TIys4zk3lfOsxUaKO1cHxbmy9dYj/jCxdX8\nmElK7CoNUZN82gexQofN5Ol81Cp/CGLH3XUhiQygzRvCCk2UVm8R1mpzT6j2j9cfEBHR9QFT7M+d\nP50fM2Qj1xFr0fnNvI7nRpYmFA07tLMt9Tb38lTy7kqF+4XRGy6q9jjc4b65rsg6w0V+BwdN1jJN\n1LHGJFtcvidjqliq5PUhIqokYiUmEUhPjgolibndhsqDsQ5S3s4Bex0mVXhNE1k87t+9lZe1+nzd\nM+dXiYjow48/lToizGcQSyMX0hfQ4MTZUeLUJzcTsscg4bpMzImedHWeLY1JqEH5npzfNX1KZSIi\nkLaMypCjwpeMZeqpMRp+1UxoSDK6CK8kU0QwY5GGmFYddX0zB6phQr4hB+LfoTropDzmAuUBGL6A\nNnc9jyqTNapUxfuSYL5rqHCoaYSdeb5pW+lbn/2YCYsbu0LyvPJtDpuLKtwew0OZs1YTWNRKM3zh\nHI/nBu5zqObwtsPvcajCuUJYjmlXWatL/BLWDrn/u+odX6vxd6m7J96XWgVqahUec722ss4xvmM1\nLz26L2E1zwNrmVpYWFhYWIwJ+zG1sLCwsLAYE8eMM00oGrSooNSOTLqcaCTmd6/Lm8UmufEwFPO7\nDRHrf/Jf/7O8bGGZXXlNiGDcvSNqGGmK9Er3xKXQ67IrwfPZlN9UbjkX7sxCKHW8d+cOriturQSx\nm0agua7IRsZ9Zwg3REKIGSIu0Ai8ExG1DpAkuiyujZXzhjBzPFeBxn7zgP7Nd75DkYoRNXGuSuee\nzq+yK+8Pf//bRETUaIi7xoFrqBjKxn0Kl0mCa1QceXZ4FsnTLqc8sTdcZ9qF7Zi0U4IAx4uuECb2\nQCBbQ7L3fl9INcnQxMIKIcCoWhXu87XOnBWxd1M35xkprsaB5zhU9x0qKzdsBvdnP5b+/djhmMcC\nUqllmTzLjInxTeSdjXZ5C6EAd5oTKOLLPvfdgS/9qYB0f47P/fBAKc0YZ2OkHjeCO8ykEyQiGoHM\n9bMP2V175wuVbKLGY2N/R2KrL0II3qTeerQm460IMkqsCGKu+4x4zBMgSzMajWLKlBuxh/6eRvI8\nCRLZm1jxQBF/BofsyttXydHNloRRU4pG0rcCKIr56hkCs4VhXMaKPDdCv4+Vo9ekW0sVcaoIV2EG\n8k1QEJbdHMhbTqK2JjBOfEKiB/VOTW21ilKSHE+R51mIKaXtpEdBWeYDB20TqlhS035mJ+XxrQf5\nsfVHPKf1FOlw9ITHwAzimt2GPEwr4+vu35eUbR9//3tERDR7ESpGy5LMI4ZIfqWmiKVLSA4eySdr\nqsRu2+hjzC0fSQq+986yG7mk5r0yFJjyOO5ErjUwrnw1p1SQWrHfez41O2uZWlhYWFhYjInjWaZp\nSsmwQ8vzEgbz0Qc/JyKi33lLFIpKoPJ74Honip68egoKGQdifRKSLbe7bOENYlnx9Ed8/mCkyAVY\nUa4/4RCZYlEsq04Pah5KO7SFNEK3bn6el4VFXrGYVXBukpEQP5oqdMAcNavDfk9o09mQz7t4WVKC\nrcyNrxXrui6VykUqF5XSCMhAtZpYzSb5riHJFF05ZixTUhqiRsnFz61KWY019/kdVFTCa2PlhyBD\n9Id6pQa1F+UJSGAlnjtzVuq4zG3ThIXaPBS1kjZ+a9UUwyiaRkhHQa2anfxf56mycVAqhnTj4tlc\nfYlIrPFBXyxTxzdkEq5joNKtTSJ1XUV7b1A5Y/34KuzIeFJ6A7Gc/AZfL0MC+/sFebqDQ+6vmSPt\nUTLJqB1Vxyr/TX/ElvWjplihX/8ah5KQUh0qFJiQ8sknrCo2VKvxAMSjqysS7tPrSXLpcZBlRHGU\n0aCtFHMwxnxlCaZoc+NV8VxpXxfPnylLIwaBboRrhb7MEX0QteJM5qUCvGcmuXUxUNeH98pXCclN\n4natHzxC6r4RPBvlilbTMQpI8i67hrsHMmRRuXfcnDglyF5AcvA0y2gQRxS5KqQH84ZOgZjhuYwm\n+d3PxOoL0Y9PL4u3KN3k9ijg25A15Fr7MdJdkpCMPrvFnpJJJLZ/e+Lt/Fgd6l3bD8WSrcNKnZkX\nUtoQs/Kpy2zdhlUZtwOMWy+V74CL0EU35e+LJnd5AfcPV4VIus7x+LnWMrWwsLCwsBgTx0sOnsTU\nO9yjh/dk7/Hm50yTPj2rwlk8FmsoYt8o8GVF5fm8gvn8h/8uL7v/K86A8lef8N5mdUKygThlrNqq\ncv14l1fxI+x7xsqSKFd4VROqTUUHAex6ledhhbuzw6ufvT0ROkiwKtPU+Tb2+JrYK+21xLI6f5rr\ne+mM7JH4iRw/KWamp+if/eN/RIEKU8n3eVT6CRMiYqj2OqGwPIJqD/xr1qYFJTB6AEEBnby7gGDx\nEfZW9g+UVYLVXVnpaHZh1TieWFtGY3QiT/otbVUITLC7WnmjkhJVpTaPnhUh8ALCBmrVKn3znW/k\nWqJERG0k/t7dkVXy5VXejynDWu52ZE9zDonUqxXpr54PoRJYRL7ax3F9toAMv4CIaOAaU5ZXywO1\nZ95p83nNlqzyzV5bpixN05JleGA2tyT06wc/+AEREV27cj4vK8L6ml1gy2J2St5Pt8Vj46Wrkny5\nc8j95L+l8ZCmKXW7Pco8ldXG7ClnKouJEUQY8biKD0UAoG5Ct5S1FSHLjFNGPx5J2wQI7euqBOsu\nGrGcmAwqUse+j8Tkqt4hkqn3VAheBRyLAazcQlFZeki6nqisJ7SA7DUmzEd1f5NFSOsBu874+9Rp\nnFB/v00jlXXIRUL07kCeJcC91iCgk8TyfhbmmQ/SUWPi/m2eu7N5bpfGGfFsDWHtz14V0Y/Zh/x7\n/SZzBm7+VEKxrrx9jYiImjtSn84azzl16fbkLPO8m05w/ecvyHdjAGGXGV/GWgyLNER9AtXe07Pc\n7ydnpI6DXc1V+OthLVMLCwsLC4sxYT+mFhYWFhYWY+JYbt5Bv0tffPIzivvyDe7A3fLzH/1FXjbz\nKruI6iGb2jqRsUkzpDMdtbfZdl9BNMnOobik1h/y+fVZUcCJQJl34WYrFsWUj4bsukkUueCVG6zj\nubgiGrTra0xeMkSK6WnZ2DbuuMOOuN72oSgTwSUyWRV3zeoCP2+QCIkiVUoaJ0WWxBS39qiotGID\nuAWTVKVlw+swSX6PsnGQZPhZ6ya4g7V6kG/CPXT4C9zAJqF3U7kdDaU8OxRFJuPmHahQipFJrg5y\nxoIiEly7wkpMoVI0So2rGom6XfVML4Js9CyEYUinVs/QcCjvcWuLwy0OmuJyWgbxolExxCzVVnBV\nF4sqzKDCNW4ewj3uCgHJ+LFLk0KQ2T3gexmi0ukl0SU2Kexu3REN0QPooXYUiakHV7WPIItRT21L\nXOZQtG/+zrvyTHDdTU0gHEBpYY+6XO/lBalH0het33GQZhkNR0NyVVJm3zSPClEyxKM6UnVVfiFp\nA8v7HKoUzglBalTkMZlBQ9vV2tFLPJ4KXdmuMEOghHSKYSwTVAEuY1eRyspo64Jytxfm+Z7FPgh+\nF87lx/buMIEnLIn7s9bg9neg/pTpkCyMV0+NzcAZv+enaUq9TpdCRRh8/PABERE9uCsuzkrKL8Eo\nlU3VZbzubnHf2DmU+XFzg9th7SaHzVSXZAshhV615vOcf4Pn4uYO/92tT4WQarKg3Xj99bzs7h2+\n7l/9XIhQX6tx2scutuhKMzJPnr7AoTbzU/JMt9d5zk9M+k+1rVSH2t3srLiKH+8fj2RnLVMLCwsL\nC4sxcbzQmGRI0eF9KmWyAgxGECyoyKqgDOsmD65XSxI3FwAQ6ybwePWzUOBjhVTILPsdBGSrVcTh\nHp+/u8urzbIis5jEwpWS0g8GOeZABbW3oR/rGTKIoknXkRz6UIUHlKBlOdjkVdBcXVauSwhl8GKx\nzmJZaJ8YnutSo1TMM2UQSaJzR2fB8I2e7tPXyLDmTtLkqWPm/ejsFguLSyhTJ+L9FRCC1NsRK2f7\nwGTRkVVzhFCKXls28LOY31kNhJCqCg/xcTNXZRfKBRkyY22rVbkxU1Udn2l5HxMpEQ0ylyJ1rxSe\ngKEif3RAdDGhCjNKJCOCBZ6FUp8qAuTTIf9dHCsyGKzaal0IS+3HTOzowAJoKF3dV6+ztfO777+V\nl+3v8gr6u//3f8rLpubYijy/ylZ0pSD1v3COQ5Z81YDDHr+rYoHfdarGc6HO5AynJmWZ0hIeB1mW\nUZREFJDq42gTrQ89Ql07Jgn1ylJ+rLXFIW9Dk8iaKLf+syHmoFmxrLrEFlV1Q2U9QX9Mq/yuRpNy\nfgaio68y72QDnhv8jlgv8Ty/J2eL+4ATiMds4iyHhnlKnzZFeyah0ZqW/l+AZewrImXsjm+Zuq5L\nlWKZQmUFF9C3O0+EUOTi/btlbgfl6KMOyJglRVIrgny1eZf1bkuT0p9nQQzylAenMsFteekdDqlc\n+/VjqU/I9/ZJ5ojWNl93f1eIosOUvQOGG7V7TzxKc9e53nqe7KLeKeYZRyliLEzwGD6vwhujhPvJ\nlsp281WwlqmFhYWFhcWYsB9TCwsLCwuLMXEsN6/vZjRXSqjgy6b7f/E+JxOerIofoJiCrAPXoqtS\nsBkFoVj5Qd3A6Gjyt/2sUs4pN9mGX9sVF0TS4funiHfs98XtmOAak8urednhAZvr3b5K+Ouz67c4\nya6E3We4DwJNbNp4SEREcxWobsyLa88IFCXDIxImY8NzXaqWS5Q8r884M+5HXWYOaU3QL+npZlLv\nElxOqXKre3BnFvCeFqfFvWOUfvpDOb8LFaq+Yk9MT3J7z08zMWCqJu4u3+ibaj1g4zZ+lmfrGSGl\nycs0kjEAAAuzSURBVAtQhyHHpcQv5DF+REQp4qJLypVLUDxy4NpKAnFHjbB9oMNeK4gX9Qpw8yp3\ndu5Oz1Qy9grSjU2yq9b1PXU++p9Sn+kMmLA3VGkQZ+FGe/0lVjsqhnKNBtyLBwfS513cM4Ab0k2k\nA/tQD/JCcfNGyTNewkmREXmu3gp6evAYF2EKvefhhQvqKFyuM2fykjTjOSHbYuLKSCmiBQFU2BRJ\nMAIj0sWYz+ZlqylIuT+01ZioTLPrctRWcYyY5/xLPD7irryP6uorRETkKGUx04QVE9dOT7t0HZUx\n/EVkdfRcl2qVMjmKFDr1+teIiCgipe0MvecDpDwrKFW14oRxASslL2wjmNSU9375ID9mYqMrajuu\ngvFfAiHrwssSw2yaef1QUqTVZvm8VNWx1+Z33MY9i7644XsgDIaevLMetvn6eHZfuc39Jtd/6Yxs\nYdagxPTJD0U576tgLVMLCwsLC4sxcSzL1M1SKqQtmi8r3U4sl0rKujFWkCHq+77SbUU2Br3KymB+\nlLHZ/e0//Af5sQ9/9GO+1kBWKVMX+LyXsan/8LFki7i/zqvtuCWbxlGPCRRRIiuREerUR/hLpgg6\nG49ZlaOsVvMrWFTNlKEUU1W0fWgJR7FsgMcvYOGeZRklaXokk4HzPPT4v+beWfYVJxgykL4NlooF\nWEgrcyrDQwZd5Y70iSYo8zdv3czLKiVeyReweg+e8RhfVS1toTpfeeLJkWZEw8yjTCnvRLBKgoKs\nzFOT/Bkkl1Qlto5T0w7KG4OsGU6e0FqFgcDS8tVQrMKKWljgpNw9lWHHZEFJlOU4jYwkN155JS8r\ngbiCqIRcsYdIlHS0vm0BJKkAGr3BQGm34lkCX+qYFsRKHQeOQ+R5Xk46IpKE3roThiDB+TGXRaFY\nVgESsvf3hAxUqfA1ovsccuHckiw4JhlUN1AeH7TrZARvwy0VuwfN3ZryfgygyOYNFJEHE15/gd9p\nuimWf/zqt7js+tW8zIUHrx1wXcuqDxiSjBdo1aPxvS+e51Gt0aBRU3TH507zPPqruxJu1JjlCa+7\ny/05jqW9q/B6DNWc70HZKcq4EaKhnL/3iD2Jh9tiVU5M8bNXkcS7PC2egLBswnLk/UwglG5uUYhh\nppvsbjJRb+WUWKZDhE82WzIvdfdBiMQ8FtbEq9CEktXummhY15TH6XlgLVMLCwsLC4sxcTxZfMqI\nKKMslj1KY4X6nuxJmO0zv2BWvrLC9LA38axUlCbrws+++3/lh1pN3isNldZtBValC+3XiVVZYVw6\nwwIAv3oolmkfiVIjT1bTQwRzd5ELcdSWIPVqkeuYHEig9wpWmzM1rmzRV6IMWKF5gTTnMHoBsTGO\nQ47jkuO8gP3AZ+ArLdRn/wUREcWRfna0hxIzncGeyvyMrCLroNoPEVKQqHyKZsX9PBYzER0Nk3mB\nyIhoFGeUqlyWxjIlZdklGDaZx30o0sMIigNeKKINccq/80wvOuOJsfZUaFa1zivsAHq52UAsU88E\n2yvXTqPMe5pTdZWTF1lukP6TCsoV4OQha3INF1az8SIFoVjivV4Pt5TzdZjD+MgoU+FzqbGc1V6x\nB0vbhxhFMRMrNHV5zyzbFauivMBcDudVFmwZqpy9VdTd6UtojAkhc+BFCKdlrnBNGJ16ZA8eKj9S\nIWpIMptWIOgwL3OWjzyfPfUevNj8hkBDqqxzE7amyuJ0/H6fZkTDOKG20nHuwxPXVlq0dYzhYpHf\nS6ulc9m6qI88e4B+GRG3aX1GrMSwxMeKrtLv3uN5t4MsVTdmxKtSQyalraZY9iZcrTSpQovAJcnQ\n7j0lsjNM+XdnW97xrz/ijEinTq/iPtInEggM9foyt936uXjWngfWMrWwsLCwsBgT9mNqYWFhYWEx\nJo6ngOS4FHsFokw0QD1DVc/EJTOMjdsTZBZ6Wn0nVZvpJmpjCNfUvduS4s132b1QUmncnIzPc+Hq\nCVVaIweaklfPiJtq45A3wHsjcdtSh+tUAkOjNi/1D6FE0lgSKnepwC6FBCpHrqKWJ3BBqixPNBxf\nmpcoyyjL0hO4Y/+GgGr4mg30ZV1gIsrg8r16SfQ5fZNsGWniPCUInL6A53sRbZRlDo1Sj1xHJzoH\nsUK52yJC+BBUgEbK/TZEgmo31W5eLktAVEpVqjC/aHRZZSgWsM1gzkqUl98kgNevoIrYrLe/9lpe\ntrvD7rMwV2JSYWF5vnitv+ziLOepY55J++fKGHHcF9MnHUrIpzZ5pNKhJezKK6jtDQ8pB9sxn9fv\nKULMFdZopYEQXDYaSM+FZO01pUg1gpyPqwhOI2h6d+G6LCodXtNVn7XTcCRFGtonxhbPaHJF/QH/\nE3bE9euCCeVhqyRzVciUixArHVbojb/dk8QRHe5sU6Uuc1tnh7e42iq1YnOD3bBlaNVGSgs8n/5V\nn/URUjQcMRk0U/WeXEAYYiZuXhMeNhgipWWi9L4PeVvjYE+23sy2SW1C5nUXYzNFhx6p1IlG8a0/\nlOtuPkaYFATFI+XmL9ZRR5WyrVwyREvRDf4qWMvUwsLCwsJiTByTgOSQ4/qUpWrV5oFcoYJ6Xaxq\nE4QJOGoFkyYmNEZWuaMRn5eZJNeOovFjxaw1Kh1jCWD1kWay8ewg4W/JkVXKmWnobUZqJVpHuELq\n4V9ZeRkyTaqzwBhBBKxcHWVZJRAdGCnN1Sx7ASSZv2EC0nOF2XypPkT0zETc+lJZLvLg60I+z5A/\njtD8zfV0+/1my+fY9X5OZORQnAXkqnc3Srh/RKnyvOD3IDFEDLmGC1JDovr8ADqrrgeNXhUa44Bs\n4ej+jdsnqMeR5s6JOoo8BO9QrdpQZd6R01IV1mIs3ViZtz7uFZlj+plAjoq1g8k75tTxG5CRQ6nj\nk6ctYdTFy2ROGbS5zfYecxhMlohVcWCcACoVVQjhkBDTwKGKchiZrFPq/D5IVsbDEgTiWRhB3EGL\nSZgQJX0Nk4nFCNIonRRyHKO1K+dPTrLlU0XGEleTsJCK3NFTdDb+PJAlCcWHLRoVxAJrt9hj1+3L\nnLn5mMk/i/B6ZKoT5mIXviJfgSxn+k1XCVaERjtcCfwWKxAHgZCJq8QpigiNWSwt5mWJyYKkLN4R\nRHgKyN4UKyGRAVLPDFSnLUIXPCzzNQ7bEh50sM+/qwtyz9rc8TIjWcvUwsLCwsJiTNiPqYWFhYWF\nxZg4lq/GoYxCyshRbtjQuLN0Il+4sWLY/FpqMyc/6Pgsk3LJM8fUteDK1W4nDxvPDohHqXLRuiBG\nhNo3Bp3JQGmiGm+LyayVHvHJwH2nVD+SxLhd4KbMFOnJlMkVcg3isZBllCbxER3jF+riPO61si/9\nqw8pt6yJ2TvCUTFqKUZ484jr2tRD/cFXEYqeVe8XQEBK0pQ6vQFRIq6hDtyFmoA0GPG9uohJC7Sr\nzxCEVJixcQn62BpIjpD1+Jjuf8aVZZpqqMZDWARZRa2DR2C+RWqgpfm4NPdRikYg86TadQd3mGti\niFUTR6ibJnPobZpx4FBGrjMgPxG97wCxil4i2zcp2qBSgotWPyv6ja/KjAJU0cO71GZDCWNXdZma\n0eFGWaoU0dKCcdE+nUryWV3RQTyx4z49jylFbEoTdlkPoS0e6qTxRlVrJPf0huLaPjGSlOJOnwaO\nkHs6Jkm62t7Y3+qgiEk7/pQkzXaMKpMac4EhU2HLodeSd7e9zW7kktrKSCNsD2ILpLMr59fO8rtw\na3p7kM874n43469gSILStnuHTGzKlF56uc7XnVviZykphaP1NX7Ojkr2Xq8IYep5YC1TCwsLCwuL\nMeEcJ6TAcZwdInr4N1ed/1/iTJZlsyf5Q9veJ4Jt798uTtzeRLbNTwjbx3+7eK72PtbH1MLCwsLC\nwuJpWDevhYWFhYXFmLAfUwsLCwsLizFhP6YWFhYWFhZjwn5MLSwsLCwsxoT9mFpYWFhYWIwJ+zG1\nsLCwsLAYE/ZjamFhYWFhMSbsx9TCwsLCwmJM2I+phYWFhYXFmPh/ALzF4+Pg3OVIAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8f0e55c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAABiCAYAAAALHznJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8f0ddbe50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_with_flip_accuracy(data_iterator, net, ctx=mx.cpu(), flip='w'):\n",
    "    def accuracy(output, label):\n",
    "        return nd.mean(output.argmax(axis=1)==label.reshape((output.shape[0],))).asscalar()\n",
    "\n",
    "    def _get_batch(batch, ctx):\n",
    "        \"\"\"return data and label on ctx\"\"\"\n",
    "        if isinstance(batch, mx.io.DataBatch):\n",
    "            data = batch.data[0]\n",
    "            label = batch.label[0]\n",
    "        else:\n",
    "            data, label = batch\n",
    "        return data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "\n",
    "    def evaluate_accuracy(data_iterator, net, ctx=mx.cpu(), flip=None):\n",
    "        acc = 0.\n",
    "        if isinstance(data_iterator, mx.io.MXDataIter):\n",
    "            data_iterator.reset()\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            data, label = _get_batch(batch, ctx)\n",
    "            output = net(data)\n",
    "            if flip is not None:\n",
    "                if flip == 'w' or flip == 'wh' or flip == 'wh2':\n",
    "                    output += net(_flip_image(data))\n",
    "                if flip == 'h' or flip == 'wh' or flip == 'wh2':\n",
    "                    output += net(_flip_image(data, width=False, height=True))\n",
    "                if flip == 'wh2':\n",
    "                    output += net(_flip_image(data, width=True, height=True))\n",
    "            acc += accuracy(output, label)\n",
    "        return acc / (i+1)\n",
    "    return evaluate_accuracy(data_iterator, net, ctx, flip)\n",
    "\n",
    "from cifar10_utils import show_images\n",
    "%matplotlib inline\n",
    "def show_data(data):\n",
    "    mean=np.array([0.4914, 0.4822, 0.4465])\n",
    "    std=np.array([0.2023, 0.1994, 0.2010])\n",
    "    images = data.transpose((0, 2, 3, 1)).asnumpy()\n",
    "    images = images * std + mean\n",
    "    images = images.transpose((0, 3, 1, 2)) * 255\n",
    "    show_images(images)\n",
    "    \n",
    "def _flip_image(batch_image, width=True, height=False):\n",
    "    h, w = batch_image.shape[-2], batch_image.shape[-1]\n",
    "    h_index = range(h-1, -1, -1) if height else range(0, h)\n",
    "    w_index = range(w-1, -1, -1) if width else range(0, w)\n",
    "    batch_image = batch_image[:, :, h_index, :]\n",
    "    batch_image = batch_image[:, :, :, w_index]\n",
    "    return batch_image\n",
    "\n",
    "train_data, valid_data = data_loader(32, transform_train_DA1, num_workers=4)\n",
    "for data, label in valid_data:\n",
    "    break\n",
    "\n",
    "data = data[:5, :, :, :]\n",
    "show_data(data)\n",
    "show_data(_flip_image(data))\n",
    "show_data(_flip_image(data, width=False, height=True))\n",
    "show_data(_flip_image(data, width=True, height=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-15T13:07:18.394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 0.952376198083 w flip: 0.958765974441\n",
      "h flip: 0.90035942492\n",
      "w+h flip:"
     ]
    }
   ],
   "source": [
    "net = MyResNet18_333_c64(10)\n",
    "net.load_params(\"../../models/resnet18_333_c64_e200\", ctx=ctx)\n",
    "print \"origin:\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "print \"w flip:\", evaluate_with_flip_accuracy(valid_data, net, ctx, flip='w')\n",
    "print \"h flip:\", evaluate_with_flip_accuracy(valid_data, net, ctx, flip='h')\n",
    "print \"w+h flip:\", evaluate_with_flip_accuracy(valid_data, net, ctx, flip='wh')\n",
    "print \"w+h+wh flip:\", evaluate_with_flip_accuracy(valid_data, net, ctx, flip='wh2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T12:44:31.816010Z",
     "start_time": "2018-03-15T12:44:11.818966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904952076677\n",
      "0.913837859425\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = data_loader(32, transform_train_DA1, _transform_test_size(36), num_workers=4)\n",
    "print utils.evaluate_accuracy(valid_data, net, ctx), evaluate_with_flip_accuracy(valid_data, net, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T12:45:45.577719Z",
     "start_time": "2018-03-15T12:45:24.781078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.891873003195 0.898562300319\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = data_loader(32, transform_train_DA1, _transform_test_size(40), num_workers=4)\n",
    "print utils.evaluate_accuracy(valid_data, net, ctx), evaluate_with_flip_accuracy(valid_data, net, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T12:46:03.467751Z",
     "start_time": "2018-03-15T12:45:50.604380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934404952077 0.940595047923\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = data_loader(32, transform_train_DA1, _transform_test_size(28), num_workers=4)\n",
    "print utils.evaluate_accuracy(valid_data, net, ctx), evaluate_with_flip_accuracy(valid_data, net, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T02:58:08.366348Z",
     "start_time": "2018-03-15T14:30:39.001969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.79793, train_acc 0.3261, valid_acc 0.4332, Time 00:03:16,lr 0.1\n",
      "epoch 1, loss 1.16314, train_acc 0.5829, valid_acc 0.6067, Time 00:03:50,lr 0.1\n",
      "epoch 2, loss 0.89921, train_acc 0.6849, valid_acc 0.7061, Time 00:03:49,lr 0.1\n",
      "epoch 3, loss 0.75125, train_acc 0.7418, valid_acc 0.6547, Time 00:03:47,lr 0.1\n",
      "epoch 4, loss 0.66331, train_acc 0.7726, valid_acc 0.7063, Time 00:03:46,lr 0.1\n",
      "epoch 5, loss 0.59731, train_acc 0.7950, valid_acc 0.7233, Time 00:03:46,lr 0.1\n",
      "epoch 6, loss 0.55572, train_acc 0.8072, valid_acc 0.7170, Time 00:03:45,lr 0.1\n",
      "epoch 7, loss 0.51778, train_acc 0.8214, valid_acc 0.7160, Time 00:03:46,lr 0.1\n",
      "epoch 8, loss 0.49655, train_acc 0.8285, valid_acc 0.8172, Time 00:03:45,lr 0.1\n",
      "epoch 9, loss 0.47225, train_acc 0.8358, valid_acc 0.7923, Time 00:03:45,lr 0.1\n",
      "epoch 10, loss 0.45314, train_acc 0.8441, valid_acc 0.7800, Time 00:03:46,lr 0.1\n",
      "epoch 11, loss 0.43043, train_acc 0.8509, valid_acc 0.7297, Time 00:03:54,lr 0.1\n",
      "epoch 12, loss 0.41771, train_acc 0.8570, valid_acc 0.7928, Time 00:03:45,lr 0.1\n",
      "epoch 13, loss 0.40686, train_acc 0.8584, valid_acc 0.8233, Time 00:03:47,lr 0.1\n",
      "epoch 14, loss 0.39543, train_acc 0.8636, valid_acc 0.8312, Time 00:03:46,lr 0.1\n",
      "epoch 15, loss 0.38814, train_acc 0.8669, valid_acc 0.8330, Time 00:03:46,lr 0.1\n",
      "epoch 16, loss 0.37064, train_acc 0.8712, valid_acc 0.8422, Time 00:03:45,lr 0.1\n",
      "epoch 17, loss 0.36491, train_acc 0.8747, valid_acc 0.7964, Time 00:03:46,lr 0.1\n",
      "epoch 18, loss 0.35779, train_acc 0.8784, valid_acc 0.8132, Time 00:03:46,lr 0.1\n",
      "epoch 19, loss 0.36175, train_acc 0.8765, valid_acc 0.8394, Time 00:03:46,lr 0.1\n",
      "epoch 20, loss 0.34830, train_acc 0.8814, valid_acc 0.8464, Time 00:03:47,lr 0.1\n",
      "epoch 21, loss 0.33743, train_acc 0.8842, valid_acc 0.8579, Time 00:03:47,lr 0.1\n",
      "epoch 22, loss 0.33722, train_acc 0.8846, valid_acc 0.8441, Time 00:03:46,lr 0.1\n",
      "epoch 23, loss 0.32869, train_acc 0.8870, valid_acc 0.8436, Time 00:03:45,lr 0.1\n",
      "epoch 24, loss 0.32346, train_acc 0.8878, valid_acc 0.8337, Time 00:03:46,lr 0.1\n",
      "epoch 25, loss 0.32232, train_acc 0.8896, valid_acc 0.8683, Time 00:03:44,lr 0.1\n",
      "epoch 26, loss 0.31810, train_acc 0.8899, valid_acc 0.8573, Time 00:03:45,lr 0.1\n",
      "epoch 27, loss 0.31091, train_acc 0.8917, valid_acc 0.8360, Time 00:03:46,lr 0.1\n",
      "epoch 28, loss 0.30956, train_acc 0.8937, valid_acc 0.8420, Time 00:03:45,lr 0.1\n",
      "epoch 29, loss 0.30554, train_acc 0.8960, valid_acc 0.8246, Time 00:03:45,lr 0.1\n",
      "epoch 30, loss 0.29982, train_acc 0.8974, valid_acc 0.8544, Time 00:03:46,lr 0.1\n",
      "epoch 31, loss 0.29795, train_acc 0.8981, valid_acc 0.8201, Time 00:03:45,lr 0.1\n",
      "epoch 32, loss 0.29734, train_acc 0.8972, valid_acc 0.8408, Time 00:03:44,lr 0.1\n",
      "epoch 33, loss 0.29307, train_acc 0.8995, valid_acc 0.7939, Time 00:03:45,lr 0.1\n",
      "epoch 34, loss 0.29182, train_acc 0.9003, valid_acc 0.8459, Time 00:03:46,lr 0.1\n",
      "epoch 35, loss 0.29116, train_acc 0.8999, valid_acc 0.7906, Time 00:03:46,lr 0.1\n",
      "epoch 36, loss 0.28893, train_acc 0.8997, valid_acc 0.8536, Time 00:03:45,lr 0.1\n",
      "epoch 37, loss 0.29032, train_acc 0.9003, valid_acc 0.8668, Time 00:03:44,lr 0.1\n",
      "epoch 38, loss 0.28643, train_acc 0.9009, valid_acc 0.8235, Time 00:03:45,lr 0.1\n",
      "epoch 39, loss 0.28098, train_acc 0.9041, valid_acc 0.8325, Time 00:03:46,lr 0.1\n",
      "epoch 40, loss 0.28551, train_acc 0.9013, valid_acc 0.8631, Time 00:03:45,lr 0.1\n",
      "epoch 41, loss 0.28200, train_acc 0.9042, valid_acc 0.8841, Time 00:03:44,lr 0.1\n",
      "epoch 42, loss 0.27143, train_acc 0.9068, valid_acc 0.8265, Time 00:03:46,lr 0.1\n",
      "epoch 43, loss 0.27948, train_acc 0.9045, valid_acc 0.8680, Time 00:03:45,lr 0.1\n",
      "epoch 44, loss 0.27788, train_acc 0.9041, valid_acc 0.8139, Time 00:03:44,lr 0.1\n",
      "epoch 45, loss 0.27704, train_acc 0.9057, valid_acc 0.8326, Time 00:03:46,lr 0.1\n",
      "epoch 46, loss 0.27404, train_acc 0.9065, valid_acc 0.8393, Time 00:03:45,lr 0.1\n",
      "epoch 47, loss 0.27368, train_acc 0.9061, valid_acc 0.8482, Time 00:03:45,lr 0.1\n",
      "epoch 48, loss 0.26943, train_acc 0.9076, valid_acc 0.8259, Time 00:03:44,lr 0.1\n",
      "epoch 49, loss 0.26970, train_acc 0.9083, valid_acc 0.8545, Time 00:03:44,lr 0.1\n",
      "epoch 50, loss 0.27066, train_acc 0.9056, valid_acc 0.7858, Time 00:03:44,lr 0.1\n",
      "epoch 51, loss 0.26115, train_acc 0.9113, valid_acc 0.8313, Time 00:03:43,lr 0.1\n",
      "epoch 52, loss 0.26522, train_acc 0.9077, valid_acc 0.8621, Time 00:03:43,lr 0.1\n",
      "epoch 53, loss 0.26276, train_acc 0.9094, valid_acc 0.8545, Time 00:03:43,lr 0.1\n",
      "epoch 54, loss 0.26136, train_acc 0.9108, valid_acc 0.8749, Time 00:03:43,lr 0.1\n",
      "epoch 55, loss 0.26616, train_acc 0.9083, valid_acc 0.8612, Time 00:03:43,lr 0.1\n",
      "epoch 56, loss 0.26314, train_acc 0.9094, valid_acc 0.7991, Time 00:03:43,lr 0.1\n",
      "epoch 57, loss 0.25969, train_acc 0.9095, valid_acc 0.8572, Time 00:03:43,lr 0.1\n",
      "epoch 58, loss 0.26141, train_acc 0.9100, valid_acc 0.8311, Time 00:03:43,lr 0.1\n",
      "epoch 59, loss 0.26014, train_acc 0.9110, valid_acc 0.8565, Time 00:03:43,lr 0.1\n",
      "epoch 60, loss 0.25856, train_acc 0.9114, valid_acc 0.8911, Time 00:03:44,lr 0.1\n",
      "epoch 61, loss 0.26046, train_acc 0.9098, valid_acc 0.8541, Time 00:03:43,lr 0.1\n",
      "epoch 62, loss 0.25440, train_acc 0.9118, valid_acc 0.8518, Time 00:03:43,lr 0.1\n",
      "epoch 63, loss 0.25845, train_acc 0.9094, valid_acc 0.8299, Time 00:03:43,lr 0.1\n",
      "epoch 64, loss 0.25876, train_acc 0.9112, valid_acc 0.8659, Time 00:03:43,lr 0.1\n",
      "epoch 65, loss 0.25596, train_acc 0.9128, valid_acc 0.8659, Time 00:03:43,lr 0.1\n",
      "epoch 66, loss 0.25375, train_acc 0.9123, valid_acc 0.8553, Time 00:03:43,lr 0.1\n",
      "epoch 67, loss 0.25618, train_acc 0.9121, valid_acc 0.8166, Time 00:03:43,lr 0.1\n",
      "epoch 68, loss 0.25335, train_acc 0.9120, valid_acc 0.8749, Time 00:03:43,lr 0.1\n",
      "epoch 69, loss 0.24684, train_acc 0.9152, valid_acc 0.8071, Time 00:03:43,lr 0.1\n",
      "epoch 70, loss 0.25760, train_acc 0.9111, valid_acc 0.8600, Time 00:03:43,lr 0.1\n",
      "epoch 71, loss 0.25411, train_acc 0.9124, valid_acc 0.8342, Time 00:03:43,lr 0.1\n",
      "epoch 72, loss 0.25362, train_acc 0.9133, valid_acc 0.8682, Time 00:03:43,lr 0.1\n",
      "epoch 73, loss 0.24795, train_acc 0.9138, valid_acc 0.8402, Time 00:03:43,lr 0.1\n",
      "epoch 74, loss 0.25301, train_acc 0.9119, valid_acc 0.8221, Time 00:03:43,lr 0.1\n",
      "epoch 75, loss 0.25601, train_acc 0.9126, valid_acc 0.8803, Time 00:03:43,lr 0.1\n",
      "epoch 76, loss 0.25268, train_acc 0.9134, valid_acc 0.8752, Time 00:03:43,lr 0.1\n",
      "epoch 77, loss 0.25291, train_acc 0.9117, valid_acc 0.8233, Time 00:03:43,lr 0.1\n",
      "epoch 78, loss 0.25095, train_acc 0.9140, valid_acc 0.8735, Time 00:03:43,lr 0.1\n",
      "epoch 79, loss 0.24892, train_acc 0.9147, valid_acc 0.8753, Time 00:03:43,lr 0.1\n",
      "epoch 80, loss 0.10754, train_acc 0.9652, valid_acc 0.9246, Time 00:03:42,lr 0.01\n",
      "epoch 81, loss 0.07022, train_acc 0.9776, valid_acc 0.9296, Time 00:03:42,lr 0.01\n",
      "epoch 82, loss 0.05423, train_acc 0.9834, valid_acc 0.9321, Time 00:03:43,lr 0.01\n",
      "epoch 83, loss 0.04680, train_acc 0.9856, valid_acc 0.9333, Time 00:03:43,lr 0.01\n",
      "epoch 84, loss 0.03843, train_acc 0.9887, valid_acc 0.9382, Time 00:03:43,lr 0.01\n",
      "epoch 85, loss 0.03217, train_acc 0.9908, valid_acc 0.9344, Time 00:03:43,lr 0.01\n",
      "epoch 86, loss 0.02857, train_acc 0.9920, valid_acc 0.9331, Time 00:03:43,lr 0.01\n",
      "epoch 87, loss 0.02451, train_acc 0.9933, valid_acc 0.9393, Time 00:03:43,lr 0.01\n",
      "epoch 88, loss 0.02059, train_acc 0.9947, valid_acc 0.9361, Time 00:03:43,lr 0.01\n",
      "epoch 89, loss 0.01861, train_acc 0.9960, valid_acc 0.9329, Time 00:03:43,lr 0.01\n",
      "epoch 90, loss 0.01690, train_acc 0.9963, valid_acc 0.9376, Time 00:03:43,lr 0.01\n",
      "epoch 91, loss 0.01445, train_acc 0.9968, valid_acc 0.9353, Time 00:03:43,lr 0.01\n",
      "epoch 92, loss 0.01252, train_acc 0.9978, valid_acc 0.9364, Time 00:03:43,lr 0.01\n",
      "epoch 93, loss 0.01189, train_acc 0.9978, valid_acc 0.9352, Time 00:03:42,lr 0.01\n",
      "epoch 94, loss 0.01172, train_acc 0.9977, valid_acc 0.9377, Time 00:03:42,lr 0.01\n",
      "epoch 95, loss 0.00976, train_acc 0.9985, valid_acc 0.9361, Time 00:03:43,lr 0.01\n",
      "epoch 96, loss 0.00957, train_acc 0.9984, valid_acc 0.9369, Time 00:03:42,lr 0.01\n",
      "epoch 97, loss 0.00921, train_acc 0.9983, valid_acc 0.9331, Time 00:03:43,lr 0.01\n",
      "epoch 98, loss 0.00854, train_acc 0.9986, valid_acc 0.9362, Time 00:03:43,lr 0.01\n",
      "epoch 99, loss 0.00845, train_acc 0.9985, valid_acc 0.9398, Time 00:03:42,lr 0.01\n",
      "epoch 100, loss 0.00805, train_acc 0.9987, valid_acc 0.9365, Time 00:03:42,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.00702, train_acc 0.9989, valid_acc 0.9401, Time 00:03:42,lr 0.01\n",
      "epoch 102, loss 0.00673, train_acc 0.9991, valid_acc 0.9378, Time 00:03:43,lr 0.01\n",
      "epoch 103, loss 0.00666, train_acc 0.9991, valid_acc 0.9386, Time 00:03:43,lr 0.01\n",
      "epoch 104, loss 0.00660, train_acc 0.9988, valid_acc 0.9398, Time 00:03:42,lr 0.01\n",
      "epoch 105, loss 0.00597, train_acc 0.9993, valid_acc 0.9377, Time 00:03:42,lr 0.01\n",
      "epoch 106, loss 0.00600, train_acc 0.9994, valid_acc 0.9362, Time 00:03:42,lr 0.01\n",
      "epoch 107, loss 0.00550, train_acc 0.9994, valid_acc 0.9352, Time 00:03:42,lr 0.01\n",
      "epoch 108, loss 0.00604, train_acc 0.9991, valid_acc 0.9354, Time 00:03:43,lr 0.01\n",
      "epoch 109, loss 0.00560, train_acc 0.9993, valid_acc 0.9324, Time 00:03:42,lr 0.01\n",
      "epoch 110, loss 0.00642, train_acc 0.9991, valid_acc 0.9415, Time 00:03:43,lr 0.01\n",
      "epoch 111, loss 0.00645, train_acc 0.9990, valid_acc 0.9343, Time 00:03:43,lr 0.01\n",
      "epoch 112, loss 0.00589, train_acc 0.9992, valid_acc 0.9380, Time 00:03:43,lr 0.01\n",
      "epoch 113, loss 0.00681, train_acc 0.9990, valid_acc 0.9355, Time 00:03:42,lr 0.01\n",
      "epoch 114, loss 0.00578, train_acc 0.9993, valid_acc 0.9378, Time 00:03:42,lr 0.01\n",
      "epoch 115, loss 0.00563, train_acc 0.9993, valid_acc 0.9323, Time 00:03:42,lr 0.01\n",
      "epoch 116, loss 0.00673, train_acc 0.9987, valid_acc 0.9314, Time 00:03:42,lr 0.01\n",
      "epoch 117, loss 0.00643, train_acc 0.9990, valid_acc 0.9376, Time 00:03:43,lr 0.01\n",
      "epoch 118, loss 0.00553, train_acc 0.9994, valid_acc 0.9392, Time 00:03:42,lr 0.01\n",
      "epoch 119, loss 0.00650, train_acc 0.9991, valid_acc 0.9380, Time 00:03:42,lr 0.01\n",
      "epoch 120, loss 0.00720, train_acc 0.9988, valid_acc 0.9347, Time 00:03:42,lr 0.01\n",
      "epoch 121, loss 0.00864, train_acc 0.9982, valid_acc 0.9377, Time 00:03:42,lr 0.01\n",
      "epoch 122, loss 0.01024, train_acc 0.9980, valid_acc 0.9353, Time 00:03:42,lr 0.01\n",
      "epoch 123, loss 0.01007, train_acc 0.9978, valid_acc 0.9362, Time 00:03:42,lr 0.01\n",
      "epoch 124, loss 0.01228, train_acc 0.9972, valid_acc 0.9314, Time 00:03:42,lr 0.01\n",
      "epoch 125, loss 0.01492, train_acc 0.9962, valid_acc 0.9232, Time 00:03:42,lr 0.01\n",
      "epoch 126, loss 0.01566, train_acc 0.9963, valid_acc 0.9312, Time 00:03:42,lr 0.01\n",
      "epoch 127, loss 0.02000, train_acc 0.9946, valid_acc 0.9328, Time 00:03:42,lr 0.01\n",
      "epoch 128, loss 0.02041, train_acc 0.9944, valid_acc 0.9162, Time 00:03:42,lr 0.01\n",
      "epoch 129, loss 0.02409, train_acc 0.9928, valid_acc 0.9254, Time 00:03:42,lr 0.01\n",
      "epoch 130, loss 0.02352, train_acc 0.9929, valid_acc 0.9302, Time 00:03:42,lr 0.01\n",
      "epoch 131, loss 0.02219, train_acc 0.9932, valid_acc 0.9332, Time 00:03:42,lr 0.01\n",
      "epoch 132, loss 0.02360, train_acc 0.9926, valid_acc 0.9205, Time 00:03:42,lr 0.01\n",
      "epoch 133, loss 0.02395, train_acc 0.9932, valid_acc 0.9183, Time 00:03:42,lr 0.01\n",
      "epoch 134, loss 0.02875, train_acc 0.9910, valid_acc 0.9231, Time 00:03:42,lr 0.01\n",
      "epoch 135, loss 0.02616, train_acc 0.9924, valid_acc 0.9213, Time 00:03:42,lr 0.01\n",
      "epoch 136, loss 0.02738, train_acc 0.9915, valid_acc 0.9316, Time 00:03:42,lr 0.01\n",
      "epoch 137, loss 0.02398, train_acc 0.9929, valid_acc 0.9293, Time 00:03:42,lr 0.01\n",
      "epoch 138, loss 0.02454, train_acc 0.9926, valid_acc 0.9171, Time 00:03:42,lr 0.01\n",
      "epoch 139, loss 0.02841, train_acc 0.9912, valid_acc 0.9172, Time 00:03:43,lr 0.01\n",
      "epoch 140, loss 0.02544, train_acc 0.9924, valid_acc 0.9246, Time 00:03:43,lr 0.01\n",
      "epoch 141, loss 0.03024, train_acc 0.9903, valid_acc 0.9031, Time 00:03:42,lr 0.01\n",
      "epoch 142, loss 0.02395, train_acc 0.9931, valid_acc 0.9181, Time 00:03:43,lr 0.01\n",
      "epoch 143, loss 0.02530, train_acc 0.9923, valid_acc 0.9287, Time 00:03:43,lr 0.01\n",
      "epoch 144, loss 0.03311, train_acc 0.9893, valid_acc 0.9292, Time 00:03:43,lr 0.01\n",
      "epoch 145, loss 0.02480, train_acc 0.9923, valid_acc 0.9096, Time 00:03:42,lr 0.01\n",
      "epoch 146, loss 0.02798, train_acc 0.9914, valid_acc 0.9189, Time 00:03:43,lr 0.01\n",
      "epoch 147, loss 0.02787, train_acc 0.9910, valid_acc 0.9235, Time 00:03:43,lr 0.01\n",
      "epoch 148, loss 0.02575, train_acc 0.9920, valid_acc 0.9185, Time 00:03:43,lr 0.01\n",
      "epoch 149, loss 0.02680, train_acc 0.9915, valid_acc 0.9244, Time 00:03:43,lr 0.01\n",
      "epoch 150, loss 0.01234, train_acc 0.9971, valid_acc 0.9317, Time 00:03:43,lr 0.001\n",
      "epoch 151, loss 0.00654, train_acc 0.9988, valid_acc 0.9393, Time 00:03:42,lr 0.001\n",
      "epoch 152, loss 0.00531, train_acc 0.9993, valid_acc 0.9347, Time 00:03:43,lr 0.001\n",
      "epoch 153, loss 0.00449, train_acc 0.9995, valid_acc 0.9383, Time 00:03:43,lr 0.001\n",
      "epoch 154, loss 0.00393, train_acc 0.9995, valid_acc 0.9398, Time 00:03:43,lr 0.001\n",
      "epoch 155, loss 0.00360, train_acc 0.9996, valid_acc 0.9384, Time 00:03:43,lr 0.001\n",
      "epoch 156, loss 0.00335, train_acc 0.9998, valid_acc 0.9384, Time 00:03:43,lr 0.001\n",
      "epoch 157, loss 0.00289, train_acc 0.9998, valid_acc 0.9385, Time 00:03:43,lr 0.001\n",
      "epoch 158, loss 0.00275, train_acc 0.9998, valid_acc 0.9405, Time 00:03:42,lr 0.001\n",
      "epoch 159, loss 0.00279, train_acc 0.9998, valid_acc 0.9357, Time 00:03:43,lr 0.001\n",
      "epoch 160, loss 0.00264, train_acc 0.9998, valid_acc 0.9397, Time 00:03:42,lr 0.001\n",
      "epoch 161, loss 0.00245, train_acc 0.9998, valid_acc 0.9405, Time 00:03:43,lr 0.001\n",
      "epoch 162, loss 0.00222, train_acc 0.9999, valid_acc 0.9378, Time 00:03:43,lr 0.001\n",
      "epoch 163, loss 0.00231, train_acc 0.9999, valid_acc 0.9394, Time 00:03:43,lr 0.001\n",
      "epoch 164, loss 0.00233, train_acc 0.9998, valid_acc 0.9401, Time 00:03:43,lr 0.001\n",
      "epoch 165, loss 0.00228, train_acc 0.9998, valid_acc 0.9370, Time 00:03:43,lr 0.001\n",
      "epoch 166, loss 0.00204, train_acc 0.9999, valid_acc 0.9415, Time 00:03:43,lr 0.001\n",
      "epoch 167, loss 0.00191, train_acc 0.9999, valid_acc 0.9395, Time 00:03:44,lr 0.001\n",
      "epoch 168, loss 0.00213, train_acc 0.9998, valid_acc 0.9421, Time 00:03:43,lr 0.001\n",
      "epoch 169, loss 0.00205, train_acc 0.9999, valid_acc 0.9381, Time 00:03:45,lr 0.001\n",
      "epoch 170, loss 0.00215, train_acc 0.9999, valid_acc 0.9405, Time 00:03:44,lr 0.001\n",
      "epoch 171, loss 0.00193, train_acc 0.9999, valid_acc 0.9408, Time 00:03:44,lr 0.001\n",
      "epoch 172, loss 0.00177, train_acc 0.9999, valid_acc 0.9363, Time 00:03:44,lr 0.001\n",
      "epoch 173, loss 0.00183, train_acc 0.9998, valid_acc 0.9394, Time 00:03:44,lr 0.001\n",
      "epoch 174, loss 0.00170, train_acc 0.9999, valid_acc 0.9424, Time 00:03:44,lr 0.001\n",
      "epoch 175, loss 0.00169, train_acc 0.9999, valid_acc 0.9384, Time 00:03:44,lr 0.001\n",
      "epoch 176, loss 0.00177, train_acc 0.9999, valid_acc 0.9396, Time 00:03:44,lr 0.001\n",
      "epoch 177, loss 0.00177, train_acc 0.9999, valid_acc 0.9411, Time 00:03:44,lr 0.001\n",
      "epoch 178, loss 0.00166, train_acc 0.9999, valid_acc 0.9402, Time 00:03:44,lr 0.001\n",
      "epoch 179, loss 0.00166, train_acc 0.9999, valid_acc 0.9398, Time 00:03:45,lr 0.001\n",
      "epoch 180, loss 0.00153, train_acc 1.0000, valid_acc 0.9377, Time 00:03:44,lr 0.001\n",
      "epoch 181, loss 0.00157, train_acc 0.9999, valid_acc 0.9426, Time 00:03:44,lr 0.001\n",
      "epoch 182, loss 0.00157, train_acc 0.9999, valid_acc 0.9423, Time 00:03:45,lr 0.001\n",
      "epoch 183, loss 0.00159, train_acc 0.9999, valid_acc 0.9437, Time 00:03:44,lr 0.001\n",
      "epoch 184, loss 0.00146, train_acc 1.0000, valid_acc 0.9373, Time 00:03:45,lr 0.001\n",
      "epoch 185, loss 0.00150, train_acc 1.0000, valid_acc 0.9424, Time 00:03:50,lr 0.001\n",
      "epoch 186, loss 0.00146, train_acc 1.0000, valid_acc 0.9405, Time 00:03:46,lr 0.001\n",
      "epoch 187, loss 0.00141, train_acc 1.0000, valid_acc 0.9392, Time 00:03:46,lr 0.001\n",
      "epoch 188, loss 0.00143, train_acc 1.0000, valid_acc 0.9394, Time 00:03:45,lr 0.001\n",
      "epoch 189, loss 0.00146, train_acc 1.0000, valid_acc 0.9390, Time 00:03:45,lr 0.001\n",
      "epoch 190, loss 0.00138, train_acc 1.0000, valid_acc 0.9358, Time 00:03:45,lr 0.001\n",
      "epoch 191, loss 0.00134, train_acc 1.0000, valid_acc 0.9407, Time 00:03:44,lr 0.001\n",
      "epoch 192, loss 0.00142, train_acc 0.9999, valid_acc 0.9398, Time 00:03:45,lr 0.001\n",
      "epoch 193, loss 0.00139, train_acc 1.0000, valid_acc 0.9414, Time 00:03:45,lr 0.001\n",
      "epoch 194, loss 0.00144, train_acc 0.9999, valid_acc 0.9389, Time 00:03:46,lr 0.001\n",
      "epoch 195, loss 0.00144, train_acc 0.9999, valid_acc 0.9397, Time 00:03:45,lr 0.001\n",
      "epoch 196, loss 0.00138, train_acc 1.0000, valid_acc 0.9425, Time 00:03:45,lr 0.001\n",
      "epoch 197, loss 0.00134, train_acc 1.0000, valid_acc 0.9382, Time 00:03:47,lr 0.001\n",
      "epoch 198, loss 0.00134, train_acc 1.0000, valid_acc 0.9397, Time 00:03:45,lr 0.001\n",
      "epoch 199, loss 0.00132, train_acc 1.0000, valid_acc 0.9408, Time 00:03:47,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train2_DA1, _transform_test_size(64), num_workers=4)\n",
    "net = MyResNet18_333_c64(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "utils.evaluate_accuracy = evaluate_with_flip_accuracy\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_333_c64_bigi_flip_e200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T03:07:18.676939Z",
     "start_time": "2018-03-16T03:04:32.627362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 0.940794728435\n",
      "w flip: 0.940794728435\n",
      "h flip: 0.878494408946\n",
      "w+h flip: 0.921525559105\n",
      "w+h+wh flip: 0.886182108626\n"
     ]
    }
   ],
   "source": [
    "net = MyResNet18_333_c64(10)\n",
    "net.load_params(\"../../models/resnet18_333_c64_bigi_flip_e200\", ctx=ctx)\n",
    "print \"origin:\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "print \"w flip:\", evaluate_with_flip_accuracy(valid_data, net, ctx, flip='w')\n",
    "print \"h flip:\", evaluate_with_flip_accuracy(valid_data, net, ctx, flip='h')\n",
    "print \"w+h flip:\", evaluate_with_flip_accuracy(valid_data, net, ctx, flip='wh')\n",
    "print \"w+h+wh flip:\", evaluate_with_flip_accuracy(valid_data, net, ctx, flip='wh2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.14.2 change pool size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet24_3333_c64(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(MyResNet24_3333_c64, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=256, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=256))\n",
    "            # block 5\n",
    "            net.add(Residual(channels=512, same_shape=False))\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=512))\n",
    "            # block 6\n",
    "            net.add(nn.GlobalAvgPool2D())\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.15 general lr policy train resnet152_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-13T16:18:20.822Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet152_v2_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet152_v2(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features[0])\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[2:4])\n",
    "        net.add(*resnet.features[5:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [100, 180, 250]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=0)\n",
    "net = get_resnet152_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet152_v2_100e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=0)\n",
    "net = get_resnet152_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "net.load_params(\"../../models/resnet152_v2_100e\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet152_v2_200e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "lr_period = [50]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=0)\n",
    "net = get_resnet152_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "net.load_params(\"../../models/resnet152_v2_200e\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet152_v2_300e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. use rewrite train function to train function to make sure not train code bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.794769Z",
     "start_time": "2018-03-03T17:35:08.750443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def get_lr(e, i, step):\n",
    "    if e >= step[i]:\n",
    "        return i + 1\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, lr_decay, wd, ctx, output_file=None, step={}):\n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "        print \" # valid_acc\", valid_acc\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    step_i = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if epoch > 0:\n",
    "            if len(step.keys()) == 0:\n",
    "                if epoch % lr_period == 0:\n",
    "                    trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            else:\n",
    "                if epoch >= step.keys()[step_i]:\n",
    "                    step_i = step_i + 1\n",
    "                    trainer.set_learning_rate(step[step.keys()[step_i]])\n",
    "        \n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            \n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += utils.accuracy(output, label)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "        \n",
    "        nd.waitall()\n",
    "        \n",
    "    if output_file != sys.stdout:\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:17:26.291786Z",
     "start_time": "2018-03-03T17:35:08.796140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.100838658147\n",
      "epoch 0, loss 2.17650, train_acc 0.1815, valid_acc 0.3233, Time 00:00:29,lr 0.1\n",
      "epoch 1, loss 1.74745, train_acc 0.3639, valid_acc 0.2111, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.48485, train_acc 0.4729, valid_acc 0.5586, Time 00:00:31,lr 0.1\n",
      "epoch 3, loss 1.32675, train_acc 0.5326, valid_acc 0.5165, Time 00:00:31,lr 0.1\n",
      "epoch 4, loss 1.23933, train_acc 0.5676, valid_acc 0.4877, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 1.18796, train_acc 0.5838, valid_acc 0.5885, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.15784, train_acc 0.5958, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 1.12028, train_acc 0.6103, valid_acc 0.6090, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 1.11800, train_acc 0.6141, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 1.09796, train_acc 0.6208, valid_acc 0.6072, Time 00:00:31,lr 0.1\n",
      "epoch 10, loss 1.08225, train_acc 0.6251, valid_acc 0.5205, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 1.07237, train_acc 0.6293, valid_acc 0.5745, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 1.06935, train_acc 0.6309, valid_acc 0.6407, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 1.05872, train_acc 0.6334, valid_acc 0.5826, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 1.04889, train_acc 0.6376, valid_acc 0.6672, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 1.05632, train_acc 0.6361, valid_acc 0.6499, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 1.05060, train_acc 0.6386, valid_acc 0.6274, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 1.04430, train_acc 0.6395, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 1.04184, train_acc 0.6421, valid_acc 0.6441, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 1.04526, train_acc 0.6398, valid_acc 0.5606, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 1.04432, train_acc 0.6404, valid_acc 0.6367, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 1.03036, train_acc 0.6456, valid_acc 0.7069, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 1.03184, train_acc 0.6459, valid_acc 0.6150, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 1.03162, train_acc 0.6419, valid_acc 0.6809, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 1.03152, train_acc 0.6440, valid_acc 0.6096, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 1.02944, train_acc 0.6447, valid_acc 0.6779, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 1.02975, train_acc 0.6446, valid_acc 0.5822, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 1.02655, train_acc 0.6464, valid_acc 0.6957, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 1.02281, train_acc 0.6490, valid_acc 0.6738, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 1.02572, train_acc 0.6455, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 1.01937, train_acc 0.6497, valid_acc 0.5929, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 1.01610, train_acc 0.6499, valid_acc 0.5739, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 1.02582, train_acc 0.6454, valid_acc 0.6808, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 1.02411, train_acc 0.6468, valid_acc 0.7068, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 1.01952, train_acc 0.6492, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 35, loss 1.02947, train_acc 0.6447, valid_acc 0.6387, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 1.01511, train_acc 0.6521, valid_acc 0.5688, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 1.02304, train_acc 0.6474, valid_acc 0.6549, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 1.01642, train_acc 0.6496, valid_acc 0.6774, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 1.01767, train_acc 0.6498, valid_acc 0.5900, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 1.01177, train_acc 0.6501, valid_acc 0.6799, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 1.01933, train_acc 0.6482, valid_acc 0.6359, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 1.01835, train_acc 0.6475, valid_acc 0.6245, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 1.02311, train_acc 0.6466, valid_acc 0.6745, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 1.01084, train_acc 0.6527, valid_acc 0.6729, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 1.01739, train_acc 0.6508, valid_acc 0.6601, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 1.02528, train_acc 0.6453, valid_acc 0.6812, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 1.01437, train_acc 0.6516, valid_acc 0.6579, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 1.02278, train_acc 0.6477, valid_acc 0.6893, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 1.01775, train_acc 0.6479, valid_acc 0.7160, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 1.01944, train_acc 0.6485, valid_acc 0.6020, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 1.02941, train_acc 0.6463, valid_acc 0.6422, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 1.01799, train_acc 0.6497, valid_acc 0.7113, Time 00:00:31,lr 0.1\n",
      "epoch 53, loss 1.01612, train_acc 0.6477, valid_acc 0.6327, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 1.02105, train_acc 0.6492, valid_acc 0.6325, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 1.02410, train_acc 0.6450, valid_acc 0.6888, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 1.01827, train_acc 0.6495, valid_acc 0.6665, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 1.01920, train_acc 0.6502, valid_acc 0.6775, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 1.02319, train_acc 0.6464, valid_acc 0.6765, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 1.01978, train_acc 0.6507, valid_acc 0.5037, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 1.02047, train_acc 0.6486, valid_acc 0.6431, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 1.01323, train_acc 0.6509, valid_acc 0.6778, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 1.01677, train_acc 0.6475, valid_acc 0.6488, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 1.02140, train_acc 0.6477, valid_acc 0.6320, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 1.01649, train_acc 0.6486, valid_acc 0.6985, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 1.01495, train_acc 0.6494, valid_acc 0.6110, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 1.02517, train_acc 0.6482, valid_acc 0.6281, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 1.01754, train_acc 0.6505, valid_acc 0.6933, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 1.01817, train_acc 0.6487, valid_acc 0.5997, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 1.02060, train_acc 0.6480, valid_acc 0.6438, Time 00:00:31,lr 0.1\n",
      "epoch 70, loss 1.01663, train_acc 0.6501, valid_acc 0.6557, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 1.01802, train_acc 0.6506, valid_acc 0.6037, Time 00:00:31,lr 0.1\n",
      "epoch 72, loss 1.02473, train_acc 0.6465, valid_acc 0.6869, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 1.02214, train_acc 0.6481, valid_acc 0.6365, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 1.02075, train_acc 0.6494, valid_acc 0.6727, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 1.02465, train_acc 0.6455, valid_acc 0.6005, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 1.01342, train_acc 0.6509, valid_acc 0.6316, Time 00:00:31,lr 0.1\n",
      "epoch 77, loss 1.02116, train_acc 0.6466, valid_acc 0.6909, Time 00:00:31,lr 0.1\n",
      "epoch 78, loss 1.02252, train_acc 0.6469, valid_acc 0.7035, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 1.01862, train_acc 0.6496, valid_acc 0.7132, Time 00:00:31,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = 80\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:33.999408Z",
     "start_time": "2018-03-03T18:17:26.293466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.713158945687\n",
      "epoch 0, loss 0.89016, train_acc 0.6943, valid_acc 0.6973, Time 00:00:29,lr 0.05\n",
      "epoch 1, loss 0.97071, train_acc 0.6649, valid_acc 0.6228, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.97855, train_acc 0.6619, valid_acc 0.5962, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.98917, train_acc 0.6604, valid_acc 0.6651, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.98467, train_acc 0.6614, valid_acc 0.6855, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.98934, train_acc 0.6582, valid_acc 0.6544, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.99133, train_acc 0.6577, valid_acc 0.6867, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.99419, train_acc 0.6565, valid_acc 0.6996, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.99295, train_acc 0.6574, valid_acc 0.6707, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.98785, train_acc 0.6583, valid_acc 0.6599, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.72795, train_acc 0.7497, valid_acc 0.7739, Time 00:00:31,lr 0.01\n",
      "epoch 11, loss 0.67062, train_acc 0.7695, valid_acc 0.7807, Time 00:00:31,lr 0.01\n",
      "epoch 12, loss 0.66014, train_acc 0.7741, valid_acc 0.8119, Time 00:00:31,lr 0.01\n",
      "epoch 13, loss 0.66186, train_acc 0.7753, valid_acc 0.8148, Time 00:00:31,lr 0.01\n",
      "epoch 14, loss 0.66272, train_acc 0.7717, valid_acc 0.7799, Time 00:00:31,lr 0.01\n",
      "epoch 15, loss 0.64879, train_acc 0.7773, valid_acc 0.8168, Time 00:00:31,lr 0.01\n",
      "epoch 16, loss 0.65313, train_acc 0.7758, valid_acc 0.8103, Time 00:00:31,lr 0.01\n",
      "epoch 17, loss 0.64368, train_acc 0.7803, valid_acc 0.8027, Time 00:00:31,lr 0.01\n",
      "epoch 18, loss 0.64573, train_acc 0.7790, valid_acc 0.7865, Time 00:00:31,lr 0.01\n",
      "epoch 19, loss 0.63965, train_acc 0.7808, valid_acc 0.8113, Time 00:00:31,lr 0.01\n",
      "epoch 20, loss 0.49768, train_acc 0.8307, valid_acc 0.8597, Time 00:00:31,lr 0.002\n",
      "epoch 21, loss 0.45085, train_acc 0.8470, valid_acc 0.8730, Time 00:00:31,lr 0.002\n",
      "epoch 22, loss 0.43494, train_acc 0.8515, valid_acc 0.8760, Time 00:00:31,lr 0.002\n",
      "epoch 23, loss 0.42184, train_acc 0.8574, valid_acc 0.8801, Time 00:00:31,lr 0.002\n",
      "epoch 24, loss 0.41728, train_acc 0.8585, valid_acc 0.8719, Time 00:00:31,lr 0.002\n",
      "epoch 25, loss 0.41803, train_acc 0.8566, valid_acc 0.8713, Time 00:00:31,lr 0.002\n",
      "epoch 26, loss 0.41157, train_acc 0.8586, valid_acc 0.8734, Time 00:00:31,lr 0.002\n",
      "epoch 27, loss 0.40844, train_acc 0.8605, valid_acc 0.8761, Time 00:00:31,lr 0.002\n",
      "epoch 28, loss 0.40713, train_acc 0.8602, valid_acc 0.8781, Time 00:00:31,lr 0.002\n",
      "epoch 29, loss 0.40274, train_acc 0.8636, valid_acc 0.8810, Time 00:00:31,lr 0.002\n",
      "epoch 30, loss 0.34908, train_acc 0.8807, valid_acc 0.8912, Time 00:00:31,lr 0.0004\n",
      "epoch 31, loss 0.32949, train_acc 0.8883, valid_acc 0.8973, Time 00:00:31,lr 0.0004\n",
      "epoch 32, loss 0.31813, train_acc 0.8929, valid_acc 0.8957, Time 00:00:31,lr 0.0004\n",
      "epoch 33, loss 0.31530, train_acc 0.8933, valid_acc 0.8949, Time 00:00:31,lr 0.0004\n",
      "epoch 34, loss 0.30938, train_acc 0.8967, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 35, loss 0.30692, train_acc 0.8977, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 36, loss 0.30172, train_acc 0.8962, valid_acc 0.8979, Time 00:00:31,lr 0.0004\n",
      "epoch 37, loss 0.29754, train_acc 0.9001, valid_acc 0.8978, Time 00:00:31,lr 0.0004\n",
      "epoch 38, loss 0.29589, train_acc 0.8995, valid_acc 0.8993, Time 00:00:31,lr 0.0004\n",
      "epoch 39, loss 0.29325, train_acc 0.9014, valid_acc 0.9010, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = 10\n",
    "lr_decay=0.2\n",
    "log_file=None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug2\", ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:34.023597Z",
     "start_time": "2018-03-03T18:38:34.000689Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.save_params('../../models/resnet18_me_9_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
