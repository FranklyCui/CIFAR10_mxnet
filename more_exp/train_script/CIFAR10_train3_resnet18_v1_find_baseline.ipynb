{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T06:32:47.406027Z",
     "start_time": "2018-03-01T06:32:42.929058Z"
    },
    "collapsed": true
   },
   "source": [
    "# 1. import needed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:07.640452Z",
     "start_time": "2018-03-04T09:17:06.929352Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.insert(0, '../../utils')\n",
    "from netlib import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data loader, data argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:07.654837Z",
     "start_time": "2018-03-04T09:17:07.641928Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data loader\n",
    "\"\"\"\n",
    "def _transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def data_loader(batch_size, transform_train, transform_test=None, num_workers=0):\n",
    "    if transform_train is None:\n",
    "        transform_train = _transform_train\n",
    "    if transform_test is None:\n",
    "        transform_test = _transform_test\n",
    "        \n",
    "    # flag=1 mean 3 channel image\n",
    "    train_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=True, transform=transform_train)\n",
    "    test_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=False, transform=transform_test)\n",
    "\n",
    "    loader = gluon.data.DataLoader\n",
    "    train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep', num_workers=num_workers)\n",
    "    test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep', num_workers=num_workers)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:07.865502Z",
     "start_time": "2018-03-04T09:17:07.805873Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data argument\n",
    "\"\"\"\n",
    "def transform_train_DA1(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def transform_train_DA2(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "    \n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "    \n",
    "\n",
    "random_clip_rate = 0.3\n",
    "def transform_train_DA3(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "#                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "#                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "        \n",
    "    if random.random() > random_clip_rate:\n",
    "        im = im.clip(0, 1)\n",
    "    _aug = image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                   std=np.array([0.2023, 0.1994, 0.2010]),)\n",
    "    im = _aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 data aurgument: mixup\n",
    "1. mixup define\n",
    "2. mixup visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 mixup: define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:08.798831Z",
     "start_time": "2018-03-04T09:17:08.782292Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mixup(x1, y1, x2, y2, alpha, num_class):\n",
    "    y1 = nd.one_hot(y1, num_class)\n",
    "    y2 = nd.one_hot(y2, num_class)\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    x = lam * x1 + (1 - lam) * x2\n",
    "    y = lam * y1 + (1 - lam) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 mixup: visulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:10.339270Z",
     "start_time": "2018-03-04T09:17:09.453635Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from time import time\n",
    "batch_size = 32\n",
    "transform_train = _transform_test#transform_train_DA1\n",
    "train_data, test_data = data_loader(batch_size, transform_train)\n",
    "mixup_alpha = 1\n",
    "\n",
    "# for x1, y1 in train_data:\n",
    "#     for x2, y2 in mixup_train_data:\n",
    "#         data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "for x, y in train_data:\n",
    "    l = x.shape[0] / 2\n",
    "    data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:10.820933Z",
     "start_time": "2018-03-04T09:17:10.340377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAADYCAYAAACwTgnaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESJEiBAhdozwYxoiRIgQIULsGP8bhmqfJeSYpx0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f854f5f5950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'airplane'), (1, 'automobile'), (2, 'bird'), (3, 'cat'), (4, 'deer'), (5, 'dog'), (6, 'frog'), (7, 'horse'), (8, 'ship'), (9, 'truck')]\n",
      "\n",
      "[[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "    0.44353923  0.          0.55646074]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.          0.\n",
      "    1.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.55646074  0.          0.44353923  0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.55646074\n",
      "    0.44353923  0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.55646074  0.44353923  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.44353923  0.          0.          0.55646074  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.44353923  0.          0.55646074  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.\n",
      "    0.55646074  0.44353923  0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.44353923  0.          0.          0.          0.\n",
      "    0.55646074  0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.55646074  0.          0.          0.44353923  0.          0.\n",
      "    0.          0.          0.        ]]]\n",
      "<NDArray 10x1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from cifar10_utils import show_images\n",
    "%matplotlib inline\n",
    "mean=np.array([0.4914, 0.4822, 0.4465])\n",
    "std=np.array([0.2023, 0.1994, 0.2010])\n",
    "images = data[:10].transpose((0, 2, 3, 1)).asnumpy()\n",
    "images = images * std + mean\n",
    "images = images.transpose((0, 3, 1, 2)) * 255\n",
    "show_images(images)\n",
    "#show_images(data[:9], rgb_mean=mean*255, std=std*255)\n",
    "\n",
    "print [(i, l) for i, l in enumerate(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])]\n",
    "print label[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 mixup: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:11.623628Z",
     "start_time": "2018-03-04T09:17:11.593888Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "mixup_test = False\n",
    "\n",
    "if mixup_test:\n",
    "    net = ResNet164_v2(10)\n",
    "    net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    loss_f = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "    num_epochs = 1\n",
    "    learning_rate = 0.1\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    cur_time = time()\n",
    "    iters = 0\n",
    "    \"\"\"\n",
    "    data loader first time run will cost about 3x time than after run.\n",
    "    \"\"\"\n",
    "    for x1, y1 in train_data:\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time\n",
    "        iters += 1\n",
    "    print \"cost time:\", time() - cur_time\n",
    "    print\n",
    "    cur_time = time()\n",
    "\n",
    "    iters = 0\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'momentum': 0.9, 'wd': 1e-4})\n",
    "    for x, y in train_data:\n",
    "        l = x.shape[0] / 2\n",
    "        data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label.as_in_context(ctx))\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time, nd.mean(loss).asscalar()\n",
    "        iters += 1\n",
    "    print iters, time() - cur_time\n",
    "\n",
    "    # load data one by one batch\n",
    "    # iters = 0\n",
    "    # for x1, y1 in train_data:\n",
    "    #     for x2, y2 in mixup_train_data:\n",
    "    #         data, label = mixup(x1, y1, x2[:x1.shape[0]], y2[:y1.shape[0]], mixup_alpha, 10)\n",
    "    #         break\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time\n",
    "    #     iters += 1\n",
    "    # print \"cost time:\", time() - cur_time\n",
    "    # print\n",
    "    # cur_time = time()\n",
    "\n",
    "    # zip will load all datas and then iterate them, too cost memory, will drop speed when memory over.\n",
    "    # iters = 0\n",
    "    # for (x1, y1), (x2, y2) in zip(train_data, mixup_train_data):\n",
    "    #     data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time#, nd.mean(loss).asscalar()\n",
    "    #     iters += 1\n",
    "    # print time() - cur_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:18.636515Z",
     "start_time": "2018-03-04T09:17:18.539997Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def abs_mean(W):\n",
    "    return nd.mean(nd.abs(W)).asscalar()\n",
    "\n",
    "def in_list(e, l):\n",
    "    for i in l:\n",
    "        if i == e:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, \n",
    "          lr_decay, wd, ctx, w_key, output_file=None, verbose=False, loss_f=gluon.loss.SoftmaxCrossEntropyLoss(), \n",
    "          use_mixup=False, mixup_alpha=0.2):\n",
    "    def train_batch(data, label, i):\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        _loss = nd.mean(loss).asscalar()\n",
    "        if not use_mixup:\n",
    "            _acc = utils.accuracy(output, label)\n",
    "        else:\n",
    "            _acc = None\n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            print \" # iter\", i,\n",
    "            print \"loss %.5f\" % _loss, \n",
    "            if not use_mixup: print \"acc %.5f\" % _acc,\n",
    "            print \"w (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.data()),\n",
    "            print \") g (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.grad()),\n",
    "            print \")\"\n",
    "        return _loss, _acc\n",
    "            \n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        stdout = sys.stdout\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = output_file\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    if verbose:\n",
    "        print \" #\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "    \n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if in_list(epoch, lr_period):\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            \n",
    "        if not use_mixup:\n",
    "            for data, label in train_data:\n",
    "                _loss, _acc = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                train_acc += _acc\n",
    "                i += 1\n",
    "        else:\n",
    "            for x, y in train_data:\n",
    "                l = x.shape[0] / 2\n",
    "                data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "                _loss, _ = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                i += 1\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "    if output_file != stdout:\n",
    "        sys.stdout = stdout\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. get net and do EXP\n",
    "```\n",
    "I want to find out the reason that resnet18_v2 get fail baseline in CIFAR10_train2_* script, design follow exp.\n",
    "\n",
    "1. 5.1 train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs.\n",
    "2. 5.2 train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18.\n",
    " there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 re-exp resnet18_v1\n",
    "train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T16:30:21.462308Z",
     "start_time": "2018-03-03T16:30:13.299911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932408146965\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(10)\n",
    "net.load_params('../../models/res18_9', ctx=ctx)\n",
    "valid_acc = utils.evaluate_accuracy(test_data, net, ctx)\n",
    "print valid_acc\n",
    "def get_net(ctx):\n",
    "    num_outputs = 10\n",
    "    net = ResNet(num_outputs)\n",
    "    net.collect_params().initialize(init=init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    return net  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T10:46:50.521728Z",
     "start_time": "2018-03-03T10:05:27.156785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.80346, train_acc 0.3266, valid_acc 0.3598, Time 00:00:28,lr 0.1\n",
      "epoch 1, loss 1.28281, train_acc 0.5411, valid_acc 0.5778, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.01865, train_acc 0.6431, valid_acc 0.6533, Time 00:00:30,lr 0.1\n",
      "epoch 3, loss 0.90869, train_acc 0.6870, valid_acc 0.5823, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 0.85972, train_acc 0.7049, valid_acc 0.6255, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 0.82922, train_acc 0.7158, valid_acc 0.6503, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 0.81416, train_acc 0.7211, valid_acc 0.6621, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 0.79129, train_acc 0.7286, valid_acc 0.6731, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.79221, train_acc 0.7293, valid_acc 0.6960, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 0.77941, train_acc 0.7357, valid_acc 0.7085, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.77304, train_acc 0.7350, valid_acc 0.6773, Time 00:00:33,lr 0.1\n",
      "epoch 11, loss 0.77300, train_acc 0.7362, valid_acc 0.6708, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 0.77379, train_acc 0.7359, valid_acc 0.7297, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.76346, train_acc 0.7400, valid_acc 0.6792, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.75981, train_acc 0.7400, valid_acc 0.7052, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 0.76189, train_acc 0.7409, valid_acc 0.7065, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 0.75300, train_acc 0.7431, valid_acc 0.7030, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 0.74843, train_acc 0.7451, valid_acc 0.7334, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 0.75400, train_acc 0.7439, valid_acc 0.6873, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 0.74822, train_acc 0.7453, valid_acc 0.6994, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 0.74226, train_acc 0.7492, valid_acc 0.6627, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 0.74857, train_acc 0.7449, valid_acc 0.6691, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 0.73817, train_acc 0.7513, valid_acc 0.6690, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.74331, train_acc 0.7469, valid_acc 0.7008, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 0.74029, train_acc 0.7473, valid_acc 0.6862, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 0.73600, train_acc 0.7491, valid_acc 0.7335, Time 00:00:30,lr 0.1\n",
      "epoch 26, loss 0.73561, train_acc 0.7486, valid_acc 0.7344, Time 00:00:30,lr 0.1\n",
      "epoch 27, loss 0.73582, train_acc 0.7491, valid_acc 0.5638, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.73652, train_acc 0.7464, valid_acc 0.5980, Time 00:00:30,lr 0.1\n",
      "epoch 29, loss 0.73622, train_acc 0.7506, valid_acc 0.7386, Time 00:00:30,lr 0.1\n",
      "epoch 30, loss 0.73189, train_acc 0.7487, valid_acc 0.6339, Time 00:00:30,lr 0.1\n",
      "epoch 31, loss 0.73360, train_acc 0.7494, valid_acc 0.6679, Time 00:00:30,lr 0.1\n",
      "epoch 32, loss 0.73711, train_acc 0.7486, valid_acc 0.6363, Time 00:00:30,lr 0.1\n",
      "epoch 33, loss 0.73315, train_acc 0.7495, valid_acc 0.6735, Time 00:00:30,lr 0.1\n",
      "epoch 34, loss 0.73185, train_acc 0.7496, valid_acc 0.7033, Time 00:00:30,lr 0.1\n",
      "epoch 35, loss 0.73066, train_acc 0.7499, valid_acc 0.7409, Time 00:00:30,lr 0.1\n",
      "epoch 36, loss 0.73286, train_acc 0.7492, valid_acc 0.6494, Time 00:00:30,lr 0.1\n",
      "epoch 37, loss 0.72791, train_acc 0.7502, valid_acc 0.6779, Time 00:00:30,lr 0.1\n",
      "epoch 38, loss 0.73055, train_acc 0.7519, valid_acc 0.6367, Time 00:00:30,lr 0.1\n",
      "epoch 39, loss 0.73307, train_acc 0.7487, valid_acc 0.7078, Time 00:00:30,lr 0.1\n",
      "epoch 40, loss 0.72583, train_acc 0.7522, valid_acc 0.6997, Time 00:00:30,lr 0.1\n",
      "epoch 41, loss 0.73065, train_acc 0.7513, valid_acc 0.6822, Time 00:00:30,lr 0.1\n",
      "epoch 42, loss 0.72384, train_acc 0.7533, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 43, loss 0.73071, train_acc 0.7507, valid_acc 0.6380, Time 00:00:30,lr 0.1\n",
      "epoch 44, loss 0.73206, train_acc 0.7511, valid_acc 0.6253, Time 00:00:30,lr 0.1\n",
      "epoch 45, loss 0.72313, train_acc 0.7533, valid_acc 0.6761, Time 00:00:30,lr 0.1\n",
      "epoch 46, loss 0.72663, train_acc 0.7531, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.72931, train_acc 0.7500, valid_acc 0.6451, Time 00:00:30,lr 0.1\n",
      "epoch 48, loss 0.72035, train_acc 0.7536, valid_acc 0.6857, Time 00:00:30,lr 0.1\n",
      "epoch 49, loss 0.73334, train_acc 0.7483, valid_acc 0.6765, Time 00:00:30,lr 0.1\n",
      "epoch 50, loss 0.72158, train_acc 0.7515, valid_acc 0.7363, Time 00:00:30,lr 0.1\n",
      "epoch 51, loss 0.72392, train_acc 0.7522, valid_acc 0.6832, Time 00:00:30,lr 0.1\n",
      "epoch 52, loss 0.72609, train_acc 0.7512, valid_acc 0.7577, Time 00:00:30,lr 0.1\n",
      "epoch 53, loss 0.72894, train_acc 0.7522, valid_acc 0.7357, Time 00:00:30,lr 0.1\n",
      "epoch 54, loss 0.72575, train_acc 0.7526, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 55, loss 0.72233, train_acc 0.7526, valid_acc 0.6178, Time 00:00:30,lr 0.1\n",
      "epoch 56, loss 0.72252, train_acc 0.7532, valid_acc 0.7112, Time 00:00:30,lr 0.1\n",
      "epoch 57, loss 0.72007, train_acc 0.7541, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 58, loss 0.72001, train_acc 0.7533, valid_acc 0.6893, Time 00:00:30,lr 0.1\n",
      "epoch 59, loss 0.72442, train_acc 0.7541, valid_acc 0.7461, Time 00:00:30,lr 0.1\n",
      "epoch 60, loss 0.72184, train_acc 0.7506, valid_acc 0.6913, Time 00:00:30,lr 0.1\n",
      "epoch 61, loss 0.72317, train_acc 0.7547, valid_acc 0.6189, Time 00:00:30,lr 0.1\n",
      "epoch 62, loss 0.72607, train_acc 0.7537, valid_acc 0.7175, Time 00:00:30,lr 0.1\n",
      "epoch 63, loss 0.71946, train_acc 0.7564, valid_acc 0.5596, Time 00:00:30,lr 0.1\n",
      "epoch 64, loss 0.72751, train_acc 0.7509, valid_acc 0.7269, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.72550, train_acc 0.7516, valid_acc 0.6568, Time 00:00:30,lr 0.1\n",
      "epoch 66, loss 0.72351, train_acc 0.7542, valid_acc 0.6292, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.72099, train_acc 0.7540, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 68, loss 0.72052, train_acc 0.7541, valid_acc 0.6509, Time 00:00:30,lr 0.1\n",
      "epoch 69, loss 0.71841, train_acc 0.7530, valid_acc 0.6510, Time 00:00:30,lr 0.1\n",
      "epoch 70, loss 0.72424, train_acc 0.7510, valid_acc 0.6668, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.71802, train_acc 0.7542, valid_acc 0.6872, Time 00:00:30,lr 0.1\n",
      "epoch 72, loss 0.72042, train_acc 0.7525, valid_acc 0.7391, Time 00:00:30,lr 0.1\n",
      "epoch 73, loss 0.72243, train_acc 0.7534, valid_acc 0.6784, Time 00:00:30,lr 0.1\n",
      "epoch 74, loss 0.72777, train_acc 0.7511, valid_acc 0.6956, Time 00:00:30,lr 0.1\n",
      "epoch 75, loss 0.72433, train_acc 0.7536, valid_acc 0.7030, Time 00:00:30,lr 0.1\n",
      "epoch 76, loss 0.72253, train_acc 0.7537, valid_acc 0.6999, Time 00:00:30,lr 0.1\n",
      "epoch 77, loss 0.72566, train_acc 0.7514, valid_acc 0.6808, Time 00:00:30,lr 0.1\n",
      "epoch 78, loss 0.72288, train_acc 0.7538, valid_acc 0.7191, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 0.72042, train_acc 0.7558, valid_acc 0.6245, Time 00:00:30,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.748897Z",
     "start_time": "2018-03-03T16:30:22.024115Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.61285, train_acc 0.7915, valid_acc 0.7278, Time 00:00:31,lr 0.05\n",
      "epoch 1, loss 0.68711, train_acc 0.7663, valid_acc 0.6986, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.70354, train_acc 0.7601, valid_acc 0.7469, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.70402, train_acc 0.7604, valid_acc 0.6044, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.70819, train_acc 0.7573, valid_acc 0.7099, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.71752, train_acc 0.7569, valid_acc 0.7387, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.71241, train_acc 0.7581, valid_acc 0.7282, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.71346, train_acc 0.7582, valid_acc 0.7717, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.71035, train_acc 0.7555, valid_acc 0.7430, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.71255, train_acc 0.7570, valid_acc 0.7069, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.71683, train_acc 0.7539, valid_acc 0.5970, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.71341, train_acc 0.7572, valid_acc 0.7481, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.71346, train_acc 0.7568, valid_acc 0.7049, Time 00:00:38,lr 0.05\n",
      "epoch 13, loss 0.70515, train_acc 0.7603, valid_acc 0.7366, Time 00:00:40,lr 0.05\n",
      "epoch 14, loss 0.71341, train_acc 0.7570, valid_acc 0.6632, Time 00:00:35,lr 0.05\n",
      "epoch 15, loss 0.70901, train_acc 0.7593, valid_acc 0.6943, Time 00:00:39,lr 0.05\n",
      "epoch 16, loss 0.71456, train_acc 0.7563, valid_acc 0.6658, Time 00:00:41,lr 0.05\n",
      "epoch 17, loss 0.71572, train_acc 0.7575, valid_acc 0.6613, Time 00:00:34,lr 0.05\n",
      "epoch 18, loss 0.71535, train_acc 0.7565, valid_acc 0.6999, Time 00:00:37,lr 0.05\n",
      "epoch 19, loss 0.70597, train_acc 0.7611, valid_acc 0.7358, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.71559, train_acc 0.7546, valid_acc 0.6635, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.71545, train_acc 0.7573, valid_acc 0.6657, Time 00:00:32,lr 0.05\n",
      "epoch 22, loss 0.71076, train_acc 0.7595, valid_acc 0.7046, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.71619, train_acc 0.7562, valid_acc 0.6314, Time 00:00:33,lr 0.05\n",
      "epoch 24, loss 0.70736, train_acc 0.7596, valid_acc 0.7478, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.70959, train_acc 0.7582, valid_acc 0.6643, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.71528, train_acc 0.7543, valid_acc 0.6441, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.72031, train_acc 0.7560, valid_acc 0.7425, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.71745, train_acc 0.7578, valid_acc 0.7464, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.70926, train_acc 0.7598, valid_acc 0.6095, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.46802, train_acc 0.8419, valid_acc 0.8559, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.42038, train_acc 0.8549, valid_acc 0.8552, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.41659, train_acc 0.8567, valid_acc 0.8418, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.41601, train_acc 0.8581, valid_acc 0.8321, Time 00:00:33,lr 0.01\n",
      "epoch 34, loss 0.41853, train_acc 0.8580, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.42103, train_acc 0.8563, valid_acc 0.8627, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.41386, train_acc 0.8588, valid_acc 0.8500, Time 00:00:31,lr 0.01\n",
      "epoch 37, loss 0.41497, train_acc 0.8594, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.40632, train_acc 0.8621, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.40439, train_acc 0.8627, valid_acc 0.8521, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.40516, train_acc 0.8633, valid_acc 0.8540, Time 00:00:31,lr 0.01\n",
      "epoch 41, loss 0.40254, train_acc 0.8627, valid_acc 0.8626, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.39611, train_acc 0.8632, valid_acc 0.8397, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.39748, train_acc 0.8639, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.39310, train_acc 0.8676, valid_acc 0.8517, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.38917, train_acc 0.8680, valid_acc 0.8554, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.38661, train_acc 0.8681, valid_acc 0.8591, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.38413, train_acc 0.8689, valid_acc 0.8555, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.37997, train_acc 0.8711, valid_acc 0.8538, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.38270, train_acc 0.8692, valid_acc 0.8484, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.38268, train_acc 0.8702, valid_acc 0.8478, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.38004, train_acc 0.8712, valid_acc 0.8528, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.37946, train_acc 0.8712, valid_acc 0.8603, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.37720, train_acc 0.8728, valid_acc 0.8533, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.37159, train_acc 0.8733, valid_acc 0.8092, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.37223, train_acc 0.8734, valid_acc 0.8453, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.37370, train_acc 0.8742, valid_acc 0.8661, Time 00:00:31,lr 0.01\n",
      "epoch 57, loss 0.37211, train_acc 0.8736, valid_acc 0.8377, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.37039, train_acc 0.8741, valid_acc 0.8310, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.37287, train_acc 0.8743, valid_acc 0.8420, Time 00:00:31,lr 0.01\n",
      "epoch 60, loss 0.23431, train_acc 0.9229, valid_acc 0.9057, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.19372, train_acc 0.9352, valid_acc 0.9090, Time 00:00:31,lr 0.002\n",
      "epoch 62, loss 0.17911, train_acc 0.9395, valid_acc 0.9086, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.17003, train_acc 0.9431, valid_acc 0.9157, Time 00:00:31,lr 0.002\n",
      "epoch 64, loss 0.16376, train_acc 0.9459, valid_acc 0.9081, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.15820, train_acc 0.9475, valid_acc 0.9153, Time 00:00:32,lr 0.002\n",
      "epoch 66, loss 0.15374, train_acc 0.9485, valid_acc 0.9127, Time 00:00:32,lr 0.002\n",
      "epoch 67, loss 0.15401, train_acc 0.9480, valid_acc 0.9136, Time 00:00:31,lr 0.002\n",
      "epoch 68, loss 0.15006, train_acc 0.9496, valid_acc 0.9137, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.15038, train_acc 0.9497, valid_acc 0.9058, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.15246, train_acc 0.9476, valid_acc 0.9106, Time 00:00:31,lr 0.002\n",
      "epoch 71, loss 0.15303, train_acc 0.9474, valid_acc 0.9042, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.15150, train_acc 0.9477, valid_acc 0.9030, Time 00:00:31,lr 0.002\n",
      "epoch 73, loss 0.15295, train_acc 0.9472, valid_acc 0.9058, Time 00:00:32,lr 0.002\n",
      "epoch 74, loss 0.15067, train_acc 0.9496, valid_acc 0.9097, Time 00:00:31,lr 0.002\n",
      "epoch 75, loss 0.15562, train_acc 0.9473, valid_acc 0.9034, Time 00:00:31,lr 0.002\n",
      "epoch 76, loss 0.15555, train_acc 0.9475, valid_acc 0.9029, Time 00:00:31,lr 0.002\n",
      "epoch 77, loss 0.15436, train_acc 0.9484, valid_acc 0.9031, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.15387, train_acc 0.9485, valid_acc 0.9032, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.15457, train_acc 0.9481, valid_acc 0.9065, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.15198, train_acc 0.9484, valid_acc 0.9091, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.15804, train_acc 0.9467, valid_acc 0.9018, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.15374, train_acc 0.9477, valid_acc 0.9049, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.15341, train_acc 0.9491, valid_acc 0.9067, Time 00:00:32,lr 0.002\n",
      "epoch 84, loss 0.15361, train_acc 0.9499, valid_acc 0.9019, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.15058, train_acc 0.9491, valid_acc 0.8981, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.15706, train_acc 0.9464, valid_acc 0.9073, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.15415, train_acc 0.9483, valid_acc 0.9021, Time 00:00:31,lr 0.002\n",
      "epoch 88, loss 0.14994, train_acc 0.9494, valid_acc 0.8976, Time 00:00:32,lr 0.002\n",
      "epoch 89, loss 0.15133, train_acc 0.9490, valid_acc 0.8961, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.08904, train_acc 0.9723, valid_acc 0.9240, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.06755, train_acc 0.9796, valid_acc 0.9256, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.05880, train_acc 0.9830, valid_acc 0.9272, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.05345, train_acc 0.9850, valid_acc 0.9282, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.05022, train_acc 0.9862, valid_acc 0.9306, Time 00:00:31,lr 0.0004\n",
      "epoch 95, loss 0.04724, train_acc 0.9864, valid_acc 0.9297, Time 00:00:31,lr 0.0004\n",
      "epoch 96, loss 0.04417, train_acc 0.9878, valid_acc 0.9303, Time 00:00:31,lr 0.0004\n",
      "epoch 97, loss 0.04230, train_acc 0.9886, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.04097, train_acc 0.9887, valid_acc 0.9272, Time 00:00:32,lr 0.0004\n",
      "epoch 99, loss 0.03953, train_acc 0.9899, valid_acc 0.9291, Time 00:00:31,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.03685, train_acc 0.9899, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.03604, train_acc 0.9903, valid_acc 0.9277, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.03277, train_acc 0.9917, valid_acc 0.9261, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.03316, train_acc 0.9913, valid_acc 0.9253, Time 00:00:32,lr 0.0004\n",
      "epoch 104, loss 0.03211, train_acc 0.9920, valid_acc 0.9288, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.03214, train_acc 0.9913, valid_acc 0.9277, Time 00:00:32,lr 0.0004\n",
      "epoch 106, loss 0.03095, train_acc 0.9922, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 107, loss 0.03007, train_acc 0.9922, valid_acc 0.9275, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.02834, train_acc 0.9926, valid_acc 0.9271, Time 00:00:31,lr 0.0004\n",
      "epoch 109, loss 0.03008, train_acc 0.9920, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.02855, train_acc 0.9928, valid_acc 0.9279, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.02935, train_acc 0.9926, valid_acc 0.9269, Time 00:00:32,lr 0.0004\n",
      "epoch 112, loss 0.02706, train_acc 0.9931, valid_acc 0.9291, Time 00:00:32,lr 0.0004\n",
      "epoch 113, loss 0.02690, train_acc 0.9933, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.02585, train_acc 0.9932, valid_acc 0.9263, Time 00:00:31,lr 0.0004\n",
      "epoch 115, loss 0.02478, train_acc 0.9938, valid_acc 0.9280, Time 00:00:31,lr 0.0004\n",
      "epoch 116, loss 0.02669, train_acc 0.9930, valid_acc 0.9264, Time 00:00:31,lr 0.0004\n",
      "epoch 117, loss 0.02483, train_acc 0.9937, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 118, loss 0.02543, train_acc 0.9936, valid_acc 0.9281, Time 00:00:31,lr 0.0004\n",
      "epoch 119, loss 0.02483, train_acc 0.9939, valid_acc 0.9300, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_me_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 use gluon renset18_v1\n",
    "train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:09.871178Z",
     "start_time": "2018-03-04T08:31:09.864612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and add must in name_scope, or may got name error\n",
    "def get_resnet18_v1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features)\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T01:41:50.461216Z",
     "start_time": "2018-03-04T01:08:08.533086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.51019, train_acc 0.2129, valid_acc 0.3261, Time 00:00:27,lr 0.1\n",
      "epoch 1, loss 1.75673, train_acc 0.3417, valid_acc 0.4151, Time 00:00:25,lr 0.1\n",
      "epoch 2, loss 1.60616, train_acc 0.4121, valid_acc 0.4824, Time 00:00:24,lr 0.1\n",
      "epoch 3, loss 1.49376, train_acc 0.4615, valid_acc 0.4921, Time 00:00:24,lr 0.1\n",
      "epoch 4, loss 1.42743, train_acc 0.4928, valid_acc 0.4964, Time 00:00:25,lr 0.1\n",
      "epoch 5, loss 1.37983, train_acc 0.5144, valid_acc 0.5435, Time 00:00:24,lr 0.1\n",
      "epoch 6, loss 1.36243, train_acc 0.5234, valid_acc 0.5876, Time 00:00:25,lr 0.1\n",
      "epoch 7, loss 1.33671, train_acc 0.5313, valid_acc 0.5255, Time 00:00:25,lr 0.1\n",
      "epoch 8, loss 1.31173, train_acc 0.5461, valid_acc 0.5368, Time 00:00:24,lr 0.1\n",
      "epoch 9, loss 1.27490, train_acc 0.5587, valid_acc 0.5911, Time 00:00:24,lr 0.1\n",
      "epoch 10, loss 1.27032, train_acc 0.5597, valid_acc 0.6087, Time 00:00:28,lr 0.1\n",
      "epoch 11, loss 1.25436, train_acc 0.5670, valid_acc 0.5886, Time 00:00:24,lr 0.1\n",
      "epoch 12, loss 1.24745, train_acc 0.5690, valid_acc 0.5414, Time 00:00:27,lr 0.1\n",
      "epoch 13, loss 1.23173, train_acc 0.5756, valid_acc 0.5944, Time 00:00:25,lr 0.1\n",
      "epoch 14, loss 1.22901, train_acc 0.5782, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 15, loss 1.22526, train_acc 0.5812, valid_acc 0.5576, Time 00:00:24,lr 0.1\n",
      "epoch 16, loss 1.22470, train_acc 0.5828, valid_acc 0.5957, Time 00:00:24,lr 0.1\n",
      "epoch 17, loss 1.21181, train_acc 0.5865, valid_acc 0.6237, Time 00:00:25,lr 0.1\n",
      "epoch 18, loss 1.21252, train_acc 0.5851, valid_acc 0.5971, Time 00:00:25,lr 0.1\n",
      "epoch 19, loss 1.20350, train_acc 0.5899, valid_acc 0.6076, Time 00:00:25,lr 0.1\n",
      "epoch 20, loss 1.20077, train_acc 0.5904, valid_acc 0.6148, Time 00:00:25,lr 0.1\n",
      "epoch 21, loss 1.19665, train_acc 0.5925, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 22, loss 1.19718, train_acc 0.5942, valid_acc 0.5745, Time 00:00:25,lr 0.1\n",
      "epoch 23, loss 1.19424, train_acc 0.5912, valid_acc 0.5711, Time 00:00:25,lr 0.1\n",
      "epoch 24, loss 1.19114, train_acc 0.5953, valid_acc 0.6069, Time 00:00:25,lr 0.1\n",
      "epoch 25, loss 1.19080, train_acc 0.5965, valid_acc 0.6262, Time 00:00:25,lr 0.1\n",
      "epoch 26, loss 1.19219, train_acc 0.5964, valid_acc 0.6112, Time 00:00:25,lr 0.1\n",
      "epoch 27, loss 1.18217, train_acc 0.5991, valid_acc 0.5783, Time 00:00:25,lr 0.1\n",
      "epoch 28, loss 1.19244, train_acc 0.5922, valid_acc 0.6090, Time 00:00:25,lr 0.1\n",
      "epoch 29, loss 1.18442, train_acc 0.5964, valid_acc 0.6002, Time 00:00:27,lr 0.1\n",
      "epoch 30, loss 1.18418, train_acc 0.5984, valid_acc 0.5866, Time 00:00:25,lr 0.1\n",
      "epoch 31, loss 1.18398, train_acc 0.5969, valid_acc 0.5779, Time 00:00:25,lr 0.1\n",
      "epoch 32, loss 1.18966, train_acc 0.5938, valid_acc 0.6059, Time 00:00:25,lr 0.1\n",
      "epoch 33, loss 1.19080, train_acc 0.5951, valid_acc 0.6048, Time 00:00:25,lr 0.1\n",
      "epoch 34, loss 1.18026, train_acc 0.5991, valid_acc 0.6034, Time 00:00:25,lr 0.1\n",
      "epoch 35, loss 1.18109, train_acc 0.5999, valid_acc 0.6194, Time 00:00:25,lr 0.1\n",
      "epoch 36, loss 1.18156, train_acc 0.6008, valid_acc 0.5972, Time 00:00:25,lr 0.1\n",
      "epoch 37, loss 1.19249, train_acc 0.5947, valid_acc 0.5651, Time 00:00:25,lr 0.1\n",
      "epoch 38, loss 1.18308, train_acc 0.5992, valid_acc 0.6097, Time 00:00:25,lr 0.1\n",
      "epoch 39, loss 1.18060, train_acc 0.6001, valid_acc 0.6363, Time 00:00:25,lr 0.1\n",
      "epoch 40, loss 1.17950, train_acc 0.6004, valid_acc 0.6394, Time 00:00:25,lr 0.1\n",
      "epoch 41, loss 1.18485, train_acc 0.5964, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 42, loss 1.18305, train_acc 0.5999, valid_acc 0.6338, Time 00:00:25,lr 0.1\n",
      "epoch 43, loss 1.18409, train_acc 0.5985, valid_acc 0.6278, Time 00:00:24,lr 0.1\n",
      "epoch 44, loss 1.17604, train_acc 0.5990, valid_acc 0.6171, Time 00:00:25,lr 0.1\n",
      "epoch 45, loss 1.18259, train_acc 0.5979, valid_acc 0.6266, Time 00:00:25,lr 0.1\n",
      "epoch 46, loss 1.17980, train_acc 0.6023, valid_acc 0.5782, Time 00:00:25,lr 0.1\n",
      "epoch 47, loss 1.18290, train_acc 0.5980, valid_acc 0.6226, Time 00:00:25,lr 0.1\n",
      "epoch 48, loss 1.17870, train_acc 0.5994, valid_acc 0.5915, Time 00:00:25,lr 0.1\n",
      "epoch 49, loss 1.17969, train_acc 0.5983, valid_acc 0.5877, Time 00:00:24,lr 0.1\n",
      "epoch 50, loss 1.18743, train_acc 0.5986, valid_acc 0.6399, Time 00:00:25,lr 0.1\n",
      "epoch 51, loss 1.17115, train_acc 0.6031, valid_acc 0.5693, Time 00:00:25,lr 0.1\n",
      "epoch 52, loss 1.16321, train_acc 0.6042, valid_acc 0.6509, Time 00:00:24,lr 0.1\n",
      "epoch 53, loss 1.17745, train_acc 0.6004, valid_acc 0.6119, Time 00:00:25,lr 0.1\n",
      "epoch 54, loss 1.18013, train_acc 0.5981, valid_acc 0.5628, Time 00:00:25,lr 0.1\n",
      "epoch 55, loss 1.17807, train_acc 0.5997, valid_acc 0.5863, Time 00:00:26,lr 0.1\n",
      "epoch 56, loss 1.18230, train_acc 0.6013, valid_acc 0.6077, Time 00:00:26,lr 0.1\n",
      "epoch 57, loss 1.17381, train_acc 0.6016, valid_acc 0.6339, Time 00:00:24,lr 0.1\n",
      "epoch 58, loss 1.17931, train_acc 0.5979, valid_acc 0.6215, Time 00:00:24,lr 0.1\n",
      "epoch 59, loss 1.17566, train_acc 0.6000, valid_acc 0.5958, Time 00:00:24,lr 0.1\n",
      "epoch 60, loss 1.16982, train_acc 0.6028, valid_acc 0.6310, Time 00:00:25,lr 0.1\n",
      "epoch 61, loss 1.17248, train_acc 0.6016, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 62, loss 1.17132, train_acc 0.6028, valid_acc 0.6392, Time 00:00:24,lr 0.1\n",
      "epoch 63, loss 1.16930, train_acc 0.6059, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 64, loss 1.17535, train_acc 0.6018, valid_acc 0.6257, Time 00:00:24,lr 0.1\n",
      "epoch 65, loss 1.16613, train_acc 0.6031, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 66, loss 1.17103, train_acc 0.6037, valid_acc 0.5942, Time 00:00:24,lr 0.1\n",
      "epoch 67, loss 1.17368, train_acc 0.6013, valid_acc 0.6163, Time 00:00:25,lr 0.1\n",
      "epoch 68, loss 1.17817, train_acc 0.5984, valid_acc 0.5712, Time 00:00:27,lr 0.1\n",
      "epoch 69, loss 1.18750, train_acc 0.5986, valid_acc 0.6246, Time 00:00:24,lr 0.1\n",
      "epoch 70, loss 1.17631, train_acc 0.6010, valid_acc 0.6244, Time 00:00:24,lr 0.1\n",
      "epoch 71, loss 1.18138, train_acc 0.5989, valid_acc 0.5719, Time 00:00:24,lr 0.1\n",
      "epoch 72, loss 1.17125, train_acc 0.6019, valid_acc 0.6373, Time 00:00:24,lr 0.1\n",
      "epoch 73, loss 1.17880, train_acc 0.6007, valid_acc 0.6064, Time 00:00:24,lr 0.1\n",
      "epoch 74, loss 1.17069, train_acc 0.6043, valid_acc 0.6049, Time 00:00:24,lr 0.1\n",
      "epoch 75, loss 1.17566, train_acc 0.6009, valid_acc 0.5390, Time 00:00:24,lr 0.1\n",
      "epoch 76, loss 1.17658, train_acc 0.5984, valid_acc 0.6027, Time 00:00:24,lr 0.1\n",
      "epoch 77, loss 1.17818, train_acc 0.5994, valid_acc 0.6432, Time 00:00:24,lr 0.1\n",
      "epoch 78, loss 1.18056, train_acc 0.6011, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 79, loss 1.18418, train_acc 0.5995, valid_acc 0.6297, Time 00:00:24,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T02:39:40.682812Z",
     "start_time": "2018-03-04T01:49:21.279182Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.03273, train_acc 0.6475, valid_acc 0.6599, Time 00:00:24,lr 0.05\n",
      "epoch 1, loss 1.10147, train_acc 0.6258, valid_acc 0.6101, Time 00:00:24,lr 0.05\n",
      "epoch 2, loss 1.10971, train_acc 0.6221, valid_acc 0.6303, Time 00:00:24,lr 0.05\n",
      "epoch 3, loss 1.11335, train_acc 0.6201, valid_acc 0.6158, Time 00:00:24,lr 0.05\n",
      "epoch 4, loss 1.11669, train_acc 0.6186, valid_acc 0.6162, Time 00:00:24,lr 0.05\n",
      "epoch 5, loss 1.10626, train_acc 0.6214, valid_acc 0.6486, Time 00:00:24,lr 0.05\n",
      "epoch 6, loss 1.11353, train_acc 0.6192, valid_acc 0.6361, Time 00:00:24,lr 0.05\n",
      "epoch 7, loss 1.10059, train_acc 0.6249, valid_acc 0.6364, Time 00:00:24,lr 0.05\n",
      "epoch 8, loss 1.11230, train_acc 0.6210, valid_acc 0.6317, Time 00:00:24,lr 0.05\n",
      "epoch 9, loss 1.10704, train_acc 0.6200, valid_acc 0.5400, Time 00:00:24,lr 0.05\n",
      "epoch 10, loss 1.10685, train_acc 0.6223, valid_acc 0.6251, Time 00:00:24,lr 0.05\n",
      "epoch 11, loss 1.11229, train_acc 0.6209, valid_acc 0.6055, Time 00:00:24,lr 0.05\n",
      "epoch 12, loss 1.11007, train_acc 0.6232, valid_acc 0.6257, Time 00:00:25,lr 0.05\n",
      "epoch 13, loss 1.10097, train_acc 0.6244, valid_acc 0.6220, Time 00:00:25,lr 0.05\n",
      "epoch 14, loss 1.10608, train_acc 0.6263, valid_acc 0.5887, Time 00:00:24,lr 0.05\n",
      "epoch 15, loss 1.10504, train_acc 0.6224, valid_acc 0.6416, Time 00:00:24,lr 0.05\n",
      "epoch 16, loss 1.10434, train_acc 0.6223, valid_acc 0.6064, Time 00:00:24,lr 0.05\n",
      "epoch 17, loss 1.10511, train_acc 0.6221, valid_acc 0.6365, Time 00:00:24,lr 0.05\n",
      "epoch 18, loss 1.10933, train_acc 0.6206, valid_acc 0.6311, Time 00:00:24,lr 0.05\n",
      "epoch 19, loss 1.10533, train_acc 0.6211, valid_acc 0.6559, Time 00:00:25,lr 0.05\n",
      "epoch 20, loss 1.10336, train_acc 0.6231, valid_acc 0.6529, Time 00:00:24,lr 0.05\n",
      "epoch 21, loss 1.11074, train_acc 0.6202, valid_acc 0.6060, Time 00:00:25,lr 0.05\n",
      "epoch 22, loss 1.10799, train_acc 0.6211, valid_acc 0.6211, Time 00:00:24,lr 0.05\n",
      "epoch 23, loss 1.11143, train_acc 0.6213, valid_acc 0.6449, Time 00:00:24,lr 0.05\n",
      "epoch 24, loss 1.10708, train_acc 0.6227, valid_acc 0.6420, Time 00:00:24,lr 0.05\n",
      "epoch 25, loss 1.11583, train_acc 0.6182, valid_acc 0.6106, Time 00:00:24,lr 0.05\n",
      "epoch 26, loss 1.10713, train_acc 0.6237, valid_acc 0.6260, Time 00:00:25,lr 0.05\n",
      "epoch 27, loss 1.10516, train_acc 0.6226, valid_acc 0.6302, Time 00:00:24,lr 0.05\n",
      "epoch 28, loss 1.10940, train_acc 0.6245, valid_acc 0.6110, Time 00:00:24,lr 0.05\n",
      "epoch 29, loss 1.10750, train_acc 0.6217, valid_acc 0.5529, Time 00:00:24,lr 0.05\n",
      "epoch 30, loss 0.82673, train_acc 0.7164, valid_acc 0.7511, Time 00:00:25,lr 0.01\n",
      "epoch 31, loss 0.76458, train_acc 0.7374, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 32, loss 0.75552, train_acc 0.7419, valid_acc 0.7292, Time 00:00:25,lr 0.01\n",
      "epoch 33, loss 0.75799, train_acc 0.7418, valid_acc 0.7546, Time 00:00:25,lr 0.01\n",
      "epoch 34, loss 0.75636, train_acc 0.7434, valid_acc 0.7697, Time 00:00:24,lr 0.01\n",
      "epoch 35, loss 0.74936, train_acc 0.7466, valid_acc 0.7569, Time 00:00:24,lr 0.01\n",
      "epoch 36, loss 0.74819, train_acc 0.7459, valid_acc 0.7532, Time 00:00:25,lr 0.01\n",
      "epoch 37, loss 0.74411, train_acc 0.7466, valid_acc 0.7663, Time 00:00:24,lr 0.01\n",
      "epoch 38, loss 0.74887, train_acc 0.7453, valid_acc 0.7722, Time 00:00:24,lr 0.01\n",
      "epoch 39, loss 0.73962, train_acc 0.7479, valid_acc 0.7629, Time 00:00:24,lr 0.01\n",
      "epoch 40, loss 0.73451, train_acc 0.7482, valid_acc 0.7582, Time 00:00:24,lr 0.01\n",
      "epoch 41, loss 0.72975, train_acc 0.7514, valid_acc 0.7482, Time 00:00:24,lr 0.01\n",
      "epoch 42, loss 0.72403, train_acc 0.7536, valid_acc 0.7622, Time 00:00:24,lr 0.01\n",
      "epoch 43, loss 0.72834, train_acc 0.7540, valid_acc 0.7566, Time 00:00:24,lr 0.01\n",
      "epoch 44, loss 0.72512, train_acc 0.7522, valid_acc 0.7666, Time 00:00:24,lr 0.01\n",
      "epoch 45, loss 0.71810, train_acc 0.7576, valid_acc 0.7462, Time 00:00:25,lr 0.01\n",
      "epoch 46, loss 0.72037, train_acc 0.7547, valid_acc 0.7660, Time 00:00:25,lr 0.01\n",
      "epoch 47, loss 0.71576, train_acc 0.7564, valid_acc 0.7687, Time 00:00:24,lr 0.01\n",
      "epoch 48, loss 0.71955, train_acc 0.7534, valid_acc 0.7430, Time 00:00:24,lr 0.01\n",
      "epoch 49, loss 0.71184, train_acc 0.7587, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 50, loss 0.71714, train_acc 0.7561, valid_acc 0.7544, Time 00:00:24,lr 0.01\n",
      "epoch 51, loss 0.71158, train_acc 0.7576, valid_acc 0.7566, Time 00:00:25,lr 0.01\n",
      "epoch 52, loss 0.70936, train_acc 0.7582, valid_acc 0.7613, Time 00:00:24,lr 0.01\n",
      "epoch 53, loss 0.70673, train_acc 0.7591, valid_acc 0.7554, Time 00:00:24,lr 0.01\n",
      "epoch 54, loss 0.70670, train_acc 0.7597, valid_acc 0.7607, Time 00:00:24,lr 0.01\n",
      "epoch 55, loss 0.70951, train_acc 0.7566, valid_acc 0.7609, Time 00:00:24,lr 0.01\n",
      "epoch 56, loss 0.70382, train_acc 0.7595, valid_acc 0.7573, Time 00:00:24,lr 0.01\n",
      "epoch 57, loss 0.70531, train_acc 0.7601, valid_acc 0.7356, Time 00:00:24,lr 0.01\n",
      "epoch 58, loss 0.70683, train_acc 0.7595, valid_acc 0.7572, Time 00:00:25,lr 0.01\n",
      "epoch 59, loss 0.70069, train_acc 0.7610, valid_acc 0.7556, Time 00:00:24,lr 0.01\n",
      "epoch 60, loss 0.54787, train_acc 0.8137, valid_acc 0.8178, Time 00:00:24,lr 0.002\n",
      "epoch 61, loss 0.49978, train_acc 0.8285, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 62, loss 0.48376, train_acc 0.8345, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 63, loss 0.47104, train_acc 0.8396, valid_acc 0.8270, Time 00:00:24,lr 0.002\n",
      "epoch 64, loss 0.46549, train_acc 0.8415, valid_acc 0.8376, Time 00:00:25,lr 0.002\n",
      "epoch 65, loss 0.45923, train_acc 0.8430, valid_acc 0.8264, Time 00:00:25,lr 0.002\n",
      "epoch 66, loss 0.45508, train_acc 0.8428, valid_acc 0.8332, Time 00:00:24,lr 0.002\n",
      "epoch 67, loss 0.45490, train_acc 0.8441, valid_acc 0.8351, Time 00:00:24,lr 0.002\n",
      "epoch 68, loss 0.45410, train_acc 0.8435, valid_acc 0.8346, Time 00:00:24,lr 0.002\n",
      "epoch 69, loss 0.44802, train_acc 0.8458, valid_acc 0.8378, Time 00:00:24,lr 0.002\n",
      "epoch 70, loss 0.44962, train_acc 0.8464, valid_acc 0.8339, Time 00:00:24,lr 0.002\n",
      "epoch 71, loss 0.45422, train_acc 0.8447, valid_acc 0.8261, Time 00:00:24,lr 0.002\n",
      "epoch 72, loss 0.44599, train_acc 0.8474, valid_acc 0.8287, Time 00:00:25,lr 0.002\n",
      "epoch 73, loss 0.44833, train_acc 0.8458, valid_acc 0.8317, Time 00:00:24,lr 0.002\n",
      "epoch 74, loss 0.44645, train_acc 0.8466, valid_acc 0.8322, Time 00:00:24,lr 0.002\n",
      "epoch 75, loss 0.45153, train_acc 0.8453, valid_acc 0.8240, Time 00:00:24,lr 0.002\n",
      "epoch 76, loss 0.44996, train_acc 0.8456, valid_acc 0.8233, Time 00:00:24,lr 0.002\n",
      "epoch 77, loss 0.44440, train_acc 0.8473, valid_acc 0.8235, Time 00:00:24,lr 0.002\n",
      "epoch 78, loss 0.44680, train_acc 0.8452, valid_acc 0.8229, Time 00:00:24,lr 0.002\n",
      "epoch 79, loss 0.44781, train_acc 0.8466, valid_acc 0.8210, Time 00:00:24,lr 0.002\n",
      "epoch 80, loss 0.44728, train_acc 0.8452, valid_acc 0.8299, Time 00:00:24,lr 0.002\n",
      "epoch 81, loss 0.44830, train_acc 0.8462, valid_acc 0.8278, Time 00:00:24,lr 0.002\n",
      "epoch 82, loss 0.44525, train_acc 0.8456, valid_acc 0.8325, Time 00:00:24,lr 0.002\n",
      "epoch 83, loss 0.44259, train_acc 0.8477, valid_acc 0.8257, Time 00:00:24,lr 0.002\n",
      "epoch 84, loss 0.44439, train_acc 0.8464, valid_acc 0.8292, Time 00:00:24,lr 0.002\n",
      "epoch 85, loss 0.44217, train_acc 0.8476, valid_acc 0.8280, Time 00:00:24,lr 0.002\n",
      "epoch 86, loss 0.44183, train_acc 0.8487, valid_acc 0.8294, Time 00:00:24,lr 0.002\n",
      "epoch 87, loss 0.43943, train_acc 0.8472, valid_acc 0.8248, Time 00:00:24,lr 0.002\n",
      "epoch 88, loss 0.43877, train_acc 0.8485, valid_acc 0.8164, Time 00:00:26,lr 0.002\n",
      "epoch 89, loss 0.43657, train_acc 0.8498, valid_acc 0.8276, Time 00:00:25,lr 0.002\n",
      "epoch 90, loss 0.35472, train_acc 0.8789, valid_acc 0.8585, Time 00:00:25,lr 0.0004\n",
      "epoch 91, loss 0.32633, train_acc 0.8876, valid_acc 0.8528, Time 00:00:25,lr 0.0004\n",
      "epoch 92, loss 0.31385, train_acc 0.8927, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 93, loss 0.30717, train_acc 0.8945, valid_acc 0.8557, Time 00:00:25,lr 0.0004\n",
      "epoch 94, loss 0.30039, train_acc 0.8969, valid_acc 0.8598, Time 00:00:27,lr 0.0004\n",
      "epoch 95, loss 0.29545, train_acc 0.8995, valid_acc 0.8572, Time 00:00:25,lr 0.0004\n",
      "epoch 96, loss 0.29173, train_acc 0.8990, valid_acc 0.8566, Time 00:00:26,lr 0.0004\n",
      "epoch 97, loss 0.28834, train_acc 0.8994, valid_acc 0.8571, Time 00:00:28,lr 0.0004\n",
      "epoch 98, loss 0.28529, train_acc 0.9006, valid_acc 0.8560, Time 00:00:27,lr 0.0004\n",
      "epoch 99, loss 0.28147, train_acc 0.9024, valid_acc 0.8564, Time 00:00:25,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.27405, train_acc 0.9057, valid_acc 0.8599, Time 00:00:26,lr 0.0004\n",
      "epoch 101, loss 0.27207, train_acc 0.9066, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 102, loss 0.27203, train_acc 0.9067, valid_acc 0.8584, Time 00:00:25,lr 0.0004\n",
      "epoch 103, loss 0.26894, train_acc 0.9070, valid_acc 0.8580, Time 00:00:27,lr 0.0004\n",
      "epoch 104, loss 0.26925, train_acc 0.9077, valid_acc 0.8566, Time 00:00:25,lr 0.0004\n",
      "epoch 105, loss 0.26636, train_acc 0.9088, valid_acc 0.8558, Time 00:00:24,lr 0.0004\n",
      "epoch 106, loss 0.26520, train_acc 0.9085, valid_acc 0.8559, Time 00:00:26,lr 0.0004\n",
      "epoch 107, loss 0.26192, train_acc 0.9094, valid_acc 0.8562, Time 00:00:27,lr 0.0004\n",
      "epoch 108, loss 0.26339, train_acc 0.9091, valid_acc 0.8547, Time 00:00:25,lr 0.0004\n",
      "epoch 109, loss 0.26113, train_acc 0.9096, valid_acc 0.8568, Time 00:00:24,lr 0.0004\n",
      "epoch 110, loss 0.25569, train_acc 0.9102, valid_acc 0.8575, Time 00:00:24,lr 0.0004\n",
      "epoch 111, loss 0.25539, train_acc 0.9117, valid_acc 0.8565, Time 00:00:25,lr 0.0004\n",
      "epoch 112, loss 0.25576, train_acc 0.9120, valid_acc 0.8555, Time 00:00:24,lr 0.0004\n",
      "epoch 113, loss 0.25426, train_acc 0.9129, valid_acc 0.8522, Time 00:00:24,lr 0.0004\n",
      "epoch 114, loss 0.25399, train_acc 0.9125, valid_acc 0.8544, Time 00:00:24,lr 0.0004\n",
      "epoch 115, loss 0.25090, train_acc 0.9127, valid_acc 0.8521, Time 00:00:24,lr 0.0004\n",
      "epoch 116, loss 0.24849, train_acc 0.9130, valid_acc 0.8567, Time 00:00:24,lr 0.0004\n",
      "epoch 117, loss 0.25278, train_acc 0.9111, valid_acc 0.8523, Time 00:00:24,lr 0.0004\n",
      "epoch 118, loss 0.25027, train_acc 0.9130, valid_acc 0.8512, Time 00:00:24,lr 0.0004\n",
      "epoch 119, loss 0.24702, train_acc 0.9142, valid_acc 0.8548, Time 00:00:24,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 gluon resnet18 vs my resnet18\n",
    "```\n",
    "there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:40:01.865661Z",
     "start_time": "2018-03-04T08:40:01.754397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (net): HybridSequential(\n",
      "    (0): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "    (2): Activation(relu)\n",
      "    (3): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (6): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (7): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (8): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (9): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (10): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (11): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (12): AvgPool2D(size=(8, 8), stride=(8, 8), padding=(0, 0), ceil_mode=False)\n",
      "    (13): Flatten\n",
      "    (14): Dense(None -> 10, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net1 = ResNet(10)\n",
    "'''\n",
    "block1: conv(32, 3, 1, 1) + BN + relu\n",
    "block2: Residual_block(32) * 3\n",
    "block3: Resisual_block(64) * 3\n",
    "block4: Residual_block(128) * 3\n",
    "block5: AvgPool(8) + Flatten + Dense(10)\n",
    "\n",
    "actually it has 19 conv + 1 fc, some code name it as resnet20 (each Residual_block has 2 conv or 2 + 1 conv(two line))\n",
    "for cifar10 dataset, most code use this arch (https://github.com/yinglang/pytorch-cifar-models).\n",
    "'''\n",
    "net2 = get_resnet18_v1()\n",
    "'''\n",
    "block1: conv(64, 7, 3, 2) + BN + relu + MaxPool(3, 1, 2)\n",
    "block2: Residual_block(64) * 2\n",
    "block3: Resisual_block(128) * 2\n",
    "block4: Residual_block(256) * 2\n",
    "block5: Residual_block(256) * 2\n",
    "block6: GlobalAvgPool() + Dense(10) \n",
    "\n",
    "has 1 + (2 + 2 + 2 + 2) * 2 = 15 conv + 1 fc, name it as resnet18\n",
    "'''\n",
    "print net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:21.510639Z",
     "start_time": "2018-03-04T08:31:21.405189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): HybridSequential(\n",
      "    (0): Conv2D(3 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n",
      "    (4): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  )\n",
      "  (1): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 gluon resnet18 vs my resnet 18: first conv is conv(k=7,p=3,s=2) vs conv(k=3,p=1,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T03:26:13.432236Z",
     "start_time": "2018-03-04T03:26:13.422231Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T04:08:38.921689Z",
     "start_time": "2018-03-04T03:26:16.924298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.20239, train_acc 0.2669, valid_acc 0.3902, Time 00:00:32,lr 0.1\n",
      "epoch 1, loss 1.65517, train_acc 0.3912, valid_acc 0.4528, Time 00:00:33,lr 0.1\n",
      "epoch 2, loss 1.47576, train_acc 0.4642, valid_acc 0.4337, Time 00:00:33,lr 0.1\n",
      "epoch 3, loss 1.34809, train_acc 0.5176, valid_acc 0.5469, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 1.25687, train_acc 0.5574, valid_acc 0.5480, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 1.20031, train_acc 0.5783, valid_acc 0.5777, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 1.15576, train_acc 0.5958, valid_acc 0.6329, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 1.11209, train_acc 0.6100, valid_acc 0.6637, Time 00:00:30,lr 0.1\n",
      "epoch 8, loss 1.06140, train_acc 0.6298, valid_acc 0.5927, Time 00:00:30,lr 0.1\n",
      "epoch 9, loss 1.02432, train_acc 0.6445, valid_acc 0.6795, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 1.00051, train_acc 0.6551, valid_acc 0.6433, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 0.98188, train_acc 0.6637, valid_acc 0.6974, Time 00:00:30,lr 0.1\n",
      "epoch 12, loss 0.96989, train_acc 0.6664, valid_acc 0.6623, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.95365, train_acc 0.6740, valid_acc 0.6860, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.95321, train_acc 0.6733, valid_acc 0.6337, Time 00:00:30,lr 0.1\n",
      "epoch 15, loss 0.94458, train_acc 0.6777, valid_acc 0.6930, Time 00:00:30,lr 0.1\n",
      "epoch 16, loss 0.93070, train_acc 0.6824, valid_acc 0.7069, Time 00:00:30,lr 0.1\n",
      "epoch 17, loss 0.93221, train_acc 0.6827, valid_acc 0.6382, Time 00:00:30,lr 0.1\n",
      "epoch 18, loss 0.92776, train_acc 0.6840, valid_acc 0.7080, Time 00:00:30,lr 0.1\n",
      "epoch 19, loss 0.92143, train_acc 0.6869, valid_acc 0.6862, Time 00:00:30,lr 0.1\n",
      "epoch 20, loss 0.92236, train_acc 0.6877, valid_acc 0.6391, Time 00:00:30,lr 0.1\n",
      "epoch 21, loss 0.92049, train_acc 0.6863, valid_acc 0.6934, Time 00:00:30,lr 0.1\n",
      "epoch 22, loss 0.91650, train_acc 0.6887, valid_acc 0.6944, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.91073, train_acc 0.6888, valid_acc 0.6668, Time 00:00:30,lr 0.1\n",
      "epoch 24, loss 0.91163, train_acc 0.6908, valid_acc 0.6598, Time 00:00:30,lr 0.1\n",
      "epoch 25, loss 0.90582, train_acc 0.6902, valid_acc 0.6914, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 0.90684, train_acc 0.6901, valid_acc 0.6054, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 0.90768, train_acc 0.6901, valid_acc 0.6733, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.90371, train_acc 0.6923, valid_acc 0.6340, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 0.90607, train_acc 0.6906, valid_acc 0.6989, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 0.90220, train_acc 0.6922, valid_acc 0.5081, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.90443, train_acc 0.6917, valid_acc 0.6913, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 0.89974, train_acc 0.6936, valid_acc 0.6244, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 0.89704, train_acc 0.6960, valid_acc 0.7255, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 0.90115, train_acc 0.6939, valid_acc 0.6980, Time 00:00:32,lr 0.1\n",
      "epoch 35, loss 0.89469, train_acc 0.6961, valid_acc 0.6947, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 0.89557, train_acc 0.6974, valid_acc 0.6494, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 0.89949, train_acc 0.6951, valid_acc 0.7107, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 0.89622, train_acc 0.6963, valid_acc 0.5968, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 0.89347, train_acc 0.6955, valid_acc 0.6612, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 0.89192, train_acc 0.6984, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 0.89004, train_acc 0.6967, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 0.88865, train_acc 0.6992, valid_acc 0.6024, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 0.90129, train_acc 0.6922, valid_acc 0.6781, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 0.89063, train_acc 0.6980, valid_acc 0.6661, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 0.89031, train_acc 0.6983, valid_acc 0.6445, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 0.88648, train_acc 0.6996, valid_acc 0.7091, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.88954, train_acc 0.6960, valid_acc 0.7163, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 0.89009, train_acc 0.6982, valid_acc 0.7027, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 0.88359, train_acc 0.7001, valid_acc 0.6971, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 0.89414, train_acc 0.6958, valid_acc 0.6732, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 0.88717, train_acc 0.6997, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 0.89172, train_acc 0.6956, valid_acc 0.6487, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.89213, train_acc 0.6993, valid_acc 0.6922, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 0.88663, train_acc 0.6969, valid_acc 0.7236, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 0.88679, train_acc 0.6988, valid_acc 0.6824, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 0.88335, train_acc 0.6991, valid_acc 0.6743, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 0.88368, train_acc 0.7007, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 0.88596, train_acc 0.6983, valid_acc 0.7060, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 0.88443, train_acc 0.6993, valid_acc 0.6722, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 0.88799, train_acc 0.6990, valid_acc 0.6508, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 0.88132, train_acc 0.7016, valid_acc 0.6703, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 0.87931, train_acc 0.7012, valid_acc 0.6996, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 0.88957, train_acc 0.6987, valid_acc 0.7010, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 0.88078, train_acc 0.7020, valid_acc 0.6454, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.88306, train_acc 0.7007, valid_acc 0.6412, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 0.88245, train_acc 0.7001, valid_acc 0.6534, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 0.87989, train_acc 0.7020, valid_acc 0.6466, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 0.89072, train_acc 0.6977, valid_acc 0.7111, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 0.88862, train_acc 0.6984, valid_acc 0.6285, Time 00:00:35,lr 0.1\n",
      "epoch 70, loss 0.87356, train_acc 0.7045, valid_acc 0.5976, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.88543, train_acc 0.6981, valid_acc 0.7188, Time 00:00:38,lr 0.1\n",
      "epoch 72, loss 0.88926, train_acc 0.6963, valid_acc 0.7050, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 0.87890, train_acc 0.7006, valid_acc 0.6988, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 0.88213, train_acc 0.6988, valid_acc 0.6710, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 0.89143, train_acc 0.6980, valid_acc 0.6485, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 0.88201, train_acc 0.6990, valid_acc 0.6410, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.87847, train_acc 0.7010, valid_acc 0.6491, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.88841, train_acc 0.6974, valid_acc 0.6903, Time 00:00:35,lr 0.1\n",
      "epoch 79, loss 0.88179, train_acc 0.7004, valid_acc 0.6880, Time 00:00:34,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T05:14:50.043433Z",
     "start_time": "2018-03-04T04:10:53.460734Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.77403, train_acc 0.7361, valid_acc 0.6271, Time 00:00:28,lr 0.05\n",
      "epoch 1, loss 0.84400, train_acc 0.7124, valid_acc 0.6732, Time 00:00:30,lr 0.05\n",
      "epoch 2, loss 0.86126, train_acc 0.7060, valid_acc 0.7009, Time 00:00:30,lr 0.05\n",
      "epoch 3, loss 0.86573, train_acc 0.7063, valid_acc 0.6787, Time 00:00:30,lr 0.05\n",
      "epoch 4, loss 0.85972, train_acc 0.7096, valid_acc 0.6623, Time 00:00:30,lr 0.05\n",
      "epoch 5, loss 0.86412, train_acc 0.7051, valid_acc 0.6821, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.86789, train_acc 0.7018, valid_acc 0.6822, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.86329, train_acc 0.7054, valid_acc 0.6411, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.87379, train_acc 0.7028, valid_acc 0.7026, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.86685, train_acc 0.7050, valid_acc 0.6352, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.86982, train_acc 0.7020, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.86871, train_acc 0.7042, valid_acc 0.5869, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.86923, train_acc 0.7037, valid_acc 0.6458, Time 00:00:31,lr 0.05\n",
      "epoch 13, loss 0.86655, train_acc 0.7048, valid_acc 0.6600, Time 00:00:31,lr 0.05\n",
      "epoch 14, loss 0.86976, train_acc 0.7038, valid_acc 0.7020, Time 00:00:31,lr 0.05\n",
      "epoch 15, loss 0.85832, train_acc 0.7083, valid_acc 0.6998, Time 00:00:31,lr 0.05\n",
      "epoch 16, loss 0.86706, train_acc 0.7046, valid_acc 0.6969, Time 00:00:31,lr 0.05\n",
      "epoch 17, loss 0.86458, train_acc 0.7044, valid_acc 0.6914, Time 00:00:31,lr 0.05\n",
      "epoch 18, loss 0.86462, train_acc 0.7072, valid_acc 0.7017, Time 00:00:31,lr 0.05\n",
      "epoch 19, loss 0.86589, train_acc 0.7067, valid_acc 0.7007, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.87160, train_acc 0.7042, valid_acc 0.7153, Time 00:00:31,lr 0.05\n",
      "epoch 21, loss 0.86976, train_acc 0.7027, valid_acc 0.6829, Time 00:00:31,lr 0.05\n",
      "epoch 22, loss 0.85921, train_acc 0.7075, valid_acc 0.7036, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.86768, train_acc 0.7058, valid_acc 0.7057, Time 00:00:31,lr 0.05\n",
      "epoch 24, loss 0.86215, train_acc 0.7067, valid_acc 0.7118, Time 00:00:31,lr 0.05\n",
      "epoch 25, loss 0.86632, train_acc 0.7029, valid_acc 0.7084, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.87295, train_acc 0.7031, valid_acc 0.6990, Time 00:00:31,lr 0.05\n",
      "epoch 27, loss 0.87456, train_acc 0.7033, valid_acc 0.6865, Time 00:00:31,lr 0.05\n",
      "epoch 28, loss 0.86871, train_acc 0.7044, valid_acc 0.6970, Time 00:00:31,lr 0.05\n",
      "epoch 29, loss 0.86998, train_acc 0.7056, valid_acc 0.6560, Time 00:00:31,lr 0.05\n",
      "epoch 30, loss 0.59346, train_acc 0.7979, valid_acc 0.8265, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.54326, train_acc 0.8155, valid_acc 0.8227, Time 00:00:31,lr 0.01\n",
      "epoch 32, loss 0.53949, train_acc 0.8170, valid_acc 0.8028, Time 00:00:31,lr 0.01\n",
      "epoch 33, loss 0.54204, train_acc 0.8157, valid_acc 0.8154, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.53898, train_acc 0.8156, valid_acc 0.8274, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.54012, train_acc 0.8164, valid_acc 0.8242, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.54208, train_acc 0.8150, valid_acc 0.8189, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.53263, train_acc 0.8171, valid_acc 0.8306, Time 00:00:35,lr 0.01\n",
      "epoch 38, loss 0.53156, train_acc 0.8190, valid_acc 0.8076, Time 00:00:33,lr 0.01\n",
      "epoch 39, loss 0.52680, train_acc 0.8204, valid_acc 0.8144, Time 00:00:33,lr 0.01\n",
      "epoch 40, loss 0.52358, train_acc 0.8228, valid_acc 0.7965, Time 00:00:33,lr 0.01\n",
      "epoch 41, loss 0.52257, train_acc 0.8211, valid_acc 0.8152, Time 00:00:34,lr 0.01\n",
      "epoch 42, loss 0.52220, train_acc 0.8232, valid_acc 0.8375, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.51315, train_acc 0.8264, valid_acc 0.8182, Time 00:00:31,lr 0.01\n",
      "epoch 44, loss 0.51323, train_acc 0.8268, valid_acc 0.8144, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.51435, train_acc 0.8240, valid_acc 0.8080, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.51004, train_acc 0.8262, valid_acc 0.8189, Time 00:00:31,lr 0.01\n",
      "epoch 47, loss 0.50912, train_acc 0.8285, valid_acc 0.8285, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.51028, train_acc 0.8259, valid_acc 0.8171, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.50687, train_acc 0.8291, valid_acc 0.8301, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.50086, train_acc 0.8297, valid_acc 0.8241, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.49983, train_acc 0.8290, valid_acc 0.8161, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.50052, train_acc 0.8278, valid_acc 0.8267, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.49692, train_acc 0.8309, valid_acc 0.8120, Time 00:00:31,lr 0.01\n",
      "epoch 54, loss 0.50006, train_acc 0.8279, valid_acc 0.8300, Time 00:00:31,lr 0.01\n",
      "epoch 55, loss 0.49913, train_acc 0.8295, valid_acc 0.7949, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.49605, train_acc 0.8320, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.49692, train_acc 0.8300, valid_acc 0.8328, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.49206, train_acc 0.8337, valid_acc 0.8387, Time 00:00:31,lr 0.01\n",
      "epoch 59, loss 0.49343, train_acc 0.8311, valid_acc 0.8150, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.34705, train_acc 0.8820, valid_acc 0.8809, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.30127, train_acc 0.8983, valid_acc 0.8846, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.28316, train_acc 0.9037, valid_acc 0.8870, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.27436, train_acc 0.9062, valid_acc 0.8866, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.26887, train_acc 0.9072, valid_acc 0.8916, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.26500, train_acc 0.9096, valid_acc 0.8894, Time 00:00:31,lr 0.002\n",
      "epoch 66, loss 0.26247, train_acc 0.9101, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 67, loss 0.26025, train_acc 0.9112, valid_acc 0.8920, Time 00:00:32,lr 0.002\n",
      "epoch 68, loss 0.25301, train_acc 0.9135, valid_acc 0.8888, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.25118, train_acc 0.9139, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.25089, train_acc 0.9137, valid_acc 0.8884, Time 00:00:32,lr 0.002\n",
      "epoch 71, loss 0.25031, train_acc 0.9147, valid_acc 0.8795, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.25265, train_acc 0.9130, valid_acc 0.8856, Time 00:00:33,lr 0.002\n",
      "epoch 73, loss 0.25271, train_acc 0.9129, valid_acc 0.8883, Time 00:00:31,lr 0.002\n",
      "epoch 74, loss 0.25271, train_acc 0.9128, valid_acc 0.8770, Time 00:00:32,lr 0.002\n",
      "epoch 75, loss 0.25109, train_acc 0.9137, valid_acc 0.8796, Time 00:00:32,lr 0.002\n",
      "epoch 76, loss 0.25106, train_acc 0.9132, valid_acc 0.8824, Time 00:00:32,lr 0.002\n",
      "epoch 77, loss 0.25249, train_acc 0.9136, valid_acc 0.8863, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.25310, train_acc 0.9126, valid_acc 0.8882, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.25188, train_acc 0.9132, valid_acc 0.8809, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.25087, train_acc 0.9143, valid_acc 0.8808, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.25043, train_acc 0.9133, valid_acc 0.8659, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.25028, train_acc 0.9126, valid_acc 0.8830, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.24958, train_acc 0.9140, valid_acc 0.8806, Time 00:00:31,lr 0.002\n",
      "epoch 84, loss 0.25462, train_acc 0.9128, valid_acc 0.8817, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.25416, train_acc 0.9109, valid_acc 0.8797, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.24963, train_acc 0.9134, valid_acc 0.8856, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.25088, train_acc 0.9143, valid_acc 0.8799, Time 00:00:32,lr 0.002\n",
      "epoch 88, loss 0.24692, train_acc 0.9147, valid_acc 0.8874, Time 00:00:31,lr 0.002\n",
      "epoch 89, loss 0.24720, train_acc 0.9146, valid_acc 0.8826, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.17336, train_acc 0.9407, valid_acc 0.9051, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.14650, train_acc 0.9515, valid_acc 0.9075, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.13297, train_acc 0.9564, valid_acc 0.9078, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.12877, train_acc 0.9568, valid_acc 0.9069, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.12118, train_acc 0.9599, valid_acc 0.9086, Time 00:00:32,lr 0.0004\n",
      "epoch 95, loss 0.11531, train_acc 0.9619, valid_acc 0.9090, Time 00:00:32,lr 0.0004\n",
      "epoch 96, loss 0.11811, train_acc 0.9601, valid_acc 0.9075, Time 00:00:35,lr 0.0004\n",
      "epoch 97, loss 0.11135, train_acc 0.9634, valid_acc 0.9066, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.10657, train_acc 0.9653, valid_acc 0.9044, Time 00:00:31,lr 0.0004\n",
      "epoch 99, loss 0.10713, train_acc 0.9637, valid_acc 0.9063, Time 00:00:31,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.10466, train_acc 0.9656, valid_acc 0.9050, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.09925, train_acc 0.9674, valid_acc 0.9043, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.09841, train_acc 0.9671, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.09499, train_acc 0.9692, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 104, loss 0.09518, train_acc 0.9685, valid_acc 0.9089, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.09390, train_acc 0.9696, valid_acc 0.9105, Time 00:00:31,lr 0.0004\n",
      "epoch 106, loss 0.09006, train_acc 0.9702, valid_acc 0.9070, Time 00:00:32,lr 0.0004\n",
      "epoch 107, loss 0.09035, train_acc 0.9709, valid_acc 0.9081, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.08887, train_acc 0.9702, valid_acc 0.9073, Time 00:00:33,lr 0.0004\n",
      "epoch 109, loss 0.08896, train_acc 0.9715, valid_acc 0.9060, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.08627, train_acc 0.9710, valid_acc 0.9103, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.08818, train_acc 0.9706, valid_acc 0.9070, Time 00:00:31,lr 0.0004\n",
      "epoch 112, loss 0.08570, train_acc 0.9718, valid_acc 0.9063, Time 00:00:33,lr 0.0004\n",
      "epoch 113, loss 0.08473, train_acc 0.9719, valid_acc 0.9086, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.08530, train_acc 0.9715, valid_acc 0.9090, Time 00:00:33,lr 0.0004\n",
      "epoch 115, loss 0.08070, train_acc 0.9733, valid_acc 0.9075, Time 00:00:33,lr 0.0004\n",
      "epoch 116, loss 0.07957, train_acc 0.9737, valid_acc 0.9041, Time 00:00:32,lr 0.0004\n",
      "epoch 117, loss 0.07966, train_acc 0.9733, valid_acc 0.9027, Time 00:00:32,lr 0.0004\n",
      "epoch 118, loss 0.08058, train_acc 0.9734, valid_acc 0.9058, Time 00:00:32,lr 0.0004\n",
      "epoch 119, loss 0.08208, train_acc 0.9728, valid_acc 0.9074, Time 00:00:35,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 I wanto find out acc impove may from cancel downsample, so use k=7 but not downsmaple and k=3 but downsample to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:41.147882Z",
     "start_time": "2018-03-04T09:17:41.140250Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k7p3s1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=7, strides=1, padding=3, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:17:41.976Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.30376, train_acc 0.2260, valid_acc 0.3361, Time 00:00:31,lr 0.1\n",
      "epoch 1, loss 1.68781, train_acc 0.3729, valid_acc 0.4189, Time 00:00:35,lr 0.1\n",
      "epoch 2, loss 1.52445, train_acc 0.4432, valid_acc 0.5236, Time 00:00:32,lr 0.1\n",
      "epoch 3, loss 1.37861, train_acc 0.5034, valid_acc 0.5116, Time 00:00:32,lr 0.1\n",
      "epoch 4, loss 1.29522, train_acc 0.5379, valid_acc 0.5644, Time 00:00:32,lr 0.1\n",
      "epoch 5, loss 1.23774, train_acc 0.5634, valid_acc 0.5271, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.18491, train_acc 0.5845, valid_acc 0.6008, Time 00:00:32,lr 0.1\n",
      "epoch 7, loss 1.13172, train_acc 0.6043, valid_acc 0.6084, Time 00:00:32,lr 0.1\n",
      "epoch 8, loss 1.10778, train_acc 0.6163, valid_acc 0.6573, Time 00:00:33,lr 0.1\n",
      "epoch 9, loss 1.06816, train_acc 0.6311, valid_acc 0.6118, Time 00:00:32,lr 0.1\n",
      "epoch 10, loss 1.03542, train_acc 0.6430, valid_acc 0.6466, Time 00:00:32,lr 0.1\n",
      "epoch 11, loss 1.02805, train_acc 0.6471, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 1.01376, train_acc 0.6525, valid_acc 0.6565, Time 00:00:33,lr 0.1\n",
      "epoch 13, loss 1.00736, train_acc 0.6535, valid_acc 0.6435, Time 00:00:32,lr 0.1\n",
      "epoch 14, loss 0.99958, train_acc 0.6599, valid_acc 0.6482, Time 00:00:32,lr 0.1\n",
      "epoch 15, loss 0.98714, train_acc 0.6617, valid_acc 0.6799, Time 00:00:32,lr 0.1\n",
      "epoch 16, loss 0.97678, train_acc 0.6662, valid_acc 0.6960, Time 00:00:32,lr 0.1\n",
      "epoch 17, loss 0.97524, train_acc 0.6663, valid_acc 0.6830, Time 00:00:32,lr 0.1\n",
      "epoch 18, loss 0.97347, train_acc 0.6669, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 19, loss 0.97192, train_acc 0.6669, valid_acc 0.6242, Time 00:00:32,lr 0.1\n",
      "epoch 20, loss 0.97201, train_acc 0.6679, valid_acc 0.6656, Time 00:00:32,lr 0.1\n",
      "epoch 21, loss 0.96170, train_acc 0.6740, valid_acc 0.6367, Time 00:00:33,lr 0.1\n",
      "epoch 22, loss 0.96810, train_acc 0.6708, valid_acc 0.6185, Time 00:00:32,lr 0.1\n",
      "epoch 23, loss 0.95803, train_acc 0.6731, valid_acc 0.6805, Time 00:00:33,lr 0.1\n",
      "epoch 24, loss 0.95777, train_acc 0.6729, valid_acc 0.6490, Time 00:00:32,lr 0.1\n",
      "epoch 25, loss 0.96024, train_acc 0.6753, valid_acc 0.6834, Time 00:00:32,lr 0.1\n",
      "epoch 26, loss 0.95364, train_acc 0.6740, valid_acc 0.6864, Time 00:00:32,lr 0.1\n",
      "epoch 27, loss 0.95158, train_acc 0.6774, valid_acc 0.6646, Time 00:00:32,lr 0.1\n",
      "epoch 28, loss 0.94868, train_acc 0.6751, valid_acc 0.6987, Time 00:00:32,lr 0.1\n",
      "epoch 29, loss 0.95404, train_acc 0.6749, valid_acc 0.6318, Time 00:00:32,lr 0.1\n",
      "epoch 30, loss 0.95379, train_acc 0.6741, valid_acc 0.6917, Time 00:00:32,lr 0.1\n",
      "epoch 31, loss 0.94801, train_acc 0.6744, valid_acc 0.6283, Time 00:00:32,lr 0.1\n",
      "epoch 32, loss 0.95098, train_acc 0.6751, valid_acc 0.6732, Time 00:00:33,lr 0.1\n",
      "epoch 33, loss 0.94359, train_acc 0.6791, valid_acc 0.6295, Time 00:00:34,lr 0.1\n",
      "epoch 34, loss 0.94709, train_acc 0.6774, valid_acc 0.6200, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.94835, train_acc 0.6774, valid_acc 0.6354, Time 00:00:32,lr 0.1\n",
      "epoch 36, loss 0.94460, train_acc 0.6784, valid_acc 0.6478, Time 00:00:32,lr 0.1\n",
      "epoch 37, loss 0.94238, train_acc 0.6794, valid_acc 0.6423, Time 00:00:32,lr 0.1\n",
      "epoch 38, loss 0.94366, train_acc 0.6784, valid_acc 0.5573, Time 00:00:32,lr 0.1\n",
      "epoch 39, loss 0.93595, train_acc 0.6804, valid_acc 0.6669, Time 00:00:32,lr 0.1\n",
      "epoch 40, loss 0.94138, train_acc 0.6788, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 41, loss 0.94039, train_acc 0.6797, valid_acc 0.6339, Time 00:00:34,lr 0.1\n",
      "epoch 42, loss 0.93921, train_acc 0.6797, valid_acc 0.6167, Time 00:00:32,lr 0.1\n",
      "epoch 43, loss 0.93964, train_acc 0.6790, valid_acc 0.6928, Time 00:00:32,lr 0.1\n",
      "epoch 44, loss 0.93891, train_acc 0.6801, valid_acc 0.6485, Time 00:00:32,lr 0.1\n",
      "epoch 45, loss 0.93846, train_acc 0.6814, valid_acc 0.5983, Time 00:00:32,lr 0.1\n",
      "epoch 46, loss 0.93314, train_acc 0.6822, valid_acc 0.6216, Time 00:00:32,lr 0.1\n",
      "epoch 47, loss 0.93256, train_acc 0.6823, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 48, loss 0.94286, train_acc 0.6781, valid_acc 0.6123, Time 00:00:32,lr 0.1\n",
      "epoch 49, loss 0.93333, train_acc 0.6848, valid_acc 0.6390, Time 00:00:32,lr 0.1\n",
      "epoch 50, loss 0.93153, train_acc 0.6829, valid_acc 0.6682, Time 00:00:32,lr 0.1\n",
      "epoch 51, loss 0.93787, train_acc 0.6817, valid_acc 0.7057, Time 00:00:32,lr 0.1\n",
      "epoch 52, loss 0.93450, train_acc 0.6823, valid_acc 0.6080, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.93795, train_acc 0.6822, valid_acc 0.6733, Time 00:00:32,lr 0.1\n",
      "epoch 54, loss 0.92976, train_acc 0.6836, valid_acc 0.6498, Time 00:00:32,lr 0.1\n",
      "epoch 55, loss 0.93671, train_acc 0.6794, valid_acc 0.6334, Time 00:00:32,lr 0.1\n",
      "epoch 56, loss 0.93371, train_acc 0.6807, valid_acc 0.6849, Time 00:00:32,lr 0.1\n",
      "epoch 57, loss 0.93283, train_acc 0.6830, valid_acc 0.6737, Time 00:00:32,lr 0.1\n",
      "epoch 58, loss 0.94008, train_acc 0.6813, valid_acc 0.6681, Time 00:00:32,lr 0.1\n",
      "epoch 59, loss 0.92572, train_acc 0.6860, valid_acc 0.6837, Time 00:00:32,lr 0.1\n",
      "epoch 60, loss 0.93572, train_acc 0.6810, valid_acc 0.6855, Time 00:00:32,lr 0.1\n",
      "epoch 61, loss 0.92939, train_acc 0.6856, valid_acc 0.6800, Time 00:00:32,lr 0.1\n",
      "epoch 62, loss 0.92851, train_acc 0.6848, valid_acc 0.6933, Time 00:00:32,lr 0.1\n",
      "epoch 63, loss 0.93826, train_acc 0.6798, valid_acc 0.5912, Time 00:00:32,lr 0.1\n",
      "epoch 64, loss 0.92919, train_acc 0.6859, valid_acc 0.6371, Time 00:00:32,lr 0.1\n",
      "epoch 65, loss 0.92681, train_acc 0.6829, valid_acc 0.6720, Time 00:00:32,lr 0.1\n",
      "epoch 66, loss 0.92689, train_acc 0.6861, valid_acc 0.6765, Time 00:00:32,lr 0.1\n",
      "epoch 67, loss 0.92941, train_acc 0.6836, valid_acc 0.6244, Time 00:00:32,lr 0.1\n",
      "epoch 68, loss 0.92887, train_acc 0.6831, valid_acc 0.6602, Time 00:00:32,lr 0.1\n",
      "epoch 69, loss 0.93088, train_acc 0.6826, valid_acc 0.6989, Time 00:00:32,lr 0.1\n",
      "epoch 70, loss 0.92295, train_acc 0.6847, valid_acc 0.7020, Time 00:00:32,lr 0.1\n",
      "epoch 71, loss 0.92583, train_acc 0.6863, valid_acc 0.6810, Time 00:00:32,lr 0.1\n",
      "epoch 72, loss 0.92959, train_acc 0.6839, valid_acc 0.6843, Time 00:00:32,lr 0.1\n",
      "epoch 73, loss 0.93152, train_acc 0.6827, valid_acc 0.6619, Time 00:00:32,lr 0.1\n",
      "epoch 74, loss 0.92909, train_acc 0.6839, valid_acc 0.6328, Time 00:00:32,lr 0.1\n",
      "epoch 75, loss 0.92891, train_acc 0.6839, valid_acc 0.6598, Time 00:00:32,lr 0.1\n",
      "epoch 76, loss 0.93157, train_acc 0.6843, valid_acc 0.6716, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.93235, train_acc 0.6823, valid_acc 0.6870, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.92986, train_acc 0.6865, valid_acc 0.6388, Time 00:00:32,lr 0.1\n",
      "epoch 79, loss 0.92761, train_acc 0.6849, valid_acc 0.6662, Time 00:00:32,lr 0.1\n",
      "epoch 0, loss 0.80896, train_acc 0.7271, valid_acc 0.6203, Time 00:00:30,lr 0.05\n",
      "epoch 1, loss 0.88513, train_acc 0.6974, valid_acc 0.6184, Time 00:00:32,lr 0.05\n",
      "epoch 2, loss 0.90256, train_acc 0.6931, valid_acc 0.7099, Time 00:00:32,lr 0.05\n",
      "epoch 3, loss 0.90922, train_acc 0.6893, valid_acc 0.6401, Time 00:00:32,lr 0.05\n",
      "epoch 4, loss 0.91353, train_acc 0.6878, valid_acc 0.6681, Time 00:00:32,lr 0.05\n",
      "epoch 5, loss 0.90272, train_acc 0.6925, valid_acc 0.6439, Time 00:00:32,lr 0.05\n",
      "epoch 6, loss 0.91182, train_acc 0.6895, valid_acc 0.7033, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.91077, train_acc 0.6885, valid_acc 0.6915, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.91324, train_acc 0.6878, valid_acc 0.6665, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.91369, train_acc 0.6868, valid_acc 0.6593, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.91246, train_acc 0.6879, valid_acc 0.6637, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.91111, train_acc 0.6916, valid_acc 0.7026, Time 00:00:32,lr 0.05\n",
      "epoch 12, loss 0.91954, train_acc 0.6874, valid_acc 0.6634, Time 00:00:32,lr 0.05\n",
      "epoch 13, loss 0.91728, train_acc 0.6863, valid_acc 0.6654, Time 00:00:32,lr 0.05\n",
      "epoch 14, loss 0.91430, train_acc 0.6878, valid_acc 0.6514, Time 00:00:32,lr 0.05\n",
      "epoch 15, loss 0.91699, train_acc 0.6876, valid_acc 0.6709, Time 00:00:32,lr 0.05\n",
      "epoch 16, loss 0.91586, train_acc 0.6865, valid_acc 0.6684, Time 00:00:32,lr 0.05\n",
      "epoch 17, loss 0.90909, train_acc 0.6914, valid_acc 0.6667, Time 00:00:32,lr 0.05\n",
      "epoch 18, loss 0.90777, train_acc 0.6873, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 19, loss 0.91364, train_acc 0.6880, valid_acc 0.6613, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.91116, train_acc 0.6876, valid_acc 0.6495, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.90711, train_acc 0.6921, valid_acc 0.6621, Time 00:00:32,lr 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss 0.91377, train_acc 0.6874, valid_acc 0.7030, Time 00:00:32,lr 0.05\n",
      "epoch 23, loss 0.91145, train_acc 0.6884, valid_acc 0.6387, Time 00:00:32,lr 0.05\n",
      "epoch 24, loss 0.91180, train_acc 0.6891, valid_acc 0.7214, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.91215, train_acc 0.6880, valid_acc 0.6815, Time 00:00:32,lr 0.05\n",
      "epoch 26, loss 0.90945, train_acc 0.6892, valid_acc 0.6991, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.91134, train_acc 0.6903, valid_acc 0.6897, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.91023, train_acc 0.6889, valid_acc 0.6460, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.90890, train_acc 0.6892, valid_acc 0.6105, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.62314, train_acc 0.7879, valid_acc 0.8119, Time 00:00:32,lr 0.01\n",
      "epoch 31, loss 0.57048, train_acc 0.8056, valid_acc 0.8057, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.56539, train_acc 0.8086, valid_acc 0.8094, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.56050, train_acc 0.8097, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.56481, train_acc 0.8090, valid_acc 0.7986, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.56655, train_acc 0.8061, valid_acc 0.8173, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.55780, train_acc 0.8100, valid_acc 0.7797, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.55701, train_acc 0.8100, valid_acc 0.8271, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.55273, train_acc 0.8132, valid_acc 0.8075, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.55390, train_acc 0.8106, valid_acc 0.8073, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.54441, train_acc 0.8154, valid_acc 0.8010, Time 00:00:32,lr 0.01\n",
      "epoch 41, loss 0.54516, train_acc 0.8139, valid_acc 0.8111, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.53969, train_acc 0.8148, valid_acc 0.8155, Time 00:00:32,lr 0.01\n",
      "epoch 43, loss 0.53975, train_acc 0.8157, valid_acc 0.8236, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.53627, train_acc 0.8176, valid_acc 0.8247, Time 00:00:32,lr 0.01\n",
      "epoch 45, loss 0.53035, train_acc 0.8196, valid_acc 0.8118, Time 00:00:32,lr 0.01\n",
      "epoch 46, loss 0.53219, train_acc 0.8190, valid_acc 0.8203, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.53024, train_acc 0.8185, valid_acc 0.8109, Time 00:00:32,lr 0.01\n",
      "epoch 48, loss 0.52706, train_acc 0.8210, valid_acc 0.8323, Time 00:00:32,lr 0.01\n",
      "epoch 49, loss 0.52530, train_acc 0.8220, valid_acc 0.7924, Time 00:00:32,lr 0.01\n",
      "epoch 50, loss 0.52793, train_acc 0.8215, valid_acc 0.8195, Time 00:00:32,lr 0.01\n",
      "epoch 51, loss 0.52560, train_acc 0.8216, valid_acc 0.8287, Time 00:00:32,lr 0.01\n",
      "epoch 52, loss 0.51781, train_acc 0.8240, valid_acc 0.8058, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.51830, train_acc 0.8228, valid_acc 0.8306, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.51420, train_acc 0.8254, valid_acc 0.8258, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.51919, train_acc 0.8242, valid_acc 0.8067, Time 00:00:32,lr 0.01\n",
      "epoch 56, loss 0.51772, train_acc 0.8231, valid_acc 0.8133, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.50818, train_acc 0.8279, valid_acc 0.8285, Time 00:00:32,lr 0.01\n",
      "epoch 58, loss 0.51014, train_acc 0.8252, valid_acc 0.8101, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.51187, train_acc 0.8270, valid_acc 0.8182, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.35304, train_acc 0.8818, valid_acc 0.8777, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.31041, train_acc 0.8950, valid_acc 0.8848, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.29029, train_acc 0.9032, valid_acc 0.8857, Time 00:00:32,lr 0.002\n",
      "epoch 63, loss 0.28051, train_acc 0.9045, valid_acc 0.8852, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.27056, train_acc 0.9079, valid_acc 0.8854, Time 00:00:32,lr 0.002\n",
      "epoch 65, loss 0.26793, train_acc 0.9099, valid_acc 0.8821, Time 00:00:33,lr 0.002\n",
      "epoch 66, loss 0.26292, train_acc 0.9110, valid_acc 0.8788, Time 00:00:35,lr 0.002\n",
      "epoch 67, loss 0.25597, train_acc 0.9123, valid_acc 0.8886, Time 00:00:33,lr 0.002\n",
      "epoch 68, loss 0.25621, train_acc 0.9120, valid_acc 0.8809, Time 00:00:34,lr 0.002\n",
      "epoch 69, loss 0.25495, train_acc 0.9139, valid_acc 0.8817, Time 00:00:34,lr 0.002\n",
      "epoch 70, loss 0.25461, train_acc 0.9128, valid_acc 0.8820, Time 00:00:35,lr 0.002\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k7p3s1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k7p3s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:17:43.020Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k3p1s2(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=2, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:17:44.325Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k3p1s2()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k3p1s2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 gluon resnet18 vs my resnet 18: use max pooling vs no max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:26.079468Z",
     "start_time": "2018-03-04T06:33:26.069002Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:3])\n",
    "        net.add(*resnet.features[4:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:31:03.467325Z",
     "start_time": "2018-03-04T05:14:50.124830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.38056, train_acc 0.1657, valid_acc 0.2737, Time 00:00:56,lr 0.1\n",
      "epoch 1, loss 1.72326, train_acc 0.3529, valid_acc 0.3568, Time 00:00:57,lr 0.1\n",
      "epoch 2, loss 1.41782, train_acc 0.4814, valid_acc 0.5539, Time 00:00:56,lr 0.1\n",
      "epoch 3, loss 1.18895, train_acc 0.5769, valid_acc 0.4499, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.06284, train_acc 0.6269, valid_acc 0.6079, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 0.94214, train_acc 0.6724, valid_acc 0.6066, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.88145, train_acc 0.6946, valid_acc 0.5705, Time 00:00:57,lr 0.1\n",
      "epoch 7, loss 0.84403, train_acc 0.7099, valid_acc 0.5912, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.81196, train_acc 0.7204, valid_acc 0.7287, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.79473, train_acc 0.7280, valid_acc 0.6951, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.78191, train_acc 0.7316, valid_acc 0.6971, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.77675, train_acc 0.7329, valid_acc 0.7301, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.75665, train_acc 0.7406, valid_acc 0.6933, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.75395, train_acc 0.7421, valid_acc 0.6948, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.74696, train_acc 0.7435, valid_acc 0.7115, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.73717, train_acc 0.7483, valid_acc 0.6977, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.73225, train_acc 0.7511, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 17, loss 0.73271, train_acc 0.7495, valid_acc 0.7008, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.72847, train_acc 0.7509, valid_acc 0.6897, Time 00:00:56,lr 0.1\n",
      "epoch 19, loss 0.72694, train_acc 0.7517, valid_acc 0.5876, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.72430, train_acc 0.7523, valid_acc 0.7296, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.71836, train_acc 0.7536, valid_acc 0.6899, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.71967, train_acc 0.7531, valid_acc 0.6885, Time 00:00:56,lr 0.1\n",
      "epoch 23, loss 0.71953, train_acc 0.7555, valid_acc 0.7130, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.70939, train_acc 0.7559, valid_acc 0.6943, Time 00:00:56,lr 0.1\n",
      "epoch 25, loss 0.71292, train_acc 0.7573, valid_acc 0.7349, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.71025, train_acc 0.7568, valid_acc 0.7351, Time 00:00:56,lr 0.1\n",
      "epoch 27, loss 0.71876, train_acc 0.7551, valid_acc 0.7187, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.70902, train_acc 0.7583, valid_acc 0.7208, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.70804, train_acc 0.7554, valid_acc 0.7308, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.69986, train_acc 0.7599, valid_acc 0.7021, Time 00:00:56,lr 0.1\n",
      "epoch 31, loss 0.70725, train_acc 0.7590, valid_acc 0.7294, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.70756, train_acc 0.7607, valid_acc 0.7474, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.70504, train_acc 0.7586, valid_acc 0.7125, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.70649, train_acc 0.7594, valid_acc 0.6955, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.70067, train_acc 0.7614, valid_acc 0.7556, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.70107, train_acc 0.7616, valid_acc 0.5762, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.69916, train_acc 0.7612, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.70580, train_acc 0.7588, valid_acc 0.6723, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.69736, train_acc 0.7607, valid_acc 0.7138, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.70295, train_acc 0.7603, valid_acc 0.7250, Time 00:00:56,lr 0.1\n",
      "epoch 41, loss 0.69599, train_acc 0.7622, valid_acc 0.6537, Time 00:00:57,lr 0.1\n",
      "epoch 42, loss 0.70267, train_acc 0.7622, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 43, loss 0.70145, train_acc 0.7609, valid_acc 0.6835, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.69758, train_acc 0.7642, valid_acc 0.6921, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.69759, train_acc 0.7621, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.69861, train_acc 0.7599, valid_acc 0.6791, Time 00:00:56,lr 0.1\n",
      "epoch 47, loss 0.69245, train_acc 0.7646, valid_acc 0.7376, Time 00:00:58,lr 0.1\n",
      "epoch 48, loss 0.70034, train_acc 0.7613, valid_acc 0.6330, Time 00:01:02,lr 0.1\n",
      "epoch 49, loss 0.69684, train_acc 0.7638, valid_acc 0.7106, Time 00:00:59,lr 0.1\n",
      "epoch 50, loss 0.69907, train_acc 0.7642, valid_acc 0.7437, Time 00:01:02,lr 0.1\n",
      "epoch 51, loss 0.69313, train_acc 0.7632, valid_acc 0.7042, Time 00:01:03,lr 0.1\n",
      "epoch 52, loss 0.69796, train_acc 0.7624, valid_acc 0.6378, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.69387, train_acc 0.7634, valid_acc 0.7631, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.69179, train_acc 0.7639, valid_acc 0.6815, Time 00:00:56,lr 0.1\n",
      "epoch 55, loss 0.70082, train_acc 0.7620, valid_acc 0.7479, Time 00:00:56,lr 0.1\n",
      "epoch 56, loss 0.69588, train_acc 0.7631, valid_acc 0.7191, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.69733, train_acc 0.7639, valid_acc 0.7269, Time 00:00:57,lr 0.1\n",
      "epoch 58, loss 0.69333, train_acc 0.7659, valid_acc 0.6627, Time 00:00:56,lr 0.1\n",
      "epoch 59, loss 0.69576, train_acc 0.7629, valid_acc 0.7450, Time 00:00:56,lr 0.1\n",
      "epoch 60, loss 0.69494, train_acc 0.7634, valid_acc 0.7199, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.69445, train_acc 0.7622, valid_acc 0.7363, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.69379, train_acc 0.7620, valid_acc 0.7078, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.69461, train_acc 0.7638, valid_acc 0.7163, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.69229, train_acc 0.7644, valid_acc 0.7119, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.69231, train_acc 0.7630, valid_acc 0.7010, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.69897, train_acc 0.7628, valid_acc 0.6015, Time 00:01:00,lr 0.1\n",
      "epoch 67, loss 0.69971, train_acc 0.7618, valid_acc 0.7291, Time 00:01:00,lr 0.1\n",
      "epoch 68, loss 0.69992, train_acc 0.7627, valid_acc 0.6869, Time 00:00:59,lr 0.1\n",
      "epoch 69, loss 0.69482, train_acc 0.7612, valid_acc 0.7649, Time 00:01:03,lr 0.1\n",
      "epoch 70, loss 0.69304, train_acc 0.7629, valid_acc 0.6619, Time 00:00:57,lr 0.1\n",
      "epoch 71, loss 0.69240, train_acc 0.7654, valid_acc 0.7104, Time 00:00:56,lr 0.1\n",
      "epoch 72, loss 0.69880, train_acc 0.7608, valid_acc 0.7013, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.69195, train_acc 0.7630, valid_acc 0.6779, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.69141, train_acc 0.7646, valid_acc 0.6973, Time 00:00:59,lr 0.1\n",
      "epoch 75, loss 0.68878, train_acc 0.7666, valid_acc 0.6534, Time 00:00:59,lr 0.1\n",
      "epoch 76, loss 0.69063, train_acc 0.7644, valid_acc 0.6998, Time 00:01:00,lr 0.1\n",
      "epoch 77, loss 0.69243, train_acc 0.7637, valid_acc 0.7247, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.70374, train_acc 0.7610, valid_acc 0.6934, Time 00:00:57,lr 0.1\n",
      "epoch 79, loss 0.69154, train_acc 0.7635, valid_acc 0.7347, Time 00:01:01,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:28:52.313297Z",
     "start_time": "2018-03-04T06:33:29.684980Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.58195, train_acc 0.8029, valid_acc 0.7700, Time 00:00:52,lr 0.05\n",
      "epoch 1, loss 0.65821, train_acc 0.7764, valid_acc 0.7590, Time 00:00:55,lr 0.05\n",
      "epoch 2, loss 0.68045, train_acc 0.7693, valid_acc 0.4827, Time 00:00:56,lr 0.05\n",
      "epoch 3, loss 0.68091, train_acc 0.7667, valid_acc 0.7464, Time 00:00:56,lr 0.05\n",
      "epoch 4, loss 0.68455, train_acc 0.7671, valid_acc 0.7237, Time 00:00:56,lr 0.05\n",
      "epoch 5, loss 0.68415, train_acc 0.7694, valid_acc 0.7485, Time 00:00:57,lr 0.05\n",
      "epoch 6, loss 0.68346, train_acc 0.7678, valid_acc 0.5743, Time 00:00:58,lr 0.05\n",
      "epoch 7, loss 0.68119, train_acc 0.7682, valid_acc 0.6967, Time 00:00:56,lr 0.05\n",
      "epoch 8, loss 0.68399, train_acc 0.7670, valid_acc 0.6504, Time 00:00:58,lr 0.05\n",
      "epoch 9, loss 0.68451, train_acc 0.7676, valid_acc 0.6430, Time 00:00:57,lr 0.05\n",
      "epoch 10, loss 0.68556, train_acc 0.7673, valid_acc 0.6783, Time 00:00:59,lr 0.05\n",
      "epoch 11, loss 0.68058, train_acc 0.7669, valid_acc 0.6660, Time 00:00:57,lr 0.05\n",
      "epoch 12, loss 0.69183, train_acc 0.7643, valid_acc 0.6486, Time 00:00:57,lr 0.05\n",
      "epoch 13, loss 0.68374, train_acc 0.7672, valid_acc 0.7040, Time 00:00:57,lr 0.05\n",
      "epoch 14, loss 0.68587, train_acc 0.7647, valid_acc 0.7557, Time 00:00:58,lr 0.05\n",
      "epoch 15, loss 0.68559, train_acc 0.7704, valid_acc 0.6823, Time 00:00:57,lr 0.05\n",
      "epoch 16, loss 0.68928, train_acc 0.7640, valid_acc 0.6444, Time 00:00:59,lr 0.05\n",
      "epoch 17, loss 0.68680, train_acc 0.7662, valid_acc 0.5039, Time 00:00:58,lr 0.05\n",
      "epoch 18, loss 0.68851, train_acc 0.7660, valid_acc 0.6354, Time 00:01:01,lr 0.05\n",
      "epoch 19, loss 0.68628, train_acc 0.7667, valid_acc 0.6202, Time 00:00:59,lr 0.05\n",
      "epoch 20, loss 0.69103, train_acc 0.7665, valid_acc 0.7290, Time 00:01:01,lr 0.05\n",
      "epoch 21, loss 0.68566, train_acc 0.7666, valid_acc 0.6471, Time 00:00:59,lr 0.05\n",
      "epoch 22, loss 0.69282, train_acc 0.7654, valid_acc 0.7272, Time 00:01:01,lr 0.05\n",
      "epoch 23, loss 0.68336, train_acc 0.7706, valid_acc 0.6115, Time 00:01:02,lr 0.05\n",
      "epoch 24, loss 0.68217, train_acc 0.7695, valid_acc 0.7322, Time 00:01:00,lr 0.05\n",
      "epoch 25, loss 0.68338, train_acc 0.7686, valid_acc 0.7349, Time 00:01:01,lr 0.05\n",
      "epoch 26, loss 0.68748, train_acc 0.7667, valid_acc 0.7034, Time 00:01:04,lr 0.05\n",
      "epoch 27, loss 0.69171, train_acc 0.7651, valid_acc 0.7070, Time 00:00:59,lr 0.05\n",
      "epoch 28, loss 0.68284, train_acc 0.7680, valid_acc 0.7419, Time 00:00:57,lr 0.05\n",
      "epoch 29, loss 0.68182, train_acc 0.7672, valid_acc 0.6750, Time 00:00:57,lr 0.05\n",
      "epoch 30, loss 0.43710, train_acc 0.8514, valid_acc 0.8458, Time 00:00:56,lr 0.01\n",
      "epoch 31, loss 0.38423, train_acc 0.8686, valid_acc 0.8636, Time 00:00:56,lr 0.01\n",
      "epoch 32, loss 0.38351, train_acc 0.8686, valid_acc 0.8521, Time 00:01:01,lr 0.01\n",
      "epoch 33, loss 0.38735, train_acc 0.8679, valid_acc 0.8384, Time 00:01:00,lr 0.01\n",
      "epoch 34, loss 0.38082, train_acc 0.8711, valid_acc 0.8641, Time 00:00:56,lr 0.01\n",
      "epoch 35, loss 0.38738, train_acc 0.8672, valid_acc 0.8248, Time 00:00:55,lr 0.01\n",
      "epoch 36, loss 0.38062, train_acc 0.8709, valid_acc 0.8618, Time 00:00:55,lr 0.01\n",
      "epoch 37, loss 0.38049, train_acc 0.8718, valid_acc 0.8291, Time 00:00:57,lr 0.01\n",
      "epoch 38, loss 0.37551, train_acc 0.8724, valid_acc 0.8629, Time 00:00:58,lr 0.01\n",
      "epoch 39, loss 0.37463, train_acc 0.8717, valid_acc 0.8566, Time 00:01:00,lr 0.01\n",
      "epoch 40, loss 0.37176, train_acc 0.8730, valid_acc 0.8559, Time 00:01:00,lr 0.01\n",
      "epoch 41, loss 0.36386, train_acc 0.8775, valid_acc 0.8362, Time 00:00:56,lr 0.01\n",
      "epoch 42, loss 0.36582, train_acc 0.8751, valid_acc 0.8740, Time 00:00:56,lr 0.01\n",
      "epoch 43, loss 0.36425, train_acc 0.8759, valid_acc 0.8561, Time 00:00:56,lr 0.01\n",
      "epoch 44, loss 0.35750, train_acc 0.8785, valid_acc 0.8442, Time 00:00:56,lr 0.01\n",
      "epoch 45, loss 0.35708, train_acc 0.8781, valid_acc 0.8600, Time 00:00:57,lr 0.01\n",
      "epoch 46, loss 0.35498, train_acc 0.8784, valid_acc 0.8639, Time 00:00:56,lr 0.01\n",
      "epoch 47, loss 0.35540, train_acc 0.8789, valid_acc 0.8602, Time 00:00:56,lr 0.01\n",
      "epoch 48, loss 0.35173, train_acc 0.8803, valid_acc 0.8466, Time 00:00:56,lr 0.01\n",
      "epoch 49, loss 0.35126, train_acc 0.8813, valid_acc 0.8660, Time 00:00:56,lr 0.01\n",
      "epoch 50, loss 0.34843, train_acc 0.8821, valid_acc 0.8473, Time 00:00:56,lr 0.01\n",
      "epoch 51, loss 0.34383, train_acc 0.8816, valid_acc 0.8726, Time 00:00:57,lr 0.01\n",
      "epoch 52, loss 0.34999, train_acc 0.8809, valid_acc 0.8602, Time 00:01:01,lr 0.01\n",
      "epoch 53, loss 0.34417, train_acc 0.8835, valid_acc 0.8419, Time 00:00:56,lr 0.01\n",
      "epoch 54, loss 0.34483, train_acc 0.8828, valid_acc 0.8654, Time 00:00:56,lr 0.01\n",
      "epoch 55, loss 0.34106, train_acc 0.8856, valid_acc 0.8673, Time 00:00:56,lr 0.01\n",
      "epoch 56, loss 0.34146, train_acc 0.8839, valid_acc 0.8758, Time 00:01:01,lr 0.01\n",
      "epoch 57, loss 0.34200, train_acc 0.8854, valid_acc 0.8456, Time 00:00:58,lr 0.01\n",
      "epoch 58, loss 0.34399, train_acc 0.8828, valid_acc 0.8639, Time 00:00:58,lr 0.01\n",
      "epoch 59, loss 0.34053, train_acc 0.8847, valid_acc 0.8597, Time 00:00:58,lr 0.01\n",
      "epoch 60, loss 0.20173, train_acc 0.9333, valid_acc 0.9198, Time 00:00:56,lr 0.002\n",
      "epoch 61, loss 0.15869, train_acc 0.9455, valid_acc 0.9215, Time 00:00:56,lr 0.002\n",
      "epoch 62, loss 0.14265, train_acc 0.9519, valid_acc 0.9222, Time 00:00:56,lr 0.002\n",
      "epoch 63, loss 0.13336, train_acc 0.9559, valid_acc 0.9245, Time 00:00:58,lr 0.002\n",
      "epoch 64, loss 0.12715, train_acc 0.9568, valid_acc 0.9178, Time 00:00:56,lr 0.002\n",
      "epoch 65, loss 0.12213, train_acc 0.9591, valid_acc 0.9210, Time 00:00:58,lr 0.002\n",
      "epoch 66, loss 0.11907, train_acc 0.9592, valid_acc 0.9224, Time 00:00:57,lr 0.002\n",
      "epoch 67, loss 0.11596, train_acc 0.9613, valid_acc 0.9201, Time 00:00:56,lr 0.002\n",
      "epoch 68, loss 0.11954, train_acc 0.9596, valid_acc 0.9202, Time 00:00:56,lr 0.002\n",
      "epoch 69, loss 0.11487, train_acc 0.9614, valid_acc 0.9212, Time 00:00:56,lr 0.002\n",
      "epoch 70, loss 0.11803, train_acc 0.9598, valid_acc 0.9105, Time 00:00:56,lr 0.002\n",
      "epoch 71, loss 0.11419, train_acc 0.9616, valid_acc 0.9124, Time 00:00:56,lr 0.002\n",
      "epoch 72, loss 0.11577, train_acc 0.9608, valid_acc 0.9108, Time 00:00:56,lr 0.002\n",
      "epoch 73, loss 0.11624, train_acc 0.9604, valid_acc 0.9160, Time 00:00:56,lr 0.002\n",
      "epoch 74, loss 0.11776, train_acc 0.9603, valid_acc 0.9185, Time 00:00:56,lr 0.002\n",
      "epoch 75, loss 0.11450, train_acc 0.9622, valid_acc 0.9117, Time 00:00:56,lr 0.002\n",
      "epoch 76, loss 0.11900, train_acc 0.9613, valid_acc 0.9097, Time 00:01:00,lr 0.002\n",
      "epoch 77, loss 0.12027, train_acc 0.9592, valid_acc 0.9142, Time 00:00:57,lr 0.002\n",
      "epoch 78, loss 0.12142, train_acc 0.9593, valid_acc 0.9121, Time 00:00:56,lr 0.002\n",
      "epoch 79, loss 0.12121, train_acc 0.9592, valid_acc 0.9147, Time 00:00:57,lr 0.002\n",
      "epoch 80, loss 0.12328, train_acc 0.9580, valid_acc 0.9057, Time 00:00:57,lr 0.002\n",
      "epoch 81, loss 0.12675, train_acc 0.9566, valid_acc 0.9083, Time 00:00:56,lr 0.002\n",
      "epoch 82, loss 0.11973, train_acc 0.9583, valid_acc 0.9099, Time 00:00:58,lr 0.002\n",
      "epoch 83, loss 0.12241, train_acc 0.9586, valid_acc 0.9161, Time 00:00:55,lr 0.002\n",
      "epoch 84, loss 0.11883, train_acc 0.9593, valid_acc 0.9097, Time 00:00:56,lr 0.002\n",
      "epoch 85, loss 0.12814, train_acc 0.9568, valid_acc 0.9161, Time 00:00:56,lr 0.002\n",
      "epoch 86, loss 0.11971, train_acc 0.9590, valid_acc 0.9075, Time 00:00:55,lr 0.002\n",
      "epoch 87, loss 0.12493, train_acc 0.9573, valid_acc 0.9143, Time 00:00:56,lr 0.002\n",
      "epoch 88, loss 0.11748, train_acc 0.9599, valid_acc 0.9079, Time 00:01:00,lr 0.002\n",
      "epoch 89, loss 0.12178, train_acc 0.9585, valid_acc 0.9043, Time 00:00:55,lr 0.002\n",
      "epoch 90, loss 0.06359, train_acc 0.9809, valid_acc 0.9297, Time 00:00:56,lr 0.0004\n",
      "epoch 91, loss 0.04226, train_acc 0.9878, valid_acc 0.9347, Time 00:00:55,lr 0.0004\n",
      "epoch 92, loss 0.03643, train_acc 0.9901, valid_acc 0.9339, Time 00:00:56,lr 0.0004\n",
      "epoch 93, loss 0.03193, train_acc 0.9917, valid_acc 0.9344, Time 00:00:55,lr 0.0004\n",
      "epoch 94, loss 0.03020, train_acc 0.9919, valid_acc 0.9354, Time 00:00:56,lr 0.0004\n",
      "epoch 95, loss 0.02928, train_acc 0.9922, valid_acc 0.9342, Time 00:00:55,lr 0.0004\n",
      "epoch 96, loss 0.02602, train_acc 0.9928, valid_acc 0.9355, Time 00:00:55,lr 0.0004\n",
      "epoch 97, loss 0.02409, train_acc 0.9935, valid_acc 0.9350, Time 00:00:55,lr 0.0004\n",
      "epoch 98, loss 0.02386, train_acc 0.9939, valid_acc 0.9329, Time 00:00:56,lr 0.0004\n",
      "epoch 99, loss 0.01987, train_acc 0.9951, valid_acc 0.9358, Time 00:00:56,lr 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.02084, train_acc 0.9946, valid_acc 0.9337, Time 00:00:57,lr 0.0004\n",
      "epoch 101, loss 0.01851, train_acc 0.9954, valid_acc 0.9360, Time 00:00:58,lr 0.0004\n",
      "epoch 102, loss 0.01870, train_acc 0.9954, valid_acc 0.9357, Time 00:00:56,lr 0.0004\n",
      "epoch 103, loss 0.01807, train_acc 0.9956, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 104, loss 0.01849, train_acc 0.9954, valid_acc 0.9336, Time 00:00:56,lr 0.0004\n",
      "epoch 105, loss 0.01738, train_acc 0.9961, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 106, loss 0.01575, train_acc 0.9961, valid_acc 0.9359, Time 00:01:01,lr 0.0004\n",
      "epoch 107, loss 0.01618, train_acc 0.9963, valid_acc 0.9350, Time 00:00:56,lr 0.0004\n",
      "epoch 108, loss 0.01708, train_acc 0.9954, valid_acc 0.9334, Time 00:00:56,lr 0.0004\n",
      "epoch 109, loss 0.01544, train_acc 0.9964, valid_acc 0.9364, Time 00:00:56,lr 0.0004\n",
      "epoch 110, loss 0.01496, train_acc 0.9965, valid_acc 0.9346, Time 00:00:57,lr 0.0004\n",
      "epoch 111, loss 0.01420, train_acc 0.9966, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 112, loss 0.01472, train_acc 0.9962, valid_acc 0.9342, Time 00:00:56,lr 0.0004\n",
      "epoch 113, loss 0.01399, train_acc 0.9968, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 114, loss 0.01392, train_acc 0.9969, valid_acc 0.9344, Time 00:00:57,lr 0.0004\n",
      "epoch 115, loss 0.01281, train_acc 0.9975, valid_acc 0.9342, Time 00:00:58,lr 0.0004\n",
      "epoch 116, loss 0.01356, train_acc 0.9968, valid_acc 0.9359, Time 00:00:56,lr 0.0004\n",
      "epoch 117, loss 0.01356, train_acc 0.9971, valid_acc 0.9362, Time 00:00:58,lr 0.0004\n",
      "epoch 118, loss 0.01320, train_acc 0.9970, valid_acc 0.9353, Time 00:00:59,lr 0.0004\n",
      "epoch 119, loss 0.01289, train_acc 0.9971, valid_acc 0.9349, Time 00:01:00,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\", ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3_no_max_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 general lr policy train resnet18_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:18:01.823Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 general lr policy train resnet18_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:18:05.213Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v2_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v2(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features[0])\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[2:4])\n",
    "        net.add(*resnet.features[5:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:18:07.512Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v2_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 try delay pooling resnet arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:24:34.961Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_delay_downsample(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(5):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:24:35.841Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_delay_downsample(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_522_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. use rewrite train function to train function to make sure not train code bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.794769Z",
     "start_time": "2018-03-03T17:35:08.750443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def get_lr(e, i, step):\n",
    "    if e >= step[i]:\n",
    "        return i + 1\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, lr_decay, wd, ctx, output_file=None, step={}):\n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "        print \" # valid_acc\", valid_acc\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    step_i = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if epoch > 0:\n",
    "            if len(step.keys()) == 0:\n",
    "                if epoch % lr_period == 0:\n",
    "                    trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            else:\n",
    "                if epoch >= step.keys()[step_i]:\n",
    "                    step_i = step_i + 1\n",
    "                    trainer.set_learning_rate(step[step.keys()[step_i]])\n",
    "        \n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            \n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += utils.accuracy(output, label)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "        \n",
    "        nd.waitall()\n",
    "        \n",
    "    if output_file != sys.stdout:\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:17:26.291786Z",
     "start_time": "2018-03-03T17:35:08.796140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.100838658147\n",
      "epoch 0, loss 2.17650, train_acc 0.1815, valid_acc 0.3233, Time 00:00:29,lr 0.1\n",
      "epoch 1, loss 1.74745, train_acc 0.3639, valid_acc 0.2111, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.48485, train_acc 0.4729, valid_acc 0.5586, Time 00:00:31,lr 0.1\n",
      "epoch 3, loss 1.32675, train_acc 0.5326, valid_acc 0.5165, Time 00:00:31,lr 0.1\n",
      "epoch 4, loss 1.23933, train_acc 0.5676, valid_acc 0.4877, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 1.18796, train_acc 0.5838, valid_acc 0.5885, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.15784, train_acc 0.5958, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 1.12028, train_acc 0.6103, valid_acc 0.6090, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 1.11800, train_acc 0.6141, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 1.09796, train_acc 0.6208, valid_acc 0.6072, Time 00:00:31,lr 0.1\n",
      "epoch 10, loss 1.08225, train_acc 0.6251, valid_acc 0.5205, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 1.07237, train_acc 0.6293, valid_acc 0.5745, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 1.06935, train_acc 0.6309, valid_acc 0.6407, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 1.05872, train_acc 0.6334, valid_acc 0.5826, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 1.04889, train_acc 0.6376, valid_acc 0.6672, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 1.05632, train_acc 0.6361, valid_acc 0.6499, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 1.05060, train_acc 0.6386, valid_acc 0.6274, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 1.04430, train_acc 0.6395, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 1.04184, train_acc 0.6421, valid_acc 0.6441, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 1.04526, train_acc 0.6398, valid_acc 0.5606, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 1.04432, train_acc 0.6404, valid_acc 0.6367, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 1.03036, train_acc 0.6456, valid_acc 0.7069, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 1.03184, train_acc 0.6459, valid_acc 0.6150, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 1.03162, train_acc 0.6419, valid_acc 0.6809, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 1.03152, train_acc 0.6440, valid_acc 0.6096, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 1.02944, train_acc 0.6447, valid_acc 0.6779, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 1.02975, train_acc 0.6446, valid_acc 0.5822, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 1.02655, train_acc 0.6464, valid_acc 0.6957, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 1.02281, train_acc 0.6490, valid_acc 0.6738, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 1.02572, train_acc 0.6455, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 1.01937, train_acc 0.6497, valid_acc 0.5929, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 1.01610, train_acc 0.6499, valid_acc 0.5739, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 1.02582, train_acc 0.6454, valid_acc 0.6808, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 1.02411, train_acc 0.6468, valid_acc 0.7068, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 1.01952, train_acc 0.6492, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 35, loss 1.02947, train_acc 0.6447, valid_acc 0.6387, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 1.01511, train_acc 0.6521, valid_acc 0.5688, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 1.02304, train_acc 0.6474, valid_acc 0.6549, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 1.01642, train_acc 0.6496, valid_acc 0.6774, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 1.01767, train_acc 0.6498, valid_acc 0.5900, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 1.01177, train_acc 0.6501, valid_acc 0.6799, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 1.01933, train_acc 0.6482, valid_acc 0.6359, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 1.01835, train_acc 0.6475, valid_acc 0.6245, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 1.02311, train_acc 0.6466, valid_acc 0.6745, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 1.01084, train_acc 0.6527, valid_acc 0.6729, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 1.01739, train_acc 0.6508, valid_acc 0.6601, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 1.02528, train_acc 0.6453, valid_acc 0.6812, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 1.01437, train_acc 0.6516, valid_acc 0.6579, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 1.02278, train_acc 0.6477, valid_acc 0.6893, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 1.01775, train_acc 0.6479, valid_acc 0.7160, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 1.01944, train_acc 0.6485, valid_acc 0.6020, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 1.02941, train_acc 0.6463, valid_acc 0.6422, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 1.01799, train_acc 0.6497, valid_acc 0.7113, Time 00:00:31,lr 0.1\n",
      "epoch 53, loss 1.01612, train_acc 0.6477, valid_acc 0.6327, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 1.02105, train_acc 0.6492, valid_acc 0.6325, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 1.02410, train_acc 0.6450, valid_acc 0.6888, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 1.01827, train_acc 0.6495, valid_acc 0.6665, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 1.01920, train_acc 0.6502, valid_acc 0.6775, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 1.02319, train_acc 0.6464, valid_acc 0.6765, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 1.01978, train_acc 0.6507, valid_acc 0.5037, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 1.02047, train_acc 0.6486, valid_acc 0.6431, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 1.01323, train_acc 0.6509, valid_acc 0.6778, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 1.01677, train_acc 0.6475, valid_acc 0.6488, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 1.02140, train_acc 0.6477, valid_acc 0.6320, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 1.01649, train_acc 0.6486, valid_acc 0.6985, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 1.01495, train_acc 0.6494, valid_acc 0.6110, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 1.02517, train_acc 0.6482, valid_acc 0.6281, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 1.01754, train_acc 0.6505, valid_acc 0.6933, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 1.01817, train_acc 0.6487, valid_acc 0.5997, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 1.02060, train_acc 0.6480, valid_acc 0.6438, Time 00:00:31,lr 0.1\n",
      "epoch 70, loss 1.01663, train_acc 0.6501, valid_acc 0.6557, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 1.01802, train_acc 0.6506, valid_acc 0.6037, Time 00:00:31,lr 0.1\n",
      "epoch 72, loss 1.02473, train_acc 0.6465, valid_acc 0.6869, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 1.02214, train_acc 0.6481, valid_acc 0.6365, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 1.02075, train_acc 0.6494, valid_acc 0.6727, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 1.02465, train_acc 0.6455, valid_acc 0.6005, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 1.01342, train_acc 0.6509, valid_acc 0.6316, Time 00:00:31,lr 0.1\n",
      "epoch 77, loss 1.02116, train_acc 0.6466, valid_acc 0.6909, Time 00:00:31,lr 0.1\n",
      "epoch 78, loss 1.02252, train_acc 0.6469, valid_acc 0.7035, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 1.01862, train_acc 0.6496, valid_acc 0.7132, Time 00:00:31,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = 80\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:33.999408Z",
     "start_time": "2018-03-03T18:17:26.293466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.713158945687\n",
      "epoch 0, loss 0.89016, train_acc 0.6943, valid_acc 0.6973, Time 00:00:29,lr 0.05\n",
      "epoch 1, loss 0.97071, train_acc 0.6649, valid_acc 0.6228, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.97855, train_acc 0.6619, valid_acc 0.5962, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.98917, train_acc 0.6604, valid_acc 0.6651, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.98467, train_acc 0.6614, valid_acc 0.6855, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.98934, train_acc 0.6582, valid_acc 0.6544, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.99133, train_acc 0.6577, valid_acc 0.6867, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.99419, train_acc 0.6565, valid_acc 0.6996, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.99295, train_acc 0.6574, valid_acc 0.6707, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.98785, train_acc 0.6583, valid_acc 0.6599, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.72795, train_acc 0.7497, valid_acc 0.7739, Time 00:00:31,lr 0.01\n",
      "epoch 11, loss 0.67062, train_acc 0.7695, valid_acc 0.7807, Time 00:00:31,lr 0.01\n",
      "epoch 12, loss 0.66014, train_acc 0.7741, valid_acc 0.8119, Time 00:00:31,lr 0.01\n",
      "epoch 13, loss 0.66186, train_acc 0.7753, valid_acc 0.8148, Time 00:00:31,lr 0.01\n",
      "epoch 14, loss 0.66272, train_acc 0.7717, valid_acc 0.7799, Time 00:00:31,lr 0.01\n",
      "epoch 15, loss 0.64879, train_acc 0.7773, valid_acc 0.8168, Time 00:00:31,lr 0.01\n",
      "epoch 16, loss 0.65313, train_acc 0.7758, valid_acc 0.8103, Time 00:00:31,lr 0.01\n",
      "epoch 17, loss 0.64368, train_acc 0.7803, valid_acc 0.8027, Time 00:00:31,lr 0.01\n",
      "epoch 18, loss 0.64573, train_acc 0.7790, valid_acc 0.7865, Time 00:00:31,lr 0.01\n",
      "epoch 19, loss 0.63965, train_acc 0.7808, valid_acc 0.8113, Time 00:00:31,lr 0.01\n",
      "epoch 20, loss 0.49768, train_acc 0.8307, valid_acc 0.8597, Time 00:00:31,lr 0.002\n",
      "epoch 21, loss 0.45085, train_acc 0.8470, valid_acc 0.8730, Time 00:00:31,lr 0.002\n",
      "epoch 22, loss 0.43494, train_acc 0.8515, valid_acc 0.8760, Time 00:00:31,lr 0.002\n",
      "epoch 23, loss 0.42184, train_acc 0.8574, valid_acc 0.8801, Time 00:00:31,lr 0.002\n",
      "epoch 24, loss 0.41728, train_acc 0.8585, valid_acc 0.8719, Time 00:00:31,lr 0.002\n",
      "epoch 25, loss 0.41803, train_acc 0.8566, valid_acc 0.8713, Time 00:00:31,lr 0.002\n",
      "epoch 26, loss 0.41157, train_acc 0.8586, valid_acc 0.8734, Time 00:00:31,lr 0.002\n",
      "epoch 27, loss 0.40844, train_acc 0.8605, valid_acc 0.8761, Time 00:00:31,lr 0.002\n",
      "epoch 28, loss 0.40713, train_acc 0.8602, valid_acc 0.8781, Time 00:00:31,lr 0.002\n",
      "epoch 29, loss 0.40274, train_acc 0.8636, valid_acc 0.8810, Time 00:00:31,lr 0.002\n",
      "epoch 30, loss 0.34908, train_acc 0.8807, valid_acc 0.8912, Time 00:00:31,lr 0.0004\n",
      "epoch 31, loss 0.32949, train_acc 0.8883, valid_acc 0.8973, Time 00:00:31,lr 0.0004\n",
      "epoch 32, loss 0.31813, train_acc 0.8929, valid_acc 0.8957, Time 00:00:31,lr 0.0004\n",
      "epoch 33, loss 0.31530, train_acc 0.8933, valid_acc 0.8949, Time 00:00:31,lr 0.0004\n",
      "epoch 34, loss 0.30938, train_acc 0.8967, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 35, loss 0.30692, train_acc 0.8977, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 36, loss 0.30172, train_acc 0.8962, valid_acc 0.8979, Time 00:00:31,lr 0.0004\n",
      "epoch 37, loss 0.29754, train_acc 0.9001, valid_acc 0.8978, Time 00:00:31,lr 0.0004\n",
      "epoch 38, loss 0.29589, train_acc 0.8995, valid_acc 0.8993, Time 00:00:31,lr 0.0004\n",
      "epoch 39, loss 0.29325, train_acc 0.9014, valid_acc 0.9010, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = 10\n",
    "lr_decay=0.2\n",
    "log_file=None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug2\", ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:34.023597Z",
     "start_time": "2018-03-03T18:38:34.000689Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.save_params('../../models/resnet18_me_9_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
