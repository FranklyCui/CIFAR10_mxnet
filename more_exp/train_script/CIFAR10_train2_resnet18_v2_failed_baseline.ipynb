{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T06:32:47.406027Z",
     "start_time": "2018-03-01T06:32:42.929058Z"
    },
    "collapsed": true
   },
   "source": [
    "# 1. import needed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:00.798383Z",
     "start_time": "2018-03-04T06:32:59.370246Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.insert(0, '../../utils')\n",
    "from netlib import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data loader, data argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:00.811106Z",
     "start_time": "2018-03-04T06:33:00.799590Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data loader\n",
    "\"\"\"\n",
    "def _transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def data_loader(batch_size, transform_train, transform_test=None, num_workers=0):\n",
    "    if transform_train is None:\n",
    "        transform_train = _transform_train\n",
    "    if transform_test is None:\n",
    "        transform_test = _transform_test\n",
    "        \n",
    "    # flag=1 mean 3 channel image\n",
    "    train_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=True, transform=transform_train)\n",
    "    test_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=False, transform=transform_test)\n",
    "\n",
    "    loader = gluon.data.DataLoader\n",
    "    train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep', num_workers=num_workers)\n",
    "    test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep', num_workers=num_workers)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:00.928403Z",
     "start_time": "2018-03-04T06:33:00.812807Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data argument\n",
    "\"\"\"\n",
    "def transform_train_DA1(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def transform_train_DA2(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "    \n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "    \n",
    "\n",
    "random_clip_rate = 0.3\n",
    "def transform_train_DA3(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "#                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "#                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "        \n",
    "    if random.random() > random_clip_rate:\n",
    "        im = im.clip(0, 1)\n",
    "    _aug = image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                   std=np.array([0.2023, 0.1994, 0.2010]),)\n",
    "    im = _aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 data aurgument: mixup\n",
    "1. mixup define\n",
    "2. mixup visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 mixup: define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:01.049784Z",
     "start_time": "2018-03-04T06:33:00.931514Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mixup(x1, y1, x2, y2, alpha, num_class):\n",
    "    y1 = nd.one_hot(y1, num_class)\n",
    "    y2 = nd.one_hot(y2, num_class)\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    x = lam * x1 + (1 - lam) * x2\n",
    "    y = lam * y1 + (1 - lam) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 mixup: visulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:02.057982Z",
     "start_time": "2018-03-04T06:33:01.134946Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from time import time\n",
    "batch_size = 32\n",
    "transform_train = _transform_test#transform_train_DA1\n",
    "train_data, test_data = data_loader(batch_size, transform_train)\n",
    "mixup_alpha = 1\n",
    "\n",
    "# for x1, y1 in train_data:\n",
    "#     for x2, y2 in mixup_train_data:\n",
    "#         data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "for x, y in train_data:\n",
    "    l = x.shape[0] / 2\n",
    "    data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:02.575878Z",
     "start_time": "2018-03-04T06:33:02.059293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAADYCAYAAACwTgnaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07b7675990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'airplane'), (1, 'automobile'), (2, 'bird'), (3, 'cat'), (4, 'deer'), (5, 'dog'), (6, 'frog'), (7, 'horse'), (8, 'ship'), (9, 'truck')]\n",
      "\n",
      "[[[ 0.21255219  0.          0.          0.          0.          0.\n",
      "    0.78744781  0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.78744781\n",
      "    0.21255219  0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.21255219  0.          0.          0.\n",
      "    0.          0.          0.78744781]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.78744781\n",
      "    0.          0.          0.21255219  0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.78744781  0.21255219  0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          1.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.21255219  0.          0.\n",
      "    0.78744781  0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.21255219  0.78744781\n",
      "    0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.78744781  0.21255219\n",
      "    0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          1.          0.          0.\n",
      "    0.          0.          0.        ]]]\n",
      "<NDArray 10x1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from cifar10_utils import show_images\n",
    "%matplotlib inline\n",
    "mean=np.array([0.4914, 0.4822, 0.4465])\n",
    "std=np.array([0.2023, 0.1994, 0.2010])\n",
    "images = data[:10].transpose((0, 2, 3, 1)).asnumpy()\n",
    "images = images * std + mean\n",
    "images = images.transpose((0, 3, 1, 2)) * 255\n",
    "show_images(images)\n",
    "#show_images(data[:9], rgb_mean=mean*255, std=std*255)\n",
    "\n",
    "print [(i, l) for i, l in enumerate(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])]\n",
    "print label[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 mixup: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:02.605857Z",
     "start_time": "2018-03-04T06:33:02.576976Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "mixup_test = False\n",
    "\n",
    "if mixup_test:\n",
    "    net = ResNet164_v2(10)\n",
    "    net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    loss_f = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "    num_epochs = 1\n",
    "    learning_rate = 0.1\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    cur_time = time()\n",
    "    iters = 0\n",
    "    \"\"\"\n",
    "    data loader first time run will cost about 3x time than after run.\n",
    "    \"\"\"\n",
    "    for x1, y1 in train_data:\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time\n",
    "        iters += 1\n",
    "    print \"cost time:\", time() - cur_time\n",
    "    print\n",
    "    cur_time = time()\n",
    "\n",
    "    iters = 0\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'momentum': 0.9, 'wd': 1e-4})\n",
    "    for x, y in train_data:\n",
    "        l = x.shape[0] / 2\n",
    "        data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label.as_in_context(ctx))\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time, nd.mean(loss).asscalar()\n",
    "        iters += 1\n",
    "    print iters, time() - cur_time\n",
    "\n",
    "    # load data one by one batch\n",
    "    # iters = 0\n",
    "    # for x1, y1 in train_data:\n",
    "    #     for x2, y2 in mixup_train_data:\n",
    "    #         data, label = mixup(x1, y1, x2[:x1.shape[0]], y2[:y1.shape[0]], mixup_alpha, 10)\n",
    "    #         break\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time\n",
    "    #     iters += 1\n",
    "    # print \"cost time:\", time() - cur_time\n",
    "    # print\n",
    "    # cur_time = time()\n",
    "\n",
    "    # zip will load all datas and then iterate them, too cost memory, will drop speed when memory over.\n",
    "    # iters = 0\n",
    "    # for (x1, y1), (x2, y2) in zip(train_data, mixup_train_data):\n",
    "    #     data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time#, nd.mean(loss).asscalar()\n",
    "    #     iters += 1\n",
    "    # print time() - cur_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:02.940931Z",
     "start_time": "2018-03-04T06:33:02.849776Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def abs_mean(W):\n",
    "    return nd.mean(nd.abs(W)).asscalar()\n",
    "\n",
    "def in_list(e, l):\n",
    "    for i in l:\n",
    "        if i == e:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, \n",
    "          lr_decay, wd, ctx, w_key, output_file=None, verbose=False, loss_f=gluon.loss.SoftmaxCrossEntropyLoss(), \n",
    "          use_mixup=False, mixup_alpha=0.2):\n",
    "    def train_batch(data, label, i):\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        _loss = nd.mean(loss).asscalar()\n",
    "        if not use_mixup:\n",
    "            _acc = utils.accuracy(output, label)\n",
    "        else:\n",
    "            _acc = None\n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            print \" # iter\", i,\n",
    "            print \"loss %.5f\" % _loss, \n",
    "            if not use_mixup: print \"acc %.5f\" % _acc,\n",
    "            print \"w (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.data()),\n",
    "            print \") g (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.grad()),\n",
    "            print \")\"\n",
    "        return _loss, _acc\n",
    "            \n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        stdout = sys.stdout\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = output_file\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    if verbose:\n",
    "        print \" #\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "    \n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if in_list(epoch, lr_period):\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            \n",
    "        if not use_mixup:\n",
    "            for data, label in train_data:\n",
    "                _loss, _acc = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                train_acc += _acc\n",
    "                i += 1\n",
    "        else:\n",
    "            for x, y in train_data:\n",
    "                l = x.shape[0] / 2\n",
    "                data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "                _loss, _ = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                i += 1\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "    if output_file != stdout:\n",
    "        sys.stdout = stdout\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. get net and do EXP\n",
    "\n",
    "1. how to use imagenet rihgt?\n",
    "2. how to use strong DA? after weak DA or directly use.\n",
    "3. how to train resnet completely.\n",
    "\n",
    "4. DA1/DA2/mixup choose and finetune order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.0 resnet18_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T09:22:28.109663Z",
     "start_time": "2018-03-02T09:22:28.099740Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and add must in name_scope, or may got name error\n",
    "def get_resnet18_v2():\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v2(pretrained=True, ctx=ctx)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features)\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    output.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "batch_size = 128\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 120      # 50 * 1563 iter about\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "w_key = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T01:16:46.526458Z",
     "start_time": "2018-03-02T00:48:23.166347Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 3.19612, train_acc 0.1812, valid_acc 0.2742, Time 00:00:10,lr 0.1\n",
      "epoch 1, loss 1.87783, train_acc 0.3034, valid_acc 0.3279, Time 00:00:13,lr 0.1\n",
      "epoch 2, loss 1.72390, train_acc 0.3515, valid_acc 0.3970, Time 00:00:13,lr 0.1\n",
      "epoch 3, loss 1.58213, train_acc 0.4137, valid_acc 0.4217, Time 00:00:14,lr 0.1\n",
      "epoch 4, loss 1.45663, train_acc 0.4659, valid_acc 0.4310, Time 00:00:13,lr 0.1\n",
      "epoch 5, loss 1.37210, train_acc 0.5031, valid_acc 0.5078, Time 00:00:14,lr 0.1\n",
      "epoch 6, loss 1.30744, train_acc 0.5246, valid_acc 0.5442, Time 00:00:14,lr 0.1\n",
      "epoch 7, loss 1.24122, train_acc 0.5529, valid_acc 0.5745, Time 00:00:13,lr 0.1\n",
      "epoch 8, loss 1.17987, train_acc 0.5778, valid_acc 0.5951, Time 00:00:14,lr 0.1\n",
      "epoch 9, loss 1.13067, train_acc 0.5949, valid_acc 0.6313, Time 00:00:13,lr 0.1\n",
      "epoch 10, loss 1.08910, train_acc 0.6098, valid_acc 0.6373, Time 00:00:13,lr 0.1\n",
      "epoch 11, loss 1.01933, train_acc 0.6365, valid_acc 0.6475, Time 00:00:14,lr 0.1\n",
      "epoch 12, loss 0.98811, train_acc 0.6482, valid_acc 0.6620, Time 00:00:14,lr 0.1\n",
      "epoch 13, loss 1.01040, train_acc 0.6436, valid_acc 0.6292, Time 00:00:13,lr 0.1\n",
      "epoch 14, loss 0.95975, train_acc 0.6599, valid_acc 0.6688, Time 00:00:13,lr 0.1\n",
      "epoch 15, loss 0.92046, train_acc 0.6747, valid_acc 0.6794, Time 00:00:14,lr 0.1\n",
      "epoch 16, loss 0.89317, train_acc 0.6840, valid_acc 0.6725, Time 00:00:13,lr 0.1\n",
      "epoch 17, loss 0.84937, train_acc 0.6993, valid_acc 0.7188, Time 00:00:13,lr 0.1\n",
      "epoch 18, loss 0.82499, train_acc 0.7079, valid_acc 0.7262, Time 00:00:13,lr 0.1\n",
      "epoch 19, loss 0.80674, train_acc 0.7152, valid_acc 0.7108, Time 00:00:14,lr 0.1\n",
      "epoch 20, loss 0.78041, train_acc 0.7247, valid_acc 0.7043, Time 00:00:14,lr 0.1\n",
      "epoch 21, loss 0.76242, train_acc 0.7335, valid_acc 0.7415, Time 00:00:13,lr 0.1\n",
      "epoch 22, loss 0.75118, train_acc 0.7383, valid_acc 0.7411, Time 00:00:14,lr 0.1\n",
      "epoch 23, loss 0.74301, train_acc 0.7413, valid_acc 0.7525, Time 00:00:14,lr 0.1\n",
      "epoch 24, loss 0.72214, train_acc 0.7470, valid_acc 0.7496, Time 00:00:13,lr 0.1\n",
      "epoch 25, loss 0.70677, train_acc 0.7537, valid_acc 0.7474, Time 00:00:14,lr 0.1\n",
      "epoch 26, loss 0.69690, train_acc 0.7558, valid_acc 0.7264, Time 00:00:14,lr 0.1\n",
      "epoch 27, loss 0.67993, train_acc 0.7624, valid_acc 0.7529, Time 00:00:14,lr 0.1\n",
      "epoch 28, loss 0.66813, train_acc 0.7662, valid_acc 0.7638, Time 00:00:13,lr 0.1\n",
      "epoch 29, loss 0.65803, train_acc 0.7694, valid_acc 0.7601, Time 00:00:14,lr 0.1\n",
      "epoch 30, loss 0.64197, train_acc 0.7776, valid_acc 0.7658, Time 00:00:15,lr 0.1\n",
      "epoch 31, loss 0.64066, train_acc 0.7752, valid_acc 0.7756, Time 00:00:14,lr 0.1\n",
      "epoch 32, loss 0.63010, train_acc 0.7803, valid_acc 0.7595, Time 00:00:14,lr 0.1\n",
      "epoch 33, loss 0.62024, train_acc 0.7831, valid_acc 0.7738, Time 00:00:13,lr 0.1\n",
      "epoch 34, loss 0.61945, train_acc 0.7835, valid_acc 0.7803, Time 00:00:13,lr 0.1\n",
      "epoch 35, loss 0.60201, train_acc 0.7901, valid_acc 0.7665, Time 00:00:14,lr 0.1\n",
      "epoch 36, loss 0.59499, train_acc 0.7926, valid_acc 0.7835, Time 00:00:13,lr 0.1\n",
      "epoch 37, loss 0.59012, train_acc 0.7948, valid_acc 0.7797, Time 00:00:14,lr 0.1\n",
      "epoch 38, loss 0.58185, train_acc 0.7969, valid_acc 0.7802, Time 00:00:14,lr 0.1\n",
      "epoch 39, loss 0.57975, train_acc 0.7977, valid_acc 0.7890, Time 00:00:14,lr 0.1\n",
      "epoch 40, loss 0.57709, train_acc 0.7979, valid_acc 0.7956, Time 00:00:14,lr 0.1\n",
      "epoch 41, loss 0.57316, train_acc 0.8003, valid_acc 0.7814, Time 00:00:13,lr 0.1\n",
      "epoch 42, loss 0.55932, train_acc 0.8040, valid_acc 0.7911, Time 00:00:14,lr 0.1\n",
      "epoch 43, loss 0.56141, train_acc 0.8026, valid_acc 0.7944, Time 00:00:14,lr 0.1\n",
      "epoch 44, loss 0.55393, train_acc 0.8056, valid_acc 0.7707, Time 00:00:14,lr 0.1\n",
      "epoch 45, loss 0.55475, train_acc 0.8073, valid_acc 0.7859, Time 00:00:14,lr 0.1\n",
      "epoch 46, loss 0.54080, train_acc 0.8107, valid_acc 0.7966, Time 00:00:13,lr 0.1\n",
      "epoch 47, loss 0.52960, train_acc 0.8136, valid_acc 0.7967, Time 00:00:13,lr 0.1\n",
      "epoch 48, loss 0.52993, train_acc 0.8150, valid_acc 0.7782, Time 00:00:13,lr 0.1\n",
      "epoch 49, loss 0.53520, train_acc 0.8143, valid_acc 0.7944, Time 00:00:14,lr 0.1\n",
      "epoch 50, loss 0.52142, train_acc 0.8181, valid_acc 0.7858, Time 00:00:13,lr 0.1\n",
      "epoch 51, loss 0.52173, train_acc 0.8171, valid_acc 0.7958, Time 00:00:14,lr 0.1\n",
      "epoch 52, loss 0.52177, train_acc 0.8168, valid_acc 0.7954, Time 00:00:13,lr 0.1\n",
      "epoch 53, loss 0.51347, train_acc 0.8203, valid_acc 0.7988, Time 00:00:13,lr 0.1\n",
      "epoch 54, loss 0.50815, train_acc 0.8240, valid_acc 0.7899, Time 00:00:13,lr 0.1\n",
      "epoch 55, loss 0.50490, train_acc 0.8218, valid_acc 0.7904, Time 00:00:14,lr 0.1\n",
      "epoch 56, loss 0.49937, train_acc 0.8260, valid_acc 0.7702, Time 00:00:14,lr 0.1\n",
      "epoch 57, loss 0.49562, train_acc 0.8264, valid_acc 0.7886, Time 00:00:13,lr 0.1\n",
      "epoch 58, loss 0.49780, train_acc 0.8262, valid_acc 0.7992, Time 00:00:13,lr 0.1\n",
      "epoch 59, loss 0.49872, train_acc 0.8245, valid_acc 0.7969, Time 00:00:14,lr 0.1\n",
      "epoch 60, loss 0.39687, train_acc 0.8620, valid_acc 0.8363, Time 00:00:14,lr 0.01\n",
      "epoch 61, loss 0.34716, train_acc 0.8775, valid_acc 0.8399, Time 00:00:14,lr 0.01\n",
      "epoch 62, loss 0.33379, train_acc 0.8829, valid_acc 0.8384, Time 00:00:14,lr 0.01\n",
      "epoch 63, loss 0.32368, train_acc 0.8879, valid_acc 0.8427, Time 00:00:14,lr 0.01\n",
      "epoch 64, loss 0.31393, train_acc 0.8903, valid_acc 0.8438, Time 00:00:14,lr 0.01\n",
      "epoch 65, loss 0.30380, train_acc 0.8935, valid_acc 0.8434, Time 00:00:14,lr 0.01\n",
      "epoch 66, loss 0.29685, train_acc 0.8943, valid_acc 0.8441, Time 00:00:14,lr 0.01\n",
      "epoch 67, loss 0.29433, train_acc 0.8960, valid_acc 0.8437, Time 00:00:13,lr 0.01\n",
      "epoch 68, loss 0.28956, train_acc 0.8968, valid_acc 0.8406, Time 00:00:13,lr 0.01\n",
      "epoch 69, loss 0.28430, train_acc 0.9005, valid_acc 0.8411, Time 00:00:14,lr 0.01\n",
      "epoch 70, loss 0.27922, train_acc 0.9017, valid_acc 0.8469, Time 00:00:13,lr 0.01\n",
      "epoch 71, loss 0.27403, train_acc 0.9031, valid_acc 0.8451, Time 00:00:14,lr 0.01\n",
      "epoch 72, loss 0.27117, train_acc 0.9049, valid_acc 0.8431, Time 00:00:13,lr 0.01\n",
      "epoch 73, loss 0.26618, train_acc 0.9047, valid_acc 0.8419, Time 00:00:13,lr 0.01\n",
      "epoch 74, loss 0.26329, train_acc 0.9064, valid_acc 0.8430, Time 00:00:14,lr 0.01\n",
      "epoch 75, loss 0.25336, train_acc 0.9102, valid_acc 0.8406, Time 00:00:13,lr 0.01\n",
      "epoch 76, loss 0.25148, train_acc 0.9105, valid_acc 0.8412, Time 00:00:13,lr 0.01\n",
      "epoch 77, loss 0.24872, train_acc 0.9129, valid_acc 0.8414, Time 00:00:14,lr 0.01\n",
      "epoch 78, loss 0.24791, train_acc 0.9130, valid_acc 0.8420, Time 00:00:14,lr 0.01\n",
      "epoch 79, loss 0.24753, train_acc 0.9127, valid_acc 0.8387, Time 00:00:14,lr 0.01\n",
      "epoch 80, loss 0.23737, train_acc 0.9166, valid_acc 0.8449, Time 00:00:14,lr 0.01\n",
      "epoch 81, loss 0.23928, train_acc 0.9145, valid_acc 0.8440, Time 00:00:13,lr 0.01\n",
      "epoch 82, loss 0.23515, train_acc 0.9167, valid_acc 0.8418, Time 00:00:14,lr 0.01\n",
      "epoch 83, loss 0.23618, train_acc 0.9160, valid_acc 0.8443, Time 00:00:15,lr 0.01\n",
      "epoch 84, loss 0.22978, train_acc 0.9188, valid_acc 0.8450, Time 00:00:14,lr 0.01\n",
      "epoch 85, loss 0.22480, train_acc 0.9206, valid_acc 0.8451, Time 00:00:14,lr 0.01\n",
      "epoch 86, loss 0.22253, train_acc 0.9205, valid_acc 0.8394, Time 00:00:14,lr 0.01\n",
      "epoch 87, loss 0.21822, train_acc 0.9231, valid_acc 0.8405, Time 00:00:14,lr 0.01\n",
      "epoch 88, loss 0.22010, train_acc 0.9217, valid_acc 0.8417, Time 00:00:14,lr 0.01\n",
      "epoch 89, loss 0.21573, train_acc 0.9230, valid_acc 0.8430, Time 00:00:14,lr 0.01\n",
      "epoch 90, loss 0.19832, train_acc 0.9290, valid_acc 0.8447, Time 00:00:14,lr 0.001\n",
      "epoch 91, loss 0.18820, train_acc 0.9345, valid_acc 0.8463, Time 00:00:14,lr 0.001\n",
      "epoch 92, loss 0.18818, train_acc 0.9335, valid_acc 0.8470, Time 00:00:14,lr 0.001\n",
      "epoch 93, loss 0.18694, train_acc 0.9338, valid_acc 0.8490, Time 00:00:13,lr 0.001\n",
      "epoch 94, loss 0.18044, train_acc 0.9373, valid_acc 0.8481, Time 00:00:13,lr 0.001\n",
      "epoch 95, loss 0.18027, train_acc 0.9367, valid_acc 0.8460, Time 00:00:14,lr 0.001\n",
      "epoch 96, loss 0.17828, train_acc 0.9376, valid_acc 0.8486, Time 00:00:14,lr 0.001\n",
      "epoch 97, loss 0.17546, train_acc 0.9376, valid_acc 0.8476, Time 00:00:14,lr 0.001\n",
      "epoch 98, loss 0.17668, train_acc 0.9380, valid_acc 0.8487, Time 00:00:14,lr 0.001\n",
      "epoch 99, loss 0.17367, train_acc 0.9393, valid_acc 0.8479, Time 00:00:15,lr 0.001\n",
      "epoch 100, loss 0.17460, train_acc 0.9386, valid_acc 0.8480, Time 00:00:14,lr 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss 0.17236, train_acc 0.9395, valid_acc 0.8473, Time 00:00:13,lr 0.001\n",
      "epoch 102, loss 0.17157, train_acc 0.9404, valid_acc 0.8481, Time 00:00:14,lr 0.001\n",
      "epoch 103, loss 0.17134, train_acc 0.9404, valid_acc 0.8475, Time 00:00:14,lr 0.001\n",
      "epoch 104, loss 0.16581, train_acc 0.9406, valid_acc 0.8474, Time 00:00:14,lr 0.001\n",
      "epoch 105, loss 0.16578, train_acc 0.9414, valid_acc 0.8470, Time 00:00:15,lr 0.001\n",
      "epoch 106, loss 0.16718, train_acc 0.9410, valid_acc 0.8475, Time 00:00:14,lr 0.001\n",
      "epoch 107, loss 0.16727, train_acc 0.9410, valid_acc 0.8489, Time 00:00:14,lr 0.001\n",
      "epoch 108, loss 0.16414, train_acc 0.9421, valid_acc 0.8488, Time 00:00:14,lr 0.001\n",
      "epoch 109, loss 0.16236, train_acc 0.9431, valid_acc 0.8474, Time 00:00:14,lr 0.001\n",
      "epoch 110, loss 0.16289, train_acc 0.9426, valid_acc 0.8462, Time 00:00:14,lr 0.0001\n",
      "epoch 111, loss 0.16164, train_acc 0.9426, valid_acc 0.8479, Time 00:00:14,lr 0.0001\n",
      "epoch 112, loss 0.16173, train_acc 0.9435, valid_acc 0.8475, Time 00:00:14,lr 0.0001\n",
      "epoch 113, loss 0.15951, train_acc 0.9446, valid_acc 0.8475, Time 00:00:14,lr 0.0001\n",
      "epoch 114, loss 0.16401, train_acc 0.9418, valid_acc 0.8476, Time 00:00:14,lr 0.0001\n",
      "epoch 115, loss 0.16163, train_acc 0.9433, valid_acc 0.8480, Time 00:00:14,lr 0.0001\n",
      "epoch 116, loss 0.16349, train_acc 0.9431, valid_acc 0.8476, Time 00:00:14,lr 0.0001\n",
      "epoch 117, loss 0.16275, train_acc 0.9413, valid_acc 0.8477, Time 00:00:14,lr 0.0001\n",
      "epoch 118, loss 0.16229, train_acc 0.9443, valid_acc 0.8481, Time 00:00:15,lr 0.0001\n",
      "epoch 119, loss 0.15669, train_acc 0.9451, valid_acc 0.8470, Time 00:00:14,lr 0.0001\n"
     ]
    }
   ],
   "source": [
    "lr_period = [60, 90, 110]\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v2()\n",
    "\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 DA1 lr_peroid=[150, 250, 350]\n",
    "I thought 5.1.1 is not good cuase not trian completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:32:31.433333Z",
     "start_time": "2018-03-02T02:04:20.644589Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.76288, train_acc 0.1891, valid_acc 0.2425, Time 00:00:10,lr 0.1\n",
      "epoch 1, loss 1.82606, train_acc 0.3212, valid_acc 0.3795, Time 00:00:11,lr 0.1\n",
      "epoch 2, loss 1.65041, train_acc 0.3860, valid_acc 0.4290, Time 00:00:11,lr 0.1\n",
      "epoch 3, loss 1.51908, train_acc 0.4404, valid_acc 0.4803, Time 00:00:11,lr 0.1\n",
      "epoch 4, loss 1.46011, train_acc 0.4700, valid_acc 0.5156, Time 00:00:11,lr 0.1\n",
      "epoch 5, loss 1.40076, train_acc 0.4915, valid_acc 0.5219, Time 00:00:11,lr 0.1\n",
      "epoch 6, loss 1.46110, train_acc 0.4724, valid_acc 0.4839, Time 00:00:11,lr 0.1\n",
      "epoch 7, loss 1.41476, train_acc 0.4857, valid_acc 0.5232, Time 00:00:11,lr 0.1\n",
      "epoch 8, loss 1.56326, train_acc 0.4328, valid_acc 0.4693, Time 00:00:11,lr 0.1\n",
      "epoch 9, loss 1.41685, train_acc 0.4872, valid_acc 0.5236, Time 00:00:11,lr 0.1\n",
      "epoch 10, loss 1.31954, train_acc 0.5228, valid_acc 0.5639, Time 00:00:11,lr 0.1\n",
      "epoch 11, loss 1.23618, train_acc 0.5568, valid_acc 0.5688, Time 00:00:11,lr 0.1\n",
      "epoch 12, loss 1.18017, train_acc 0.5765, valid_acc 0.6149, Time 00:00:11,lr 0.1\n",
      "epoch 13, loss 1.13208, train_acc 0.5980, valid_acc 0.6257, Time 00:00:11,lr 0.1\n",
      "epoch 14, loss 1.08368, train_acc 0.6134, valid_acc 0.6362, Time 00:00:11,lr 0.1\n",
      "epoch 15, loss 1.20246, train_acc 0.5711, valid_acc 0.6138, Time 00:00:12,lr 0.1\n",
      "epoch 16, loss 1.07978, train_acc 0.6138, valid_acc 0.6243, Time 00:00:11,lr 0.1\n",
      "epoch 17, loss 1.02701, train_acc 0.6353, valid_acc 0.6646, Time 00:00:11,lr 0.1\n",
      "epoch 18, loss 0.98416, train_acc 0.6506, valid_acc 0.6684, Time 00:00:11,lr 0.1\n",
      "epoch 19, loss 0.94517, train_acc 0.6654, valid_acc 0.6699, Time 00:00:11,lr 0.1\n",
      "epoch 20, loss 0.91998, train_acc 0.6738, valid_acc 0.6925, Time 00:00:11,lr 0.1\n",
      "epoch 21, loss 0.90343, train_acc 0.6812, valid_acc 0.6981, Time 00:00:11,lr 0.1\n",
      "epoch 22, loss 0.88002, train_acc 0.6907, valid_acc 0.6976, Time 00:00:11,lr 0.1\n",
      "epoch 23, loss 0.92472, train_acc 0.6737, valid_acc 0.6824, Time 00:00:11,lr 0.1\n",
      "epoch 24, loss 0.86019, train_acc 0.6964, valid_acc 0.7018, Time 00:00:11,lr 0.1\n",
      "epoch 25, loss 0.82525, train_acc 0.7110, valid_acc 0.7114, Time 00:00:11,lr 0.1\n",
      "epoch 26, loss 0.79939, train_acc 0.7170, valid_acc 0.7177, Time 00:00:11,lr 0.1\n",
      "epoch 27, loss 0.78870, train_acc 0.7232, valid_acc 0.7322, Time 00:00:11,lr 0.1\n",
      "epoch 28, loss 0.77436, train_acc 0.7282, valid_acc 0.7228, Time 00:00:11,lr 0.1\n",
      "epoch 29, loss 0.75901, train_acc 0.7338, valid_acc 0.7328, Time 00:00:11,lr 0.1\n",
      "epoch 30, loss 0.74714, train_acc 0.7376, valid_acc 0.7295, Time 00:00:11,lr 0.1\n",
      "epoch 31, loss 0.73447, train_acc 0.7432, valid_acc 0.7433, Time 00:00:11,lr 0.1\n",
      "epoch 32, loss 0.72194, train_acc 0.7443, valid_acc 0.7479, Time 00:00:11,lr 0.1\n",
      "epoch 33, loss 0.71649, train_acc 0.7466, valid_acc 0.7510, Time 00:00:11,lr 0.1\n",
      "epoch 34, loss 0.69564, train_acc 0.7574, valid_acc 0.7525, Time 00:00:11,lr 0.1\n",
      "epoch 35, loss 0.68833, train_acc 0.7596, valid_acc 0.7598, Time 00:00:11,lr 0.1\n",
      "epoch 36, loss 0.67870, train_acc 0.7628, valid_acc 0.7535, Time 00:00:11,lr 0.1\n",
      "epoch 37, loss 0.67188, train_acc 0.7637, valid_acc 0.7605, Time 00:00:11,lr 0.1\n",
      "epoch 38, loss 0.65865, train_acc 0.7688, valid_acc 0.7532, Time 00:00:11,lr 0.1\n",
      "epoch 39, loss 0.66084, train_acc 0.7668, valid_acc 0.7518, Time 00:00:11,lr 0.1\n",
      "epoch 40, loss 0.64839, train_acc 0.7744, valid_acc 0.7493, Time 00:00:11,lr 0.1\n",
      "epoch 41, loss 0.63722, train_acc 0.7757, valid_acc 0.7649, Time 00:00:11,lr 0.1\n",
      "epoch 42, loss 0.62347, train_acc 0.7812, valid_acc 0.7565, Time 00:00:11,lr 0.1\n",
      "epoch 43, loss 0.61804, train_acc 0.7848, valid_acc 0.7623, Time 00:00:11,lr 0.1\n",
      "epoch 44, loss 0.61543, train_acc 0.7851, valid_acc 0.7662, Time 00:00:11,lr 0.1\n",
      "epoch 45, loss 0.60951, train_acc 0.7871, valid_acc 0.7650, Time 00:00:11,lr 0.1\n",
      "epoch 46, loss 0.60344, train_acc 0.7875, valid_acc 0.7671, Time 00:00:11,lr 0.1\n",
      "epoch 47, loss 0.59642, train_acc 0.7905, valid_acc 0.7626, Time 00:00:11,lr 0.1\n",
      "epoch 48, loss 0.59299, train_acc 0.7909, valid_acc 0.7646, Time 00:00:11,lr 0.1\n",
      "epoch 49, loss 0.58948, train_acc 0.7934, valid_acc 0.7754, Time 00:00:11,lr 0.1\n",
      "epoch 50, loss 0.57861, train_acc 0.7963, valid_acc 0.7777, Time 00:00:11,lr 0.1\n",
      "epoch 51, loss 0.57473, train_acc 0.7971, valid_acc 0.7739, Time 00:00:11,lr 0.1\n",
      "epoch 52, loss 0.57393, train_acc 0.7985, valid_acc 0.7797, Time 00:00:11,lr 0.1\n",
      "epoch 53, loss 0.56627, train_acc 0.8043, valid_acc 0.7870, Time 00:00:11,lr 0.1\n",
      "epoch 54, loss 0.56432, train_acc 0.8021, valid_acc 0.7729, Time 00:00:11,lr 0.1\n",
      "epoch 55, loss 0.55943, train_acc 0.8034, valid_acc 0.7636, Time 00:00:11,lr 0.1\n",
      "epoch 56, loss 0.55210, train_acc 0.8070, valid_acc 0.7772, Time 00:00:11,lr 0.1\n",
      "epoch 57, loss 0.55122, train_acc 0.8066, valid_acc 0.7845, Time 00:00:11,lr 0.1\n",
      "epoch 58, loss 0.54119, train_acc 0.8095, valid_acc 0.7781, Time 00:00:11,lr 0.1\n",
      "epoch 59, loss 0.53922, train_acc 0.8097, valid_acc 0.7871, Time 00:00:11,lr 0.1\n",
      "epoch 60, loss 0.53632, train_acc 0.8120, valid_acc 0.7828, Time 00:00:11,lr 0.1\n",
      "epoch 61, loss 0.52792, train_acc 0.8145, valid_acc 0.7900, Time 00:00:11,lr 0.1\n",
      "epoch 62, loss 0.52427, train_acc 0.8159, valid_acc 0.7789, Time 00:00:11,lr 0.1\n",
      "epoch 63, loss 0.52227, train_acc 0.8172, valid_acc 0.7924, Time 00:00:11,lr 0.1\n",
      "epoch 64, loss 0.51558, train_acc 0.8193, valid_acc 0.7771, Time 00:00:11,lr 0.1\n",
      "epoch 65, loss 0.51132, train_acc 0.8205, valid_acc 0.7721, Time 00:00:11,lr 0.1\n",
      "epoch 66, loss 0.51750, train_acc 0.8192, valid_acc 0.7961, Time 00:00:11,lr 0.1\n",
      "epoch 67, loss 0.51284, train_acc 0.8202, valid_acc 0.7982, Time 00:00:11,lr 0.1\n",
      "epoch 68, loss 0.50330, train_acc 0.8246, valid_acc 0.7872, Time 00:00:11,lr 0.1\n",
      "epoch 69, loss 0.49553, train_acc 0.8269, valid_acc 0.7892, Time 00:00:11,lr 0.1\n",
      "epoch 70, loss 0.50233, train_acc 0.8250, valid_acc 0.7895, Time 00:00:11,lr 0.1\n",
      "epoch 71, loss 0.49849, train_acc 0.8224, valid_acc 0.7926, Time 00:00:11,lr 0.1\n",
      "epoch 72, loss 0.49596, train_acc 0.8263, valid_acc 0.8064, Time 00:00:11,lr 0.1\n",
      "epoch 73, loss 0.49503, train_acc 0.8264, valid_acc 0.7956, Time 00:00:11,lr 0.1\n",
      "epoch 74, loss 0.48956, train_acc 0.8301, valid_acc 0.7902, Time 00:00:11,lr 0.1\n",
      "epoch 75, loss 0.48751, train_acc 0.8305, valid_acc 0.7874, Time 00:00:11,lr 0.1\n",
      "epoch 76, loss 0.48400, train_acc 0.8292, valid_acc 0.8033, Time 00:00:11,lr 0.1\n",
      "epoch 77, loss 0.48910, train_acc 0.8289, valid_acc 0.7978, Time 00:00:11,lr 0.1\n",
      "epoch 78, loss 0.48089, train_acc 0.8308, valid_acc 0.7893, Time 00:00:11,lr 0.1\n",
      "epoch 79, loss 0.47633, train_acc 0.8332, valid_acc 0.8051, Time 00:00:11,lr 0.1\n",
      "epoch 80, loss 0.47532, train_acc 0.8331, valid_acc 0.7947, Time 00:00:11,lr 0.1\n",
      "epoch 81, loss 0.46937, train_acc 0.8364, valid_acc 0.7809, Time 00:00:11,lr 0.1\n",
      "epoch 82, loss 0.46351, train_acc 0.8378, valid_acc 0.7989, Time 00:00:11,lr 0.1\n",
      "epoch 83, loss 0.46876, train_acc 0.8364, valid_acc 0.7866, Time 00:00:11,lr 0.1\n",
      "epoch 84, loss 0.46200, train_acc 0.8381, valid_acc 0.8025, Time 00:00:11,lr 0.1\n",
      "epoch 85, loss 0.46241, train_acc 0.8377, valid_acc 0.8013, Time 00:00:11,lr 0.1\n",
      "epoch 86, loss 0.45696, train_acc 0.8407, valid_acc 0.7898, Time 00:00:11,lr 0.1\n",
      "epoch 87, loss 0.45813, train_acc 0.8389, valid_acc 0.8047, Time 00:00:11,lr 0.1\n",
      "epoch 88, loss 0.45434, train_acc 0.8399, valid_acc 0.7988, Time 00:00:11,lr 0.1\n",
      "epoch 89, loss 0.45635, train_acc 0.8394, valid_acc 0.7969, Time 00:00:11,lr 0.1\n",
      "epoch 90, loss 0.44878, train_acc 0.8437, valid_acc 0.8032, Time 00:00:11,lr 0.1\n",
      "epoch 91, loss 0.45446, train_acc 0.8396, valid_acc 0.7999, Time 00:00:11,lr 0.1\n",
      "epoch 92, loss 0.45062, train_acc 0.8420, valid_acc 0.8057, Time 00:00:11,lr 0.1\n",
      "epoch 93, loss 0.45301, train_acc 0.8425, valid_acc 0.8023, Time 00:00:11,lr 0.1\n",
      "epoch 94, loss 0.44938, train_acc 0.8421, valid_acc 0.8044, Time 00:00:11,lr 0.1\n",
      "epoch 95, loss 0.44392, train_acc 0.8429, valid_acc 0.7972, Time 00:00:11,lr 0.1\n",
      "epoch 96, loss 0.44713, train_acc 0.8433, valid_acc 0.7986, Time 00:00:11,lr 0.1\n",
      "epoch 97, loss 0.43787, train_acc 0.8473, valid_acc 0.8080, Time 00:00:11,lr 0.1\n",
      "epoch 98, loss 0.44187, train_acc 0.8447, valid_acc 0.8018, Time 00:00:11,lr 0.1\n",
      "epoch 99, loss 0.44241, train_acc 0.8442, valid_acc 0.7973, Time 00:00:11,lr 0.1\n",
      "epoch 100, loss 0.43270, train_acc 0.8480, valid_acc 0.8171, Time 00:00:11,lr 0.1\n",
      "epoch 101, loss 0.43912, train_acc 0.8451, valid_acc 0.7982, Time 00:00:11,lr 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 102, loss 0.43860, train_acc 0.8476, valid_acc 0.8165, Time 00:00:11,lr 0.1\n",
      "epoch 103, loss 0.43661, train_acc 0.8479, valid_acc 0.7952, Time 00:00:11,lr 0.1\n",
      "epoch 104, loss 0.43444, train_acc 0.8457, valid_acc 0.7984, Time 00:00:11,lr 0.1\n",
      "epoch 105, loss 0.43892, train_acc 0.8464, valid_acc 0.8182, Time 00:00:11,lr 0.1\n",
      "epoch 106, loss 0.42864, train_acc 0.8483, valid_acc 0.8088, Time 00:00:11,lr 0.1\n",
      "epoch 107, loss 0.42585, train_acc 0.8514, valid_acc 0.7831, Time 00:00:11,lr 0.1\n",
      "epoch 108, loss 0.43077, train_acc 0.8504, valid_acc 0.8051, Time 00:00:11,lr 0.1\n",
      "epoch 109, loss 0.42805, train_acc 0.8493, valid_acc 0.8106, Time 00:00:11,lr 0.1\n",
      "epoch 110, loss 0.42019, train_acc 0.8511, valid_acc 0.8154, Time 00:00:11,lr 0.1\n",
      "epoch 111, loss 0.42386, train_acc 0.8498, valid_acc 0.8085, Time 00:00:11,lr 0.1\n",
      "epoch 112, loss 0.42199, train_acc 0.8514, valid_acc 0.8059, Time 00:00:11,lr 0.1\n",
      "epoch 113, loss 0.41956, train_acc 0.8521, valid_acc 0.8120, Time 00:00:11,lr 0.1\n",
      "epoch 114, loss 0.43258, train_acc 0.8495, valid_acc 0.7993, Time 00:00:11,lr 0.1\n",
      "epoch 115, loss 0.43624, train_acc 0.8474, valid_acc 0.8053, Time 00:00:11,lr 0.1\n",
      "epoch 116, loss 0.42834, train_acc 0.8507, valid_acc 0.8039, Time 00:00:11,lr 0.1\n",
      "epoch 117, loss 0.42472, train_acc 0.8505, valid_acc 0.8165, Time 00:00:11,lr 0.1\n",
      "epoch 118, loss 0.42678, train_acc 0.8504, valid_acc 0.8088, Time 00:00:11,lr 0.1\n",
      "epoch 119, loss 0.41998, train_acc 0.8516, valid_acc 0.8097, Time 00:00:11,lr 0.1\n",
      "epoch 120, loss 0.42254, train_acc 0.8517, valid_acc 0.7653, Time 00:00:11,lr 0.1\n",
      "epoch 121, loss 0.41829, train_acc 0.8533, valid_acc 0.8125, Time 00:00:11,lr 0.1\n",
      "epoch 122, loss 0.41250, train_acc 0.8553, valid_acc 0.8005, Time 00:00:11,lr 0.1\n",
      "epoch 123, loss 0.41365, train_acc 0.8531, valid_acc 0.7967, Time 00:00:11,lr 0.1\n",
      "epoch 124, loss 0.41167, train_acc 0.8539, valid_acc 0.8163, Time 00:00:11,lr 0.1\n",
      "epoch 125, loss 0.40877, train_acc 0.8562, valid_acc 0.8170, Time 00:00:11,lr 0.1\n",
      "epoch 126, loss 0.41211, train_acc 0.8545, valid_acc 0.8058, Time 00:00:11,lr 0.1\n",
      "epoch 127, loss 0.40685, train_acc 0.8577, valid_acc 0.7980, Time 00:00:11,lr 0.1\n",
      "epoch 128, loss 0.40743, train_acc 0.8566, valid_acc 0.7869, Time 00:00:11,lr 0.1\n",
      "epoch 129, loss 0.40888, train_acc 0.8563, valid_acc 0.8070, Time 00:00:11,lr 0.1\n",
      "epoch 130, loss 0.39895, train_acc 0.8605, valid_acc 0.8127, Time 00:00:11,lr 0.1\n",
      "epoch 131, loss 0.40903, train_acc 0.8557, valid_acc 0.8157, Time 00:00:11,lr 0.1\n",
      "epoch 132, loss 0.39877, train_acc 0.8587, valid_acc 0.8113, Time 00:00:11,lr 0.1\n",
      "epoch 133, loss 0.39898, train_acc 0.8610, valid_acc 0.8135, Time 00:00:11,lr 0.1\n",
      "epoch 134, loss 0.40009, train_acc 0.8597, valid_acc 0.8136, Time 00:00:11,lr 0.1\n",
      "epoch 135, loss 0.40296, train_acc 0.8579, valid_acc 0.8080, Time 00:00:11,lr 0.1\n",
      "epoch 136, loss 0.40115, train_acc 0.8568, valid_acc 0.7995, Time 00:00:11,lr 0.1\n",
      "epoch 137, loss 0.39105, train_acc 0.8638, valid_acc 0.8176, Time 00:00:11,lr 0.1\n",
      "epoch 138, loss 0.39663, train_acc 0.8605, valid_acc 0.7984, Time 00:00:11,lr 0.1\n",
      "epoch 139, loss 0.39679, train_acc 0.8609, valid_acc 0.8113, Time 00:00:11,lr 0.1\n",
      "epoch 140, loss 0.40253, train_acc 0.8587, valid_acc 0.8022, Time 00:00:11,lr 0.1\n",
      "epoch 141, loss 0.39330, train_acc 0.8629, valid_acc 0.8085, Time 00:00:11,lr 0.1\n",
      "epoch 142, loss 0.39550, train_acc 0.8612, valid_acc 0.8056, Time 00:00:11,lr 0.1\n",
      "epoch 143, loss 0.40002, train_acc 0.8609, valid_acc 0.8189, Time 00:00:11,lr 0.1\n",
      "epoch 144, loss 0.38797, train_acc 0.8629, valid_acc 0.8126, Time 00:00:11,lr 0.1\n",
      "epoch 145, loss 0.39147, train_acc 0.8620, valid_acc 0.8210, Time 00:00:11,lr 0.1\n",
      "epoch 146, loss 0.39022, train_acc 0.8634, valid_acc 0.8161, Time 00:00:11,lr 0.1\n",
      "epoch 147, loss 0.38559, train_acc 0.8639, valid_acc 0.7831, Time 00:00:11,lr 0.1\n",
      "epoch 148, loss 0.39108, train_acc 0.8611, valid_acc 0.8140, Time 00:00:11,lr 0.1\n",
      "epoch 149, loss 0.38864, train_acc 0.8626, valid_acc 0.7920, Time 00:00:11,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "net = get_resnet18_v2()\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "\n",
    "num_epochs = 150      # 50 * 1563 iter about\n",
    "learning_rate = 0.1\n",
    "lr_period = [150]\n",
    "\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_DA1_e150\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:51:17.793686Z",
     "start_time": "2018-03-02T02:32:31.434759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.27494, train_acc 0.9036, valid_acc 0.8531, Time 00:00:09,lr 0.01\n",
      "epoch 1, loss 0.23308, train_acc 0.9173, valid_acc 0.8568, Time 00:00:11,lr 0.01\n",
      "epoch 2, loss 0.21313, train_acc 0.9240, valid_acc 0.8548, Time 00:00:11,lr 0.01\n",
      "epoch 3, loss 0.20037, train_acc 0.9292, valid_acc 0.8565, Time 00:00:11,lr 0.01\n",
      "epoch 4, loss 0.19582, train_acc 0.9303, valid_acc 0.8583, Time 00:00:11,lr 0.01\n",
      "epoch 5, loss 0.18779, train_acc 0.9335, valid_acc 0.8574, Time 00:00:11,lr 0.01\n",
      "epoch 6, loss 0.18050, train_acc 0.9360, valid_acc 0.8593, Time 00:00:11,lr 0.01\n",
      "epoch 7, loss 0.17190, train_acc 0.9377, valid_acc 0.8585, Time 00:00:11,lr 0.01\n",
      "epoch 8, loss 0.16845, train_acc 0.9406, valid_acc 0.8590, Time 00:00:11,lr 0.01\n",
      "epoch 9, loss 0.16648, train_acc 0.9400, valid_acc 0.8552, Time 00:00:11,lr 0.01\n",
      "epoch 10, loss 0.16247, train_acc 0.9421, valid_acc 0.8586, Time 00:00:11,lr 0.01\n",
      "epoch 11, loss 0.15294, train_acc 0.9463, valid_acc 0.8601, Time 00:00:11,lr 0.01\n",
      "epoch 12, loss 0.15478, train_acc 0.9445, valid_acc 0.8592, Time 00:00:11,lr 0.01\n",
      "epoch 13, loss 0.15023, train_acc 0.9462, valid_acc 0.8572, Time 00:00:11,lr 0.01\n",
      "epoch 14, loss 0.14658, train_acc 0.9483, valid_acc 0.8609, Time 00:00:11,lr 0.01\n",
      "epoch 15, loss 0.14086, train_acc 0.9495, valid_acc 0.8616, Time 00:00:11,lr 0.01\n",
      "epoch 16, loss 0.13300, train_acc 0.9525, valid_acc 0.8615, Time 00:00:11,lr 0.01\n",
      "epoch 17, loss 0.13827, train_acc 0.9504, valid_acc 0.8614, Time 00:00:11,lr 0.01\n",
      "epoch 18, loss 0.13247, train_acc 0.9530, valid_acc 0.8609, Time 00:00:11,lr 0.01\n",
      "epoch 19, loss 0.13240, train_acc 0.9525, valid_acc 0.8598, Time 00:00:11,lr 0.01\n",
      "epoch 20, loss 0.12631, train_acc 0.9551, valid_acc 0.8588, Time 00:00:11,lr 0.01\n",
      "epoch 21, loss 0.12759, train_acc 0.9541, valid_acc 0.8599, Time 00:00:11,lr 0.01\n",
      "epoch 22, loss 0.12232, train_acc 0.9570, valid_acc 0.8573, Time 00:00:11,lr 0.01\n",
      "epoch 23, loss 0.11921, train_acc 0.9583, valid_acc 0.8581, Time 00:00:11,lr 0.01\n",
      "epoch 24, loss 0.11827, train_acc 0.9589, valid_acc 0.8611, Time 00:00:11,lr 0.01\n",
      "epoch 25, loss 0.11519, train_acc 0.9593, valid_acc 0.8619, Time 00:00:11,lr 0.01\n",
      "epoch 26, loss 0.11584, train_acc 0.9589, valid_acc 0.8583, Time 00:00:11,lr 0.01\n",
      "epoch 27, loss 0.11290, train_acc 0.9588, valid_acc 0.8590, Time 00:00:11,lr 0.01\n",
      "epoch 28, loss 0.11159, train_acc 0.9607, valid_acc 0.8601, Time 00:00:11,lr 0.01\n",
      "epoch 29, loss 0.11350, train_acc 0.9597, valid_acc 0.8594, Time 00:00:11,lr 0.01\n",
      "epoch 30, loss 0.10545, train_acc 0.9633, valid_acc 0.8573, Time 00:00:11,lr 0.01\n",
      "epoch 31, loss 0.10657, train_acc 0.9617, valid_acc 0.8590, Time 00:00:11,lr 0.01\n",
      "epoch 32, loss 0.10543, train_acc 0.9626, valid_acc 0.8545, Time 00:00:11,lr 0.01\n",
      "epoch 33, loss 0.10275, train_acc 0.9626, valid_acc 0.8587, Time 00:00:11,lr 0.01\n",
      "epoch 34, loss 0.09869, train_acc 0.9655, valid_acc 0.8568, Time 00:00:11,lr 0.01\n",
      "epoch 35, loss 0.09868, train_acc 0.9653, valid_acc 0.8562, Time 00:00:11,lr 0.01\n",
      "epoch 36, loss 0.09748, train_acc 0.9661, valid_acc 0.8554, Time 00:00:11,lr 0.01\n",
      "epoch 37, loss 0.09735, train_acc 0.9647, valid_acc 0.8580, Time 00:00:11,lr 0.01\n",
      "epoch 38, loss 0.09648, train_acc 0.9662, valid_acc 0.8587, Time 00:00:11,lr 0.01\n",
      "epoch 39, loss 0.09564, train_acc 0.9653, valid_acc 0.8556, Time 00:00:11,lr 0.01\n",
      "epoch 40, loss 0.09231, train_acc 0.9674, valid_acc 0.8567, Time 00:00:11,lr 0.01\n",
      "epoch 41, loss 0.09156, train_acc 0.9679, valid_acc 0.8574, Time 00:00:11,lr 0.01\n",
      "epoch 42, loss 0.09322, train_acc 0.9667, valid_acc 0.8578, Time 00:00:11,lr 0.01\n",
      "epoch 43, loss 0.08897, train_acc 0.9685, valid_acc 0.8550, Time 00:00:11,lr 0.01\n",
      "epoch 44, loss 0.09148, train_acc 0.9676, valid_acc 0.8593, Time 00:00:11,lr 0.01\n",
      "epoch 45, loss 0.09191, train_acc 0.9672, valid_acc 0.8551, Time 00:00:11,lr 0.01\n",
      "epoch 46, loss 0.09023, train_acc 0.9672, valid_acc 0.8570, Time 00:00:11,lr 0.01\n",
      "epoch 47, loss 0.08791, train_acc 0.9690, valid_acc 0.8556, Time 00:00:11,lr 0.01\n",
      "epoch 48, loss 0.08722, train_acc 0.9689, valid_acc 0.8590, Time 00:00:11,lr 0.01\n",
      "epoch 49, loss 0.08566, train_acc 0.9698, valid_acc 0.8575, Time 00:00:11,lr 0.01\n",
      "epoch 50, loss 0.08682, train_acc 0.9690, valid_acc 0.8553, Time 00:00:11,lr 0.01\n",
      "epoch 51, loss 0.08353, train_acc 0.9701, valid_acc 0.8553, Time 00:00:11,lr 0.01\n",
      "epoch 52, loss 0.08220, train_acc 0.9707, valid_acc 0.8548, Time 00:00:11,lr 0.01\n",
      "epoch 53, loss 0.08313, train_acc 0.9707, valid_acc 0.8556, Time 00:00:11,lr 0.01\n",
      "epoch 54, loss 0.07930, train_acc 0.9723, valid_acc 0.8574, Time 00:00:11,lr 0.01\n",
      "epoch 55, loss 0.08073, train_acc 0.9715, valid_acc 0.8597, Time 00:00:11,lr 0.01\n",
      "epoch 56, loss 0.07982, train_acc 0.9723, valid_acc 0.8596, Time 00:00:11,lr 0.01\n",
      "epoch 57, loss 0.07858, train_acc 0.9725, valid_acc 0.8580, Time 00:00:11,lr 0.01\n",
      "epoch 58, loss 0.07973, train_acc 0.9722, valid_acc 0.8600, Time 00:00:11,lr 0.01\n",
      "epoch 59, loss 0.07884, train_acc 0.9717, valid_acc 0.8579, Time 00:00:11,lr 0.01\n",
      "epoch 60, loss 0.07889, train_acc 0.9721, valid_acc 0.8592, Time 00:00:11,lr 0.01\n",
      "epoch 61, loss 0.07616, train_acc 0.9729, valid_acc 0.8543, Time 00:00:11,lr 0.01\n",
      "epoch 62, loss 0.07786, train_acc 0.9722, valid_acc 0.8578, Time 00:00:11,lr 0.01\n",
      "epoch 63, loss 0.07802, train_acc 0.9722, valid_acc 0.8548, Time 00:00:11,lr 0.01\n",
      "epoch 64, loss 0.07374, train_acc 0.9733, valid_acc 0.8563, Time 00:00:11,lr 0.01\n",
      "epoch 65, loss 0.08094, train_acc 0.9721, valid_acc 0.8568, Time 00:00:11,lr 0.01\n",
      "epoch 66, loss 0.07863, train_acc 0.9724, valid_acc 0.8535, Time 00:00:11,lr 0.01\n",
      "epoch 67, loss 0.07564, train_acc 0.9733, valid_acc 0.8547, Time 00:00:11,lr 0.01\n",
      "epoch 68, loss 0.07307, train_acc 0.9743, valid_acc 0.8575, Time 00:00:11,lr 0.01\n",
      "epoch 69, loss 0.07609, train_acc 0.9731, valid_acc 0.8535, Time 00:00:11,lr 0.01\n",
      "epoch 70, loss 0.07310, train_acc 0.9735, valid_acc 0.8554, Time 00:00:11,lr 0.01\n",
      "epoch 71, loss 0.07438, train_acc 0.9746, valid_acc 0.8571, Time 00:00:11,lr 0.01\n",
      "epoch 72, loss 0.07436, train_acc 0.9735, valid_acc 0.8537, Time 00:00:11,lr 0.01\n",
      "epoch 73, loss 0.07308, train_acc 0.9738, valid_acc 0.8573, Time 00:00:11,lr 0.01\n",
      "epoch 74, loss 0.07607, train_acc 0.9737, valid_acc 0.8542, Time 00:00:11,lr 0.01\n",
      "epoch 75, loss 0.07318, train_acc 0.9738, valid_acc 0.8548, Time 00:00:11,lr 0.01\n",
      "epoch 76, loss 0.07091, train_acc 0.9753, valid_acc 0.8552, Time 00:00:11,lr 0.01\n",
      "epoch 77, loss 0.07452, train_acc 0.9738, valid_acc 0.8543, Time 00:00:11,lr 0.01\n",
      "epoch 78, loss 0.07331, train_acc 0.9747, valid_acc 0.8564, Time 00:00:11,lr 0.01\n",
      "epoch 79, loss 0.07124, train_acc 0.9749, valid_acc 0.8563, Time 00:00:11,lr 0.01\n",
      "epoch 80, loss 0.07005, train_acc 0.9756, valid_acc 0.8549, Time 00:00:11,lr 0.01\n",
      "epoch 81, loss 0.07065, train_acc 0.9745, valid_acc 0.8569, Time 00:00:11,lr 0.01\n",
      "epoch 82, loss 0.07080, train_acc 0.9746, valid_acc 0.8569, Time 00:00:11,lr 0.01\n",
      "epoch 83, loss 0.07548, train_acc 0.9728, valid_acc 0.8544, Time 00:00:11,lr 0.01\n",
      "epoch 84, loss 0.06766, train_acc 0.9761, valid_acc 0.8553, Time 00:00:11,lr 0.01\n",
      "epoch 85, loss 0.06973, train_acc 0.9760, valid_acc 0.8555, Time 00:00:11,lr 0.01\n",
      "epoch 86, loss 0.06824, train_acc 0.9758, valid_acc 0.8549, Time 00:00:11,lr 0.01\n",
      "epoch 87, loss 0.07301, train_acc 0.9741, valid_acc 0.8519, Time 00:00:11,lr 0.01\n",
      "epoch 88, loss 0.07096, train_acc 0.9752, valid_acc 0.8507, Time 00:00:11,lr 0.01\n",
      "epoch 89, loss 0.06898, train_acc 0.9757, valid_acc 0.8522, Time 00:00:11,lr 0.01\n",
      "epoch 90, loss 0.06991, train_acc 0.9757, valid_acc 0.8532, Time 00:00:11,lr 0.01\n",
      "epoch 91, loss 0.06794, train_acc 0.9760, valid_acc 0.8529, Time 00:00:11,lr 0.01\n",
      "epoch 92, loss 0.06768, train_acc 0.9762, valid_acc 0.8523, Time 00:00:11,lr 0.01\n",
      "epoch 93, loss 0.06721, train_acc 0.9768, valid_acc 0.8530, Time 00:00:11,lr 0.01\n",
      "epoch 94, loss 0.07162, train_acc 0.9750, valid_acc 0.8582, Time 00:00:11,lr 0.01\n",
      "epoch 95, loss 0.07175, train_acc 0.9755, valid_acc 0.8512, Time 00:00:11,lr 0.01\n",
      "epoch 96, loss 0.06785, train_acc 0.9753, valid_acc 0.8563, Time 00:00:11,lr 0.01\n",
      "epoch 97, loss 0.07115, train_acc 0.9744, valid_acc 0.8529, Time 00:00:11,lr 0.01\n",
      "epoch 98, loss 0.07004, train_acc 0.9761, valid_acc 0.8527, Time 00:00:11,lr 0.01\n",
      "epoch 99, loss 0.06979, train_acc 0.9757, valid_acc 0.8572, Time 00:00:11,lr 0.01\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100      # 100 + 150 epoch\n",
    "learning_rate = 0.01\n",
    "lr_period = [100]\n",
    "\n",
    "net.load_params(\"../../models/train3_resnet18_v2_finetune_DA1_e150\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_DA1_e250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:10:10.176682Z",
     "start_time": "2018-03-02T02:51:17.795304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.05284, train_acc 0.9826, valid_acc 0.8580, Time 00:00:10,lr 0.001\n",
      "epoch 1, loss 0.04562, train_acc 0.9853, valid_acc 0.8588, Time 00:00:11,lr 0.001\n",
      "epoch 2, loss 0.04212, train_acc 0.9863, valid_acc 0.8604, Time 00:00:11,lr 0.001\n",
      "epoch 3, loss 0.04051, train_acc 0.9867, valid_acc 0.8598, Time 00:00:11,lr 0.001\n",
      "epoch 4, loss 0.03794, train_acc 0.9878, valid_acc 0.8611, Time 00:00:11,lr 0.001\n",
      "epoch 5, loss 0.03741, train_acc 0.9884, valid_acc 0.8600, Time 00:00:11,lr 0.001\n",
      "epoch 6, loss 0.03595, train_acc 0.9886, valid_acc 0.8595, Time 00:00:11,lr 0.001\n",
      "epoch 7, loss 0.03529, train_acc 0.9894, valid_acc 0.8614, Time 00:00:11,lr 0.001\n",
      "epoch 8, loss 0.03540, train_acc 0.9892, valid_acc 0.8615, Time 00:00:11,lr 0.001\n",
      "epoch 9, loss 0.03170, train_acc 0.9902, valid_acc 0.8606, Time 00:00:11,lr 0.001\n",
      "epoch 10, loss 0.03246, train_acc 0.9902, valid_acc 0.8608, Time 00:00:11,lr 0.001\n",
      "epoch 11, loss 0.03148, train_acc 0.9902, valid_acc 0.8621, Time 00:00:11,lr 0.001\n",
      "epoch 12, loss 0.03185, train_acc 0.9901, valid_acc 0.8616, Time 00:00:11,lr 0.001\n",
      "epoch 13, loss 0.03160, train_acc 0.9899, valid_acc 0.8620, Time 00:00:11,lr 0.001\n",
      "epoch 14, loss 0.03255, train_acc 0.9899, valid_acc 0.8611, Time 00:00:11,lr 0.001\n",
      "epoch 15, loss 0.03047, train_acc 0.9902, valid_acc 0.8619, Time 00:00:11,lr 0.001\n",
      "epoch 16, loss 0.02961, train_acc 0.9906, valid_acc 0.8626, Time 00:00:11,lr 0.001\n",
      "epoch 17, loss 0.03091, train_acc 0.9905, valid_acc 0.8618, Time 00:00:11,lr 0.001\n",
      "epoch 18, loss 0.02852, train_acc 0.9915, valid_acc 0.8628, Time 00:00:11,lr 0.001\n",
      "epoch 19, loss 0.02834, train_acc 0.9916, valid_acc 0.8631, Time 00:00:11,lr 0.001\n",
      "epoch 20, loss 0.02876, train_acc 0.9907, valid_acc 0.8625, Time 00:00:11,lr 0.001\n",
      "epoch 21, loss 0.02834, train_acc 0.9910, valid_acc 0.8647, Time 00:00:11,lr 0.001\n",
      "epoch 22, loss 0.02799, train_acc 0.9919, valid_acc 0.8633, Time 00:00:11,lr 0.001\n",
      "epoch 23, loss 0.02635, train_acc 0.9921, valid_acc 0.8625, Time 00:00:11,lr 0.001\n",
      "epoch 24, loss 0.02650, train_acc 0.9917, valid_acc 0.8630, Time 00:00:11,lr 0.001\n",
      "epoch 25, loss 0.02712, train_acc 0.9919, valid_acc 0.8632, Time 00:00:11,lr 0.001\n",
      "epoch 26, loss 0.02672, train_acc 0.9918, valid_acc 0.8645, Time 00:00:11,lr 0.001\n",
      "epoch 27, loss 0.02632, train_acc 0.9917, valid_acc 0.8648, Time 00:00:11,lr 0.001\n",
      "epoch 28, loss 0.02722, train_acc 0.9918, valid_acc 0.8641, Time 00:00:11,lr 0.001\n",
      "epoch 29, loss 0.02668, train_acc 0.9915, valid_acc 0.8639, Time 00:00:11,lr 0.001\n",
      "epoch 30, loss 0.02407, train_acc 0.9926, valid_acc 0.8636, Time 00:00:11,lr 0.001\n",
      "epoch 31, loss 0.02457, train_acc 0.9927, valid_acc 0.8631, Time 00:00:11,lr 0.001\n",
      "epoch 32, loss 0.02441, train_acc 0.9932, valid_acc 0.8624, Time 00:00:11,lr 0.001\n",
      "epoch 33, loss 0.02448, train_acc 0.9925, valid_acc 0.8629, Time 00:00:11,lr 0.001\n",
      "epoch 34, loss 0.02306, train_acc 0.9931, valid_acc 0.8646, Time 00:00:11,lr 0.001\n",
      "epoch 35, loss 0.02335, train_acc 0.9928, valid_acc 0.8622, Time 00:00:11,lr 0.001\n",
      "epoch 36, loss 0.02469, train_acc 0.9928, valid_acc 0.8630, Time 00:00:11,lr 0.001\n",
      "epoch 37, loss 0.02492, train_acc 0.9924, valid_acc 0.8643, Time 00:00:11,lr 0.001\n",
      "epoch 38, loss 0.02411, train_acc 0.9927, valid_acc 0.8625, Time 00:00:11,lr 0.001\n",
      "epoch 39, loss 0.02389, train_acc 0.9930, valid_acc 0.8622, Time 00:00:11,lr 0.001\n",
      "epoch 40, loss 0.02218, train_acc 0.9935, valid_acc 0.8625, Time 00:00:11,lr 0.001\n",
      "epoch 41, loss 0.02305, train_acc 0.9930, valid_acc 0.8630, Time 00:00:11,lr 0.001\n",
      "epoch 42, loss 0.02373, train_acc 0.9925, valid_acc 0.8612, Time 00:00:11,lr 0.001\n",
      "epoch 43, loss 0.02334, train_acc 0.9928, valid_acc 0.8628, Time 00:00:11,lr 0.001\n",
      "epoch 44, loss 0.02275, train_acc 0.9928, valid_acc 0.8643, Time 00:00:11,lr 0.001\n",
      "epoch 45, loss 0.02262, train_acc 0.9934, valid_acc 0.8635, Time 00:00:11,lr 0.001\n",
      "epoch 46, loss 0.02260, train_acc 0.9930, valid_acc 0.8617, Time 00:00:11,lr 0.001\n",
      "epoch 47, loss 0.02169, train_acc 0.9937, valid_acc 0.8625, Time 00:00:11,lr 0.001\n",
      "epoch 48, loss 0.02156, train_acc 0.9939, valid_acc 0.8649, Time 00:00:11,lr 0.001\n",
      "epoch 49, loss 0.02132, train_acc 0.9937, valid_acc 0.8630, Time 00:00:11,lr 0.001\n",
      "epoch 50, loss 0.02088, train_acc 0.9936, valid_acc 0.8621, Time 00:00:11,lr 0.001\n",
      "epoch 51, loss 0.02424, train_acc 0.9923, valid_acc 0.8643, Time 00:00:11,lr 0.001\n",
      "epoch 52, loss 0.02095, train_acc 0.9942, valid_acc 0.8623, Time 00:00:11,lr 0.001\n",
      "epoch 53, loss 0.02118, train_acc 0.9936, valid_acc 0.8635, Time 00:00:11,lr 0.001\n",
      "epoch 54, loss 0.02140, train_acc 0.9937, valid_acc 0.8635, Time 00:00:11,lr 0.001\n",
      "epoch 55, loss 0.02053, train_acc 0.9942, valid_acc 0.8639, Time 00:00:11,lr 0.001\n",
      "epoch 56, loss 0.02028, train_acc 0.9939, valid_acc 0.8638, Time 00:00:11,lr 0.001\n",
      "epoch 57, loss 0.02145, train_acc 0.9935, valid_acc 0.8620, Time 00:00:11,lr 0.001\n",
      "epoch 58, loss 0.01969, train_acc 0.9942, valid_acc 0.8621, Time 00:00:11,lr 0.001\n",
      "epoch 59, loss 0.02003, train_acc 0.9940, valid_acc 0.8639, Time 00:00:11,lr 0.001\n",
      "epoch 60, loss 0.01957, train_acc 0.9944, valid_acc 0.8627, Time 00:00:11,lr 0.001\n",
      "epoch 61, loss 0.02009, train_acc 0.9940, valid_acc 0.8628, Time 00:00:11,lr 0.001\n",
      "epoch 62, loss 0.02010, train_acc 0.9940, valid_acc 0.8616, Time 00:00:11,lr 0.001\n",
      "epoch 63, loss 0.02006, train_acc 0.9941, valid_acc 0.8630, Time 00:00:11,lr 0.001\n",
      "epoch 64, loss 0.01962, train_acc 0.9939, valid_acc 0.8632, Time 00:00:11,lr 0.001\n",
      "epoch 65, loss 0.02012, train_acc 0.9942, valid_acc 0.8615, Time 00:00:11,lr 0.001\n",
      "epoch 66, loss 0.01982, train_acc 0.9945, valid_acc 0.8628, Time 00:00:11,lr 0.001\n",
      "epoch 67, loss 0.01937, train_acc 0.9942, valid_acc 0.8636, Time 00:00:11,lr 0.001\n",
      "epoch 68, loss 0.01860, train_acc 0.9948, valid_acc 0.8626, Time 00:00:11,lr 0.001\n",
      "epoch 69, loss 0.01901, train_acc 0.9945, valid_acc 0.8635, Time 00:00:11,lr 0.001\n",
      "epoch 70, loss 0.01963, train_acc 0.9939, valid_acc 0.8644, Time 00:00:11,lr 0.001\n",
      "epoch 71, loss 0.01941, train_acc 0.9941, valid_acc 0.8637, Time 00:00:11,lr 0.001\n",
      "epoch 72, loss 0.01928, train_acc 0.9942, valid_acc 0.8627, Time 00:00:11,lr 0.001\n",
      "epoch 73, loss 0.01877, train_acc 0.9942, valid_acc 0.8647, Time 00:00:11,lr 0.001\n",
      "epoch 74, loss 0.01855, train_acc 0.9949, valid_acc 0.8622, Time 00:00:11,lr 0.001\n",
      "epoch 75, loss 0.01923, train_acc 0.9942, valid_acc 0.8626, Time 00:00:11,lr 0.001\n",
      "epoch 76, loss 0.01977, train_acc 0.9936, valid_acc 0.8637, Time 00:00:11,lr 0.001\n",
      "epoch 77, loss 0.01736, train_acc 0.9946, valid_acc 0.8631, Time 00:00:11,lr 0.001\n",
      "epoch 78, loss 0.01781, train_acc 0.9950, valid_acc 0.8619, Time 00:00:11,lr 0.001\n",
      "epoch 79, loss 0.01892, train_acc 0.9942, valid_acc 0.8623, Time 00:00:11,lr 0.001\n",
      "epoch 80, loss 0.01889, train_acc 0.9943, valid_acc 0.8638, Time 00:00:11,lr 0.001\n",
      "epoch 81, loss 0.01762, train_acc 0.9950, valid_acc 0.8634, Time 00:00:11,lr 0.001\n",
      "epoch 82, loss 0.01786, train_acc 0.9949, valid_acc 0.8635, Time 00:00:11,lr 0.001\n",
      "epoch 83, loss 0.01742, train_acc 0.9950, valid_acc 0.8615, Time 00:00:11,lr 0.001\n",
      "epoch 84, loss 0.01787, train_acc 0.9945, valid_acc 0.8604, Time 00:00:11,lr 0.001\n",
      "epoch 85, loss 0.01826, train_acc 0.9945, valid_acc 0.8630, Time 00:00:11,lr 0.001\n",
      "epoch 86, loss 0.01832, train_acc 0.9943, valid_acc 0.8630, Time 00:00:11,lr 0.001\n",
      "epoch 87, loss 0.01713, train_acc 0.9952, valid_acc 0.8627, Time 00:00:11,lr 0.001\n",
      "epoch 88, loss 0.01715, train_acc 0.9951, valid_acc 0.8617, Time 00:00:11,lr 0.001\n",
      "epoch 89, loss 0.01689, train_acc 0.9952, valid_acc 0.8651, Time 00:00:11,lr 0.001\n",
      "epoch 90, loss 0.01794, train_acc 0.9946, valid_acc 0.8636, Time 00:00:11,lr 0.001\n",
      "epoch 91, loss 0.01744, train_acc 0.9946, valid_acc 0.8623, Time 00:00:11,lr 0.001\n",
      "epoch 92, loss 0.01761, train_acc 0.9948, valid_acc 0.8610, Time 00:00:11,lr 0.001\n",
      "epoch 93, loss 0.01772, train_acc 0.9948, valid_acc 0.8613, Time 00:00:11,lr 0.001\n",
      "epoch 94, loss 0.01742, train_acc 0.9947, valid_acc 0.8643, Time 00:00:11,lr 0.001\n",
      "epoch 95, loss 0.01692, train_acc 0.9951, valid_acc 0.8622, Time 00:00:11,lr 0.001\n",
      "epoch 96, loss 0.01661, train_acc 0.9950, valid_acc 0.8621, Time 00:00:11,lr 0.001\n",
      "epoch 97, loss 0.01634, train_acc 0.9956, valid_acc 0.8642, Time 00:00:11,lr 0.001\n",
      "epoch 98, loss 0.01721, train_acc 0.9947, valid_acc 0.8631, Time 00:00:11,lr 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss 0.01719, train_acc 0.9948, valid_acc 0.8623, Time 00:00:11,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100      # 100 + 150 epoch\n",
    "learning_rate = 0.001\n",
    "lr_period = [100]\n",
    "\n",
    "net.load_params(\"../../models/train3_resnet18_v2_finetune_DA1_e250\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_DA1_e350\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 finetune begin with small lr\n",
    "I thought use big lr will not show the benifit of finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:25:18.410854Z",
     "start_time": "2018-03-02T03:10:10.180085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.28070, train_acc 0.5484, valid_acc 0.6659, Time 00:00:09,lr 0.001\n",
      "epoch 1, loss 0.87181, train_acc 0.6942, valid_acc 0.7318, Time 00:00:11,lr 0.001\n",
      "epoch 2, loss 0.75666, train_acc 0.7330, valid_acc 0.7585, Time 00:00:11,lr 0.001\n",
      "epoch 3, loss 0.67932, train_acc 0.7616, valid_acc 0.7797, Time 00:00:11,lr 0.001\n",
      "epoch 4, loss 0.62995, train_acc 0.7807, valid_acc 0.7881, Time 00:00:11,lr 0.001\n",
      "epoch 5, loss 0.59185, train_acc 0.7945, valid_acc 0.7990, Time 00:00:11,lr 0.001\n",
      "epoch 6, loss 0.55680, train_acc 0.8047, valid_acc 0.8007, Time 00:00:11,lr 0.001\n",
      "epoch 7, loss 0.53603, train_acc 0.8130, valid_acc 0.8134, Time 00:00:11,lr 0.001\n",
      "epoch 8, loss 0.50757, train_acc 0.8222, valid_acc 0.8152, Time 00:00:11,lr 0.001\n",
      "epoch 9, loss 0.48074, train_acc 0.8317, valid_acc 0.8184, Time 00:00:11,lr 0.001\n",
      "epoch 10, loss 0.45911, train_acc 0.8391, valid_acc 0.8214, Time 00:00:11,lr 0.001\n",
      "epoch 11, loss 0.44425, train_acc 0.8454, valid_acc 0.8180, Time 00:00:11,lr 0.001\n",
      "epoch 12, loss 0.42952, train_acc 0.8489, valid_acc 0.8235, Time 00:00:11,lr 0.001\n",
      "epoch 13, loss 0.40571, train_acc 0.8552, valid_acc 0.8287, Time 00:00:11,lr 0.001\n",
      "epoch 14, loss 0.39024, train_acc 0.8624, valid_acc 0.8291, Time 00:00:11,lr 0.001\n",
      "epoch 15, loss 0.38157, train_acc 0.8640, valid_acc 0.8268, Time 00:00:11,lr 0.001\n",
      "epoch 16, loss 0.36596, train_acc 0.8694, valid_acc 0.8325, Time 00:00:11,lr 0.001\n",
      "epoch 17, loss 0.34866, train_acc 0.8761, valid_acc 0.8344, Time 00:00:11,lr 0.001\n",
      "epoch 18, loss 0.33592, train_acc 0.8797, valid_acc 0.8347, Time 00:00:11,lr 0.001\n",
      "epoch 19, loss 0.32504, train_acc 0.8835, valid_acc 0.8359, Time 00:00:11,lr 0.001\n",
      "epoch 20, loss 0.31255, train_acc 0.8887, valid_acc 0.8325, Time 00:00:11,lr 0.001\n",
      "epoch 21, loss 0.29850, train_acc 0.8937, valid_acc 0.8332, Time 00:00:11,lr 0.001\n",
      "epoch 22, loss 0.28633, train_acc 0.8993, valid_acc 0.8355, Time 00:00:11,lr 0.001\n",
      "epoch 23, loss 0.28496, train_acc 0.8980, valid_acc 0.8385, Time 00:00:11,lr 0.001\n",
      "epoch 24, loss 0.27356, train_acc 0.9034, valid_acc 0.8413, Time 00:00:11,lr 0.001\n",
      "epoch 25, loss 0.26162, train_acc 0.9055, valid_acc 0.8396, Time 00:00:11,lr 0.001\n",
      "epoch 26, loss 0.25043, train_acc 0.9110, valid_acc 0.8408, Time 00:00:11,lr 0.001\n",
      "epoch 27, loss 0.23963, train_acc 0.9145, valid_acc 0.8384, Time 00:00:11,lr 0.001\n",
      "epoch 28, loss 0.22899, train_acc 0.9181, valid_acc 0.8387, Time 00:00:11,lr 0.001\n",
      "epoch 29, loss 0.22901, train_acc 0.9182, valid_acc 0.8397, Time 00:00:11,lr 0.001\n",
      "epoch 30, loss 0.21234, train_acc 0.9234, valid_acc 0.8391, Time 00:00:11,lr 0.001\n",
      "epoch 31, loss 0.20503, train_acc 0.9280, valid_acc 0.8407, Time 00:00:11,lr 0.001\n",
      "epoch 32, loss 0.19802, train_acc 0.9295, valid_acc 0.8376, Time 00:00:11,lr 0.001\n",
      "epoch 33, loss 0.19614, train_acc 0.9296, valid_acc 0.8365, Time 00:00:11,lr 0.001\n",
      "epoch 34, loss 0.18965, train_acc 0.9327, valid_acc 0.8370, Time 00:00:11,lr 0.001\n",
      "epoch 35, loss 0.18015, train_acc 0.9354, valid_acc 0.8408, Time 00:00:11,lr 0.001\n",
      "epoch 36, loss 0.17122, train_acc 0.9404, valid_acc 0.8398, Time 00:00:11,lr 0.001\n",
      "epoch 37, loss 0.17036, train_acc 0.9392, valid_acc 0.8375, Time 00:00:11,lr 0.001\n",
      "epoch 38, loss 0.16285, train_acc 0.9419, valid_acc 0.8397, Time 00:00:11,lr 0.001\n",
      "epoch 39, loss 0.15669, train_acc 0.9440, valid_acc 0.8414, Time 00:00:11,lr 0.001\n",
      "epoch 40, loss 0.15191, train_acc 0.9462, valid_acc 0.8378, Time 00:00:11,lr 0.001\n",
      "epoch 41, loss 0.14769, train_acc 0.9472, valid_acc 0.8408, Time 00:00:11,lr 0.001\n",
      "epoch 42, loss 0.14525, train_acc 0.9483, valid_acc 0.8420, Time 00:00:11,lr 0.001\n",
      "epoch 43, loss 0.13800, train_acc 0.9511, valid_acc 0.8434, Time 00:00:11,lr 0.001\n",
      "epoch 44, loss 0.13653, train_acc 0.9516, valid_acc 0.8425, Time 00:00:11,lr 0.001\n",
      "epoch 45, loss 0.12865, train_acc 0.9554, valid_acc 0.8435, Time 00:00:11,lr 0.001\n",
      "epoch 46, loss 0.12368, train_acc 0.9554, valid_acc 0.8443, Time 00:00:11,lr 0.001\n",
      "epoch 47, loss 0.12321, train_acc 0.9566, valid_acc 0.8426, Time 00:00:11,lr 0.001\n",
      "epoch 48, loss 0.12107, train_acc 0.9577, valid_acc 0.8475, Time 00:00:11,lr 0.001\n",
      "epoch 49, loss 0.10998, train_acc 0.9609, valid_acc 0.8437, Time 00:00:11,lr 0.001\n",
      "epoch 50, loss 0.11569, train_acc 0.9593, valid_acc 0.8434, Time 00:00:11,lr 0.001\n",
      "epoch 51, loss 0.11210, train_acc 0.9613, valid_acc 0.8436, Time 00:00:11,lr 0.001\n",
      "epoch 52, loss 0.10516, train_acc 0.9619, valid_acc 0.8455, Time 00:00:11,lr 0.001\n",
      "epoch 53, loss 0.09938, train_acc 0.9648, valid_acc 0.8451, Time 00:00:11,lr 0.001\n",
      "epoch 54, loss 0.09466, train_acc 0.9663, valid_acc 0.8477, Time 00:00:11,lr 0.001\n",
      "epoch 55, loss 0.09440, train_acc 0.9670, valid_acc 0.8443, Time 00:00:11,lr 0.001\n",
      "epoch 56, loss 0.09222, train_acc 0.9680, valid_acc 0.8474, Time 00:00:11,lr 0.001\n",
      "epoch 57, loss 0.09156, train_acc 0.9683, valid_acc 0.8441, Time 00:00:11,lr 0.001\n",
      "epoch 58, loss 0.08860, train_acc 0.9695, valid_acc 0.8479, Time 00:00:11,lr 0.001\n",
      "epoch 59, loss 0.08653, train_acc 0.9695, valid_acc 0.8448, Time 00:00:11,lr 0.001\n",
      "epoch 60, loss 0.08493, train_acc 0.9695, valid_acc 0.8438, Time 00:00:11,lr 0.001\n",
      "epoch 61, loss 0.08224, train_acc 0.9715, valid_acc 0.8419, Time 00:00:11,lr 0.001\n",
      "epoch 62, loss 0.08147, train_acc 0.9718, valid_acc 0.8477, Time 00:00:11,lr 0.001\n",
      "epoch 63, loss 0.08008, train_acc 0.9719, valid_acc 0.8423, Time 00:00:11,lr 0.001\n",
      "epoch 64, loss 0.07786, train_acc 0.9728, valid_acc 0.8474, Time 00:00:11,lr 0.001\n",
      "epoch 65, loss 0.07561, train_acc 0.9729, valid_acc 0.8469, Time 00:00:11,lr 0.001\n",
      "epoch 66, loss 0.07311, train_acc 0.9748, valid_acc 0.8453, Time 00:00:11,lr 0.001\n",
      "epoch 67, loss 0.07063, train_acc 0.9762, valid_acc 0.8455, Time 00:00:11,lr 0.001\n",
      "epoch 68, loss 0.06772, train_acc 0.9757, valid_acc 0.8494, Time 00:00:11,lr 0.001\n",
      "epoch 69, loss 0.06816, train_acc 0.9765, valid_acc 0.8451, Time 00:00:11,lr 0.001\n",
      "epoch 70, loss 0.07025, train_acc 0.9755, valid_acc 0.8446, Time 00:00:11,lr 0.001\n",
      "epoch 71, loss 0.06601, train_acc 0.9773, valid_acc 0.8475, Time 00:00:11,lr 0.001\n",
      "epoch 72, loss 0.06536, train_acc 0.9765, valid_acc 0.8481, Time 00:00:11,lr 0.001\n",
      "epoch 73, loss 0.06210, train_acc 0.9788, valid_acc 0.8434, Time 00:00:11,lr 0.001\n",
      "epoch 74, loss 0.06137, train_acc 0.9794, valid_acc 0.8446, Time 00:00:11,lr 0.001\n",
      "epoch 75, loss 0.06220, train_acc 0.9790, valid_acc 0.8442, Time 00:00:11,lr 0.001\n",
      "epoch 76, loss 0.05752, train_acc 0.9810, valid_acc 0.8448, Time 00:00:11,lr 0.001\n",
      "epoch 77, loss 0.05889, train_acc 0.9793, valid_acc 0.8513, Time 00:00:11,lr 0.001\n",
      "epoch 78, loss 0.05707, train_acc 0.9807, valid_acc 0.8474, Time 00:00:11,lr 0.001\n",
      "epoch 79, loss 0.05533, train_acc 0.9815, valid_acc 0.8438, Time 00:00:11,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "net = get_resnet18_v2()\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "\n",
    "num_epochs = 80      # 50 * 1563 iter about\n",
    "learning_rate = 0.001\n",
    "lr_period = [80]\n",
    "\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_small_lr_e80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:32:52.921166Z",
     "start_time": "2018-03-02T03:25:18.412674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.04797, train_acc 0.9834, valid_acc 0.8493, Time 00:00:09,lr 0.0001\n",
      "epoch 1, loss 0.04121, train_acc 0.9862, valid_acc 0.8484, Time 00:00:11,lr 0.0001\n",
      "epoch 2, loss 0.03944, train_acc 0.9867, valid_acc 0.8523, Time 00:00:11,lr 0.0001\n",
      "epoch 3, loss 0.03901, train_acc 0.9874, valid_acc 0.8501, Time 00:00:11,lr 0.0001\n",
      "epoch 4, loss 0.03566, train_acc 0.9884, valid_acc 0.8516, Time 00:00:11,lr 0.0001\n",
      "epoch 5, loss 0.03694, train_acc 0.9884, valid_acc 0.8539, Time 00:00:11,lr 0.0001\n",
      "epoch 6, loss 0.03435, train_acc 0.9887, valid_acc 0.8518, Time 00:00:11,lr 0.0001\n",
      "epoch 7, loss 0.03511, train_acc 0.9885, valid_acc 0.8508, Time 00:00:11,lr 0.0001\n",
      "epoch 8, loss 0.03544, train_acc 0.9886, valid_acc 0.8509, Time 00:00:11,lr 0.0001\n",
      "epoch 9, loss 0.03100, train_acc 0.9902, valid_acc 0.8514, Time 00:00:11,lr 0.0001\n",
      "epoch 10, loss 0.03248, train_acc 0.9900, valid_acc 0.8529, Time 00:00:11,lr 0.0001\n",
      "epoch 11, loss 0.03225, train_acc 0.9901, valid_acc 0.8531, Time 00:00:11,lr 0.0001\n",
      "epoch 12, loss 0.03250, train_acc 0.9895, valid_acc 0.8507, Time 00:00:11,lr 0.0001\n",
      "epoch 13, loss 0.03177, train_acc 0.9897, valid_acc 0.8508, Time 00:00:11,lr 0.0001\n",
      "epoch 14, loss 0.03059, train_acc 0.9902, valid_acc 0.8515, Time 00:00:11,lr 0.0001\n",
      "epoch 15, loss 0.03115, train_acc 0.9898, valid_acc 0.8522, Time 00:00:11,lr 0.0001\n",
      "epoch 16, loss 0.03083, train_acc 0.9905, valid_acc 0.8527, Time 00:00:11,lr 0.0001\n",
      "epoch 17, loss 0.03123, train_acc 0.9898, valid_acc 0.8531, Time 00:00:11,lr 0.0001\n",
      "epoch 18, loss 0.02858, train_acc 0.9911, valid_acc 0.8508, Time 00:00:11,lr 0.0001\n",
      "epoch 19, loss 0.02856, train_acc 0.9913, valid_acc 0.8525, Time 00:00:11,lr 0.0001\n",
      "epoch 20, loss 0.02801, train_acc 0.9914, valid_acc 0.8535, Time 00:00:11,lr 0.0001\n",
      "epoch 21, loss 0.02800, train_acc 0.9915, valid_acc 0.8523, Time 00:00:11,lr 0.0001\n",
      "epoch 22, loss 0.02759, train_acc 0.9912, valid_acc 0.8546, Time 00:00:11,lr 0.0001\n",
      "epoch 23, loss 0.02829, train_acc 0.9911, valid_acc 0.8537, Time 00:00:11,lr 0.0001\n",
      "epoch 24, loss 0.02775, train_acc 0.9910, valid_acc 0.8527, Time 00:00:11,lr 0.0001\n",
      "epoch 25, loss 0.02567, train_acc 0.9921, valid_acc 0.8541, Time 00:00:11,lr 0.0001\n",
      "epoch 26, loss 0.02561, train_acc 0.9920, valid_acc 0.8536, Time 00:00:11,lr 0.0001\n",
      "epoch 27, loss 0.02783, train_acc 0.9909, valid_acc 0.8525, Time 00:00:11,lr 0.0001\n",
      "epoch 28, loss 0.02628, train_acc 0.9918, valid_acc 0.8541, Time 00:00:11,lr 0.0001\n",
      "epoch 29, loss 0.02772, train_acc 0.9914, valid_acc 0.8537, Time 00:00:11,lr 0.0001\n",
      "epoch 30, loss 0.02634, train_acc 0.9920, valid_acc 0.8532, Time 00:00:11,lr 0.0001\n",
      "epoch 31, loss 0.02648, train_acc 0.9917, valid_acc 0.8528, Time 00:00:11,lr 0.0001\n",
      "epoch 32, loss 0.02602, train_acc 0.9923, valid_acc 0.8536, Time 00:00:11,lr 0.0001\n",
      "epoch 33, loss 0.02537, train_acc 0.9922, valid_acc 0.8526, Time 00:00:11,lr 0.0001\n",
      "epoch 34, loss 0.02574, train_acc 0.9917, valid_acc 0.8527, Time 00:00:11,lr 0.0001\n",
      "epoch 35, loss 0.02530, train_acc 0.9923, valid_acc 0.8516, Time 00:00:11,lr 0.0001\n",
      "epoch 36, loss 0.02520, train_acc 0.9921, valid_acc 0.8531, Time 00:00:11,lr 0.0001\n",
      "epoch 37, loss 0.02446, train_acc 0.9929, valid_acc 0.8521, Time 00:00:11,lr 0.0001\n",
      "epoch 38, loss 0.02461, train_acc 0.9922, valid_acc 0.8537, Time 00:00:11,lr 0.0001\n",
      "epoch 39, loss 0.02530, train_acc 0.9920, valid_acc 0.8527, Time 00:00:11,lr 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40      # 50 * 1563 iter about\n",
    "learning_rate = 0.0001\n",
    "lr_period = [40]\n",
    "net.load_params(\"../../models/train3_resnet18_v2_finetune_small_lr_e80\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_small_lr_e120\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.4 try DA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T04:26:49.405695Z",
     "start_time": "2018-03-02T03:32:52.922841Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 3.11658, train_acc 0.1416, valid_acc 0.2111, Time 00:00:20,lr 0.1\n",
      "epoch 1, loss 2.09582, train_acc 0.2126, valid_acc 0.3123, Time 00:00:21,lr 0.1\n",
      "epoch 2, loss 1.95817, train_acc 0.2712, valid_acc 0.3694, Time 00:00:21,lr 0.1\n",
      "epoch 3, loss 1.84647, train_acc 0.3231, valid_acc 0.4040, Time 00:00:21,lr 0.1\n",
      "epoch 4, loss 1.76677, train_acc 0.3577, valid_acc 0.4454, Time 00:00:21,lr 0.1\n",
      "epoch 5, loss 1.70275, train_acc 0.3834, valid_acc 0.4674, Time 00:00:21,lr 0.1\n",
      "epoch 6, loss 1.64178, train_acc 0.4087, valid_acc 0.4802, Time 00:00:21,lr 0.1\n",
      "epoch 7, loss 1.60851, train_acc 0.4211, valid_acc 0.5013, Time 00:00:21,lr 0.1\n",
      "epoch 8, loss 1.53296, train_acc 0.4511, valid_acc 0.5281, Time 00:00:21,lr 0.1\n",
      "epoch 9, loss 1.47802, train_acc 0.4725, valid_acc 0.5324, Time 00:00:21,lr 0.1\n",
      "epoch 10, loss 1.43122, train_acc 0.4918, valid_acc 0.5658, Time 00:00:21,lr 0.1\n",
      "epoch 11, loss 1.53871, train_acc 0.4492, valid_acc 0.2993, Time 00:00:21,lr 0.1\n",
      "epoch 12, loss 1.84977, train_acc 0.3270, valid_acc 0.4725, Time 00:00:21,lr 0.1\n",
      "epoch 13, loss 1.69150, train_acc 0.3920, valid_acc 0.5099, Time 00:00:21,lr 0.1\n",
      "epoch 14, loss 1.61971, train_acc 0.4239, valid_acc 0.5320, Time 00:00:21,lr 0.1\n",
      "epoch 15, loss 1.54674, train_acc 0.4479, valid_acc 0.5457, Time 00:00:21,lr 0.1\n",
      "epoch 16, loss 1.47876, train_acc 0.4759, valid_acc 0.5586, Time 00:00:21,lr 0.1\n",
      "epoch 17, loss 1.42175, train_acc 0.4955, valid_acc 0.5898, Time 00:00:21,lr 0.1\n",
      "epoch 18, loss 1.37033, train_acc 0.5165, valid_acc 0.5936, Time 00:00:21,lr 0.1\n",
      "epoch 19, loss 1.34337, train_acc 0.5262, valid_acc 0.6074, Time 00:00:21,lr 0.1\n",
      "epoch 20, loss 1.31890, train_acc 0.5352, valid_acc 0.6196, Time 00:00:21,lr 0.1\n",
      "epoch 21, loss 1.27578, train_acc 0.5524, valid_acc 0.6333, Time 00:00:21,lr 0.1\n",
      "epoch 22, loss 1.25617, train_acc 0.5576, valid_acc 0.6274, Time 00:00:21,lr 0.1\n",
      "epoch 23, loss 1.23998, train_acc 0.5643, valid_acc 0.6419, Time 00:00:21,lr 0.1\n",
      "epoch 24, loss 1.20687, train_acc 0.5773, valid_acc 0.6442, Time 00:00:21,lr 0.1\n",
      "epoch 25, loss 1.19323, train_acc 0.5843, valid_acc 0.6603, Time 00:00:21,lr 0.1\n",
      "epoch 26, loss 1.17770, train_acc 0.5865, valid_acc 0.6353, Time 00:00:21,lr 0.1\n",
      "epoch 27, loss 1.14914, train_acc 0.5969, valid_acc 0.6591, Time 00:00:21,lr 0.1\n",
      "epoch 28, loss 1.12389, train_acc 0.6077, valid_acc 0.6737, Time 00:00:21,lr 0.1\n",
      "epoch 29, loss 1.10253, train_acc 0.6145, valid_acc 0.6658, Time 00:00:21,lr 0.1\n",
      "epoch 30, loss 1.09169, train_acc 0.6168, valid_acc 0.6774, Time 00:00:21,lr 0.1\n",
      "epoch 31, loss 1.07391, train_acc 0.6240, valid_acc 0.7026, Time 00:00:21,lr 0.1\n",
      "epoch 32, loss 1.03957, train_acc 0.6374, valid_acc 0.6956, Time 00:00:21,lr 0.1\n",
      "epoch 33, loss 1.03640, train_acc 0.6386, valid_acc 0.6952, Time 00:00:21,lr 0.1\n",
      "epoch 34, loss 1.02023, train_acc 0.6445, valid_acc 0.7021, Time 00:00:21,lr 0.1\n",
      "epoch 35, loss 1.00806, train_acc 0.6495, valid_acc 0.7044, Time 00:00:21,lr 0.1\n",
      "epoch 36, loss 0.99415, train_acc 0.6540, valid_acc 0.6918, Time 00:00:21,lr 0.1\n",
      "epoch 37, loss 0.98240, train_acc 0.6576, valid_acc 0.7011, Time 00:00:21,lr 0.1\n",
      "epoch 38, loss 0.96824, train_acc 0.6624, valid_acc 0.7247, Time 00:00:21,lr 0.1\n",
      "epoch 39, loss 0.96214, train_acc 0.6653, valid_acc 0.7176, Time 00:00:21,lr 0.1\n",
      "epoch 40, loss 0.94749, train_acc 0.6722, valid_acc 0.6934, Time 00:00:21,lr 0.1\n",
      "epoch 41, loss 0.94345, train_acc 0.6743, valid_acc 0.7062, Time 00:00:21,lr 0.1\n",
      "epoch 42, loss 0.92819, train_acc 0.6795, valid_acc 0.7304, Time 00:00:21,lr 0.1\n",
      "epoch 43, loss 0.92148, train_acc 0.6797, valid_acc 0.7245, Time 00:00:21,lr 0.1\n",
      "epoch 44, loss 0.91474, train_acc 0.6830, valid_acc 0.7234, Time 00:00:21,lr 0.1\n",
      "epoch 45, loss 0.90811, train_acc 0.6853, valid_acc 0.7283, Time 00:00:22,lr 0.1\n",
      "epoch 46, loss 0.89448, train_acc 0.6898, valid_acc 0.7094, Time 00:00:21,lr 0.1\n",
      "epoch 47, loss 0.88712, train_acc 0.6947, valid_acc 0.7360, Time 00:00:21,lr 0.1\n",
      "epoch 48, loss 0.89035, train_acc 0.6916, valid_acc 0.7463, Time 00:00:21,lr 0.1\n",
      "epoch 49, loss 0.87290, train_acc 0.6975, valid_acc 0.7507, Time 00:00:21,lr 0.1\n",
      "epoch 50, loss 0.86660, train_acc 0.6992, valid_acc 0.7391, Time 00:00:21,lr 0.1\n",
      "epoch 51, loss 0.85922, train_acc 0.7020, valid_acc 0.7296, Time 00:00:21,lr 0.1\n",
      "epoch 52, loss 0.85432, train_acc 0.7048, valid_acc 0.7404, Time 00:00:21,lr 0.1\n",
      "epoch 53, loss 0.85272, train_acc 0.7042, valid_acc 0.7400, Time 00:00:21,lr 0.1\n",
      "epoch 54, loss 0.83921, train_acc 0.7103, valid_acc 0.7203, Time 00:00:21,lr 0.1\n",
      "epoch 55, loss 0.83884, train_acc 0.7123, valid_acc 0.7419, Time 00:00:21,lr 0.1\n",
      "epoch 56, loss 0.82632, train_acc 0.7151, valid_acc 0.7566, Time 00:00:21,lr 0.1\n",
      "epoch 57, loss 0.82471, train_acc 0.7164, valid_acc 0.7541, Time 00:00:21,lr 0.1\n",
      "epoch 58, loss 0.81686, train_acc 0.7185, valid_acc 0.7577, Time 00:00:21,lr 0.1\n",
      "epoch 59, loss 0.81452, train_acc 0.7163, valid_acc 0.7403, Time 00:00:21,lr 0.1\n",
      "epoch 60, loss 0.80729, train_acc 0.7235, valid_acc 0.7451, Time 00:00:21,lr 0.1\n",
      "epoch 61, loss 0.79842, train_acc 0.7237, valid_acc 0.7501, Time 00:00:21,lr 0.1\n",
      "epoch 62, loss 0.79740, train_acc 0.7252, valid_acc 0.7624, Time 00:00:21,lr 0.1\n",
      "epoch 63, loss 0.79414, train_acc 0.7265, valid_acc 0.7635, Time 00:00:21,lr 0.1\n",
      "epoch 64, loss 0.79002, train_acc 0.7262, valid_acc 0.7594, Time 00:00:21,lr 0.1\n",
      "epoch 65, loss 0.79050, train_acc 0.7264, valid_acc 0.7559, Time 00:00:21,lr 0.1\n",
      "epoch 66, loss 0.77959, train_acc 0.7300, valid_acc 0.7634, Time 00:00:21,lr 0.1\n",
      "epoch 67, loss 0.77985, train_acc 0.7307, valid_acc 0.7505, Time 00:00:21,lr 0.1\n",
      "epoch 68, loss 0.77456, train_acc 0.7322, valid_acc 0.7559, Time 00:00:21,lr 0.1\n",
      "epoch 69, loss 0.76402, train_acc 0.7374, valid_acc 0.7668, Time 00:00:21,lr 0.1\n",
      "epoch 70, loss 0.76829, train_acc 0.7357, valid_acc 0.7655, Time 00:00:21,lr 0.1\n",
      "epoch 71, loss 0.76313, train_acc 0.7380, valid_acc 0.7713, Time 00:00:21,lr 0.1\n",
      "epoch 72, loss 0.76575, train_acc 0.7361, valid_acc 0.7698, Time 00:00:21,lr 0.1\n",
      "epoch 73, loss 0.76049, train_acc 0.7383, valid_acc 0.7790, Time 00:00:21,lr 0.1\n",
      "epoch 74, loss 0.74716, train_acc 0.7441, valid_acc 0.7635, Time 00:00:21,lr 0.1\n",
      "epoch 75, loss 0.74522, train_acc 0.7426, valid_acc 0.7746, Time 00:00:21,lr 0.1\n",
      "epoch 76, loss 0.75552, train_acc 0.7402, valid_acc 0.7771, Time 00:00:21,lr 0.1\n",
      "epoch 77, loss 0.74099, train_acc 0.7437, valid_acc 0.7676, Time 00:00:21,lr 0.1\n",
      "epoch 78, loss 0.73329, train_acc 0.7469, valid_acc 0.7596, Time 00:00:21,lr 0.1\n",
      "epoch 79, loss 0.73058, train_acc 0.7478, valid_acc 0.7770, Time 00:00:21,lr 0.1\n",
      "epoch 80, loss 0.73485, train_acc 0.7466, valid_acc 0.7800, Time 00:00:21,lr 0.1\n",
      "epoch 81, loss 0.72577, train_acc 0.7503, valid_acc 0.7855, Time 00:00:21,lr 0.1\n",
      "epoch 82, loss 0.72735, train_acc 0.7495, valid_acc 0.7787, Time 00:00:21,lr 0.1\n",
      "epoch 83, loss 0.72139, train_acc 0.7517, valid_acc 0.7796, Time 00:00:21,lr 0.1\n",
      "epoch 84, loss 0.72150, train_acc 0.7533, valid_acc 0.7673, Time 00:00:21,lr 0.1\n",
      "epoch 85, loss 0.71393, train_acc 0.7551, valid_acc 0.7787, Time 00:00:21,lr 0.1\n",
      "epoch 86, loss 0.71681, train_acc 0.7533, valid_acc 0.7857, Time 00:00:21,lr 0.1\n",
      "epoch 87, loss 0.70170, train_acc 0.7568, valid_acc 0.7896, Time 00:00:21,lr 0.1\n",
      "epoch 88, loss 0.70562, train_acc 0.7558, valid_acc 0.7779, Time 00:00:21,lr 0.1\n",
      "epoch 89, loss 0.70347, train_acc 0.7589, valid_acc 0.7573, Time 00:00:21,lr 0.1\n",
      "epoch 90, loss 0.69812, train_acc 0.7589, valid_acc 0.7856, Time 00:00:21,lr 0.1\n",
      "epoch 91, loss 0.68913, train_acc 0.7606, valid_acc 0.7824, Time 00:00:21,lr 0.1\n",
      "epoch 92, loss 0.70933, train_acc 0.7556, valid_acc 0.7753, Time 00:00:21,lr 0.1\n",
      "epoch 93, loss 0.70442, train_acc 0.7572, valid_acc 0.7869, Time 00:00:21,lr 0.1\n",
      "epoch 94, loss 0.69777, train_acc 0.7592, valid_acc 0.7800, Time 00:00:21,lr 0.1\n",
      "epoch 95, loss 0.68607, train_acc 0.7643, valid_acc 0.7905, Time 00:00:21,lr 0.1\n",
      "epoch 96, loss 0.68276, train_acc 0.7657, valid_acc 0.7776, Time 00:00:21,lr 0.1\n",
      "epoch 97, loss 0.68862, train_acc 0.7644, valid_acc 0.7760, Time 00:00:21,lr 0.1\n",
      "epoch 98, loss 0.68275, train_acc 0.7661, valid_acc 0.7838, Time 00:00:21,lr 0.1\n",
      "epoch 99, loss 0.67443, train_acc 0.7678, valid_acc 0.7847, Time 00:00:21,lr 0.1\n",
      "epoch 100, loss 0.68213, train_acc 0.7665, valid_acc 0.7936, Time 00:00:21,lr 0.1\n",
      "epoch 101, loss 0.68095, train_acc 0.7651, valid_acc 0.7914, Time 00:00:21,lr 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 102, loss 0.67687, train_acc 0.7662, valid_acc 0.7782, Time 00:00:21,lr 0.1\n",
      "epoch 103, loss 0.67751, train_acc 0.7670, valid_acc 0.7813, Time 00:00:21,lr 0.1\n",
      "epoch 104, loss 0.67318, train_acc 0.7685, valid_acc 0.7808, Time 00:00:21,lr 0.1\n",
      "epoch 105, loss 0.66943, train_acc 0.7691, valid_acc 0.7823, Time 00:00:21,lr 0.1\n",
      "epoch 106, loss 0.67118, train_acc 0.7677, valid_acc 0.7928, Time 00:00:21,lr 0.1\n",
      "epoch 107, loss 0.66830, train_acc 0.7686, valid_acc 0.7750, Time 00:00:21,lr 0.1\n",
      "epoch 108, loss 0.66392, train_acc 0.7721, valid_acc 0.8018, Time 00:00:21,lr 0.1\n",
      "epoch 109, loss 0.65623, train_acc 0.7736, valid_acc 0.7922, Time 00:00:21,lr 0.1\n",
      "epoch 110, loss 0.65318, train_acc 0.7747, valid_acc 0.7908, Time 00:00:21,lr 0.1\n",
      "epoch 111, loss 0.65489, train_acc 0.7735, valid_acc 0.7892, Time 00:00:21,lr 0.1\n",
      "epoch 112, loss 0.65816, train_acc 0.7740, valid_acc 0.7902, Time 00:00:21,lr 0.1\n",
      "epoch 113, loss 0.65512, train_acc 0.7769, valid_acc 0.7834, Time 00:00:21,lr 0.1\n",
      "epoch 114, loss 0.65101, train_acc 0.7771, valid_acc 0.7858, Time 00:00:21,lr 0.1\n",
      "epoch 115, loss 0.65831, train_acc 0.7737, valid_acc 0.7856, Time 00:00:21,lr 0.1\n",
      "epoch 116, loss 0.64652, train_acc 0.7793, valid_acc 0.7814, Time 00:00:21,lr 0.1\n",
      "epoch 117, loss 0.65595, train_acc 0.7745, valid_acc 0.7952, Time 00:00:21,lr 0.1\n",
      "epoch 118, loss 0.64459, train_acc 0.7786, valid_acc 0.7880, Time 00:00:21,lr 0.1\n",
      "epoch 119, loss 0.64453, train_acc 0.7789, valid_acc 0.7985, Time 00:00:21,lr 0.1\n",
      "epoch 120, loss 0.65007, train_acc 0.7763, valid_acc 0.7930, Time 00:00:21,lr 0.1\n",
      "epoch 121, loss 0.63937, train_acc 0.7811, valid_acc 0.7883, Time 00:00:21,lr 0.1\n",
      "epoch 122, loss 0.64279, train_acc 0.7784, valid_acc 0.7932, Time 00:00:21,lr 0.1\n",
      "epoch 123, loss 0.64558, train_acc 0.7777, valid_acc 0.8021, Time 00:00:21,lr 0.1\n",
      "epoch 124, loss 0.64537, train_acc 0.7768, valid_acc 0.7920, Time 00:00:21,lr 0.1\n",
      "epoch 125, loss 0.64060, train_acc 0.7805, valid_acc 0.7805, Time 00:00:21,lr 0.1\n",
      "epoch 126, loss 0.64606, train_acc 0.7773, valid_acc 0.7931, Time 00:00:21,lr 0.1\n",
      "epoch 127, loss 0.63683, train_acc 0.7809, valid_acc 0.7968, Time 00:00:21,lr 0.1\n",
      "epoch 128, loss 0.63184, train_acc 0.7818, valid_acc 0.7751, Time 00:00:21,lr 0.1\n",
      "epoch 129, loss 0.63997, train_acc 0.7809, valid_acc 0.8044, Time 00:00:21,lr 0.1\n",
      "epoch 130, loss 0.63618, train_acc 0.7811, valid_acc 0.7901, Time 00:00:21,lr 0.1\n",
      "epoch 131, loss 0.62924, train_acc 0.7820, valid_acc 0.8001, Time 00:00:21,lr 0.1\n",
      "epoch 132, loss 0.63617, train_acc 0.7823, valid_acc 0.7963, Time 00:00:21,lr 0.1\n",
      "epoch 133, loss 0.63670, train_acc 0.7818, valid_acc 0.7991, Time 00:00:21,lr 0.1\n",
      "epoch 134, loss 0.62374, train_acc 0.7865, valid_acc 0.7850, Time 00:00:21,lr 0.1\n",
      "epoch 135, loss 0.63331, train_acc 0.7807, valid_acc 0.7958, Time 00:00:21,lr 0.1\n",
      "epoch 136, loss 0.62887, train_acc 0.7845, valid_acc 0.8016, Time 00:00:21,lr 0.1\n",
      "epoch 137, loss 0.62180, train_acc 0.7860, valid_acc 0.7877, Time 00:00:21,lr 0.1\n",
      "epoch 138, loss 0.62687, train_acc 0.7849, valid_acc 0.8016, Time 00:00:21,lr 0.1\n",
      "epoch 139, loss 0.61581, train_acc 0.7900, valid_acc 0.7979, Time 00:00:21,lr 0.1\n",
      "epoch 140, loss 0.61859, train_acc 0.7860, valid_acc 0.7826, Time 00:00:21,lr 0.1\n",
      "epoch 141, loss 0.61711, train_acc 0.7879, valid_acc 0.7807, Time 00:00:21,lr 0.1\n",
      "epoch 142, loss 0.61437, train_acc 0.7884, valid_acc 0.7894, Time 00:00:21,lr 0.1\n",
      "epoch 143, loss 0.61843, train_acc 0.7893, valid_acc 0.8053, Time 00:00:21,lr 0.1\n",
      "epoch 144, loss 0.62009, train_acc 0.7871, valid_acc 0.7927, Time 00:00:21,lr 0.1\n",
      "epoch 145, loss 0.62408, train_acc 0.7864, valid_acc 0.7888, Time 00:00:21,lr 0.1\n",
      "epoch 146, loss 0.61431, train_acc 0.7896, valid_acc 0.7996, Time 00:00:21,lr 0.1\n",
      "epoch 147, loss 0.61943, train_acc 0.7873, valid_acc 0.8066, Time 00:00:21,lr 0.1\n",
      "epoch 148, loss 0.61379, train_acc 0.7869, valid_acc 0.7961, Time 00:00:21,lr 0.1\n",
      "epoch 149, loss 0.62664, train_acc 0.7837, valid_acc 0.7950, Time 00:00:21,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=3)\n",
    "net = get_resnet18_v2()\n",
    "\n",
    "num_epochs = 150      # 50 * 1563 iter about\n",
    "learning_rate = 0.1\n",
    "lr_period = [150]\n",
    "\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_DA2_e150\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T05:02:51.575662Z",
     "start_time": "2018-03-02T04:26:49.407415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.51322, train_acc 0.8236, valid_acc 0.8331, Time 00:00:20,lr 0.01\n",
      "epoch 1, loss 0.46838, train_acc 0.8386, valid_acc 0.8359, Time 00:00:21,lr 0.01\n",
      "epoch 2, loss 0.44826, train_acc 0.8455, valid_acc 0.8392, Time 00:00:21,lr 0.01\n",
      "epoch 3, loss 0.43397, train_acc 0.8479, valid_acc 0.8416, Time 00:00:21,lr 0.01\n",
      "epoch 4, loss 0.42680, train_acc 0.8532, valid_acc 0.8440, Time 00:00:21,lr 0.01\n",
      "epoch 5, loss 0.42267, train_acc 0.8544, valid_acc 0.8429, Time 00:00:21,lr 0.01\n",
      "epoch 6, loss 0.41589, train_acc 0.8567, valid_acc 0.8442, Time 00:00:21,lr 0.01\n",
      "epoch 7, loss 0.40887, train_acc 0.8574, valid_acc 0.8473, Time 00:00:21,lr 0.01\n",
      "epoch 8, loss 0.40575, train_acc 0.8579, valid_acc 0.8425, Time 00:00:21,lr 0.01\n",
      "epoch 9, loss 0.39781, train_acc 0.8612, valid_acc 0.8458, Time 00:00:21,lr 0.01\n",
      "epoch 10, loss 0.39084, train_acc 0.8652, valid_acc 0.8451, Time 00:00:21,lr 0.01\n",
      "epoch 11, loss 0.38765, train_acc 0.8656, valid_acc 0.8448, Time 00:00:21,lr 0.01\n",
      "epoch 12, loss 0.38949, train_acc 0.8641, valid_acc 0.8465, Time 00:00:21,lr 0.01\n",
      "epoch 13, loss 0.37845, train_acc 0.8684, valid_acc 0.8462, Time 00:00:21,lr 0.01\n",
      "epoch 14, loss 0.37988, train_acc 0.8687, valid_acc 0.8488, Time 00:00:21,lr 0.01\n",
      "epoch 15, loss 0.37487, train_acc 0.8706, valid_acc 0.8490, Time 00:00:21,lr 0.01\n",
      "epoch 16, loss 0.37835, train_acc 0.8704, valid_acc 0.8456, Time 00:00:21,lr 0.01\n",
      "epoch 17, loss 0.37238, train_acc 0.8699, valid_acc 0.8509, Time 00:00:21,lr 0.01\n",
      "epoch 18, loss 0.36749, train_acc 0.8711, valid_acc 0.8528, Time 00:00:21,lr 0.01\n",
      "epoch 19, loss 0.36624, train_acc 0.8726, valid_acc 0.8516, Time 00:00:21,lr 0.01\n",
      "epoch 20, loss 0.36831, train_acc 0.8725, valid_acc 0.8540, Time 00:00:21,lr 0.01\n",
      "epoch 21, loss 0.35824, train_acc 0.8750, valid_acc 0.8495, Time 00:00:21,lr 0.01\n",
      "epoch 22, loss 0.36049, train_acc 0.8755, valid_acc 0.8505, Time 00:00:21,lr 0.01\n",
      "epoch 23, loss 0.35712, train_acc 0.8762, valid_acc 0.8527, Time 00:00:21,lr 0.01\n",
      "epoch 24, loss 0.35367, train_acc 0.8766, valid_acc 0.8513, Time 00:00:21,lr 0.01\n",
      "epoch 25, loss 0.35595, train_acc 0.8772, valid_acc 0.8525, Time 00:00:21,lr 0.01\n",
      "epoch 26, loss 0.34549, train_acc 0.8808, valid_acc 0.8519, Time 00:00:21,lr 0.01\n",
      "epoch 27, loss 0.34621, train_acc 0.8808, valid_acc 0.8485, Time 00:00:21,lr 0.01\n",
      "epoch 28, loss 0.35368, train_acc 0.8778, valid_acc 0.8527, Time 00:00:21,lr 0.01\n",
      "epoch 29, loss 0.34598, train_acc 0.8795, valid_acc 0.8489, Time 00:00:21,lr 0.01\n",
      "epoch 30, loss 0.34259, train_acc 0.8823, valid_acc 0.8469, Time 00:00:21,lr 0.01\n",
      "epoch 31, loss 0.33989, train_acc 0.8816, valid_acc 0.8518, Time 00:00:21,lr 0.01\n",
      "epoch 32, loss 0.34395, train_acc 0.8799, valid_acc 0.8502, Time 00:00:21,lr 0.01\n",
      "epoch 33, loss 0.33645, train_acc 0.8836, valid_acc 0.8515, Time 00:00:21,lr 0.01\n",
      "epoch 34, loss 0.33424, train_acc 0.8836, valid_acc 0.8538, Time 00:00:21,lr 0.01\n",
      "epoch 35, loss 0.33821, train_acc 0.8823, valid_acc 0.8526, Time 00:00:21,lr 0.01\n",
      "epoch 36, loss 0.33564, train_acc 0.8833, valid_acc 0.8529, Time 00:00:21,lr 0.01\n",
      "epoch 37, loss 0.32975, train_acc 0.8856, valid_acc 0.8527, Time 00:00:21,lr 0.01\n",
      "epoch 38, loss 0.33104, train_acc 0.8865, valid_acc 0.8522, Time 00:00:21,lr 0.01\n",
      "epoch 39, loss 0.32996, train_acc 0.8853, valid_acc 0.8525, Time 00:00:21,lr 0.01\n",
      "epoch 40, loss 0.32914, train_acc 0.8857, valid_acc 0.8534, Time 00:00:21,lr 0.01\n",
      "epoch 41, loss 0.32797, train_acc 0.8869, valid_acc 0.8531, Time 00:00:21,lr 0.01\n",
      "epoch 42, loss 0.33073, train_acc 0.8852, valid_acc 0.8513, Time 00:00:21,lr 0.01\n",
      "epoch 43, loss 0.32416, train_acc 0.8874, valid_acc 0.8495, Time 00:00:21,lr 0.01\n",
      "epoch 44, loss 0.32527, train_acc 0.8875, valid_acc 0.8522, Time 00:00:21,lr 0.01\n",
      "epoch 45, loss 0.32531, train_acc 0.8869, valid_acc 0.8485, Time 00:00:21,lr 0.01\n",
      "epoch 46, loss 0.32216, train_acc 0.8890, valid_acc 0.8522, Time 00:00:21,lr 0.01\n",
      "epoch 47, loss 0.32339, train_acc 0.8868, valid_acc 0.8496, Time 00:00:21,lr 0.01\n",
      "epoch 48, loss 0.31869, train_acc 0.8896, valid_acc 0.8511, Time 00:00:21,lr 0.01\n",
      "epoch 49, loss 0.32032, train_acc 0.8883, valid_acc 0.8510, Time 00:00:21,lr 0.01\n",
      "epoch 50, loss 0.31499, train_acc 0.8906, valid_acc 0.8496, Time 00:00:21,lr 0.01\n",
      "epoch 51, loss 0.31383, train_acc 0.8914, valid_acc 0.8535, Time 00:00:21,lr 0.01\n",
      "epoch 52, loss 0.31263, train_acc 0.8919, valid_acc 0.8520, Time 00:00:21,lr 0.01\n",
      "epoch 53, loss 0.31388, train_acc 0.8906, valid_acc 0.8536, Time 00:00:21,lr 0.01\n",
      "epoch 54, loss 0.31122, train_acc 0.8930, valid_acc 0.8529, Time 00:00:21,lr 0.01\n",
      "epoch 55, loss 0.30628, train_acc 0.8937, valid_acc 0.8539, Time 00:00:21,lr 0.01\n",
      "epoch 56, loss 0.30827, train_acc 0.8928, valid_acc 0.8553, Time 00:00:21,lr 0.01\n",
      "epoch 57, loss 0.31109, train_acc 0.8918, valid_acc 0.8537, Time 00:00:21,lr 0.01\n",
      "epoch 58, loss 0.30791, train_acc 0.8938, valid_acc 0.8519, Time 00:00:21,lr 0.01\n",
      "epoch 59, loss 0.30720, train_acc 0.8930, valid_acc 0.8525, Time 00:00:21,lr 0.01\n",
      "epoch 60, loss 0.30762, train_acc 0.8941, valid_acc 0.8527, Time 00:00:21,lr 0.01\n",
      "epoch 61, loss 0.30819, train_acc 0.8924, valid_acc 0.8537, Time 00:00:21,lr 0.01\n",
      "epoch 62, loss 0.30591, train_acc 0.8938, valid_acc 0.8564, Time 00:00:21,lr 0.01\n",
      "epoch 63, loss 0.30001, train_acc 0.8960, valid_acc 0.8522, Time 00:00:21,lr 0.01\n",
      "epoch 64, loss 0.30677, train_acc 0.8942, valid_acc 0.8509, Time 00:00:21,lr 0.01\n",
      "epoch 65, loss 0.30374, train_acc 0.8965, valid_acc 0.8523, Time 00:00:21,lr 0.01\n",
      "epoch 66, loss 0.30280, train_acc 0.8944, valid_acc 0.8566, Time 00:00:21,lr 0.01\n",
      "epoch 67, loss 0.30056, train_acc 0.8958, valid_acc 0.8531, Time 00:00:21,lr 0.01\n",
      "epoch 68, loss 0.30299, train_acc 0.8944, valid_acc 0.8538, Time 00:00:21,lr 0.01\n",
      "epoch 69, loss 0.30133, train_acc 0.8960, valid_acc 0.8531, Time 00:00:21,lr 0.01\n",
      "epoch 70, loss 0.30411, train_acc 0.8942, valid_acc 0.8534, Time 00:00:21,lr 0.01\n",
      "epoch 71, loss 0.29844, train_acc 0.8973, valid_acc 0.8567, Time 00:00:21,lr 0.01\n",
      "epoch 72, loss 0.29059, train_acc 0.8996, valid_acc 0.8508, Time 00:00:21,lr 0.01\n",
      "epoch 73, loss 0.30196, train_acc 0.8967, valid_acc 0.8560, Time 00:00:21,lr 0.01\n",
      "epoch 74, loss 0.29851, train_acc 0.8968, valid_acc 0.8490, Time 00:00:21,lr 0.01\n",
      "epoch 75, loss 0.29626, train_acc 0.8966, valid_acc 0.8519, Time 00:00:21,lr 0.01\n",
      "epoch 76, loss 0.29403, train_acc 0.8969, valid_acc 0.8555, Time 00:00:21,lr 0.01\n",
      "epoch 77, loss 0.29754, train_acc 0.8960, valid_acc 0.8507, Time 00:00:21,lr 0.01\n",
      "epoch 78, loss 0.29281, train_acc 0.9003, valid_acc 0.8485, Time 00:00:21,lr 0.01\n",
      "epoch 79, loss 0.29157, train_acc 0.8976, valid_acc 0.8578, Time 00:00:21,lr 0.01\n",
      "epoch 80, loss 0.29840, train_acc 0.8966, valid_acc 0.8550, Time 00:00:21,lr 0.01\n",
      "epoch 81, loss 0.28965, train_acc 0.8999, valid_acc 0.8558, Time 00:00:21,lr 0.01\n",
      "epoch 82, loss 0.29548, train_acc 0.8978, valid_acc 0.8565, Time 00:00:21,lr 0.01\n",
      "epoch 83, loss 0.29554, train_acc 0.8982, valid_acc 0.8527, Time 00:00:21,lr 0.01\n",
      "epoch 84, loss 0.29363, train_acc 0.8988, valid_acc 0.8565, Time 00:00:21,lr 0.01\n",
      "epoch 85, loss 0.29041, train_acc 0.8981, valid_acc 0.8489, Time 00:00:21,lr 0.01\n",
      "epoch 86, loss 0.29064, train_acc 0.8981, valid_acc 0.8512, Time 00:00:21,lr 0.01\n",
      "epoch 87, loss 0.28614, train_acc 0.9025, valid_acc 0.8548, Time 00:00:21,lr 0.01\n",
      "epoch 88, loss 0.29014, train_acc 0.9009, valid_acc 0.8547, Time 00:00:21,lr 0.01\n",
      "epoch 89, loss 0.29150, train_acc 0.8985, valid_acc 0.8542, Time 00:00:21,lr 0.01\n",
      "epoch 90, loss 0.28805, train_acc 0.9002, valid_acc 0.8487, Time 00:00:21,lr 0.01\n",
      "epoch 91, loss 0.28848, train_acc 0.9013, valid_acc 0.8536, Time 00:00:21,lr 0.01\n",
      "epoch 92, loss 0.28433, train_acc 0.9023, valid_acc 0.8517, Time 00:00:21,lr 0.01\n",
      "epoch 93, loss 0.29005, train_acc 0.8992, valid_acc 0.8546, Time 00:00:21,lr 0.01\n",
      "epoch 94, loss 0.29061, train_acc 0.8992, valid_acc 0.8536, Time 00:00:21,lr 0.01\n",
      "epoch 95, loss 0.28550, train_acc 0.9021, valid_acc 0.8552, Time 00:00:21,lr 0.01\n",
      "epoch 96, loss 0.28086, train_acc 0.9043, valid_acc 0.8538, Time 00:00:21,lr 0.01\n",
      "epoch 97, loss 0.28470, train_acc 0.9022, valid_acc 0.8490, Time 00:00:21,lr 0.01\n",
      "epoch 98, loss 0.28611, train_acc 0.9005, valid_acc 0.8499, Time 00:00:21,lr 0.01\n",
      "epoch 99, loss 0.28330, train_acc 0.9019, valid_acc 0.8551, Time 00:00:21,lr 0.01\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100      # 100 + 150 epoch\n",
    "learning_rate = 0.01\n",
    "lr_period = [100]\n",
    "\n",
    "net.load_params(\"../../models/train3_resnet18_v2_finetune_DA2_e150\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_DA2_e250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T05:38:49.462287Z",
     "start_time": "2018-03-02T05:02:51.577318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.57669, train_acc 0.8041, valid_acc 0.8167, Time 00:00:20,lr 0.001\n",
      "epoch 1, loss 0.53490, train_acc 0.8162, valid_acc 0.8238, Time 00:00:21,lr 0.001\n",
      "epoch 2, loss 0.51747, train_acc 0.8224, valid_acc 0.8280, Time 00:00:21,lr 0.001\n",
      "epoch 3, loss 0.50795, train_acc 0.8260, valid_acc 0.8278, Time 00:00:21,lr 0.001\n",
      "epoch 4, loss 0.49265, train_acc 0.8292, valid_acc 0.8269, Time 00:00:21,lr 0.001\n",
      "epoch 5, loss 0.48383, train_acc 0.8342, valid_acc 0.8315, Time 00:00:21,lr 0.001\n",
      "epoch 6, loss 0.47790, train_acc 0.8353, valid_acc 0.8309, Time 00:00:21,lr 0.001\n",
      "epoch 7, loss 0.47156, train_acc 0.8379, valid_acc 0.8309, Time 00:00:21,lr 0.001\n",
      "epoch 8, loss 0.47058, train_acc 0.8381, valid_acc 0.8314, Time 00:00:21,lr 0.001\n",
      "epoch 9, loss 0.46141, train_acc 0.8386, valid_acc 0.8338, Time 00:00:21,lr 0.001\n",
      "epoch 10, loss 0.46309, train_acc 0.8405, valid_acc 0.8344, Time 00:00:21,lr 0.001\n",
      "epoch 11, loss 0.45508, train_acc 0.8403, valid_acc 0.8358, Time 00:00:21,lr 0.001\n",
      "epoch 12, loss 0.45483, train_acc 0.8444, valid_acc 0.8353, Time 00:00:21,lr 0.001\n",
      "epoch 13, loss 0.44582, train_acc 0.8467, valid_acc 0.8360, Time 00:00:21,lr 0.001\n",
      "epoch 14, loss 0.44717, train_acc 0.8458, valid_acc 0.8354, Time 00:00:21,lr 0.001\n",
      "epoch 15, loss 0.44468, train_acc 0.8471, valid_acc 0.8360, Time 00:00:21,lr 0.001\n",
      "epoch 16, loss 0.44842, train_acc 0.8465, valid_acc 0.8375, Time 00:00:21,lr 0.001\n",
      "epoch 17, loss 0.43674, train_acc 0.8495, valid_acc 0.8382, Time 00:00:21,lr 0.001\n",
      "epoch 18, loss 0.44127, train_acc 0.8495, valid_acc 0.8372, Time 00:00:21,lr 0.001\n",
      "epoch 19, loss 0.44067, train_acc 0.8474, valid_acc 0.8380, Time 00:00:21,lr 0.001\n",
      "epoch 20, loss 0.43912, train_acc 0.8484, valid_acc 0.8387, Time 00:00:21,lr 0.001\n",
      "epoch 21, loss 0.43613, train_acc 0.8465, valid_acc 0.8387, Time 00:00:21,lr 0.001\n",
      "epoch 22, loss 0.43449, train_acc 0.8512, valid_acc 0.8392, Time 00:00:21,lr 0.001\n",
      "epoch 23, loss 0.42812, train_acc 0.8500, valid_acc 0.8389, Time 00:00:21,lr 0.001\n",
      "epoch 24, loss 0.43483, train_acc 0.8486, valid_acc 0.8393, Time 00:00:21,lr 0.001\n",
      "epoch 25, loss 0.42666, train_acc 0.8528, valid_acc 0.8410, Time 00:00:21,lr 0.001\n",
      "epoch 26, loss 0.42341, train_acc 0.8526, valid_acc 0.8405, Time 00:00:21,lr 0.001\n",
      "epoch 27, loss 0.42213, train_acc 0.8545, valid_acc 0.8403, Time 00:00:21,lr 0.001\n",
      "epoch 28, loss 0.42847, train_acc 0.8531, valid_acc 0.8414, Time 00:00:21,lr 0.001\n",
      "epoch 29, loss 0.42479, train_acc 0.8521, valid_acc 0.8393, Time 00:00:21,lr 0.001\n",
      "epoch 30, loss 0.41519, train_acc 0.8570, valid_acc 0.8427, Time 00:00:21,lr 0.001\n",
      "epoch 31, loss 0.41868, train_acc 0.8560, valid_acc 0.8420, Time 00:00:21,lr 0.001\n",
      "epoch 32, loss 0.42247, train_acc 0.8525, valid_acc 0.8419, Time 00:00:21,lr 0.001\n",
      "epoch 33, loss 0.41435, train_acc 0.8581, valid_acc 0.8439, Time 00:00:21,lr 0.001\n",
      "epoch 34, loss 0.42118, train_acc 0.8552, valid_acc 0.8437, Time 00:00:21,lr 0.001\n",
      "epoch 35, loss 0.41513, train_acc 0.8575, valid_acc 0.8416, Time 00:00:21,lr 0.001\n",
      "epoch 36, loss 0.41110, train_acc 0.8587, valid_acc 0.8424, Time 00:00:21,lr 0.001\n",
      "epoch 37, loss 0.41619, train_acc 0.8572, valid_acc 0.8436, Time 00:00:21,lr 0.001\n",
      "epoch 38, loss 0.41162, train_acc 0.8583, valid_acc 0.8422, Time 00:00:21,lr 0.001\n",
      "epoch 39, loss 0.40931, train_acc 0.8583, valid_acc 0.8437, Time 00:00:21,lr 0.001\n",
      "epoch 40, loss 0.40370, train_acc 0.8613, valid_acc 0.8440, Time 00:00:21,lr 0.001\n",
      "epoch 41, loss 0.40437, train_acc 0.8603, valid_acc 0.8441, Time 00:00:21,lr 0.001\n",
      "epoch 42, loss 0.40416, train_acc 0.8607, valid_acc 0.8445, Time 00:00:21,lr 0.001\n",
      "epoch 43, loss 0.40627, train_acc 0.8596, valid_acc 0.8436, Time 00:00:21,lr 0.001\n",
      "epoch 44, loss 0.41031, train_acc 0.8587, valid_acc 0.8441, Time 00:00:21,lr 0.001\n",
      "epoch 45, loss 0.40576, train_acc 0.8584, valid_acc 0.8435, Time 00:00:21,lr 0.001\n",
      "epoch 46, loss 0.40360, train_acc 0.8609, valid_acc 0.8450, Time 00:00:21,lr 0.001\n",
      "epoch 47, loss 0.40221, train_acc 0.8607, valid_acc 0.8446, Time 00:00:21,lr 0.001\n",
      "epoch 48, loss 0.40334, train_acc 0.8604, valid_acc 0.8438, Time 00:00:21,lr 0.001\n",
      "epoch 49, loss 0.39528, train_acc 0.8631, valid_acc 0.8448, Time 00:00:21,lr 0.001\n",
      "epoch 50, loss 0.39528, train_acc 0.8648, valid_acc 0.8447, Time 00:00:21,lr 0.001\n",
      "epoch 51, loss 0.39524, train_acc 0.8631, valid_acc 0.8453, Time 00:00:21,lr 0.001\n",
      "epoch 52, loss 0.39207, train_acc 0.8645, valid_acc 0.8466, Time 00:00:21,lr 0.001\n",
      "epoch 53, loss 0.39143, train_acc 0.8656, valid_acc 0.8462, Time 00:00:21,lr 0.001\n",
      "epoch 54, loss 0.39537, train_acc 0.8628, valid_acc 0.8457, Time 00:00:21,lr 0.001\n",
      "epoch 55, loss 0.39569, train_acc 0.8642, valid_acc 0.8460, Time 00:00:21,lr 0.001\n",
      "epoch 56, loss 0.39207, train_acc 0.8650, valid_acc 0.8444, Time 00:00:21,lr 0.001\n",
      "epoch 57, loss 0.39393, train_acc 0.8628, valid_acc 0.8457, Time 00:00:21,lr 0.001\n",
      "epoch 58, loss 0.39588, train_acc 0.8634, valid_acc 0.8446, Time 00:00:21,lr 0.001\n",
      "epoch 59, loss 0.39445, train_acc 0.8629, valid_acc 0.8457, Time 00:00:21,lr 0.001\n",
      "epoch 60, loss 0.39691, train_acc 0.8620, valid_acc 0.8453, Time 00:00:21,lr 0.001\n",
      "epoch 61, loss 0.38715, train_acc 0.8664, valid_acc 0.8482, Time 00:00:21,lr 0.001\n",
      "epoch 62, loss 0.38552, train_acc 0.8667, valid_acc 0.8474, Time 00:00:21,lr 0.001\n",
      "epoch 63, loss 0.38874, train_acc 0.8647, valid_acc 0.8459, Time 00:00:21,lr 0.001\n",
      "epoch 64, loss 0.39362, train_acc 0.8639, valid_acc 0.8457, Time 00:00:21,lr 0.001\n",
      "epoch 65, loss 0.38645, train_acc 0.8681, valid_acc 0.8457, Time 00:00:21,lr 0.001\n",
      "epoch 66, loss 0.39088, train_acc 0.8646, valid_acc 0.8479, Time 00:00:21,lr 0.001\n",
      "epoch 67, loss 0.38822, train_acc 0.8652, valid_acc 0.8454, Time 00:00:21,lr 0.001\n",
      "epoch 68, loss 0.37749, train_acc 0.8693, valid_acc 0.8461, Time 00:00:21,lr 0.001\n",
      "epoch 69, loss 0.38569, train_acc 0.8647, valid_acc 0.8469, Time 00:00:21,lr 0.001\n",
      "epoch 70, loss 0.38221, train_acc 0.8657, valid_acc 0.8479, Time 00:00:21,lr 0.001\n",
      "epoch 71, loss 0.37855, train_acc 0.8698, valid_acc 0.8479, Time 00:00:21,lr 0.001\n",
      "epoch 72, loss 0.37544, train_acc 0.8709, valid_acc 0.8477, Time 00:00:21,lr 0.001\n",
      "epoch 73, loss 0.38522, train_acc 0.8667, valid_acc 0.8482, Time 00:00:21,lr 0.001\n",
      "epoch 74, loss 0.38278, train_acc 0.8673, valid_acc 0.8458, Time 00:00:21,lr 0.001\n",
      "epoch 75, loss 0.38112, train_acc 0.8676, valid_acc 0.8481, Time 00:00:21,lr 0.001\n",
      "epoch 76, loss 0.38214, train_acc 0.8678, valid_acc 0.8476, Time 00:00:21,lr 0.001\n",
      "epoch 77, loss 0.37670, train_acc 0.8707, valid_acc 0.8464, Time 00:00:21,lr 0.001\n",
      "epoch 78, loss 0.37911, train_acc 0.8684, valid_acc 0.8475, Time 00:00:21,lr 0.001\n",
      "epoch 79, loss 0.37431, train_acc 0.8721, valid_acc 0.8482, Time 00:00:21,lr 0.001\n",
      "epoch 80, loss 0.37702, train_acc 0.8692, valid_acc 0.8491, Time 00:00:21,lr 0.001\n",
      "epoch 81, loss 0.37713, train_acc 0.8684, valid_acc 0.8485, Time 00:00:21,lr 0.001\n",
      "epoch 82, loss 0.37883, train_acc 0.8683, valid_acc 0.8478, Time 00:00:21,lr 0.001\n",
      "epoch 83, loss 0.37840, train_acc 0.8691, valid_acc 0.8487, Time 00:00:21,lr 0.001\n",
      "epoch 84, loss 0.37155, train_acc 0.8715, valid_acc 0.8478, Time 00:00:22,lr 0.001\n",
      "epoch 85, loss 0.37982, train_acc 0.8690, valid_acc 0.8503, Time 00:00:21,lr 0.001\n",
      "epoch 86, loss 0.37465, train_acc 0.8721, valid_acc 0.8498, Time 00:00:21,lr 0.001\n",
      "epoch 87, loss 0.37255, train_acc 0.8711, valid_acc 0.8485, Time 00:00:21,lr 0.001\n",
      "epoch 88, loss 0.37229, train_acc 0.8723, valid_acc 0.8499, Time 00:00:21,lr 0.001\n",
      "epoch 89, loss 0.37136, train_acc 0.8717, valid_acc 0.8506, Time 00:00:21,lr 0.001\n",
      "epoch 90, loss 0.37068, train_acc 0.8716, valid_acc 0.8498, Time 00:00:21,lr 0.001\n",
      "epoch 91, loss 0.36837, train_acc 0.8725, valid_acc 0.8469, Time 00:00:21,lr 0.001\n",
      "epoch 92, loss 0.37442, train_acc 0.8702, valid_acc 0.8490, Time 00:00:21,lr 0.001\n",
      "epoch 93, loss 0.36945, train_acc 0.8723, valid_acc 0.8497, Time 00:00:21,lr 0.001\n",
      "epoch 94, loss 0.37561, train_acc 0.8696, valid_acc 0.8518, Time 00:00:21,lr 0.001\n",
      "epoch 95, loss 0.36839, train_acc 0.8740, valid_acc 0.8499, Time 00:00:21,lr 0.001\n",
      "epoch 96, loss 0.36758, train_acc 0.8736, valid_acc 0.8513, Time 00:00:21,lr 0.001\n",
      "epoch 97, loss 0.36478, train_acc 0.8745, valid_acc 0.8512, Time 00:00:21,lr 0.001\n",
      "epoch 98, loss 0.37112, train_acc 0.8722, valid_acc 0.8503, Time 00:00:21,lr 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss 0.36646, train_acc 0.8726, valid_acc 0.8503, Time 00:00:21,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100      # 100 + 150 epoch\n",
    "learning_rate = 0.001\n",
    "lr_period = [100]\n",
    "\n",
    "net.load_params(\"../../models/train3_resnet18_v2_finetune_DA2_e150\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_DA2_e250\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.5 try DA2 use net train with DA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T06:14:51.600393Z",
     "start_time": "2018-03-02T05:38:49.477320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.99687, train_acc 0.7539, valid_acc 0.8322, Time 00:00:20,lr 0.001\n",
      "epoch 1, loss 0.75326, train_acc 0.7850, valid_acc 0.8325, Time 00:00:21,lr 0.001\n",
      "epoch 2, loss 0.68016, train_acc 0.7939, valid_acc 0.8352, Time 00:00:21,lr 0.001\n",
      "epoch 3, loss 0.62057, train_acc 0.8034, valid_acc 0.8363, Time 00:00:22,lr 0.001\n",
      "epoch 4, loss 0.59461, train_acc 0.8088, valid_acc 0.8396, Time 00:00:21,lr 0.001\n",
      "epoch 5, loss 0.55887, train_acc 0.8153, valid_acc 0.8420, Time 00:00:21,lr 0.001\n",
      "epoch 6, loss 0.54601, train_acc 0.8200, valid_acc 0.8440, Time 00:00:21,lr 0.001\n",
      "epoch 7, loss 0.52265, train_acc 0.8250, valid_acc 0.8448, Time 00:00:21,lr 0.001\n",
      "epoch 8, loss 0.52356, train_acc 0.8250, valid_acc 0.8432, Time 00:00:21,lr 0.001\n",
      "epoch 9, loss 0.50462, train_acc 0.8294, valid_acc 0.8438, Time 00:00:21,lr 0.001\n",
      "epoch 10, loss 0.50508, train_acc 0.8290, valid_acc 0.8449, Time 00:00:21,lr 0.001\n",
      "epoch 11, loss 0.49400, train_acc 0.8350, valid_acc 0.8457, Time 00:00:21,lr 0.001\n",
      "epoch 12, loss 0.49014, train_acc 0.8324, valid_acc 0.8471, Time 00:00:21,lr 0.001\n",
      "epoch 13, loss 0.48417, train_acc 0.8374, valid_acc 0.8466, Time 00:00:21,lr 0.001\n",
      "epoch 14, loss 0.47858, train_acc 0.8363, valid_acc 0.8473, Time 00:00:21,lr 0.001\n",
      "epoch 15, loss 0.47226, train_acc 0.8388, valid_acc 0.8457, Time 00:00:21,lr 0.001\n",
      "epoch 16, loss 0.47208, train_acc 0.8390, valid_acc 0.8456, Time 00:00:21,lr 0.001\n",
      "epoch 17, loss 0.46624, train_acc 0.8422, valid_acc 0.8496, Time 00:00:21,lr 0.001\n",
      "epoch 18, loss 0.46401, train_acc 0.8407, valid_acc 0.8477, Time 00:00:21,lr 0.001\n",
      "epoch 19, loss 0.45836, train_acc 0.8432, valid_acc 0.8489, Time 00:00:21,lr 0.001\n",
      "epoch 20, loss 0.45551, train_acc 0.8442, valid_acc 0.8481, Time 00:00:21,lr 0.001\n",
      "epoch 21, loss 0.44363, train_acc 0.8489, valid_acc 0.8485, Time 00:00:21,lr 0.001\n",
      "epoch 22, loss 0.44932, train_acc 0.8467, valid_acc 0.8472, Time 00:00:21,lr 0.001\n",
      "epoch 23, loss 0.45086, train_acc 0.8464, valid_acc 0.8485, Time 00:00:21,lr 0.001\n",
      "epoch 24, loss 0.44657, train_acc 0.8473, valid_acc 0.8492, Time 00:00:21,lr 0.001\n",
      "epoch 25, loss 0.44077, train_acc 0.8476, valid_acc 0.8503, Time 00:00:21,lr 0.001\n",
      "epoch 26, loss 0.43511, train_acc 0.8510, valid_acc 0.8507, Time 00:00:21,lr 0.001\n",
      "epoch 27, loss 0.43856, train_acc 0.8491, valid_acc 0.8500, Time 00:00:21,lr 0.001\n",
      "epoch 28, loss 0.43241, train_acc 0.8502, valid_acc 0.8483, Time 00:00:21,lr 0.001\n",
      "epoch 29, loss 0.43117, train_acc 0.8522, valid_acc 0.8482, Time 00:00:21,lr 0.001\n",
      "epoch 30, loss 0.42537, train_acc 0.8534, valid_acc 0.8478, Time 00:00:21,lr 0.001\n",
      "epoch 31, loss 0.42770, train_acc 0.8535, valid_acc 0.8476, Time 00:00:21,lr 0.001\n",
      "epoch 32, loss 0.42385, train_acc 0.8552, valid_acc 0.8522, Time 00:00:21,lr 0.001\n",
      "epoch 33, loss 0.41824, train_acc 0.8568, valid_acc 0.8511, Time 00:00:21,lr 0.001\n",
      "epoch 34, loss 0.42627, train_acc 0.8537, valid_acc 0.8507, Time 00:00:21,lr 0.001\n",
      "epoch 35, loss 0.42293, train_acc 0.8543, valid_acc 0.8488, Time 00:00:21,lr 0.001\n",
      "epoch 36, loss 0.42117, train_acc 0.8546, valid_acc 0.8498, Time 00:00:21,lr 0.001\n",
      "epoch 37, loss 0.41266, train_acc 0.8581, valid_acc 0.8487, Time 00:00:21,lr 0.001\n",
      "epoch 38, loss 0.41668, train_acc 0.8550, valid_acc 0.8469, Time 00:00:21,lr 0.001\n",
      "epoch 39, loss 0.41355, train_acc 0.8584, valid_acc 0.8493, Time 00:00:21,lr 0.001\n",
      "epoch 40, loss 0.41021, train_acc 0.8578, valid_acc 0.8530, Time 00:00:21,lr 0.001\n",
      "epoch 41, loss 0.40879, train_acc 0.8598, valid_acc 0.8482, Time 00:00:21,lr 0.001\n",
      "epoch 42, loss 0.40801, train_acc 0.8605, valid_acc 0.8510, Time 00:00:21,lr 0.001\n",
      "epoch 43, loss 0.40439, train_acc 0.8615, valid_acc 0.8500, Time 00:00:21,lr 0.001\n",
      "epoch 44, loss 0.40213, train_acc 0.8631, valid_acc 0.8516, Time 00:00:21,lr 0.001\n",
      "epoch 45, loss 0.40143, train_acc 0.8613, valid_acc 0.8513, Time 00:00:21,lr 0.001\n",
      "epoch 46, loss 0.39666, train_acc 0.8623, valid_acc 0.8518, Time 00:00:21,lr 0.001\n",
      "epoch 47, loss 0.39537, train_acc 0.8641, valid_acc 0.8490, Time 00:00:21,lr 0.001\n",
      "epoch 48, loss 0.39476, train_acc 0.8645, valid_acc 0.8524, Time 00:00:21,lr 0.001\n",
      "epoch 49, loss 0.39666, train_acc 0.8631, valid_acc 0.8498, Time 00:00:21,lr 0.001\n",
      "epoch 50, loss 0.38757, train_acc 0.8655, valid_acc 0.8527, Time 00:00:21,lr 0.001\n",
      "epoch 51, loss 0.39361, train_acc 0.8651, valid_acc 0.8520, Time 00:00:21,lr 0.001\n",
      "epoch 52, loss 0.39306, train_acc 0.8660, valid_acc 0.8538, Time 00:00:21,lr 0.001\n",
      "epoch 53, loss 0.38852, train_acc 0.8655, valid_acc 0.8519, Time 00:00:21,lr 0.001\n",
      "epoch 54, loss 0.39297, train_acc 0.8652, valid_acc 0.8521, Time 00:00:21,lr 0.001\n",
      "epoch 55, loss 0.38401, train_acc 0.8677, valid_acc 0.8535, Time 00:00:21,lr 0.001\n",
      "epoch 56, loss 0.38518, train_acc 0.8673, valid_acc 0.8534, Time 00:00:21,lr 0.001\n",
      "epoch 57, loss 0.38219, train_acc 0.8670, valid_acc 0.8537, Time 00:00:21,lr 0.001\n",
      "epoch 58, loss 0.38522, train_acc 0.8693, valid_acc 0.8533, Time 00:00:21,lr 0.001\n",
      "epoch 59, loss 0.38491, train_acc 0.8688, valid_acc 0.8521, Time 00:00:21,lr 0.001\n",
      "epoch 60, loss 0.38274, train_acc 0.8684, valid_acc 0.8537, Time 00:00:21,lr 0.001\n",
      "epoch 61, loss 0.37956, train_acc 0.8696, valid_acc 0.8528, Time 00:00:21,lr 0.001\n",
      "epoch 62, loss 0.37874, train_acc 0.8689, valid_acc 0.8546, Time 00:00:21,lr 0.001\n",
      "epoch 63, loss 0.37598, train_acc 0.8706, valid_acc 0.8532, Time 00:00:21,lr 0.001\n",
      "epoch 64, loss 0.37355, train_acc 0.8731, valid_acc 0.8518, Time 00:00:21,lr 0.001\n",
      "epoch 65, loss 0.37149, train_acc 0.8738, valid_acc 0.8521, Time 00:00:21,lr 0.001\n",
      "epoch 66, loss 0.37667, train_acc 0.8696, valid_acc 0.8524, Time 00:00:21,lr 0.001\n",
      "epoch 67, loss 0.36867, train_acc 0.8731, valid_acc 0.8547, Time 00:00:21,lr 0.001\n",
      "epoch 68, loss 0.37102, train_acc 0.8732, valid_acc 0.8548, Time 00:00:21,lr 0.001\n",
      "epoch 69, loss 0.37669, train_acc 0.8710, valid_acc 0.8528, Time 00:00:21,lr 0.001\n",
      "epoch 70, loss 0.37151, train_acc 0.8712, valid_acc 0.8538, Time 00:00:21,lr 0.001\n",
      "epoch 71, loss 0.36660, train_acc 0.8741, valid_acc 0.8518, Time 00:00:21,lr 0.001\n",
      "epoch 72, loss 0.37310, train_acc 0.8726, valid_acc 0.8528, Time 00:00:21,lr 0.001\n",
      "epoch 73, loss 0.36925, train_acc 0.8738, valid_acc 0.8547, Time 00:00:21,lr 0.001\n",
      "epoch 74, loss 0.36612, train_acc 0.8754, valid_acc 0.8521, Time 00:00:21,lr 0.001\n",
      "epoch 75, loss 0.36686, train_acc 0.8737, valid_acc 0.8522, Time 00:00:21,lr 0.001\n",
      "epoch 76, loss 0.35685, train_acc 0.8776, valid_acc 0.8516, Time 00:00:21,lr 0.001\n",
      "epoch 77, loss 0.36853, train_acc 0.8744, valid_acc 0.8537, Time 00:00:21,lr 0.001\n",
      "epoch 78, loss 0.36281, train_acc 0.8750, valid_acc 0.8529, Time 00:00:21,lr 0.001\n",
      "epoch 79, loss 0.35936, train_acc 0.8779, valid_acc 0.8544, Time 00:00:21,lr 0.001\n",
      "epoch 80, loss 0.36089, train_acc 0.8772, valid_acc 0.8529, Time 00:00:21,lr 0.001\n",
      "epoch 81, loss 0.36800, train_acc 0.8740, valid_acc 0.8551, Time 00:00:21,lr 0.001\n",
      "epoch 82, loss 0.35478, train_acc 0.8781, valid_acc 0.8545, Time 00:00:21,lr 0.001\n",
      "epoch 83, loss 0.36383, train_acc 0.8743, valid_acc 0.8554, Time 00:00:21,lr 0.001\n",
      "epoch 84, loss 0.36634, train_acc 0.8741, valid_acc 0.8528, Time 00:00:21,lr 0.001\n",
      "epoch 85, loss 0.35735, train_acc 0.8771, valid_acc 0.8541, Time 00:00:21,lr 0.001\n",
      "epoch 86, loss 0.35795, train_acc 0.8773, valid_acc 0.8527, Time 00:00:21,lr 0.001\n",
      "epoch 87, loss 0.35041, train_acc 0.8809, valid_acc 0.8532, Time 00:00:21,lr 0.001\n",
      "epoch 88, loss 0.35601, train_acc 0.8790, valid_acc 0.8553, Time 00:00:21,lr 0.001\n",
      "epoch 89, loss 0.35046, train_acc 0.8797, valid_acc 0.8543, Time 00:00:21,lr 0.001\n",
      "epoch 90, loss 0.34508, train_acc 0.8807, valid_acc 0.8543, Time 00:00:21,lr 0.001\n",
      "epoch 91, loss 0.35194, train_acc 0.8807, valid_acc 0.8545, Time 00:00:21,lr 0.001\n",
      "epoch 92, loss 0.35148, train_acc 0.8786, valid_acc 0.8540, Time 00:00:21,lr 0.001\n",
      "epoch 93, loss 0.35001, train_acc 0.8804, valid_acc 0.8560, Time 00:00:21,lr 0.001\n",
      "epoch 94, loss 0.35086, train_acc 0.8810, valid_acc 0.8521, Time 00:00:21,lr 0.001\n",
      "epoch 95, loss 0.34865, train_acc 0.8799, valid_acc 0.8547, Time 00:00:21,lr 0.001\n",
      "epoch 96, loss 0.33574, train_acc 0.8841, valid_acc 0.8528, Time 00:00:21,lr 0.001\n",
      "epoch 97, loss 0.34881, train_acc 0.8798, valid_acc 0.8526, Time 00:00:21,lr 0.001\n",
      "epoch 98, loss 0.34003, train_acc 0.8825, valid_acc 0.8528, Time 00:00:21,lr 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss 0.34729, train_acc 0.8795, valid_acc 0.8534, Time 00:00:21,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=3)\n",
    "net = get_resnet18_v2()\n",
    "\n",
    "num_epochs = 100      # 50 * 1563 iter about\n",
    "learning_rate = 0.001\n",
    "lr_period = [150]\n",
    "\n",
    "net.load_params(\"../../models/train3_resnet18_v2_finetune_DA1_e250\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_DA2_afterDA1e250_e100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.5 try DA2 use imagenet finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T06:58:06.884065Z",
     "start_time": "2018-03-02T06:14:51.602144Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.37907, train_acc 0.5171, valid_acc 0.6732, Time 00:00:20,lr 0.001\n",
      "epoch 1, loss 0.98552, train_acc 0.6540, valid_acc 0.7287, Time 00:00:21,lr 0.001\n",
      "epoch 2, loss 0.88241, train_acc 0.6923, valid_acc 0.7486, Time 00:00:21,lr 0.001\n",
      "epoch 3, loss 0.82404, train_acc 0.7127, valid_acc 0.7690, Time 00:00:21,lr 0.001\n",
      "epoch 4, loss 0.76971, train_acc 0.7321, valid_acc 0.7729, Time 00:00:21,lr 0.001\n",
      "epoch 5, loss 0.74007, train_acc 0.7424, valid_acc 0.7854, Time 00:00:21,lr 0.001\n",
      "epoch 6, loss 0.70770, train_acc 0.7541, valid_acc 0.7922, Time 00:00:21,lr 0.001\n",
      "epoch 7, loss 0.68088, train_acc 0.7619, valid_acc 0.7990, Time 00:00:21,lr 0.001\n",
      "epoch 8, loss 0.66286, train_acc 0.7699, valid_acc 0.8018, Time 00:00:21,lr 0.001\n",
      "epoch 9, loss 0.64915, train_acc 0.7741, valid_acc 0.8026, Time 00:00:21,lr 0.001\n",
      "epoch 10, loss 0.62471, train_acc 0.7834, valid_acc 0.8100, Time 00:00:21,lr 0.001\n",
      "epoch 11, loss 0.61559, train_acc 0.7847, valid_acc 0.8126, Time 00:00:21,lr 0.001\n",
      "epoch 12, loss 0.59959, train_acc 0.7920, valid_acc 0.8131, Time 00:00:22,lr 0.001\n",
      "epoch 13, loss 0.58246, train_acc 0.7975, valid_acc 0.8177, Time 00:00:23,lr 0.001\n",
      "epoch 14, loss 0.57836, train_acc 0.7993, valid_acc 0.8200, Time 00:00:22,lr 0.001\n",
      "epoch 15, loss 0.56919, train_acc 0.7999, valid_acc 0.8201, Time 00:00:21,lr 0.001\n",
      "epoch 16, loss 0.55545, train_acc 0.8050, valid_acc 0.8195, Time 00:00:21,lr 0.001\n",
      "epoch 17, loss 0.53824, train_acc 0.8121, valid_acc 0.8226, Time 00:00:21,lr 0.001\n",
      "epoch 18, loss 0.53569, train_acc 0.8133, valid_acc 0.8270, Time 00:00:21,lr 0.001\n",
      "epoch 19, loss 0.52518, train_acc 0.8175, valid_acc 0.8284, Time 00:00:21,lr 0.001\n",
      "epoch 20, loss 0.51397, train_acc 0.8209, valid_acc 0.8265, Time 00:00:21,lr 0.001\n",
      "epoch 21, loss 0.51048, train_acc 0.8233, valid_acc 0.8282, Time 00:00:21,lr 0.001\n",
      "epoch 22, loss 0.50079, train_acc 0.8241, valid_acc 0.8359, Time 00:00:21,lr 0.001\n",
      "epoch 23, loss 0.48659, train_acc 0.8308, valid_acc 0.8346, Time 00:00:21,lr 0.001\n",
      "epoch 24, loss 0.48909, train_acc 0.8278, valid_acc 0.8335, Time 00:00:21,lr 0.001\n",
      "epoch 25, loss 0.48533, train_acc 0.8303, valid_acc 0.8394, Time 00:00:21,lr 0.001\n",
      "epoch 26, loss 0.47935, train_acc 0.8331, valid_acc 0.8331, Time 00:00:21,lr 0.001\n",
      "epoch 27, loss 0.46480, train_acc 0.8379, valid_acc 0.8361, Time 00:00:21,lr 0.001\n",
      "epoch 28, loss 0.46211, train_acc 0.8391, valid_acc 0.8354, Time 00:00:21,lr 0.001\n",
      "epoch 29, loss 0.45108, train_acc 0.8429, valid_acc 0.8426, Time 00:00:21,lr 0.001\n",
      "epoch 30, loss 0.45408, train_acc 0.8424, valid_acc 0.8411, Time 00:00:21,lr 0.001\n",
      "epoch 31, loss 0.44604, train_acc 0.8447, valid_acc 0.8417, Time 00:00:21,lr 0.001\n",
      "epoch 32, loss 0.44009, train_acc 0.8441, valid_acc 0.8397, Time 00:00:21,lr 0.001\n",
      "epoch 33, loss 0.43175, train_acc 0.8477, valid_acc 0.8413, Time 00:00:21,lr 0.001\n",
      "epoch 34, loss 0.42500, train_acc 0.8515, valid_acc 0.8429, Time 00:00:21,lr 0.001\n",
      "epoch 35, loss 0.42523, train_acc 0.8516, valid_acc 0.8419, Time 00:00:21,lr 0.001\n",
      "epoch 36, loss 0.42325, train_acc 0.8517, valid_acc 0.8480, Time 00:00:21,lr 0.001\n",
      "epoch 37, loss 0.41294, train_acc 0.8552, valid_acc 0.8428, Time 00:00:21,lr 0.001\n",
      "epoch 38, loss 0.41231, train_acc 0.8574, valid_acc 0.8408, Time 00:00:21,lr 0.001\n",
      "epoch 39, loss 0.41051, train_acc 0.8573, valid_acc 0.8450, Time 00:00:21,lr 0.001\n",
      "epoch 40, loss 0.40293, train_acc 0.8586, valid_acc 0.8446, Time 00:00:21,lr 0.001\n",
      "epoch 41, loss 0.40146, train_acc 0.8588, valid_acc 0.8438, Time 00:00:21,lr 0.001\n",
      "epoch 42, loss 0.39427, train_acc 0.8601, valid_acc 0.8468, Time 00:00:21,lr 0.001\n",
      "epoch 43, loss 0.38900, train_acc 0.8648, valid_acc 0.8465, Time 00:00:21,lr 0.001\n",
      "epoch 44, loss 0.38218, train_acc 0.8657, valid_acc 0.8501, Time 00:00:21,lr 0.001\n",
      "epoch 45, loss 0.38333, train_acc 0.8647, valid_acc 0.8444, Time 00:00:21,lr 0.001\n",
      "epoch 46, loss 0.37863, train_acc 0.8665, valid_acc 0.8457, Time 00:00:21,lr 0.001\n",
      "epoch 47, loss 0.37279, train_acc 0.8698, valid_acc 0.8478, Time 00:00:21,lr 0.001\n",
      "epoch 48, loss 0.37501, train_acc 0.8684, valid_acc 0.8517, Time 00:00:21,lr 0.001\n",
      "epoch 49, loss 0.36627, train_acc 0.8718, valid_acc 0.8457, Time 00:00:21,lr 0.001\n",
      "epoch 50, loss 0.36894, train_acc 0.8697, valid_acc 0.8511, Time 00:00:21,lr 0.001\n",
      "epoch 51, loss 0.35315, train_acc 0.8749, valid_acc 0.8506, Time 00:00:21,lr 0.001\n",
      "epoch 52, loss 0.36533, train_acc 0.8732, valid_acc 0.8490, Time 00:00:21,lr 0.001\n",
      "epoch 53, loss 0.35612, train_acc 0.8752, valid_acc 0.8522, Time 00:00:21,lr 0.001\n",
      "epoch 54, loss 0.35341, train_acc 0.8767, valid_acc 0.8465, Time 00:00:21,lr 0.001\n",
      "epoch 55, loss 0.34664, train_acc 0.8794, valid_acc 0.8516, Time 00:00:21,lr 0.001\n",
      "epoch 56, loss 0.34053, train_acc 0.8811, valid_acc 0.8550, Time 00:00:21,lr 0.001\n",
      "epoch 57, loss 0.34391, train_acc 0.8793, valid_acc 0.8524, Time 00:00:21,lr 0.001\n",
      "epoch 58, loss 0.34109, train_acc 0.8804, valid_acc 0.8488, Time 00:00:21,lr 0.001\n",
      "epoch 59, loss 0.33218, train_acc 0.8842, valid_acc 0.8527, Time 00:00:21,lr 0.001\n",
      "epoch 60, loss 0.32935, train_acc 0.8840, valid_acc 0.8549, Time 00:00:21,lr 0.001\n",
      "epoch 61, loss 0.32631, train_acc 0.8872, valid_acc 0.8515, Time 00:00:21,lr 0.001\n",
      "epoch 62, loss 0.33507, train_acc 0.8824, valid_acc 0.8554, Time 00:00:21,lr 0.001\n",
      "epoch 63, loss 0.32498, train_acc 0.8870, valid_acc 0.8524, Time 00:00:21,lr 0.001\n",
      "epoch 64, loss 0.32281, train_acc 0.8860, valid_acc 0.8515, Time 00:00:21,lr 0.001\n",
      "epoch 65, loss 0.31687, train_acc 0.8882, valid_acc 0.8492, Time 00:00:21,lr 0.001\n",
      "epoch 66, loss 0.31817, train_acc 0.8902, valid_acc 0.8485, Time 00:00:21,lr 0.001\n",
      "epoch 67, loss 0.31531, train_acc 0.8891, valid_acc 0.8541, Time 00:00:21,lr 0.001\n",
      "epoch 68, loss 0.30747, train_acc 0.8936, valid_acc 0.8530, Time 00:00:21,lr 0.001\n",
      "epoch 69, loss 0.30839, train_acc 0.8919, valid_acc 0.8513, Time 00:00:21,lr 0.001\n",
      "epoch 70, loss 0.30543, train_acc 0.8930, valid_acc 0.8540, Time 00:00:21,lr 0.001\n",
      "epoch 71, loss 0.30058, train_acc 0.8958, valid_acc 0.8563, Time 00:00:21,lr 0.001\n",
      "epoch 72, loss 0.30322, train_acc 0.8949, valid_acc 0.8571, Time 00:00:21,lr 0.001\n",
      "epoch 73, loss 0.29601, train_acc 0.8953, valid_acc 0.8522, Time 00:00:21,lr 0.001\n",
      "epoch 74, loss 0.29636, train_acc 0.8969, valid_acc 0.8558, Time 00:00:21,lr 0.001\n",
      "epoch 75, loss 0.29744, train_acc 0.8954, valid_acc 0.8553, Time 00:00:21,lr 0.001\n",
      "epoch 76, loss 0.29464, train_acc 0.8969, valid_acc 0.8527, Time 00:00:21,lr 0.001\n",
      "epoch 77, loss 0.29247, train_acc 0.8967, valid_acc 0.8498, Time 00:00:21,lr 0.001\n",
      "epoch 78, loss 0.28860, train_acc 0.8992, valid_acc 0.8568, Time 00:00:21,lr 0.001\n",
      "epoch 79, loss 0.28514, train_acc 0.8988, valid_acc 0.8545, Time 00:00:21,lr 0.001\n",
      "epoch 80, loss 0.27148, train_acc 0.9049, valid_acc 0.8574, Time 00:00:21,lr 0.0001\n",
      "epoch 81, loss 0.25888, train_acc 0.9108, valid_acc 0.8594, Time 00:00:21,lr 0.0001\n",
      "epoch 82, loss 0.25457, train_acc 0.9130, valid_acc 0.8592, Time 00:00:21,lr 0.0001\n",
      "epoch 83, loss 0.25208, train_acc 0.9119, valid_acc 0.8596, Time 00:00:21,lr 0.0001\n",
      "epoch 84, loss 0.25018, train_acc 0.9134, valid_acc 0.8561, Time 00:00:21,lr 0.0001\n",
      "epoch 85, loss 0.24424, train_acc 0.9165, valid_acc 0.8609, Time 00:00:21,lr 0.0001\n",
      "epoch 86, loss 0.24857, train_acc 0.9134, valid_acc 0.8604, Time 00:00:21,lr 0.0001\n",
      "epoch 87, loss 0.24919, train_acc 0.9126, valid_acc 0.8603, Time 00:00:21,lr 0.0001\n",
      "epoch 88, loss 0.24528, train_acc 0.9157, valid_acc 0.8618, Time 00:00:21,lr 0.0001\n",
      "epoch 89, loss 0.24927, train_acc 0.9139, valid_acc 0.8611, Time 00:00:21,lr 0.0001\n",
      "epoch 90, loss 0.24223, train_acc 0.9157, valid_acc 0.8594, Time 00:00:21,lr 0.0001\n",
      "epoch 91, loss 0.24225, train_acc 0.9156, valid_acc 0.8599, Time 00:00:21,lr 0.0001\n",
      "epoch 92, loss 0.23879, train_acc 0.9180, valid_acc 0.8587, Time 00:00:21,lr 0.0001\n",
      "epoch 93, loss 0.24210, train_acc 0.9169, valid_acc 0.8600, Time 00:00:21,lr 0.0001\n",
      "epoch 94, loss 0.23873, train_acc 0.9152, valid_acc 0.8602, Time 00:00:21,lr 0.0001\n",
      "epoch 95, loss 0.23772, train_acc 0.9179, valid_acc 0.8583, Time 00:00:21,lr 0.0001\n",
      "epoch 96, loss 0.23711, train_acc 0.9176, valid_acc 0.8611, Time 00:00:21,lr 0.0001\n",
      "epoch 97, loss 0.23209, train_acc 0.9200, valid_acc 0.8599, Time 00:00:21,lr 0.0001\n",
      "epoch 98, loss 0.23190, train_acc 0.9194, valid_acc 0.8607, Time 00:00:21,lr 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss 0.23610, train_acc 0.9179, valid_acc 0.8596, Time 00:00:21,lr 0.0001\n",
      "epoch 100, loss 0.23601, train_acc 0.9181, valid_acc 0.8603, Time 00:00:21,lr 0.0001\n",
      "epoch 101, loss 0.23521, train_acc 0.9176, valid_acc 0.8606, Time 00:00:21,lr 0.0001\n",
      "epoch 102, loss 0.23208, train_acc 0.9182, valid_acc 0.8599, Time 00:00:21,lr 0.0001\n",
      "epoch 103, loss 0.23638, train_acc 0.9196, valid_acc 0.8619, Time 00:00:21,lr 0.0001\n",
      "epoch 104, loss 0.23010, train_acc 0.9197, valid_acc 0.8614, Time 00:00:21,lr 0.0001\n",
      "epoch 105, loss 0.22912, train_acc 0.9209, valid_acc 0.8612, Time 00:00:21,lr 0.0001\n",
      "epoch 106, loss 0.22961, train_acc 0.9217, valid_acc 0.8596, Time 00:00:21,lr 0.0001\n",
      "epoch 107, loss 0.22829, train_acc 0.9202, valid_acc 0.8620, Time 00:00:21,lr 0.0001\n",
      "epoch 108, loss 0.23068, train_acc 0.9199, valid_acc 0.8628, Time 00:00:21,lr 0.0001\n",
      "epoch 109, loss 0.22720, train_acc 0.9208, valid_acc 0.8641, Time 00:00:22,lr 0.0001\n",
      "epoch 110, loss 0.23003, train_acc 0.9202, valid_acc 0.8620, Time 00:00:21,lr 0.0001\n",
      "epoch 111, loss 0.22834, train_acc 0.9208, valid_acc 0.8612, Time 00:00:21,lr 0.0001\n",
      "epoch 112, loss 0.22497, train_acc 0.9230, valid_acc 0.8626, Time 00:00:21,lr 0.0001\n",
      "epoch 113, loss 0.22436, train_acc 0.9220, valid_acc 0.8637, Time 00:00:21,lr 0.0001\n",
      "epoch 114, loss 0.22540, train_acc 0.9228, valid_acc 0.8624, Time 00:00:21,lr 0.0001\n",
      "epoch 115, loss 0.23235, train_acc 0.9198, valid_acc 0.8606, Time 00:00:21,lr 0.0001\n",
      "epoch 116, loss 0.22800, train_acc 0.9216, valid_acc 0.8620, Time 00:00:21,lr 0.0001\n",
      "epoch 117, loss 0.22698, train_acc 0.9218, valid_acc 0.8632, Time 00:00:21,lr 0.0001\n",
      "epoch 118, loss 0.22568, train_acc 0.9197, valid_acc 0.8633, Time 00:00:21,lr 0.0001\n",
      "epoch 119, loss 0.23081, train_acc 0.9203, valid_acc 0.8613, Time 00:00:21,lr 0.0001\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=3)\n",
    "net = get_resnet18_v2()\n",
    "\n",
    "num_epochs = 120      # 50 * 1563 iter about\n",
    "learning_rate = 0.001\n",
    "lr_period = [80]\n",
    "\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_DA2_small_lr_e120\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.6 try mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:10:13.250728Z",
     "start_time": "2018-03-02T06:58:06.885737Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 3.20680, train_acc 0.0000, valid_acc 0.2596, Time 00:00:11,lr 0.1\n",
      "epoch 1, loss 1.96925, train_acc 0.0000, valid_acc 0.3252, Time 00:00:12,lr 0.1\n",
      "epoch 2, loss 1.84118, train_acc 0.0000, valid_acc 0.3862, Time 00:00:12,lr 0.1\n",
      "epoch 3, loss 1.76241, train_acc 0.0000, valid_acc 0.3886, Time 00:00:12,lr 0.1\n",
      "epoch 4, loss 1.72654, train_acc 0.0000, valid_acc 0.4518, Time 00:00:12,lr 0.1\n",
      "epoch 5, loss 1.69166, train_acc 0.0000, valid_acc 0.4454, Time 00:00:12,lr 0.1\n",
      "epoch 6, loss 1.68581, train_acc 0.0000, valid_acc 0.4613, Time 00:00:12,lr 0.1\n",
      "epoch 7, loss 1.63838, train_acc 0.0000, valid_acc 0.5050, Time 00:00:12,lr 0.1\n",
      "epoch 8, loss 1.55155, train_acc 0.0000, valid_acc 0.5331, Time 00:00:12,lr 0.1\n",
      "epoch 9, loss 1.57070, train_acc 0.0000, valid_acc 0.5259, Time 00:00:12,lr 0.1\n",
      "epoch 10, loss 1.55299, train_acc 0.0000, valid_acc 0.5209, Time 00:00:12,lr 0.1\n",
      "epoch 11, loss 1.63187, train_acc 0.0000, valid_acc 0.4830, Time 00:00:12,lr 0.1\n",
      "epoch 12, loss 1.53881, train_acc 0.0000, valid_acc 0.5321, Time 00:00:12,lr 0.1\n",
      "epoch 13, loss 1.47465, train_acc 0.0000, valid_acc 0.5734, Time 00:00:12,lr 0.1\n",
      "epoch 14, loss 1.48686, train_acc 0.0000, valid_acc 0.5560, Time 00:00:12,lr 0.1\n",
      "epoch 15, loss 1.42834, train_acc 0.0000, valid_acc 0.6213, Time 00:00:12,lr 0.1\n",
      "epoch 16, loss 1.38650, train_acc 0.0000, valid_acc 0.6275, Time 00:00:12,lr 0.1\n",
      "epoch 17, loss 1.35677, train_acc 0.0000, valid_acc 0.6261, Time 00:00:12,lr 0.1\n",
      "epoch 18, loss 1.34850, train_acc 0.0000, valid_acc 0.6402, Time 00:00:12,lr 0.1\n",
      "epoch 19, loss 1.30834, train_acc 0.0000, valid_acc 0.6399, Time 00:00:12,lr 0.1\n",
      "epoch 20, loss 1.33571, train_acc 0.0000, valid_acc 0.6656, Time 00:00:12,lr 0.1\n",
      "epoch 21, loss 1.28376, train_acc 0.0000, valid_acc 0.6494, Time 00:00:12,lr 0.1\n",
      "epoch 22, loss 1.27055, train_acc 0.0000, valid_acc 0.6694, Time 00:00:12,lr 0.1\n",
      "epoch 23, loss 1.25923, train_acc 0.0000, valid_acc 0.6834, Time 00:00:12,lr 0.1\n",
      "epoch 24, loss 1.23220, train_acc 0.0000, valid_acc 0.6654, Time 00:00:12,lr 0.1\n",
      "epoch 25, loss 1.22819, train_acc 0.0000, valid_acc 0.6964, Time 00:00:12,lr 0.1\n",
      "epoch 26, loss 1.23312, train_acc 0.0000, valid_acc 0.6897, Time 00:00:12,lr 0.1\n",
      "epoch 27, loss 1.20643, train_acc 0.0000, valid_acc 0.6908, Time 00:00:12,lr 0.1\n",
      "epoch 28, loss 1.18733, train_acc 0.0000, valid_acc 0.7009, Time 00:00:12,lr 0.1\n",
      "epoch 29, loss 1.20757, train_acc 0.0000, valid_acc 0.6847, Time 00:00:12,lr 0.1\n",
      "epoch 30, loss 1.18979, train_acc 0.0000, valid_acc 0.6611, Time 00:00:12,lr 0.1\n",
      "epoch 31, loss 1.17485, train_acc 0.0000, valid_acc 0.6998, Time 00:00:12,lr 0.1\n",
      "epoch 32, loss 1.17839, train_acc 0.0000, valid_acc 0.7210, Time 00:00:12,lr 0.1\n",
      "epoch 33, loss 1.17699, train_acc 0.0000, valid_acc 0.7058, Time 00:00:12,lr 0.1\n",
      "epoch 34, loss 1.16397, train_acc 0.0000, valid_acc 0.7132, Time 00:00:12,lr 0.1\n",
      "epoch 35, loss 1.17331, train_acc 0.0000, valid_acc 0.7267, Time 00:00:12,lr 0.1\n",
      "epoch 36, loss 1.14620, train_acc 0.0000, valid_acc 0.6859, Time 00:00:12,lr 0.1\n",
      "epoch 37, loss 1.11279, train_acc 0.0000, valid_acc 0.7132, Time 00:00:12,lr 0.1\n",
      "epoch 38, loss 1.12213, train_acc 0.0000, valid_acc 0.7219, Time 00:00:12,lr 0.1\n",
      "epoch 39, loss 1.14052, train_acc 0.0000, valid_acc 0.7260, Time 00:00:12,lr 0.1\n",
      "epoch 40, loss 1.12532, train_acc 0.0000, valid_acc 0.7218, Time 00:00:12,lr 0.1\n",
      "epoch 41, loss 1.21321, train_acc 0.0000, valid_acc 0.7294, Time 00:00:12,lr 0.1\n",
      "epoch 42, loss 1.09637, train_acc 0.0000, valid_acc 0.7369, Time 00:00:12,lr 0.1\n",
      "epoch 43, loss 1.09997, train_acc 0.0000, valid_acc 0.7269, Time 00:00:12,lr 0.1\n",
      "epoch 44, loss 1.09141, train_acc 0.0000, valid_acc 0.7538, Time 00:00:12,lr 0.1\n",
      "epoch 45, loss 1.10582, train_acc 0.0000, valid_acc 0.7264, Time 00:00:12,lr 0.1\n",
      "epoch 46, loss 1.09920, train_acc 0.0000, valid_acc 0.7522, Time 00:00:12,lr 0.1\n",
      "epoch 47, loss 1.07344, train_acc 0.0000, valid_acc 0.7372, Time 00:00:12,lr 0.1\n",
      "epoch 48, loss 1.07723, train_acc 0.0000, valid_acc 0.7426, Time 00:00:12,lr 0.1\n",
      "epoch 49, loss 1.09251, train_acc 0.0000, valid_acc 0.7515, Time 00:00:12,lr 0.1\n",
      "epoch 50, loss 1.07259, train_acc 0.0000, valid_acc 0.7545, Time 00:00:12,lr 0.1\n",
      "epoch 51, loss 1.01059, train_acc 0.0000, valid_acc 0.7197, Time 00:00:12,lr 0.1\n",
      "epoch 52, loss 1.05142, train_acc 0.0000, valid_acc 0.7468, Time 00:00:12,lr 0.1\n",
      "epoch 53, loss 1.04906, train_acc 0.0000, valid_acc 0.7435, Time 00:00:12,lr 0.1\n",
      "epoch 54, loss 1.07304, train_acc 0.0000, valid_acc 0.7585, Time 00:00:12,lr 0.1\n",
      "epoch 55, loss 1.04409, train_acc 0.0000, valid_acc 0.7618, Time 00:00:12,lr 0.1\n",
      "epoch 56, loss 1.07647, train_acc 0.0000, valid_acc 0.7394, Time 00:00:12,lr 0.1\n",
      "epoch 57, loss 1.07284, train_acc 0.0000, valid_acc 0.7540, Time 00:00:12,lr 0.1\n",
      "epoch 58, loss 1.04476, train_acc 0.0000, valid_acc 0.7478, Time 00:00:12,lr 0.1\n",
      "epoch 59, loss 1.04539, train_acc 0.0000, valid_acc 0.7634, Time 00:00:12,lr 0.1\n",
      "epoch 60, loss 1.06532, train_acc 0.0000, valid_acc 0.7592, Time 00:00:12,lr 0.1\n",
      "epoch 61, loss 1.03179, train_acc 0.0000, valid_acc 0.7720, Time 00:00:12,lr 0.1\n",
      "epoch 62, loss 1.01756, train_acc 0.0000, valid_acc 0.7523, Time 00:00:12,lr 0.1\n",
      "epoch 63, loss 1.03467, train_acc 0.0000, valid_acc 0.7706, Time 00:00:12,lr 0.1\n",
      "epoch 64, loss 1.05409, train_acc 0.0000, valid_acc 0.7664, Time 00:00:12,lr 0.1\n",
      "epoch 65, loss 1.05715, train_acc 0.0000, valid_acc 0.7551, Time 00:00:12,lr 0.1\n",
      "epoch 66, loss 1.02712, train_acc 0.0000, valid_acc 0.7307, Time 00:00:12,lr 0.1\n",
      "epoch 67, loss 0.99500, train_acc 0.0000, valid_acc 0.7329, Time 00:00:12,lr 0.1\n",
      "epoch 68, loss 1.02924, train_acc 0.0000, valid_acc 0.7581, Time 00:00:12,lr 0.1\n",
      "epoch 69, loss 1.00977, train_acc 0.0000, valid_acc 0.7722, Time 00:00:12,lr 0.1\n",
      "epoch 70, loss 1.01638, train_acc 0.0000, valid_acc 0.7647, Time 00:00:12,lr 0.1\n",
      "epoch 71, loss 1.01082, train_acc 0.0000, valid_acc 0.6976, Time 00:00:12,lr 0.1\n",
      "epoch 72, loss 1.01468, train_acc 0.0000, valid_acc 0.7653, Time 00:00:12,lr 0.1\n",
      "epoch 73, loss 0.97668, train_acc 0.0000, valid_acc 0.7642, Time 00:00:12,lr 0.1\n",
      "epoch 74, loss 1.02034, train_acc 0.0000, valid_acc 0.7649, Time 00:00:12,lr 0.1\n",
      "epoch 75, loss 0.95468, train_acc 0.0000, valid_acc 0.7584, Time 00:00:12,lr 0.1\n",
      "epoch 76, loss 1.01756, train_acc 0.0000, valid_acc 0.7448, Time 00:00:12,lr 0.1\n",
      "epoch 77, loss 0.96203, train_acc 0.0000, valid_acc 0.7652, Time 00:00:12,lr 0.1\n",
      "epoch 78, loss 0.99365, train_acc 0.0000, valid_acc 0.7625, Time 00:00:12,lr 0.1\n",
      "epoch 79, loss 0.98956, train_acc 0.0000, valid_acc 0.7631, Time 00:00:12,lr 0.1\n",
      "epoch 80, loss 0.97952, train_acc 0.0000, valid_acc 0.7691, Time 00:00:12,lr 0.1\n",
      "epoch 81, loss 0.97271, train_acc 0.0000, valid_acc 0.7647, Time 00:00:12,lr 0.1\n",
      "epoch 82, loss 0.95636, train_acc 0.0000, valid_acc 0.7732, Time 00:00:12,lr 0.1\n",
      "epoch 83, loss 0.99493, train_acc 0.0000, valid_acc 0.7438, Time 00:00:12,lr 0.1\n",
      "epoch 84, loss 0.95414, train_acc 0.0000, valid_acc 0.7739, Time 00:00:12,lr 0.1\n",
      "epoch 85, loss 0.95911, train_acc 0.0000, valid_acc 0.7598, Time 00:00:12,lr 0.1\n",
      "epoch 86, loss 0.95058, train_acc 0.0000, valid_acc 0.7826, Time 00:00:12,lr 0.1\n",
      "epoch 87, loss 0.96495, train_acc 0.0000, valid_acc 0.7712, Time 00:00:12,lr 0.1\n",
      "epoch 88, loss 0.98746, train_acc 0.0000, valid_acc 0.7470, Time 00:00:12,lr 0.1\n",
      "epoch 89, loss 1.00017, train_acc 0.0000, valid_acc 0.7764, Time 00:00:12,lr 0.1\n",
      "epoch 90, loss 0.99734, train_acc 0.0000, valid_acc 0.7695, Time 00:00:12,lr 0.1\n",
      "epoch 91, loss 0.97140, train_acc 0.0000, valid_acc 0.7763, Time 00:00:12,lr 0.1\n",
      "epoch 92, loss 0.95261, train_acc 0.0000, valid_acc 0.7639, Time 00:00:12,lr 0.1\n",
      "epoch 93, loss 0.99321, train_acc 0.0000, valid_acc 0.7629, Time 00:00:12,lr 0.1\n",
      "epoch 94, loss 0.98602, train_acc 0.0000, valid_acc 0.7667, Time 00:00:12,lr 0.1\n",
      "epoch 95, loss 0.98113, train_acc 0.0000, valid_acc 0.7894, Time 00:00:12,lr 0.1\n",
      "epoch 96, loss 0.94321, train_acc 0.0000, valid_acc 0.7579, Time 00:00:12,lr 0.1\n",
      "epoch 97, loss 0.99696, train_acc 0.0000, valid_acc 0.7856, Time 00:00:12,lr 0.1\n",
      "epoch 98, loss 0.95928, train_acc 0.0000, valid_acc 0.7417, Time 00:00:12,lr 0.1\n",
      "epoch 99, loss 0.94883, train_acc 0.0000, valid_acc 0.7932, Time 00:00:12,lr 0.1\n",
      "epoch 100, loss 0.97283, train_acc 0.0000, valid_acc 0.7733, Time 00:00:12,lr 0.1\n",
      "epoch 101, loss 0.96041, train_acc 0.0000, valid_acc 0.7594, Time 00:00:12,lr 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 102, loss 0.95967, train_acc 0.0000, valid_acc 0.7734, Time 00:00:12,lr 0.1\n",
      "epoch 103, loss 0.92482, train_acc 0.0000, valid_acc 0.7807, Time 00:00:12,lr 0.1\n",
      "epoch 104, loss 0.99490, train_acc 0.0000, valid_acc 0.7628, Time 00:00:12,lr 0.1\n",
      "epoch 105, loss 0.97915, train_acc 0.0000, valid_acc 0.7627, Time 00:00:12,lr 0.1\n",
      "epoch 106, loss 0.96403, train_acc 0.0000, valid_acc 0.7781, Time 00:00:12,lr 0.1\n",
      "epoch 107, loss 0.94653, train_acc 0.0000, valid_acc 0.7798, Time 00:00:12,lr 0.1\n",
      "epoch 108, loss 0.92119, train_acc 0.0000, valid_acc 0.7889, Time 00:00:12,lr 0.1\n",
      "epoch 109, loss 0.93430, train_acc 0.0000, valid_acc 0.7683, Time 00:00:12,lr 0.1\n",
      "epoch 110, loss 0.94448, train_acc 0.0000, valid_acc 0.7991, Time 00:00:12,lr 0.1\n",
      "epoch 111, loss 0.94373, train_acc 0.0000, valid_acc 0.7870, Time 00:00:12,lr 0.1\n",
      "epoch 112, loss 0.96400, train_acc 0.0000, valid_acc 0.7738, Time 00:00:12,lr 0.1\n",
      "epoch 113, loss 0.92215, train_acc 0.0000, valid_acc 0.7690, Time 00:00:12,lr 0.1\n",
      "epoch 114, loss 0.93983, train_acc 0.0000, valid_acc 0.7685, Time 00:00:12,lr 0.1\n",
      "epoch 115, loss 0.93516, train_acc 0.0000, valid_acc 0.7865, Time 00:00:12,lr 0.1\n",
      "epoch 116, loss 0.93777, train_acc 0.0000, valid_acc 0.7845, Time 00:00:12,lr 0.1\n",
      "epoch 117, loss 0.94435, train_acc 0.0000, valid_acc 0.7796, Time 00:00:12,lr 0.1\n",
      "epoch 118, loss 0.93221, train_acc 0.0000, valid_acc 0.7828, Time 00:00:12,lr 0.1\n",
      "epoch 119, loss 0.94421, train_acc 0.0000, valid_acc 0.7659, Time 00:00:12,lr 0.1\n",
      "epoch 120, loss 0.93768, train_acc 0.0000, valid_acc 0.7973, Time 00:00:12,lr 0.1\n",
      "epoch 121, loss 0.90898, train_acc 0.0000, valid_acc 0.7888, Time 00:00:12,lr 0.1\n",
      "epoch 122, loss 0.93060, train_acc 0.0000, valid_acc 0.7915, Time 00:00:12,lr 0.1\n",
      "epoch 123, loss 0.95434, train_acc 0.0000, valid_acc 0.7658, Time 00:00:12,lr 0.1\n",
      "epoch 124, loss 0.97644, train_acc 0.0000, valid_acc 0.7745, Time 00:00:12,lr 0.1\n",
      "epoch 125, loss 0.96484, train_acc 0.0000, valid_acc 0.7757, Time 00:00:12,lr 0.1\n",
      "epoch 126, loss 0.91697, train_acc 0.0000, valid_acc 0.7602, Time 00:00:12,lr 0.1\n",
      "epoch 127, loss 0.94166, train_acc 0.0000, valid_acc 0.7923, Time 00:00:12,lr 0.1\n",
      "epoch 128, loss 0.92631, train_acc 0.0000, valid_acc 0.7641, Time 00:00:12,lr 0.1\n",
      "epoch 129, loss 0.92285, train_acc 0.0000, valid_acc 0.7845, Time 00:00:12,lr 0.1\n",
      "epoch 130, loss 0.94284, train_acc 0.0000, valid_acc 0.7908, Time 00:00:12,lr 0.1\n",
      "epoch 131, loss 0.96852, train_acc 0.0000, valid_acc 0.7779, Time 00:00:12,lr 0.1\n",
      "epoch 132, loss 0.92785, train_acc 0.0000, valid_acc 0.7762, Time 00:00:12,lr 0.1\n",
      "epoch 133, loss 0.92246, train_acc 0.0000, valid_acc 0.7888, Time 00:00:12,lr 0.1\n",
      "epoch 134, loss 0.92745, train_acc 0.0000, valid_acc 0.7825, Time 00:00:12,lr 0.1\n",
      "epoch 135, loss 0.93927, train_acc 0.0000, valid_acc 0.7682, Time 00:00:12,lr 0.1\n",
      "epoch 136, loss 0.90833, train_acc 0.0000, valid_acc 0.7851, Time 00:00:12,lr 0.1\n",
      "epoch 137, loss 0.96226, train_acc 0.0000, valid_acc 0.7940, Time 00:00:12,lr 0.1\n",
      "epoch 138, loss 0.94671, train_acc 0.0000, valid_acc 0.7844, Time 00:00:12,lr 0.1\n",
      "epoch 139, loss 0.94144, train_acc 0.0000, valid_acc 0.7915, Time 00:00:12,lr 0.1\n",
      "epoch 140, loss 0.90194, train_acc 0.0000, valid_acc 0.7750, Time 00:00:12,lr 0.1\n",
      "epoch 141, loss 0.95462, train_acc 0.0000, valid_acc 0.7598, Time 00:00:12,lr 0.1\n",
      "epoch 142, loss 0.93920, train_acc 0.0000, valid_acc 0.7888, Time 00:00:12,lr 0.1\n",
      "epoch 143, loss 0.93819, train_acc 0.0000, valid_acc 0.7910, Time 00:00:12,lr 0.1\n",
      "epoch 144, loss 0.93136, train_acc 0.0000, valid_acc 0.7634, Time 00:00:12,lr 0.1\n",
      "epoch 145, loss 0.92617, train_acc 0.0000, valid_acc 0.7952, Time 00:00:12,lr 0.1\n",
      "epoch 146, loss 0.90406, train_acc 0.0000, valid_acc 0.7701, Time 00:00:12,lr 0.1\n",
      "epoch 147, loss 0.94046, train_acc 0.0000, valid_acc 0.7773, Time 00:00:12,lr 0.1\n",
      "epoch 148, loss 0.92717, train_acc 0.0000, valid_acc 0.7747, Time 00:00:12,lr 0.1\n",
      "epoch 149, loss 0.95557, train_acc 0.0000, valid_acc 0.7791, Time 00:00:12,lr 0.1\n",
      "epoch 150, loss 0.84115, train_acc 0.0000, valid_acc 0.8357, Time 00:00:12,lr 0.01\n",
      "epoch 151, loss 0.80480, train_acc 0.0000, valid_acc 0.8421, Time 00:00:12,lr 0.01\n",
      "epoch 152, loss 0.75903, train_acc 0.0000, valid_acc 0.8402, Time 00:00:12,lr 0.01\n",
      "epoch 153, loss 0.75228, train_acc 0.0000, valid_acc 0.8435, Time 00:00:12,lr 0.01\n",
      "epoch 154, loss 0.75124, train_acc 0.0000, valid_acc 0.8466, Time 00:00:12,lr 0.01\n",
      "epoch 155, loss 0.79339, train_acc 0.0000, valid_acc 0.8458, Time 00:00:12,lr 0.01\n",
      "epoch 156, loss 0.76670, train_acc 0.0000, valid_acc 0.8477, Time 00:00:12,lr 0.01\n",
      "epoch 157, loss 0.74745, train_acc 0.0000, valid_acc 0.8447, Time 00:00:12,lr 0.01\n",
      "epoch 158, loss 0.72452, train_acc 0.0000, valid_acc 0.8497, Time 00:00:12,lr 0.01\n",
      "epoch 159, loss 0.70881, train_acc 0.0000, valid_acc 0.8483, Time 00:00:12,lr 0.01\n",
      "epoch 160, loss 0.75024, train_acc 0.0000, valid_acc 0.8503, Time 00:00:12,lr 0.01\n",
      "epoch 161, loss 0.75704, train_acc 0.0000, valid_acc 0.8513, Time 00:00:12,lr 0.01\n",
      "epoch 162, loss 0.70833, train_acc 0.0000, valid_acc 0.8538, Time 00:00:12,lr 0.01\n",
      "epoch 163, loss 0.70128, train_acc 0.0000, valid_acc 0.8511, Time 00:00:12,lr 0.01\n",
      "epoch 164, loss 0.71348, train_acc 0.0000, valid_acc 0.8475, Time 00:00:12,lr 0.01\n",
      "epoch 165, loss 0.68799, train_acc 0.0000, valid_acc 0.8485, Time 00:00:12,lr 0.01\n",
      "epoch 166, loss 0.73935, train_acc 0.0000, valid_acc 0.8478, Time 00:00:12,lr 0.01\n",
      "epoch 167, loss 0.70925, train_acc 0.0000, valid_acc 0.8490, Time 00:00:12,lr 0.01\n",
      "epoch 168, loss 0.69299, train_acc 0.0000, valid_acc 0.8550, Time 00:00:12,lr 0.01\n",
      "epoch 169, loss 0.71747, train_acc 0.0000, valid_acc 0.8548, Time 00:00:12,lr 0.01\n",
      "epoch 170, loss 0.68007, train_acc 0.0000, valid_acc 0.8555, Time 00:00:12,lr 0.01\n",
      "epoch 171, loss 0.69706, train_acc 0.0000, valid_acc 0.8531, Time 00:00:12,lr 0.01\n",
      "epoch 172, loss 0.72555, train_acc 0.0000, valid_acc 0.8558, Time 00:00:12,lr 0.01\n",
      "epoch 173, loss 0.66718, train_acc 0.0000, valid_acc 0.8512, Time 00:00:12,lr 0.01\n",
      "epoch 174, loss 0.68042, train_acc 0.0000, valid_acc 0.8527, Time 00:00:12,lr 0.01\n",
      "epoch 175, loss 0.73001, train_acc 0.0000, valid_acc 0.8557, Time 00:00:12,lr 0.01\n",
      "epoch 176, loss 0.67265, train_acc 0.0000, valid_acc 0.8583, Time 00:00:12,lr 0.01\n",
      "epoch 177, loss 0.69345, train_acc 0.0000, valid_acc 0.8551, Time 00:00:12,lr 0.01\n",
      "epoch 178, loss 0.69747, train_acc 0.0000, valid_acc 0.8568, Time 00:00:12,lr 0.01\n",
      "epoch 179, loss 0.67588, train_acc 0.0000, valid_acc 0.8547, Time 00:00:12,lr 0.01\n",
      "epoch 180, loss 0.67465, train_acc 0.0000, valid_acc 0.8557, Time 00:00:12,lr 0.01\n",
      "epoch 181, loss 0.68130, train_acc 0.0000, valid_acc 0.8546, Time 00:00:12,lr 0.01\n",
      "epoch 182, loss 0.66621, train_acc 0.0000, valid_acc 0.8517, Time 00:00:12,lr 0.01\n",
      "epoch 183, loss 0.66178, train_acc 0.0000, valid_acc 0.8590, Time 00:00:12,lr 0.01\n",
      "epoch 184, loss 0.68370, train_acc 0.0000, valid_acc 0.8554, Time 00:00:12,lr 0.01\n",
      "epoch 185, loss 0.71540, train_acc 0.0000, valid_acc 0.8523, Time 00:00:12,lr 0.01\n",
      "epoch 186, loss 0.69726, train_acc 0.0000, valid_acc 0.8535, Time 00:00:12,lr 0.01\n",
      "epoch 187, loss 0.65084, train_acc 0.0000, valid_acc 0.8553, Time 00:00:12,lr 0.01\n",
      "epoch 188, loss 0.66525, train_acc 0.0000, valid_acc 0.8562, Time 00:00:12,lr 0.01\n",
      "epoch 189, loss 0.65295, train_acc 0.0000, valid_acc 0.8560, Time 00:00:12,lr 0.01\n",
      "epoch 190, loss 0.66886, train_acc 0.0000, valid_acc 0.8560, Time 00:00:12,lr 0.01\n",
      "epoch 191, loss 0.69912, train_acc 0.0000, valid_acc 0.8457, Time 00:00:12,lr 0.01\n",
      "epoch 192, loss 0.66232, train_acc 0.0000, valid_acc 0.8548, Time 00:00:12,lr 0.01\n",
      "epoch 193, loss 0.71371, train_acc 0.0000, valid_acc 0.8582, Time 00:00:12,lr 0.01\n",
      "epoch 194, loss 0.70265, train_acc 0.0000, valid_acc 0.8556, Time 00:00:12,lr 0.01\n",
      "epoch 195, loss 0.66021, train_acc 0.0000, valid_acc 0.8539, Time 00:00:12,lr 0.01\n",
      "epoch 196, loss 0.68156, train_acc 0.0000, valid_acc 0.8597, Time 00:00:12,lr 0.01\n",
      "epoch 197, loss 0.69520, train_acc 0.0000, valid_acc 0.8550, Time 00:00:12,lr 0.01\n",
      "epoch 198, loss 0.70459, train_acc 0.0000, valid_acc 0.8508, Time 00:00:12,lr 0.01\n",
      "epoch 199, loss 0.64043, train_acc 0.0000, valid_acc 0.8587, Time 00:00:12,lr 0.01\n",
      "epoch 200, loss 0.66041, train_acc 0.0000, valid_acc 0.8561, Time 00:00:12,lr 0.01\n",
      "epoch 201, loss 0.71141, train_acc 0.0000, valid_acc 0.8523, Time 00:00:12,lr 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 202, loss 0.68213, train_acc 0.0000, valid_acc 0.8616, Time 00:00:12,lr 0.01\n",
      "epoch 203, loss 0.66956, train_acc 0.0000, valid_acc 0.8566, Time 00:00:12,lr 0.01\n",
      "epoch 204, loss 0.68738, train_acc 0.0000, valid_acc 0.8548, Time 00:00:12,lr 0.01\n",
      "epoch 205, loss 0.64160, train_acc 0.0000, valid_acc 0.8508, Time 00:00:12,lr 0.01\n",
      "epoch 206, loss 0.67941, train_acc 0.0000, valid_acc 0.8530, Time 00:00:12,lr 0.01\n",
      "epoch 207, loss 0.64575, train_acc 0.0000, valid_acc 0.8585, Time 00:00:12,lr 0.01\n",
      "epoch 208, loss 0.66259, train_acc 0.0000, valid_acc 0.8629, Time 00:00:12,lr 0.01\n",
      "epoch 209, loss 0.66337, train_acc 0.0000, valid_acc 0.8558, Time 00:00:12,lr 0.01\n",
      "epoch 210, loss 0.65602, train_acc 0.0000, valid_acc 0.8577, Time 00:00:12,lr 0.01\n",
      "epoch 211, loss 0.64681, train_acc 0.0000, valid_acc 0.8563, Time 00:00:12,lr 0.01\n",
      "epoch 212, loss 0.66006, train_acc 0.0000, valid_acc 0.8543, Time 00:00:12,lr 0.01\n",
      "epoch 213, loss 0.69104, train_acc 0.0000, valid_acc 0.8580, Time 00:00:12,lr 0.01\n",
      "epoch 214, loss 0.66138, train_acc 0.0000, valid_acc 0.8548, Time 00:00:12,lr 0.01\n",
      "epoch 215, loss 0.69263, train_acc 0.0000, valid_acc 0.8564, Time 00:00:12,lr 0.01\n",
      "epoch 216, loss 0.65086, train_acc 0.0000, valid_acc 0.8570, Time 00:00:12,lr 0.01\n",
      "epoch 217, loss 0.66599, train_acc 0.0000, valid_acc 0.8515, Time 00:00:12,lr 0.01\n",
      "epoch 218, loss 0.66435, train_acc 0.0000, valid_acc 0.8579, Time 00:00:12,lr 0.01\n",
      "epoch 219, loss 0.67021, train_acc 0.0000, valid_acc 0.8539, Time 00:00:12,lr 0.01\n",
      "epoch 220, loss 0.65423, train_acc 0.0000, valid_acc 0.8550, Time 00:00:12,lr 0.01\n",
      "epoch 221, loss 0.61697, train_acc 0.0000, valid_acc 0.8490, Time 00:00:12,lr 0.01\n",
      "epoch 222, loss 0.65184, train_acc 0.0000, valid_acc 0.8562, Time 00:00:12,lr 0.01\n",
      "epoch 223, loss 0.67574, train_acc 0.0000, valid_acc 0.8558, Time 00:00:12,lr 0.01\n",
      "epoch 224, loss 0.64664, train_acc 0.0000, valid_acc 0.8530, Time 00:00:12,lr 0.01\n",
      "epoch 225, loss 0.64429, train_acc 0.0000, valid_acc 0.8560, Time 00:00:12,lr 0.01\n",
      "epoch 226, loss 0.63697, train_acc 0.0000, valid_acc 0.8525, Time 00:00:12,lr 0.01\n",
      "epoch 227, loss 0.65928, train_acc 0.0000, valid_acc 0.8528, Time 00:00:12,lr 0.01\n",
      "epoch 228, loss 0.61896, train_acc 0.0000, valid_acc 0.8504, Time 00:00:12,lr 0.01\n",
      "epoch 229, loss 0.68739, train_acc 0.0000, valid_acc 0.8538, Time 00:00:12,lr 0.01\n",
      "epoch 230, loss 0.68417, train_acc 0.0000, valid_acc 0.8547, Time 00:00:12,lr 0.01\n",
      "epoch 231, loss 0.66517, train_acc 0.0000, valid_acc 0.8505, Time 00:00:12,lr 0.01\n",
      "epoch 232, loss 0.67533, train_acc 0.0000, valid_acc 0.8526, Time 00:00:12,lr 0.01\n",
      "epoch 233, loss 0.66627, train_acc 0.0000, valid_acc 0.8529, Time 00:00:12,lr 0.01\n",
      "epoch 234, loss 0.64847, train_acc 0.0000, valid_acc 0.8550, Time 00:00:12,lr 0.01\n",
      "epoch 235, loss 0.61907, train_acc 0.0000, valid_acc 0.8551, Time 00:00:12,lr 0.01\n",
      "epoch 236, loss 0.66851, train_acc 0.0000, valid_acc 0.8584, Time 00:00:12,lr 0.01\n",
      "epoch 237, loss 0.65347, train_acc 0.0000, valid_acc 0.8548, Time 00:00:12,lr 0.01\n",
      "epoch 238, loss 0.65095, train_acc 0.0000, valid_acc 0.8588, Time 00:00:12,lr 0.01\n",
      "epoch 239, loss 0.65128, train_acc 0.0000, valid_acc 0.8531, Time 00:00:12,lr 0.01\n",
      "epoch 240, loss 0.66159, train_acc 0.0000, valid_acc 0.8514, Time 00:00:12,lr 0.01\n",
      "epoch 241, loss 0.62167, train_acc 0.0000, valid_acc 0.8493, Time 00:00:12,lr 0.01\n",
      "epoch 242, loss 0.66373, train_acc 0.0000, valid_acc 0.8545, Time 00:00:12,lr 0.01\n",
      "epoch 243, loss 0.65094, train_acc 0.0000, valid_acc 0.8569, Time 00:00:12,lr 0.01\n",
      "epoch 244, loss 0.63717, train_acc 0.0000, valid_acc 0.8506, Time 00:00:12,lr 0.01\n",
      "epoch 245, loss 0.61256, train_acc 0.0000, valid_acc 0.8547, Time 00:00:12,lr 0.01\n",
      "epoch 246, loss 0.68577, train_acc 0.0000, valid_acc 0.8523, Time 00:00:12,lr 0.01\n",
      "epoch 247, loss 0.62739, train_acc 0.0000, valid_acc 0.8519, Time 00:00:12,lr 0.01\n",
      "epoch 248, loss 0.64313, train_acc 0.0000, valid_acc 0.8543, Time 00:00:12,lr 0.01\n",
      "epoch 249, loss 0.61270, train_acc 0.0000, valid_acc 0.8551, Time 00:00:12,lr 0.01\n",
      "epoch 250, loss 0.61832, train_acc 0.0000, valid_acc 0.8638, Time 00:00:12,lr 0.001\n",
      "epoch 251, loss 0.61247, train_acc 0.0000, valid_acc 0.8622, Time 00:00:12,lr 0.001\n",
      "epoch 252, loss 0.56691, train_acc 0.0000, valid_acc 0.8668, Time 00:00:12,lr 0.001\n",
      "epoch 253, loss 0.57503, train_acc 0.0000, valid_acc 0.8629, Time 00:00:12,lr 0.001\n",
      "epoch 254, loss 0.57278, train_acc 0.0000, valid_acc 0.8677, Time 00:00:12,lr 0.001\n",
      "epoch 255, loss 0.64170, train_acc 0.0000, valid_acc 0.8666, Time 00:00:12,lr 0.001\n",
      "epoch 256, loss 0.59176, train_acc 0.0000, valid_acc 0.8666, Time 00:00:12,lr 0.001\n",
      "epoch 257, loss 0.63879, train_acc 0.0000, valid_acc 0.8657, Time 00:00:12,lr 0.001\n",
      "epoch 258, loss 0.62311, train_acc 0.0000, valid_acc 0.8657, Time 00:00:12,lr 0.001\n",
      "epoch 259, loss 0.59667, train_acc 0.0000, valid_acc 0.8664, Time 00:00:12,lr 0.001\n",
      "epoch 260, loss 0.61594, train_acc 0.0000, valid_acc 0.8674, Time 00:00:12,lr 0.001\n",
      "epoch 261, loss 0.60573, train_acc 0.0000, valid_acc 0.8679, Time 00:00:12,lr 0.001\n",
      "epoch 262, loss 0.59641, train_acc 0.0000, valid_acc 0.8683, Time 00:00:12,lr 0.001\n",
      "epoch 263, loss 0.62193, train_acc 0.0000, valid_acc 0.8677, Time 00:00:12,lr 0.001\n",
      "epoch 264, loss 0.58233, train_acc 0.0000, valid_acc 0.8686, Time 00:00:12,lr 0.001\n",
      "epoch 265, loss 0.58906, train_acc 0.0000, valid_acc 0.8702, Time 00:00:12,lr 0.001\n",
      "epoch 266, loss 0.60493, train_acc 0.0000, valid_acc 0.8644, Time 00:00:12,lr 0.001\n",
      "epoch 267, loss 0.56191, train_acc 0.0000, valid_acc 0.8665, Time 00:00:12,lr 0.001\n",
      "epoch 268, loss 0.55902, train_acc 0.0000, valid_acc 0.8667, Time 00:00:12,lr 0.001\n",
      "epoch 269, loss 0.55750, train_acc 0.0000, valid_acc 0.8669, Time 00:00:12,lr 0.001\n",
      "epoch 270, loss 0.60847, train_acc 0.0000, valid_acc 0.8663, Time 00:00:12,lr 0.001\n",
      "epoch 271, loss 0.58352, train_acc 0.0000, valid_acc 0.8678, Time 00:00:12,lr 0.001\n",
      "epoch 272, loss 0.51846, train_acc 0.0000, valid_acc 0.8701, Time 00:00:12,lr 0.001\n",
      "epoch 273, loss 0.59868, train_acc 0.0000, valid_acc 0.8702, Time 00:00:12,lr 0.001\n",
      "epoch 274, loss 0.56868, train_acc 0.0000, valid_acc 0.8659, Time 00:00:12,lr 0.001\n",
      "epoch 275, loss 0.59048, train_acc 0.0000, valid_acc 0.8664, Time 00:00:12,lr 0.001\n",
      "epoch 276, loss 0.62009, train_acc 0.0000, valid_acc 0.8678, Time 00:00:12,lr 0.001\n",
      "epoch 277, loss 0.56054, train_acc 0.0000, valid_acc 0.8668, Time 00:00:12,lr 0.001\n",
      "epoch 278, loss 0.55805, train_acc 0.0000, valid_acc 0.8694, Time 00:00:12,lr 0.001\n",
      "epoch 279, loss 0.53810, train_acc 0.0000, valid_acc 0.8684, Time 00:00:12,lr 0.001\n",
      "epoch 280, loss 0.56775, train_acc 0.0000, valid_acc 0.8676, Time 00:00:12,lr 0.001\n",
      "epoch 281, loss 0.61796, train_acc 0.0000, valid_acc 0.8674, Time 00:00:12,lr 0.001\n",
      "epoch 282, loss 0.52921, train_acc 0.0000, valid_acc 0.8675, Time 00:00:12,lr 0.001\n",
      "epoch 283, loss 0.61031, train_acc 0.0000, valid_acc 0.8647, Time 00:00:12,lr 0.001\n",
      "epoch 284, loss 0.59074, train_acc 0.0000, valid_acc 0.8705, Time 00:00:12,lr 0.001\n",
      "epoch 285, loss 0.59286, train_acc 0.0000, valid_acc 0.8652, Time 00:00:12,lr 0.001\n",
      "epoch 286, loss 0.61293, train_acc 0.0000, valid_acc 0.8687, Time 00:00:12,lr 0.001\n",
      "epoch 287, loss 0.53750, train_acc 0.0000, valid_acc 0.8673, Time 00:00:12,lr 0.001\n",
      "epoch 288, loss 0.56555, train_acc 0.0000, valid_acc 0.8659, Time 00:00:12,lr 0.001\n",
      "epoch 289, loss 0.58345, train_acc 0.0000, valid_acc 0.8667, Time 00:00:12,lr 0.001\n",
      "epoch 290, loss 0.54332, train_acc 0.0000, valid_acc 0.8671, Time 00:00:12,lr 0.001\n",
      "epoch 291, loss 0.55275, train_acc 0.0000, valid_acc 0.8702, Time 00:00:12,lr 0.001\n",
      "epoch 292, loss 0.55861, train_acc 0.0000, valid_acc 0.8664, Time 00:00:12,lr 0.001\n",
      "epoch 293, loss 0.57197, train_acc 0.0000, valid_acc 0.8666, Time 00:00:12,lr 0.001\n",
      "epoch 294, loss 0.57366, train_acc 0.0000, valid_acc 0.8669, Time 00:00:12,lr 0.001\n",
      "epoch 295, loss 0.55294, train_acc 0.0000, valid_acc 0.8672, Time 00:00:12,lr 0.001\n",
      "epoch 296, loss 0.56102, train_acc 0.0000, valid_acc 0.8694, Time 00:00:12,lr 0.001\n",
      "epoch 297, loss 0.56374, train_acc 0.0000, valid_acc 0.8663, Time 00:00:12,lr 0.001\n",
      "epoch 298, loss 0.57677, train_acc 0.0000, valid_acc 0.8680, Time 00:00:12,lr 0.001\n",
      "epoch 299, loss 0.59011, train_acc 0.0000, valid_acc 0.8715, Time 00:00:12,lr 0.001\n",
      "epoch 300, loss 0.56410, train_acc 0.0000, valid_acc 0.8684, Time 00:00:12,lr 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 301, loss 0.55164, train_acc 0.0000, valid_acc 0.8673, Time 00:00:12,lr 0.001\n",
      "epoch 302, loss 0.56074, train_acc 0.0000, valid_acc 0.8693, Time 00:00:12,lr 0.001\n",
      "epoch 303, loss 0.59132, train_acc 0.0000, valid_acc 0.8688, Time 00:00:12,lr 0.001\n",
      "epoch 304, loss 0.57373, train_acc 0.0000, valid_acc 0.8646, Time 00:00:12,lr 0.001\n",
      "epoch 305, loss 0.59050, train_acc 0.0000, valid_acc 0.8676, Time 00:00:12,lr 0.001\n",
      "epoch 306, loss 0.54857, train_acc 0.0000, valid_acc 0.8705, Time 00:00:12,lr 0.001\n",
      "epoch 307, loss 0.52638, train_acc 0.0000, valid_acc 0.8730, Time 00:00:12,lr 0.001\n",
      "epoch 308, loss 0.52827, train_acc 0.0000, valid_acc 0.8663, Time 00:00:12,lr 0.001\n",
      "epoch 309, loss 0.55569, train_acc 0.0000, valid_acc 0.8687, Time 00:00:12,lr 0.001\n",
      "epoch 310, loss 0.51560, train_acc 0.0000, valid_acc 0.8689, Time 00:00:12,lr 0.001\n",
      "epoch 311, loss 0.53696, train_acc 0.0000, valid_acc 0.8661, Time 00:00:12,lr 0.001\n",
      "epoch 312, loss 0.56155, train_acc 0.0000, valid_acc 0.8695, Time 00:00:12,lr 0.001\n",
      "epoch 313, loss 0.55693, train_acc 0.0000, valid_acc 0.8683, Time 00:00:12,lr 0.001\n",
      "epoch 314, loss 0.56468, train_acc 0.0000, valid_acc 0.8673, Time 00:00:12,lr 0.001\n",
      "epoch 315, loss 0.58233, train_acc 0.0000, valid_acc 0.8659, Time 00:00:12,lr 0.001\n",
      "epoch 316, loss 0.51298, train_acc 0.0000, valid_acc 0.8631, Time 00:00:12,lr 0.001\n",
      "epoch 317, loss 0.53303, train_acc 0.0000, valid_acc 0.8671, Time 00:00:12,lr 0.001\n",
      "epoch 318, loss 0.55902, train_acc 0.0000, valid_acc 0.8694, Time 00:00:12,lr 0.001\n",
      "epoch 319, loss 0.57843, train_acc 0.0000, valid_acc 0.8665, Time 00:00:12,lr 0.001\n",
      "epoch 320, loss 0.59208, train_acc 0.0000, valid_acc 0.8681, Time 00:00:12,lr 0.001\n",
      "epoch 321, loss 0.53208, train_acc 0.0000, valid_acc 0.8618, Time 00:00:12,lr 0.001\n",
      "epoch 322, loss 0.56361, train_acc 0.0000, valid_acc 0.8675, Time 00:00:12,lr 0.001\n",
      "epoch 323, loss 0.55613, train_acc 0.0000, valid_acc 0.8647, Time 00:00:12,lr 0.001\n",
      "epoch 324, loss 0.55465, train_acc 0.0000, valid_acc 0.8693, Time 00:00:12,lr 0.001\n",
      "epoch 325, loss 0.49811, train_acc 0.0000, valid_acc 0.8705, Time 00:00:12,lr 0.001\n",
      "epoch 326, loss 0.60275, train_acc 0.0000, valid_acc 0.8671, Time 00:00:12,lr 0.001\n",
      "epoch 327, loss 0.53508, train_acc 0.0000, valid_acc 0.8687, Time 00:00:12,lr 0.001\n",
      "epoch 328, loss 0.56311, train_acc 0.0000, valid_acc 0.8688, Time 00:00:12,lr 0.001\n",
      "epoch 329, loss 0.55569, train_acc 0.0000, valid_acc 0.8652, Time 00:00:12,lr 0.001\n",
      "epoch 330, loss 0.58284, train_acc 0.0000, valid_acc 0.8667, Time 00:00:12,lr 0.001\n",
      "epoch 331, loss 0.58517, train_acc 0.0000, valid_acc 0.8678, Time 00:00:12,lr 0.001\n",
      "epoch 332, loss 0.58679, train_acc 0.0000, valid_acc 0.8687, Time 00:00:12,lr 0.001\n",
      "epoch 333, loss 0.59870, train_acc 0.0000, valid_acc 0.8682, Time 00:00:12,lr 0.001\n",
      "epoch 334, loss 0.53251, train_acc 0.0000, valid_acc 0.8681, Time 00:00:12,lr 0.001\n",
      "epoch 335, loss 0.58461, train_acc 0.0000, valid_acc 0.8694, Time 00:00:12,lr 0.001\n",
      "epoch 336, loss 0.58170, train_acc 0.0000, valid_acc 0.8635, Time 00:00:12,lr 0.001\n",
      "epoch 337, loss 0.56242, train_acc 0.0000, valid_acc 0.8652, Time 00:00:12,lr 0.001\n",
      "epoch 338, loss 0.56096, train_acc 0.0000, valid_acc 0.8696, Time 00:00:12,lr 0.001\n",
      "epoch 339, loss 0.54412, train_acc 0.0000, valid_acc 0.8704, Time 00:00:12,lr 0.001\n",
      "epoch 340, loss 0.55013, train_acc 0.0000, valid_acc 0.8677, Time 00:00:12,lr 0.001\n",
      "epoch 341, loss 0.57040, train_acc 0.0000, valid_acc 0.8670, Time 00:00:12,lr 0.001\n",
      "epoch 342, loss 0.55359, train_acc 0.0000, valid_acc 0.8686, Time 00:00:12,lr 0.001\n",
      "epoch 343, loss 0.56753, train_acc 0.0000, valid_acc 0.8673, Time 00:00:12,lr 0.001\n",
      "epoch 344, loss 0.58434, train_acc 0.0000, valid_acc 0.8646, Time 00:00:12,lr 0.001\n",
      "epoch 345, loss 0.55299, train_acc 0.0000, valid_acc 0.8672, Time 00:00:12,lr 0.001\n",
      "epoch 346, loss 0.56326, train_acc 0.0000, valid_acc 0.8646, Time 00:00:12,lr 0.001\n",
      "epoch 347, loss 0.55318, train_acc 0.0000, valid_acc 0.8667, Time 00:00:12,lr 0.001\n",
      "epoch 348, loss 0.48371, train_acc 0.0000, valid_acc 0.8677, Time 00:00:12,lr 0.001\n",
      "epoch 349, loss 0.54573, train_acc 0.0000, valid_acc 0.8696, Time 00:00:12,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=3)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "net = get_resnet18_v2()\n",
    "\n",
    "num_epochs = 350      # 50 * 1563 iter about\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 250]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f, use_mixup=True, mixup_alpha=0.2)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_mixup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.7 mixup finetune DA1e250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:35:04.998977Z",
     "start_time": "2018-03-02T08:10:13.255525Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.04917, train_acc 0.0000, valid_acc 0.8296, Time 00:00:10,lr 0.001\n",
      "epoch 1, loss 0.83445, train_acc 0.0000, valid_acc 0.8164, Time 00:00:12,lr 0.001\n",
      "epoch 2, loss 0.80262, train_acc 0.0000, valid_acc 0.8288, Time 00:00:12,lr 0.001\n",
      "epoch 3, loss 0.75816, train_acc 0.0000, valid_acc 0.8304, Time 00:00:12,lr 0.001\n",
      "epoch 4, loss 0.66389, train_acc 0.0000, valid_acc 0.8422, Time 00:00:12,lr 0.001\n",
      "epoch 5, loss 0.68028, train_acc 0.0000, valid_acc 0.8384, Time 00:00:12,lr 0.001\n",
      "epoch 6, loss 0.68301, train_acc 0.0000, valid_acc 0.8416, Time 00:00:12,lr 0.001\n",
      "epoch 7, loss 0.66096, train_acc 0.0000, valid_acc 0.8482, Time 00:00:12,lr 0.001\n",
      "epoch 8, loss 0.68089, train_acc 0.0000, valid_acc 0.8482, Time 00:00:12,lr 0.001\n",
      "epoch 9, loss 0.60168, train_acc 0.0000, valid_acc 0.8499, Time 00:00:12,lr 0.001\n",
      "epoch 10, loss 0.58366, train_acc 0.0000, valid_acc 0.8509, Time 00:00:12,lr 0.001\n",
      "epoch 11, loss 0.63316, train_acc 0.0000, valid_acc 0.8511, Time 00:00:12,lr 0.001\n",
      "epoch 12, loss 0.68382, train_acc 0.0000, valid_acc 0.8499, Time 00:00:12,lr 0.001\n",
      "epoch 13, loss 0.60909, train_acc 0.0000, valid_acc 0.8543, Time 00:00:12,lr 0.001\n",
      "epoch 14, loss 0.65065, train_acc 0.0000, valid_acc 0.8549, Time 00:00:12,lr 0.001\n",
      "epoch 15, loss 0.60335, train_acc 0.0000, valid_acc 0.8551, Time 00:00:12,lr 0.001\n",
      "epoch 16, loss 0.65350, train_acc 0.0000, valid_acc 0.8554, Time 00:00:12,lr 0.001\n",
      "epoch 17, loss 0.56340, train_acc 0.0000, valid_acc 0.8590, Time 00:00:12,lr 0.001\n",
      "epoch 18, loss 0.60451, train_acc 0.0000, valid_acc 0.8572, Time 00:00:12,lr 0.001\n",
      "epoch 19, loss 0.65111, train_acc 0.0000, valid_acc 0.8594, Time 00:00:12,lr 0.001\n",
      "epoch 20, loss 0.63504, train_acc 0.0000, valid_acc 0.8578, Time 00:00:12,lr 0.001\n",
      "epoch 21, loss 0.61984, train_acc 0.0000, valid_acc 0.8560, Time 00:00:12,lr 0.001\n",
      "epoch 22, loss 0.58813, train_acc 0.0000, valid_acc 0.8584, Time 00:00:12,lr 0.001\n",
      "epoch 23, loss 0.66315, train_acc 0.0000, valid_acc 0.8529, Time 00:00:12,lr 0.001\n",
      "epoch 24, loss 0.62087, train_acc 0.0000, valid_acc 0.8595, Time 00:00:12,lr 0.001\n",
      "epoch 25, loss 0.56767, train_acc 0.0000, valid_acc 0.8589, Time 00:00:12,lr 0.001\n",
      "epoch 26, loss 0.55546, train_acc 0.0000, valid_acc 0.8607, Time 00:00:12,lr 0.001\n",
      "epoch 27, loss 0.58277, train_acc 0.0000, valid_acc 0.8616, Time 00:00:12,lr 0.001\n",
      "epoch 28, loss 0.57724, train_acc 0.0000, valid_acc 0.8626, Time 00:00:12,lr 0.001\n",
      "epoch 29, loss 0.60828, train_acc 0.0000, valid_acc 0.8602, Time 00:00:12,lr 0.001\n",
      "epoch 30, loss 0.61943, train_acc 0.0000, valid_acc 0.8638, Time 00:00:12,lr 0.001\n",
      "epoch 31, loss 0.64867, train_acc 0.0000, valid_acc 0.8616, Time 00:00:12,lr 0.001\n",
      "epoch 32, loss 0.58573, train_acc 0.0000, valid_acc 0.8653, Time 00:00:12,lr 0.001\n",
      "epoch 33, loss 0.59403, train_acc 0.0000, valid_acc 0.8614, Time 00:00:12,lr 0.001\n",
      "epoch 34, loss 0.59626, train_acc 0.0000, valid_acc 0.8608, Time 00:00:12,lr 0.001\n",
      "epoch 35, loss 0.60751, train_acc 0.0000, valid_acc 0.8610, Time 00:00:12,lr 0.001\n",
      "epoch 36, loss 0.60082, train_acc 0.0000, valid_acc 0.8607, Time 00:00:12,lr 0.001\n",
      "epoch 37, loss 0.57443, train_acc 0.0000, valid_acc 0.8638, Time 00:00:12,lr 0.001\n",
      "epoch 38, loss 0.64751, train_acc 0.0000, valid_acc 0.8598, Time 00:00:12,lr 0.001\n",
      "epoch 39, loss 0.62489, train_acc 0.0000, valid_acc 0.8588, Time 00:00:12,lr 0.001\n",
      "epoch 40, loss 0.59857, train_acc 0.0000, valid_acc 0.8589, Time 00:00:12,lr 0.001\n",
      "epoch 41, loss 0.53652, train_acc 0.0000, valid_acc 0.8646, Time 00:00:12,lr 0.001\n",
      "epoch 42, loss 0.61969, train_acc 0.0000, valid_acc 0.8621, Time 00:00:12,lr 0.001\n",
      "epoch 43, loss 0.60692, train_acc 0.0000, valid_acc 0.8644, Time 00:00:12,lr 0.001\n",
      "epoch 44, loss 0.61657, train_acc 0.0000, valid_acc 0.8625, Time 00:00:12,lr 0.001\n",
      "epoch 45, loss 0.56512, train_acc 0.0000, valid_acc 0.8645, Time 00:00:12,lr 0.001\n",
      "epoch 46, loss 0.55140, train_acc 0.0000, valid_acc 0.8593, Time 00:00:12,lr 0.001\n",
      "epoch 47, loss 0.58292, train_acc 0.0000, valid_acc 0.8638, Time 00:00:12,lr 0.001\n",
      "epoch 48, loss 0.60024, train_acc 0.0000, valid_acc 0.8672, Time 00:00:12,lr 0.001\n",
      "epoch 49, loss 0.56714, train_acc 0.0000, valid_acc 0.8655, Time 00:00:12,lr 0.001\n",
      "epoch 50, loss 0.59305, train_acc 0.0000, valid_acc 0.8644, Time 00:00:12,lr 0.001\n",
      "epoch 51, loss 0.56624, train_acc 0.0000, valid_acc 0.8635, Time 00:00:12,lr 0.001\n",
      "epoch 52, loss 0.58343, train_acc 0.0000, valid_acc 0.8673, Time 00:00:12,lr 0.001\n",
      "epoch 53, loss 0.56003, train_acc 0.0000, valid_acc 0.8641, Time 00:00:12,lr 0.001\n",
      "epoch 54, loss 0.58216, train_acc 0.0000, valid_acc 0.8583, Time 00:00:12,lr 0.001\n",
      "epoch 55, loss 0.57309, train_acc 0.0000, valid_acc 0.8644, Time 00:00:12,lr 0.001\n",
      "epoch 56, loss 0.51446, train_acc 0.0000, valid_acc 0.8662, Time 00:00:12,lr 0.001\n",
      "epoch 57, loss 0.57103, train_acc 0.0000, valid_acc 0.8677, Time 00:00:12,lr 0.001\n",
      "epoch 58, loss 0.60401, train_acc 0.0000, valid_acc 0.8611, Time 00:00:12,lr 0.001\n",
      "epoch 59, loss 0.61387, train_acc 0.0000, valid_acc 0.8675, Time 00:00:12,lr 0.001\n",
      "epoch 60, loss 0.54641, train_acc 0.0000, valid_acc 0.8653, Time 00:00:12,lr 0.001\n",
      "epoch 61, loss 0.60835, train_acc 0.0000, valid_acc 0.8649, Time 00:00:12,lr 0.001\n",
      "epoch 62, loss 0.54844, train_acc 0.0000, valid_acc 0.8632, Time 00:00:12,lr 0.001\n",
      "epoch 63, loss 0.59112, train_acc 0.0000, valid_acc 0.8660, Time 00:00:12,lr 0.001\n",
      "epoch 64, loss 0.54976, train_acc 0.0000, valid_acc 0.8658, Time 00:00:12,lr 0.001\n",
      "epoch 65, loss 0.54135, train_acc 0.0000, valid_acc 0.8670, Time 00:00:12,lr 0.001\n",
      "epoch 66, loss 0.58644, train_acc 0.0000, valid_acc 0.8638, Time 00:00:12,lr 0.001\n",
      "epoch 67, loss 0.63712, train_acc 0.0000, valid_acc 0.8617, Time 00:00:12,lr 0.001\n",
      "epoch 68, loss 0.55391, train_acc 0.0000, valid_acc 0.8650, Time 00:00:12,lr 0.001\n",
      "epoch 69, loss 0.55907, train_acc 0.0000, valid_acc 0.8657, Time 00:00:12,lr 0.001\n",
      "epoch 70, loss 0.58530, train_acc 0.0000, valid_acc 0.8649, Time 00:00:12,lr 0.001\n",
      "epoch 71, loss 0.58443, train_acc 0.0000, valid_acc 0.8656, Time 00:00:12,lr 0.001\n",
      "epoch 72, loss 0.54506, train_acc 0.0000, valid_acc 0.8689, Time 00:00:12,lr 0.001\n",
      "epoch 73, loss 0.55503, train_acc 0.0000, valid_acc 0.8669, Time 00:00:12,lr 0.001\n",
      "epoch 74, loss 0.61006, train_acc 0.0000, valid_acc 0.8647, Time 00:00:12,lr 0.001\n",
      "epoch 75, loss 0.53397, train_acc 0.0000, valid_acc 0.8666, Time 00:00:12,lr 0.001\n",
      "epoch 76, loss 0.53762, train_acc 0.0000, valid_acc 0.8648, Time 00:00:12,lr 0.001\n",
      "epoch 77, loss 0.54904, train_acc 0.0000, valid_acc 0.8677, Time 00:00:12,lr 0.001\n",
      "epoch 78, loss 0.53778, train_acc 0.0000, valid_acc 0.8683, Time 00:00:12,lr 0.001\n",
      "epoch 79, loss 0.60604, train_acc 0.0000, valid_acc 0.8644, Time 00:00:12,lr 0.001\n",
      "epoch 80, loss 0.58034, train_acc 0.0000, valid_acc 0.8673, Time 00:00:12,lr 0.0001\n",
      "epoch 81, loss 0.54357, train_acc 0.0000, valid_acc 0.8690, Time 00:00:12,lr 0.0001\n",
      "epoch 82, loss 0.53124, train_acc 0.0000, valid_acc 0.8686, Time 00:00:12,lr 0.0001\n",
      "epoch 83, loss 0.55999, train_acc 0.0000, valid_acc 0.8671, Time 00:00:12,lr 0.0001\n",
      "epoch 84, loss 0.55794, train_acc 0.0000, valid_acc 0.8705, Time 00:00:12,lr 0.0001\n",
      "epoch 85, loss 0.54287, train_acc 0.0000, valid_acc 0.8674, Time 00:00:12,lr 0.0001\n",
      "epoch 86, loss 0.59244, train_acc 0.0000, valid_acc 0.8676, Time 00:00:12,lr 0.0001\n",
      "epoch 87, loss 0.52220, train_acc 0.0000, valid_acc 0.8719, Time 00:00:12,lr 0.0001\n",
      "epoch 88, loss 0.51682, train_acc 0.0000, valid_acc 0.8706, Time 00:00:12,lr 0.0001\n",
      "epoch 89, loss 0.57252, train_acc 0.0000, valid_acc 0.8685, Time 00:00:12,lr 0.0001\n",
      "epoch 90, loss 0.61488, train_acc 0.0000, valid_acc 0.8679, Time 00:00:12,lr 0.0001\n",
      "epoch 91, loss 0.61770, train_acc 0.0000, valid_acc 0.8668, Time 00:00:12,lr 0.0001\n",
      "epoch 92, loss 0.53211, train_acc 0.0000, valid_acc 0.8683, Time 00:00:12,lr 0.0001\n",
      "epoch 93, loss 0.55579, train_acc 0.0000, valid_acc 0.8689, Time 00:00:12,lr 0.0001\n",
      "epoch 94, loss 0.56170, train_acc 0.0000, valid_acc 0.8694, Time 00:00:12,lr 0.0001\n",
      "epoch 95, loss 0.55699, train_acc 0.0000, valid_acc 0.8684, Time 00:00:12,lr 0.0001\n",
      "epoch 96, loss 0.54951, train_acc 0.0000, valid_acc 0.8693, Time 00:00:12,lr 0.0001\n",
      "epoch 97, loss 0.51965, train_acc 0.0000, valid_acc 0.8675, Time 00:00:12,lr 0.0001\n",
      "epoch 98, loss 0.54736, train_acc 0.0000, valid_acc 0.8654, Time 00:00:12,lr 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss 0.54879, train_acc 0.0000, valid_acc 0.8683, Time 00:00:12,lr 0.0001\n",
      "epoch 100, loss 0.57475, train_acc 0.0000, valid_acc 0.8681, Time 00:00:12,lr 0.0001\n",
      "epoch 101, loss 0.55174, train_acc 0.0000, valid_acc 0.8699, Time 00:00:12,lr 0.0001\n",
      "epoch 102, loss 0.55255, train_acc 0.0000, valid_acc 0.8683, Time 00:00:12,lr 0.0001\n",
      "epoch 103, loss 0.54742, train_acc 0.0000, valid_acc 0.8690, Time 00:00:12,lr 0.0001\n",
      "epoch 104, loss 0.61386, train_acc 0.0000, valid_acc 0.8638, Time 00:00:12,lr 0.0001\n",
      "epoch 105, loss 0.57141, train_acc 0.0000, valid_acc 0.8691, Time 00:00:12,lr 0.0001\n",
      "epoch 106, loss 0.53535, train_acc 0.0000, valid_acc 0.8705, Time 00:00:12,lr 0.0001\n",
      "epoch 107, loss 0.52410, train_acc 0.0000, valid_acc 0.8711, Time 00:00:12,lr 0.0001\n",
      "epoch 108, loss 0.51386, train_acc 0.0000, valid_acc 0.8666, Time 00:00:12,lr 0.0001\n",
      "epoch 109, loss 0.56098, train_acc 0.0000, valid_acc 0.8675, Time 00:00:12,lr 0.0001\n",
      "epoch 110, loss 0.55949, train_acc 0.0000, valid_acc 0.8704, Time 00:00:12,lr 0.0001\n",
      "epoch 111, loss 0.56072, train_acc 0.0000, valid_acc 0.8651, Time 00:00:12,lr 0.0001\n",
      "epoch 112, loss 0.52428, train_acc 0.0000, valid_acc 0.8703, Time 00:00:12,lr 0.0001\n",
      "epoch 113, loss 0.55186, train_acc 0.0000, valid_acc 0.8696, Time 00:00:12,lr 0.0001\n",
      "epoch 114, loss 0.56627, train_acc 0.0000, valid_acc 0.8682, Time 00:00:12,lr 0.0001\n",
      "epoch 115, loss 0.53173, train_acc 0.0000, valid_acc 0.8678, Time 00:00:12,lr 0.0001\n",
      "epoch 116, loss 0.50252, train_acc 0.0000, valid_acc 0.8651, Time 00:00:12,lr 0.0001\n",
      "epoch 117, loss 0.53947, train_acc 0.0000, valid_acc 0.8683, Time 00:00:12,lr 0.0001\n",
      "epoch 118, loss 0.53269, train_acc 0.0000, valid_acc 0.8676, Time 00:00:12,lr 0.0001\n",
      "epoch 119, loss 0.54335, train_acc 0.0000, valid_acc 0.8684, Time 00:00:12,lr 0.0001\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=3)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "net = get_resnet18_v2()\n",
    "\n",
    "num_epochs = 120      # 50 * 1563 iter about\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80]\n",
    "\n",
    "net.load_params(\"../../models/train3_resnet18_v2_finetune_DA1_e250\", ctx=ctx)\n",
    "\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f, use_mixup=True, mixup_alpha=0.2)\n",
    "net.save_params(\"../../models/train3_resnet18_v2_finetune_agterDA1e250_mixup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T14:14:29.728373Z",
     "start_time": "2018-03-01T14:13:29.698Z"
    }
   },
   "source": [
    "## 5.2 resnet152_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T09:21:54.987370Z",
     "start_time": "2018-03-02T09:21:54.981259Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and add must in name_scope, or may got name error\n",
    "def get_resnet152_v2():\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet152_v2(pretrained=True, ctx=ctx)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features)\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    output.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-02T09:22:50.613Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = get_resnet152_v2()\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=3)\n",
    "\n",
    "num_epochs = 150      # 50 * 1563 iter about\n",
    "learning_rate = 0.1\n",
    "lr_period = [150]\n",
    "\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet152_v2_finetune_DA1_e150\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-02T09:22:56.910Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100      # 100 + 150 epoch\n",
    "learning_rate = 0.01\n",
    "lr_period = [100]\n",
    "\n",
    "net.load_params(\"../../models/train3_resnet152_v2_finetune_DA1_e150\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet152_v2_finetune_DA1_e250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-02T09:22:57.661Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100      # 100 + 150 epoch\n",
    "learning_rate = 0.001\n",
    "lr_period = [100]\n",
    "\n",
    "net.load_params(\"../../models/train3_resnet152_v2_finetune_DA1_e250\", ctx=ctx)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"../../models/train3_resnet152_v2_finetune_DA1_e350\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
