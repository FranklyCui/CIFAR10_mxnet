{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T06:32:47.406027Z",
     "start_time": "2018-03-01T06:32:42.929058Z"
    },
    "collapsed": true
   },
   "source": [
    "# 1. import needed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:07.640452Z",
     "start_time": "2018-03-04T09:17:06.929352Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.insert(0, '../../utils')\n",
    "from netlib import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ctx = mx.gpu(4)\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data loader, data argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:07.654837Z",
     "start_time": "2018-03-04T09:17:07.641928Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data loader\n",
    "\"\"\"\n",
    "def _transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def data_loader(batch_size, transform_train, transform_test=None, num_workers=0):\n",
    "    if transform_train is None:\n",
    "        transform_train = _transform_train\n",
    "    if transform_test is None:\n",
    "        transform_test = _transform_test\n",
    "        \n",
    "    # flag=1 mean 3 channel image\n",
    "    train_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=True, transform=transform_train)\n",
    "    test_ds = gluon.data.vision.datasets.CIFAR10(root='~/.mxnet/datasets/cifar10', train=False, transform=transform_test)\n",
    "\n",
    "    loader = gluon.data.DataLoader\n",
    "    train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep', num_workers=num_workers)\n",
    "    test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep', num_workers=num_workers)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:07.865502Z",
     "start_time": "2018-03-04T09:17:07.805873Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data argument\n",
    "\"\"\"\n",
    "def transform_train_DA1(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def transform_train_DA2(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "    \n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "    \n",
    "\n",
    "random_clip_rate = 0.3\n",
    "def transform_train_DA3(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "#                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "#                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "        \n",
    "    if random.random() > random_clip_rate:\n",
    "        im = im.clip(0, 1)\n",
    "    _aug = image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                   std=np.array([0.2023, 0.1994, 0.2010]),)\n",
    "    im = _aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 data aurgument: mixup\n",
    "1. mixup define\n",
    "2. mixup visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 mixup: define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:08.798831Z",
     "start_time": "2018-03-04T09:17:08.782292Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mixup(x1, y1, x2, y2, alpha, num_class):\n",
    "    y1 = nd.one_hot(y1, num_class)\n",
    "    y2 = nd.one_hot(y2, num_class)\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    x = lam * x1 + (1 - lam) * x2\n",
    "    y = lam * y1 + (1 - lam) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 mixup: visulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:10.339270Z",
     "start_time": "2018-03-04T09:17:09.453635Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /root/.mxnet/datasets/cifar10/cifar-10-binary.tar.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/cifar10/cifar-10-binary.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "from time import time\n",
    "batch_size = 32\n",
    "transform_train = _transform_test#transform_train_DA1\n",
    "train_data, test_data = data_loader(batch_size, transform_train)\n",
    "mixup_alpha = 1\n",
    "\n",
    "# for x1, y1 in train_data:\n",
    "#     for x2, y2 in mixup_train_data:\n",
    "#         data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "for x, y in train_data:\n",
    "    l = x.shape[0] / 2\n",
    "    data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:10.820933Z",
     "start_time": "2018-03-04T09:17:10.340377Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cifar10_utils import show_images\n",
    "%matplotlib inline\n",
    "mean=np.array([0.4914, 0.4822, 0.4465])\n",
    "std=np.array([0.2023, 0.1994, 0.2010])\n",
    "images = data[:10].transpose((0, 2, 3, 1)).asnumpy()\n",
    "images = images * std + mean\n",
    "images = images.transpose((0, 3, 1, 2)) * 255\n",
    "show_images(images)\n",
    "#show_images(data[:9], rgb_mean=mean*255, std=std*255)\n",
    "\n",
    "print [(i, l) for i, l in enumerate(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])]\n",
    "print label[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 mixup: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:11.623628Z",
     "start_time": "2018-03-04T09:17:11.593888Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "mixup_test = False\n",
    "\n",
    "if mixup_test:\n",
    "    net = ResNet164_v2(10)\n",
    "    net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    loss_f = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "    num_epochs = 1\n",
    "    learning_rate = 0.1\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    cur_time = time()\n",
    "    iters = 0\n",
    "    \"\"\"\n",
    "    data loader first time run will cost about 3x time than after run.\n",
    "    \"\"\"\n",
    "    for x1, y1 in train_data:\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time\n",
    "        iters += 1\n",
    "    print \"cost time:\", time() - cur_time\n",
    "    print\n",
    "    cur_time = time()\n",
    "\n",
    "    iters = 0\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'momentum': 0.9, 'wd': 1e-4})\n",
    "    for x, y in train_data:\n",
    "        l = x.shape[0] / 2\n",
    "        data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label.as_in_context(ctx))\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "            print iters, time() - cur_time, nd.mean(loss).asscalar()\n",
    "        iters += 1\n",
    "    print iters, time() - cur_time\n",
    "\n",
    "    # load data one by one batch\n",
    "    # iters = 0\n",
    "    # for x1, y1 in train_data:\n",
    "    #     for x2, y2 in mixup_train_data:\n",
    "    #         data, label = mixup(x1, y1, x2[:x1.shape[0]], y2[:y1.shape[0]], mixup_alpha, 10)\n",
    "    #         break\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time\n",
    "    #     iters += 1\n",
    "    # print \"cost time:\", time() - cur_time\n",
    "    # print\n",
    "    # cur_time = time()\n",
    "\n",
    "    # zip will load all datas and then iterate them, too cost memory, will drop speed when memory over.\n",
    "    # iters = 0\n",
    "    # for (x1, y1), (x2, y2) in zip(train_data, mixup_train_data):\n",
    "    #     data, label = mixup(x1, y1, x2, y2, mixup_alpha, 10)\n",
    "    #     if iters % 100 == 0:\n",
    "    #         print iters, time() - cur_time#, nd.mean(loss).asscalar()\n",
    "    #     iters += 1\n",
    "    # print time() - cur_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:18.636515Z",
     "start_time": "2018-03-04T09:17:18.539997Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def abs_mean(W):\n",
    "    return nd.mean(nd.abs(W)).asscalar()\n",
    "\n",
    "def in_list(e, l):\n",
    "    for i in l:\n",
    "        if i == e:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, \n",
    "          lr_decay, wd, ctx, w_key, output_file=None, verbose=False, loss_f=gluon.loss.SoftmaxCrossEntropyLoss(), \n",
    "          use_mixup=False, mixup_alpha=0.2):\n",
    "    def train_batch(data, label, i):\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        _loss = nd.mean(loss).asscalar()\n",
    "        if not use_mixup:\n",
    "            _acc = utils.accuracy(output, label)\n",
    "        else:\n",
    "            _acc = None\n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            print \" # iter\", i,\n",
    "            print \"loss %.5f\" % _loss, \n",
    "            if not use_mixup: print \"acc %.5f\" % _acc,\n",
    "            print \"w (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.data()),\n",
    "            print \") g (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.grad()),\n",
    "            print \")\"\n",
    "        return _loss, _acc\n",
    "            \n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        stdout = sys.stdout\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = output_file\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    if verbose:\n",
    "        print \" #\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "    \n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if in_list(epoch, lr_period):\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            \n",
    "        if not use_mixup:\n",
    "            for data, label in train_data:\n",
    "                _loss, _acc = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                train_acc += _acc\n",
    "                i += 1\n",
    "        else:\n",
    "            for x, y in train_data:\n",
    "                l = x.shape[0] / 2\n",
    "                data, label = mixup(x[:l], y[:l], x[l:2*l], y[l:2*l], mixup_alpha, 10)\n",
    "                _loss, _ = train_batch(data, label, i)\n",
    "                train_loss += _loss\n",
    "                i += 1\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "    if output_file != stdout:\n",
    "        sys.stdout = stdout\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. get net and do EXP\n",
    "```\n",
    "I want to find out the reason that resnet18_v2 get fail baseline in CIFAR10_train2_* script, design follow exp.\n",
    "\n",
    "1. 5.1 train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs.\n",
    "2. 5.2 train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18.\n",
    " there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 re-exp resnet18_v1\n",
    "train my resnet18 to repreduce baseline(0.93 acc), to make sure code is no bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T16:30:21.462308Z",
     "start_time": "2018-03-03T16:30:13.299911Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932408146965\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(10)\n",
    "net.load_params('../../models/res18_9', ctx=ctx)\n",
    "valid_acc = utils.evaluate_accuracy(test_data, net, ctx)\n",
    "print valid_acc\n",
    "def get_net(ctx):\n",
    "    num_outputs = 10\n",
    "    net = ResNet(num_outputs)\n",
    "    net.collect_params().initialize(init=init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "    return net  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T10:46:50.521728Z",
     "start_time": "2018-03-03T10:05:27.156785Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.80346, train_acc 0.3266, valid_acc 0.3598, Time 00:00:28,lr 0.1\n",
      "epoch 1, loss 1.28281, train_acc 0.5411, valid_acc 0.5778, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.01865, train_acc 0.6431, valid_acc 0.6533, Time 00:00:30,lr 0.1\n",
      "epoch 3, loss 0.90869, train_acc 0.6870, valid_acc 0.5823, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 0.85972, train_acc 0.7049, valid_acc 0.6255, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 0.82922, train_acc 0.7158, valid_acc 0.6503, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 0.81416, train_acc 0.7211, valid_acc 0.6621, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 0.79129, train_acc 0.7286, valid_acc 0.6731, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 0.79221, train_acc 0.7293, valid_acc 0.6960, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 0.77941, train_acc 0.7357, valid_acc 0.7085, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 0.77304, train_acc 0.7350, valid_acc 0.6773, Time 00:00:33,lr 0.1\n",
      "epoch 11, loss 0.77300, train_acc 0.7362, valid_acc 0.6708, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 0.77379, train_acc 0.7359, valid_acc 0.7297, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.76346, train_acc 0.7400, valid_acc 0.6792, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.75981, train_acc 0.7400, valid_acc 0.7052, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 0.76189, train_acc 0.7409, valid_acc 0.7065, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 0.75300, train_acc 0.7431, valid_acc 0.7030, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 0.74843, train_acc 0.7451, valid_acc 0.7334, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 0.75400, train_acc 0.7439, valid_acc 0.6873, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 0.74822, train_acc 0.7453, valid_acc 0.6994, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 0.74226, train_acc 0.7492, valid_acc 0.6627, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 0.74857, train_acc 0.7449, valid_acc 0.6691, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 0.73817, train_acc 0.7513, valid_acc 0.6690, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.74331, train_acc 0.7469, valid_acc 0.7008, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 0.74029, train_acc 0.7473, valid_acc 0.6862, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 0.73600, train_acc 0.7491, valid_acc 0.7335, Time 00:00:30,lr 0.1\n",
      "epoch 26, loss 0.73561, train_acc 0.7486, valid_acc 0.7344, Time 00:00:30,lr 0.1\n",
      "epoch 27, loss 0.73582, train_acc 0.7491, valid_acc 0.5638, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.73652, train_acc 0.7464, valid_acc 0.5980, Time 00:00:30,lr 0.1\n",
      "epoch 29, loss 0.73622, train_acc 0.7506, valid_acc 0.7386, Time 00:00:30,lr 0.1\n",
      "epoch 30, loss 0.73189, train_acc 0.7487, valid_acc 0.6339, Time 00:00:30,lr 0.1\n",
      "epoch 31, loss 0.73360, train_acc 0.7494, valid_acc 0.6679, Time 00:00:30,lr 0.1\n",
      "epoch 32, loss 0.73711, train_acc 0.7486, valid_acc 0.6363, Time 00:00:30,lr 0.1\n",
      "epoch 33, loss 0.73315, train_acc 0.7495, valid_acc 0.6735, Time 00:00:30,lr 0.1\n",
      "epoch 34, loss 0.73185, train_acc 0.7496, valid_acc 0.7033, Time 00:00:30,lr 0.1\n",
      "epoch 35, loss 0.73066, train_acc 0.7499, valid_acc 0.7409, Time 00:00:30,lr 0.1\n",
      "epoch 36, loss 0.73286, train_acc 0.7492, valid_acc 0.6494, Time 00:00:30,lr 0.1\n",
      "epoch 37, loss 0.72791, train_acc 0.7502, valid_acc 0.6779, Time 00:00:30,lr 0.1\n",
      "epoch 38, loss 0.73055, train_acc 0.7519, valid_acc 0.6367, Time 00:00:30,lr 0.1\n",
      "epoch 39, loss 0.73307, train_acc 0.7487, valid_acc 0.7078, Time 00:00:30,lr 0.1\n",
      "epoch 40, loss 0.72583, train_acc 0.7522, valid_acc 0.6997, Time 00:00:30,lr 0.1\n",
      "epoch 41, loss 0.73065, train_acc 0.7513, valid_acc 0.6822, Time 00:00:30,lr 0.1\n",
      "epoch 42, loss 0.72384, train_acc 0.7533, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 43, loss 0.73071, train_acc 0.7507, valid_acc 0.6380, Time 00:00:30,lr 0.1\n",
      "epoch 44, loss 0.73206, train_acc 0.7511, valid_acc 0.6253, Time 00:00:30,lr 0.1\n",
      "epoch 45, loss 0.72313, train_acc 0.7533, valid_acc 0.6761, Time 00:00:30,lr 0.1\n",
      "epoch 46, loss 0.72663, train_acc 0.7531, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.72931, train_acc 0.7500, valid_acc 0.6451, Time 00:00:30,lr 0.1\n",
      "epoch 48, loss 0.72035, train_acc 0.7536, valid_acc 0.6857, Time 00:00:30,lr 0.1\n",
      "epoch 49, loss 0.73334, train_acc 0.7483, valid_acc 0.6765, Time 00:00:30,lr 0.1\n",
      "epoch 50, loss 0.72158, train_acc 0.7515, valid_acc 0.7363, Time 00:00:30,lr 0.1\n",
      "epoch 51, loss 0.72392, train_acc 0.7522, valid_acc 0.6832, Time 00:00:30,lr 0.1\n",
      "epoch 52, loss 0.72609, train_acc 0.7512, valid_acc 0.7577, Time 00:00:30,lr 0.1\n",
      "epoch 53, loss 0.72894, train_acc 0.7522, valid_acc 0.7357, Time 00:00:30,lr 0.1\n",
      "epoch 54, loss 0.72575, train_acc 0.7526, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 55, loss 0.72233, train_acc 0.7526, valid_acc 0.6178, Time 00:00:30,lr 0.1\n",
      "epoch 56, loss 0.72252, train_acc 0.7532, valid_acc 0.7112, Time 00:00:30,lr 0.1\n",
      "epoch 57, loss 0.72007, train_acc 0.7541, valid_acc 0.7108, Time 00:00:30,lr 0.1\n",
      "epoch 58, loss 0.72001, train_acc 0.7533, valid_acc 0.6893, Time 00:00:30,lr 0.1\n",
      "epoch 59, loss 0.72442, train_acc 0.7541, valid_acc 0.7461, Time 00:00:30,lr 0.1\n",
      "epoch 60, loss 0.72184, train_acc 0.7506, valid_acc 0.6913, Time 00:00:30,lr 0.1\n",
      "epoch 61, loss 0.72317, train_acc 0.7547, valid_acc 0.6189, Time 00:00:30,lr 0.1\n",
      "epoch 62, loss 0.72607, train_acc 0.7537, valid_acc 0.7175, Time 00:00:30,lr 0.1\n",
      "epoch 63, loss 0.71946, train_acc 0.7564, valid_acc 0.5596, Time 00:00:30,lr 0.1\n",
      "epoch 64, loss 0.72751, train_acc 0.7509, valid_acc 0.7269, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.72550, train_acc 0.7516, valid_acc 0.6568, Time 00:00:30,lr 0.1\n",
      "epoch 66, loss 0.72351, train_acc 0.7542, valid_acc 0.6292, Time 00:00:30,lr 0.1\n",
      "epoch 67, loss 0.72099, train_acc 0.7540, valid_acc 0.7067, Time 00:00:30,lr 0.1\n",
      "epoch 68, loss 0.72052, train_acc 0.7541, valid_acc 0.6509, Time 00:00:30,lr 0.1\n",
      "epoch 69, loss 0.71841, train_acc 0.7530, valid_acc 0.6510, Time 00:00:30,lr 0.1\n",
      "epoch 70, loss 0.72424, train_acc 0.7510, valid_acc 0.6668, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.71802, train_acc 0.7542, valid_acc 0.6872, Time 00:00:30,lr 0.1\n",
      "epoch 72, loss 0.72042, train_acc 0.7525, valid_acc 0.7391, Time 00:00:30,lr 0.1\n",
      "epoch 73, loss 0.72243, train_acc 0.7534, valid_acc 0.6784, Time 00:00:30,lr 0.1\n",
      "epoch 74, loss 0.72777, train_acc 0.7511, valid_acc 0.6956, Time 00:00:30,lr 0.1\n",
      "epoch 75, loss 0.72433, train_acc 0.7536, valid_acc 0.7030, Time 00:00:30,lr 0.1\n",
      "epoch 76, loss 0.72253, train_acc 0.7537, valid_acc 0.6999, Time 00:00:30,lr 0.1\n",
      "epoch 77, loss 0.72566, train_acc 0.7514, valid_acc 0.6808, Time 00:00:30,lr 0.1\n",
      "epoch 78, loss 0.72288, train_acc 0.7538, valid_acc 0.7191, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 0.72042, train_acc 0.7558, valid_acc 0.6245, Time 00:00:30,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.748897Z",
     "start_time": "2018-03-03T16:30:22.024115Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.61285, train_acc 0.7915, valid_acc 0.7278, Time 00:00:31,lr 0.05\n",
      "epoch 1, loss 0.68711, train_acc 0.7663, valid_acc 0.6986, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.70354, train_acc 0.7601, valid_acc 0.7469, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.70402, train_acc 0.7604, valid_acc 0.6044, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.70819, train_acc 0.7573, valid_acc 0.7099, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.71752, train_acc 0.7569, valid_acc 0.7387, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.71241, train_acc 0.7581, valid_acc 0.7282, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.71346, train_acc 0.7582, valid_acc 0.7717, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.71035, train_acc 0.7555, valid_acc 0.7430, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.71255, train_acc 0.7570, valid_acc 0.7069, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.71683, train_acc 0.7539, valid_acc 0.5970, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.71341, train_acc 0.7572, valid_acc 0.7481, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.71346, train_acc 0.7568, valid_acc 0.7049, Time 00:00:38,lr 0.05\n",
      "epoch 13, loss 0.70515, train_acc 0.7603, valid_acc 0.7366, Time 00:00:40,lr 0.05\n",
      "epoch 14, loss 0.71341, train_acc 0.7570, valid_acc 0.6632, Time 00:00:35,lr 0.05\n",
      "epoch 15, loss 0.70901, train_acc 0.7593, valid_acc 0.6943, Time 00:00:39,lr 0.05\n",
      "epoch 16, loss 0.71456, train_acc 0.7563, valid_acc 0.6658, Time 00:00:41,lr 0.05\n",
      "epoch 17, loss 0.71572, train_acc 0.7575, valid_acc 0.6613, Time 00:00:34,lr 0.05\n",
      "epoch 18, loss 0.71535, train_acc 0.7565, valid_acc 0.6999, Time 00:00:37,lr 0.05\n",
      "epoch 19, loss 0.70597, train_acc 0.7611, valid_acc 0.7358, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.71559, train_acc 0.7546, valid_acc 0.6635, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.71545, train_acc 0.7573, valid_acc 0.6657, Time 00:00:32,lr 0.05\n",
      "epoch 22, loss 0.71076, train_acc 0.7595, valid_acc 0.7046, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.71619, train_acc 0.7562, valid_acc 0.6314, Time 00:00:33,lr 0.05\n",
      "epoch 24, loss 0.70736, train_acc 0.7596, valid_acc 0.7478, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.70959, train_acc 0.7582, valid_acc 0.6643, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.71528, train_acc 0.7543, valid_acc 0.6441, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.72031, train_acc 0.7560, valid_acc 0.7425, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.71745, train_acc 0.7578, valid_acc 0.7464, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.70926, train_acc 0.7598, valid_acc 0.6095, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.46802, train_acc 0.8419, valid_acc 0.8559, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.42038, train_acc 0.8549, valid_acc 0.8552, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.41659, train_acc 0.8567, valid_acc 0.8418, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.41601, train_acc 0.8581, valid_acc 0.8321, Time 00:00:33,lr 0.01\n",
      "epoch 34, loss 0.41853, train_acc 0.8580, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.42103, train_acc 0.8563, valid_acc 0.8627, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.41386, train_acc 0.8588, valid_acc 0.8500, Time 00:00:31,lr 0.01\n",
      "epoch 37, loss 0.41497, train_acc 0.8594, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.40632, train_acc 0.8621, valid_acc 0.8522, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.40439, train_acc 0.8627, valid_acc 0.8521, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.40516, train_acc 0.8633, valid_acc 0.8540, Time 00:00:31,lr 0.01\n",
      "epoch 41, loss 0.40254, train_acc 0.8627, valid_acc 0.8626, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.39611, train_acc 0.8632, valid_acc 0.8397, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.39748, train_acc 0.8639, valid_acc 0.8526, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.39310, train_acc 0.8676, valid_acc 0.8517, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.38917, train_acc 0.8680, valid_acc 0.8554, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.38661, train_acc 0.8681, valid_acc 0.8591, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.38413, train_acc 0.8689, valid_acc 0.8555, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.37997, train_acc 0.8711, valid_acc 0.8538, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.38270, train_acc 0.8692, valid_acc 0.8484, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.38268, train_acc 0.8702, valid_acc 0.8478, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.38004, train_acc 0.8712, valid_acc 0.8528, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.37946, train_acc 0.8712, valid_acc 0.8603, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.37720, train_acc 0.8728, valid_acc 0.8533, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.37159, train_acc 0.8733, valid_acc 0.8092, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.37223, train_acc 0.8734, valid_acc 0.8453, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.37370, train_acc 0.8742, valid_acc 0.8661, Time 00:00:31,lr 0.01\n",
      "epoch 57, loss 0.37211, train_acc 0.8736, valid_acc 0.8377, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.37039, train_acc 0.8741, valid_acc 0.8310, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.37287, train_acc 0.8743, valid_acc 0.8420, Time 00:00:31,lr 0.01\n",
      "epoch 60, loss 0.23431, train_acc 0.9229, valid_acc 0.9057, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.19372, train_acc 0.9352, valid_acc 0.9090, Time 00:00:31,lr 0.002\n",
      "epoch 62, loss 0.17911, train_acc 0.9395, valid_acc 0.9086, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.17003, train_acc 0.9431, valid_acc 0.9157, Time 00:00:31,lr 0.002\n",
      "epoch 64, loss 0.16376, train_acc 0.9459, valid_acc 0.9081, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.15820, train_acc 0.9475, valid_acc 0.9153, Time 00:00:32,lr 0.002\n",
      "epoch 66, loss 0.15374, train_acc 0.9485, valid_acc 0.9127, Time 00:00:32,lr 0.002\n",
      "epoch 67, loss 0.15401, train_acc 0.9480, valid_acc 0.9136, Time 00:00:31,lr 0.002\n",
      "epoch 68, loss 0.15006, train_acc 0.9496, valid_acc 0.9137, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.15038, train_acc 0.9497, valid_acc 0.9058, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.15246, train_acc 0.9476, valid_acc 0.9106, Time 00:00:31,lr 0.002\n",
      "epoch 71, loss 0.15303, train_acc 0.9474, valid_acc 0.9042, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.15150, train_acc 0.9477, valid_acc 0.9030, Time 00:00:31,lr 0.002\n",
      "epoch 73, loss 0.15295, train_acc 0.9472, valid_acc 0.9058, Time 00:00:32,lr 0.002\n",
      "epoch 74, loss 0.15067, train_acc 0.9496, valid_acc 0.9097, Time 00:00:31,lr 0.002\n",
      "epoch 75, loss 0.15562, train_acc 0.9473, valid_acc 0.9034, Time 00:00:31,lr 0.002\n",
      "epoch 76, loss 0.15555, train_acc 0.9475, valid_acc 0.9029, Time 00:00:31,lr 0.002\n",
      "epoch 77, loss 0.15436, train_acc 0.9484, valid_acc 0.9031, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.15387, train_acc 0.9485, valid_acc 0.9032, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.15457, train_acc 0.9481, valid_acc 0.9065, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.15198, train_acc 0.9484, valid_acc 0.9091, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.15804, train_acc 0.9467, valid_acc 0.9018, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.15374, train_acc 0.9477, valid_acc 0.9049, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.15341, train_acc 0.9491, valid_acc 0.9067, Time 00:00:32,lr 0.002\n",
      "epoch 84, loss 0.15361, train_acc 0.9499, valid_acc 0.9019, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.15058, train_acc 0.9491, valid_acc 0.8981, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.15706, train_acc 0.9464, valid_acc 0.9073, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.15415, train_acc 0.9483, valid_acc 0.9021, Time 00:00:31,lr 0.002\n",
      "epoch 88, loss 0.14994, train_acc 0.9494, valid_acc 0.8976, Time 00:00:32,lr 0.002\n",
      "epoch 89, loss 0.15133, train_acc 0.9490, valid_acc 0.8961, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.08904, train_acc 0.9723, valid_acc 0.9240, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.06755, train_acc 0.9796, valid_acc 0.9256, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.05880, train_acc 0.9830, valid_acc 0.9272, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.05345, train_acc 0.9850, valid_acc 0.9282, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.05022, train_acc 0.9862, valid_acc 0.9306, Time 00:00:31,lr 0.0004\n",
      "epoch 95, loss 0.04724, train_acc 0.9864, valid_acc 0.9297, Time 00:00:31,lr 0.0004\n",
      "epoch 96, loss 0.04417, train_acc 0.9878, valid_acc 0.9303, Time 00:00:31,lr 0.0004\n",
      "epoch 97, loss 0.04230, train_acc 0.9886, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.04097, train_acc 0.9887, valid_acc 0.9272, Time 00:00:32,lr 0.0004\n",
      "epoch 99, loss 0.03953, train_acc 0.9899, valid_acc 0.9291, Time 00:00:31,lr 0.0004\n",
      "epoch 100, loss 0.03685, train_acc 0.9899, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.03604, train_acc 0.9903, valid_acc 0.9277, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.03277, train_acc 0.9917, valid_acc 0.9261, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.03316, train_acc 0.9913, valid_acc 0.9253, Time 00:00:32,lr 0.0004\n",
      "epoch 104, loss 0.03211, train_acc 0.9920, valid_acc 0.9288, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.03214, train_acc 0.9913, valid_acc 0.9277, Time 00:00:32,lr 0.0004\n",
      "epoch 106, loss 0.03095, train_acc 0.9922, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 107, loss 0.03007, train_acc 0.9922, valid_acc 0.9275, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.02834, train_acc 0.9926, valid_acc 0.9271, Time 00:00:31,lr 0.0004\n",
      "epoch 109, loss 0.03008, train_acc 0.9920, valid_acc 0.9286, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.02855, train_acc 0.9928, valid_acc 0.9279, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.02935, train_acc 0.9926, valid_acc 0.9269, Time 00:00:32,lr 0.0004\n",
      "epoch 112, loss 0.02706, train_acc 0.9931, valid_acc 0.9291, Time 00:00:32,lr 0.0004\n",
      "epoch 113, loss 0.02690, train_acc 0.9933, valid_acc 0.9265, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.02585, train_acc 0.9932, valid_acc 0.9263, Time 00:00:31,lr 0.0004\n",
      "epoch 115, loss 0.02478, train_acc 0.9938, valid_acc 0.9280, Time 00:00:31,lr 0.0004\n",
      "epoch 116, loss 0.02669, train_acc 0.9930, valid_acc 0.9264, Time 00:00:31,lr 0.0004\n",
      "epoch 117, loss 0.02483, train_acc 0.9937, valid_acc 0.9267, Time 00:00:31,lr 0.0004\n",
      "epoch 118, loss 0.02543, train_acc 0.9936, valid_acc 0.9281, Time 00:00:31,lr 0.0004\n",
      "epoch 119, loss 0.02483, train_acc 0.9939, valid_acc 0.9300, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_me_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 use gluon renset18_v1\n",
    "train gluon resnet18 use same hyper-param, get failed baseline(0.85 acc), so i thought the reason may cause by gluon resnet18 and my resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:09.871178Z",
     "start_time": "2018-03-04T08:31:09.864612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and add must in name_scope, or may got name error\n",
    "def get_resnet18_v1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features)\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T01:41:50.461216Z",
     "start_time": "2018-03-04T01:08:08.533086Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.51019, train_acc 0.2129, valid_acc 0.3261, Time 00:00:27,lr 0.1\n",
      "epoch 1, loss 1.75673, train_acc 0.3417, valid_acc 0.4151, Time 00:00:25,lr 0.1\n",
      "epoch 2, loss 1.60616, train_acc 0.4121, valid_acc 0.4824, Time 00:00:24,lr 0.1\n",
      "epoch 3, loss 1.49376, train_acc 0.4615, valid_acc 0.4921, Time 00:00:24,lr 0.1\n",
      "epoch 4, loss 1.42743, train_acc 0.4928, valid_acc 0.4964, Time 00:00:25,lr 0.1\n",
      "epoch 5, loss 1.37983, train_acc 0.5144, valid_acc 0.5435, Time 00:00:24,lr 0.1\n",
      "epoch 6, loss 1.36243, train_acc 0.5234, valid_acc 0.5876, Time 00:00:25,lr 0.1\n",
      "epoch 7, loss 1.33671, train_acc 0.5313, valid_acc 0.5255, Time 00:00:25,lr 0.1\n",
      "epoch 8, loss 1.31173, train_acc 0.5461, valid_acc 0.5368, Time 00:00:24,lr 0.1\n",
      "epoch 9, loss 1.27490, train_acc 0.5587, valid_acc 0.5911, Time 00:00:24,lr 0.1\n",
      "epoch 10, loss 1.27032, train_acc 0.5597, valid_acc 0.6087, Time 00:00:28,lr 0.1\n",
      "epoch 11, loss 1.25436, train_acc 0.5670, valid_acc 0.5886, Time 00:00:24,lr 0.1\n",
      "epoch 12, loss 1.24745, train_acc 0.5690, valid_acc 0.5414, Time 00:00:27,lr 0.1\n",
      "epoch 13, loss 1.23173, train_acc 0.5756, valid_acc 0.5944, Time 00:00:25,lr 0.1\n",
      "epoch 14, loss 1.22901, train_acc 0.5782, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 15, loss 1.22526, train_acc 0.5812, valid_acc 0.5576, Time 00:00:24,lr 0.1\n",
      "epoch 16, loss 1.22470, train_acc 0.5828, valid_acc 0.5957, Time 00:00:24,lr 0.1\n",
      "epoch 17, loss 1.21181, train_acc 0.5865, valid_acc 0.6237, Time 00:00:25,lr 0.1\n",
      "epoch 18, loss 1.21252, train_acc 0.5851, valid_acc 0.5971, Time 00:00:25,lr 0.1\n",
      "epoch 19, loss 1.20350, train_acc 0.5899, valid_acc 0.6076, Time 00:00:25,lr 0.1\n",
      "epoch 20, loss 1.20077, train_acc 0.5904, valid_acc 0.6148, Time 00:00:25,lr 0.1\n",
      "epoch 21, loss 1.19665, train_acc 0.5925, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 22, loss 1.19718, train_acc 0.5942, valid_acc 0.5745, Time 00:00:25,lr 0.1\n",
      "epoch 23, loss 1.19424, train_acc 0.5912, valid_acc 0.5711, Time 00:00:25,lr 0.1\n",
      "epoch 24, loss 1.19114, train_acc 0.5953, valid_acc 0.6069, Time 00:00:25,lr 0.1\n",
      "epoch 25, loss 1.19080, train_acc 0.5965, valid_acc 0.6262, Time 00:00:25,lr 0.1\n",
      "epoch 26, loss 1.19219, train_acc 0.5964, valid_acc 0.6112, Time 00:00:25,lr 0.1\n",
      "epoch 27, loss 1.18217, train_acc 0.5991, valid_acc 0.5783, Time 00:00:25,lr 0.1\n",
      "epoch 28, loss 1.19244, train_acc 0.5922, valid_acc 0.6090, Time 00:00:25,lr 0.1\n",
      "epoch 29, loss 1.18442, train_acc 0.5964, valid_acc 0.6002, Time 00:00:27,lr 0.1\n",
      "epoch 30, loss 1.18418, train_acc 0.5984, valid_acc 0.5866, Time 00:00:25,lr 0.1\n",
      "epoch 31, loss 1.18398, train_acc 0.5969, valid_acc 0.5779, Time 00:00:25,lr 0.1\n",
      "epoch 32, loss 1.18966, train_acc 0.5938, valid_acc 0.6059, Time 00:00:25,lr 0.1\n",
      "epoch 33, loss 1.19080, train_acc 0.5951, valid_acc 0.6048, Time 00:00:25,lr 0.1\n",
      "epoch 34, loss 1.18026, train_acc 0.5991, valid_acc 0.6034, Time 00:00:25,lr 0.1\n",
      "epoch 35, loss 1.18109, train_acc 0.5999, valid_acc 0.6194, Time 00:00:25,lr 0.1\n",
      "epoch 36, loss 1.18156, train_acc 0.6008, valid_acc 0.5972, Time 00:00:25,lr 0.1\n",
      "epoch 37, loss 1.19249, train_acc 0.5947, valid_acc 0.5651, Time 00:00:25,lr 0.1\n",
      "epoch 38, loss 1.18308, train_acc 0.5992, valid_acc 0.6097, Time 00:00:25,lr 0.1\n",
      "epoch 39, loss 1.18060, train_acc 0.6001, valid_acc 0.6363, Time 00:00:25,lr 0.1\n",
      "epoch 40, loss 1.17950, train_acc 0.6004, valid_acc 0.6394, Time 00:00:25,lr 0.1\n",
      "epoch 41, loss 1.18485, train_acc 0.5964, valid_acc 0.6099, Time 00:00:25,lr 0.1\n",
      "epoch 42, loss 1.18305, train_acc 0.5999, valid_acc 0.6338, Time 00:00:25,lr 0.1\n",
      "epoch 43, loss 1.18409, train_acc 0.5985, valid_acc 0.6278, Time 00:00:24,lr 0.1\n",
      "epoch 44, loss 1.17604, train_acc 0.5990, valid_acc 0.6171, Time 00:00:25,lr 0.1\n",
      "epoch 45, loss 1.18259, train_acc 0.5979, valid_acc 0.6266, Time 00:00:25,lr 0.1\n",
      "epoch 46, loss 1.17980, train_acc 0.6023, valid_acc 0.5782, Time 00:00:25,lr 0.1\n",
      "epoch 47, loss 1.18290, train_acc 0.5980, valid_acc 0.6226, Time 00:00:25,lr 0.1\n",
      "epoch 48, loss 1.17870, train_acc 0.5994, valid_acc 0.5915, Time 00:00:25,lr 0.1\n",
      "epoch 49, loss 1.17969, train_acc 0.5983, valid_acc 0.5877, Time 00:00:24,lr 0.1\n",
      "epoch 50, loss 1.18743, train_acc 0.5986, valid_acc 0.6399, Time 00:00:25,lr 0.1\n",
      "epoch 51, loss 1.17115, train_acc 0.6031, valid_acc 0.5693, Time 00:00:25,lr 0.1\n",
      "epoch 52, loss 1.16321, train_acc 0.6042, valid_acc 0.6509, Time 00:00:24,lr 0.1\n",
      "epoch 53, loss 1.17745, train_acc 0.6004, valid_acc 0.6119, Time 00:00:25,lr 0.1\n",
      "epoch 54, loss 1.18013, train_acc 0.5981, valid_acc 0.5628, Time 00:00:25,lr 0.1\n",
      "epoch 55, loss 1.17807, train_acc 0.5997, valid_acc 0.5863, Time 00:00:26,lr 0.1\n",
      "epoch 56, loss 1.18230, train_acc 0.6013, valid_acc 0.6077, Time 00:00:26,lr 0.1\n",
      "epoch 57, loss 1.17381, train_acc 0.6016, valid_acc 0.6339, Time 00:00:24,lr 0.1\n",
      "epoch 58, loss 1.17931, train_acc 0.5979, valid_acc 0.6215, Time 00:00:24,lr 0.1\n",
      "epoch 59, loss 1.17566, train_acc 0.6000, valid_acc 0.5958, Time 00:00:24,lr 0.1\n",
      "epoch 60, loss 1.16982, train_acc 0.6028, valid_acc 0.6310, Time 00:00:25,lr 0.1\n",
      "epoch 61, loss 1.17248, train_acc 0.6016, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 62, loss 1.17132, train_acc 0.6028, valid_acc 0.6392, Time 00:00:24,lr 0.1\n",
      "epoch 63, loss 1.16930, train_acc 0.6059, valid_acc 0.5884, Time 00:00:24,lr 0.1\n",
      "epoch 64, loss 1.17535, train_acc 0.6018, valid_acc 0.6257, Time 00:00:24,lr 0.1\n",
      "epoch 65, loss 1.16613, train_acc 0.6031, valid_acc 0.6155, Time 00:00:24,lr 0.1\n",
      "epoch 66, loss 1.17103, train_acc 0.6037, valid_acc 0.5942, Time 00:00:24,lr 0.1\n",
      "epoch 67, loss 1.17368, train_acc 0.6013, valid_acc 0.6163, Time 00:00:25,lr 0.1\n",
      "epoch 68, loss 1.17817, train_acc 0.5984, valid_acc 0.5712, Time 00:00:27,lr 0.1\n",
      "epoch 69, loss 1.18750, train_acc 0.5986, valid_acc 0.6246, Time 00:00:24,lr 0.1\n",
      "epoch 70, loss 1.17631, train_acc 0.6010, valid_acc 0.6244, Time 00:00:24,lr 0.1\n",
      "epoch 71, loss 1.18138, train_acc 0.5989, valid_acc 0.5719, Time 00:00:24,lr 0.1\n",
      "epoch 72, loss 1.17125, train_acc 0.6019, valid_acc 0.6373, Time 00:00:24,lr 0.1\n",
      "epoch 73, loss 1.17880, train_acc 0.6007, valid_acc 0.6064, Time 00:00:24,lr 0.1\n",
      "epoch 74, loss 1.17069, train_acc 0.6043, valid_acc 0.6049, Time 00:00:24,lr 0.1\n",
      "epoch 75, loss 1.17566, train_acc 0.6009, valid_acc 0.5390, Time 00:00:24,lr 0.1\n",
      "epoch 76, loss 1.17658, train_acc 0.5984, valid_acc 0.6027, Time 00:00:24,lr 0.1\n",
      "epoch 77, loss 1.17818, train_acc 0.5994, valid_acc 0.6432, Time 00:00:24,lr 0.1\n",
      "epoch 78, loss 1.18056, train_acc 0.6011, valid_acc 0.6201, Time 00:00:24,lr 0.1\n",
      "epoch 79, loss 1.18418, train_acc 0.5995, valid_acc 0.6297, Time 00:00:24,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T02:39:40.682812Z",
     "start_time": "2018-03-04T01:49:21.279182Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.03273, train_acc 0.6475, valid_acc 0.6599, Time 00:00:24,lr 0.05\n",
      "epoch 1, loss 1.10147, train_acc 0.6258, valid_acc 0.6101, Time 00:00:24,lr 0.05\n",
      "epoch 2, loss 1.10971, train_acc 0.6221, valid_acc 0.6303, Time 00:00:24,lr 0.05\n",
      "epoch 3, loss 1.11335, train_acc 0.6201, valid_acc 0.6158, Time 00:00:24,lr 0.05\n",
      "epoch 4, loss 1.11669, train_acc 0.6186, valid_acc 0.6162, Time 00:00:24,lr 0.05\n",
      "epoch 5, loss 1.10626, train_acc 0.6214, valid_acc 0.6486, Time 00:00:24,lr 0.05\n",
      "epoch 6, loss 1.11353, train_acc 0.6192, valid_acc 0.6361, Time 00:00:24,lr 0.05\n",
      "epoch 7, loss 1.10059, train_acc 0.6249, valid_acc 0.6364, Time 00:00:24,lr 0.05\n",
      "epoch 8, loss 1.11230, train_acc 0.6210, valid_acc 0.6317, Time 00:00:24,lr 0.05\n",
      "epoch 9, loss 1.10704, train_acc 0.6200, valid_acc 0.5400, Time 00:00:24,lr 0.05\n",
      "epoch 10, loss 1.10685, train_acc 0.6223, valid_acc 0.6251, Time 00:00:24,lr 0.05\n",
      "epoch 11, loss 1.11229, train_acc 0.6209, valid_acc 0.6055, Time 00:00:24,lr 0.05\n",
      "epoch 12, loss 1.11007, train_acc 0.6232, valid_acc 0.6257, Time 00:00:25,lr 0.05\n",
      "epoch 13, loss 1.10097, train_acc 0.6244, valid_acc 0.6220, Time 00:00:25,lr 0.05\n",
      "epoch 14, loss 1.10608, train_acc 0.6263, valid_acc 0.5887, Time 00:00:24,lr 0.05\n",
      "epoch 15, loss 1.10504, train_acc 0.6224, valid_acc 0.6416, Time 00:00:24,lr 0.05\n",
      "epoch 16, loss 1.10434, train_acc 0.6223, valid_acc 0.6064, Time 00:00:24,lr 0.05\n",
      "epoch 17, loss 1.10511, train_acc 0.6221, valid_acc 0.6365, Time 00:00:24,lr 0.05\n",
      "epoch 18, loss 1.10933, train_acc 0.6206, valid_acc 0.6311, Time 00:00:24,lr 0.05\n",
      "epoch 19, loss 1.10533, train_acc 0.6211, valid_acc 0.6559, Time 00:00:25,lr 0.05\n",
      "epoch 20, loss 1.10336, train_acc 0.6231, valid_acc 0.6529, Time 00:00:24,lr 0.05\n",
      "epoch 21, loss 1.11074, train_acc 0.6202, valid_acc 0.6060, Time 00:00:25,lr 0.05\n",
      "epoch 22, loss 1.10799, train_acc 0.6211, valid_acc 0.6211, Time 00:00:24,lr 0.05\n",
      "epoch 23, loss 1.11143, train_acc 0.6213, valid_acc 0.6449, Time 00:00:24,lr 0.05\n",
      "epoch 24, loss 1.10708, train_acc 0.6227, valid_acc 0.6420, Time 00:00:24,lr 0.05\n",
      "epoch 25, loss 1.11583, train_acc 0.6182, valid_acc 0.6106, Time 00:00:24,lr 0.05\n",
      "epoch 26, loss 1.10713, train_acc 0.6237, valid_acc 0.6260, Time 00:00:25,lr 0.05\n",
      "epoch 27, loss 1.10516, train_acc 0.6226, valid_acc 0.6302, Time 00:00:24,lr 0.05\n",
      "epoch 28, loss 1.10940, train_acc 0.6245, valid_acc 0.6110, Time 00:00:24,lr 0.05\n",
      "epoch 29, loss 1.10750, train_acc 0.6217, valid_acc 0.5529, Time 00:00:24,lr 0.05\n",
      "epoch 30, loss 0.82673, train_acc 0.7164, valid_acc 0.7511, Time 00:00:25,lr 0.01\n",
      "epoch 31, loss 0.76458, train_acc 0.7374, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 32, loss 0.75552, train_acc 0.7419, valid_acc 0.7292, Time 00:00:25,lr 0.01\n",
      "epoch 33, loss 0.75799, train_acc 0.7418, valid_acc 0.7546, Time 00:00:25,lr 0.01\n",
      "epoch 34, loss 0.75636, train_acc 0.7434, valid_acc 0.7697, Time 00:00:24,lr 0.01\n",
      "epoch 35, loss 0.74936, train_acc 0.7466, valid_acc 0.7569, Time 00:00:24,lr 0.01\n",
      "epoch 36, loss 0.74819, train_acc 0.7459, valid_acc 0.7532, Time 00:00:25,lr 0.01\n",
      "epoch 37, loss 0.74411, train_acc 0.7466, valid_acc 0.7663, Time 00:00:24,lr 0.01\n",
      "epoch 38, loss 0.74887, train_acc 0.7453, valid_acc 0.7722, Time 00:00:24,lr 0.01\n",
      "epoch 39, loss 0.73962, train_acc 0.7479, valid_acc 0.7629, Time 00:00:24,lr 0.01\n",
      "epoch 40, loss 0.73451, train_acc 0.7482, valid_acc 0.7582, Time 00:00:24,lr 0.01\n",
      "epoch 41, loss 0.72975, train_acc 0.7514, valid_acc 0.7482, Time 00:00:24,lr 0.01\n",
      "epoch 42, loss 0.72403, train_acc 0.7536, valid_acc 0.7622, Time 00:00:24,lr 0.01\n",
      "epoch 43, loss 0.72834, train_acc 0.7540, valid_acc 0.7566, Time 00:00:24,lr 0.01\n",
      "epoch 44, loss 0.72512, train_acc 0.7522, valid_acc 0.7666, Time 00:00:24,lr 0.01\n",
      "epoch 45, loss 0.71810, train_acc 0.7576, valid_acc 0.7462, Time 00:00:25,lr 0.01\n",
      "epoch 46, loss 0.72037, train_acc 0.7547, valid_acc 0.7660, Time 00:00:25,lr 0.01\n",
      "epoch 47, loss 0.71576, train_acc 0.7564, valid_acc 0.7687, Time 00:00:24,lr 0.01\n",
      "epoch 48, loss 0.71955, train_acc 0.7534, valid_acc 0.7430, Time 00:00:24,lr 0.01\n",
      "epoch 49, loss 0.71184, train_acc 0.7587, valid_acc 0.7466, Time 00:00:24,lr 0.01\n",
      "epoch 50, loss 0.71714, train_acc 0.7561, valid_acc 0.7544, Time 00:00:24,lr 0.01\n",
      "epoch 51, loss 0.71158, train_acc 0.7576, valid_acc 0.7566, Time 00:00:25,lr 0.01\n",
      "epoch 52, loss 0.70936, train_acc 0.7582, valid_acc 0.7613, Time 00:00:24,lr 0.01\n",
      "epoch 53, loss 0.70673, train_acc 0.7591, valid_acc 0.7554, Time 00:00:24,lr 0.01\n",
      "epoch 54, loss 0.70670, train_acc 0.7597, valid_acc 0.7607, Time 00:00:24,lr 0.01\n",
      "epoch 55, loss 0.70951, train_acc 0.7566, valid_acc 0.7609, Time 00:00:24,lr 0.01\n",
      "epoch 56, loss 0.70382, train_acc 0.7595, valid_acc 0.7573, Time 00:00:24,lr 0.01\n",
      "epoch 57, loss 0.70531, train_acc 0.7601, valid_acc 0.7356, Time 00:00:24,lr 0.01\n",
      "epoch 58, loss 0.70683, train_acc 0.7595, valid_acc 0.7572, Time 00:00:25,lr 0.01\n",
      "epoch 59, loss 0.70069, train_acc 0.7610, valid_acc 0.7556, Time 00:00:24,lr 0.01\n",
      "epoch 60, loss 0.54787, train_acc 0.8137, valid_acc 0.8178, Time 00:00:24,lr 0.002\n",
      "epoch 61, loss 0.49978, train_acc 0.8285, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 62, loss 0.48376, train_acc 0.8345, valid_acc 0.8283, Time 00:00:24,lr 0.002\n",
      "epoch 63, loss 0.47104, train_acc 0.8396, valid_acc 0.8270, Time 00:00:24,lr 0.002\n",
      "epoch 64, loss 0.46549, train_acc 0.8415, valid_acc 0.8376, Time 00:00:25,lr 0.002\n",
      "epoch 65, loss 0.45923, train_acc 0.8430, valid_acc 0.8264, Time 00:00:25,lr 0.002\n",
      "epoch 66, loss 0.45508, train_acc 0.8428, valid_acc 0.8332, Time 00:00:24,lr 0.002\n",
      "epoch 67, loss 0.45490, train_acc 0.8441, valid_acc 0.8351, Time 00:00:24,lr 0.002\n",
      "epoch 68, loss 0.45410, train_acc 0.8435, valid_acc 0.8346, Time 00:00:24,lr 0.002\n",
      "epoch 69, loss 0.44802, train_acc 0.8458, valid_acc 0.8378, Time 00:00:24,lr 0.002\n",
      "epoch 70, loss 0.44962, train_acc 0.8464, valid_acc 0.8339, Time 00:00:24,lr 0.002\n",
      "epoch 71, loss 0.45422, train_acc 0.8447, valid_acc 0.8261, Time 00:00:24,lr 0.002\n",
      "epoch 72, loss 0.44599, train_acc 0.8474, valid_acc 0.8287, Time 00:00:25,lr 0.002\n",
      "epoch 73, loss 0.44833, train_acc 0.8458, valid_acc 0.8317, Time 00:00:24,lr 0.002\n",
      "epoch 74, loss 0.44645, train_acc 0.8466, valid_acc 0.8322, Time 00:00:24,lr 0.002\n",
      "epoch 75, loss 0.45153, train_acc 0.8453, valid_acc 0.8240, Time 00:00:24,lr 0.002\n",
      "epoch 76, loss 0.44996, train_acc 0.8456, valid_acc 0.8233, Time 00:00:24,lr 0.002\n",
      "epoch 77, loss 0.44440, train_acc 0.8473, valid_acc 0.8235, Time 00:00:24,lr 0.002\n",
      "epoch 78, loss 0.44680, train_acc 0.8452, valid_acc 0.8229, Time 00:00:24,lr 0.002\n",
      "epoch 79, loss 0.44781, train_acc 0.8466, valid_acc 0.8210, Time 00:00:24,lr 0.002\n",
      "epoch 80, loss 0.44728, train_acc 0.8452, valid_acc 0.8299, Time 00:00:24,lr 0.002\n",
      "epoch 81, loss 0.44830, train_acc 0.8462, valid_acc 0.8278, Time 00:00:24,lr 0.002\n",
      "epoch 82, loss 0.44525, train_acc 0.8456, valid_acc 0.8325, Time 00:00:24,lr 0.002\n",
      "epoch 83, loss 0.44259, train_acc 0.8477, valid_acc 0.8257, Time 00:00:24,lr 0.002\n",
      "epoch 84, loss 0.44439, train_acc 0.8464, valid_acc 0.8292, Time 00:00:24,lr 0.002\n",
      "epoch 85, loss 0.44217, train_acc 0.8476, valid_acc 0.8280, Time 00:00:24,lr 0.002\n",
      "epoch 86, loss 0.44183, train_acc 0.8487, valid_acc 0.8294, Time 00:00:24,lr 0.002\n",
      "epoch 87, loss 0.43943, train_acc 0.8472, valid_acc 0.8248, Time 00:00:24,lr 0.002\n",
      "epoch 88, loss 0.43877, train_acc 0.8485, valid_acc 0.8164, Time 00:00:26,lr 0.002\n",
      "epoch 89, loss 0.43657, train_acc 0.8498, valid_acc 0.8276, Time 00:00:25,lr 0.002\n",
      "epoch 90, loss 0.35472, train_acc 0.8789, valid_acc 0.8585, Time 00:00:25,lr 0.0004\n",
      "epoch 91, loss 0.32633, train_acc 0.8876, valid_acc 0.8528, Time 00:00:25,lr 0.0004\n",
      "epoch 92, loss 0.31385, train_acc 0.8927, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 93, loss 0.30717, train_acc 0.8945, valid_acc 0.8557, Time 00:00:25,lr 0.0004\n",
      "epoch 94, loss 0.30039, train_acc 0.8969, valid_acc 0.8598, Time 00:00:27,lr 0.0004\n",
      "epoch 95, loss 0.29545, train_acc 0.8995, valid_acc 0.8572, Time 00:00:25,lr 0.0004\n",
      "epoch 96, loss 0.29173, train_acc 0.8990, valid_acc 0.8566, Time 00:00:26,lr 0.0004\n",
      "epoch 97, loss 0.28834, train_acc 0.8994, valid_acc 0.8571, Time 00:00:28,lr 0.0004\n",
      "epoch 98, loss 0.28529, train_acc 0.9006, valid_acc 0.8560, Time 00:00:27,lr 0.0004\n",
      "epoch 99, loss 0.28147, train_acc 0.9024, valid_acc 0.8564, Time 00:00:25,lr 0.0004\n",
      "epoch 100, loss 0.27405, train_acc 0.9057, valid_acc 0.8599, Time 00:00:26,lr 0.0004\n",
      "epoch 101, loss 0.27207, train_acc 0.9066, valid_acc 0.8568, Time 00:00:25,lr 0.0004\n",
      "epoch 102, loss 0.27203, train_acc 0.9067, valid_acc 0.8584, Time 00:00:25,lr 0.0004\n",
      "epoch 103, loss 0.26894, train_acc 0.9070, valid_acc 0.8580, Time 00:00:27,lr 0.0004\n",
      "epoch 104, loss 0.26925, train_acc 0.9077, valid_acc 0.8566, Time 00:00:25,lr 0.0004\n",
      "epoch 105, loss 0.26636, train_acc 0.9088, valid_acc 0.8558, Time 00:00:24,lr 0.0004\n",
      "epoch 106, loss 0.26520, train_acc 0.9085, valid_acc 0.8559, Time 00:00:26,lr 0.0004\n",
      "epoch 107, loss 0.26192, train_acc 0.9094, valid_acc 0.8562, Time 00:00:27,lr 0.0004\n",
      "epoch 108, loss 0.26339, train_acc 0.9091, valid_acc 0.8547, Time 00:00:25,lr 0.0004\n",
      "epoch 109, loss 0.26113, train_acc 0.9096, valid_acc 0.8568, Time 00:00:24,lr 0.0004\n",
      "epoch 110, loss 0.25569, train_acc 0.9102, valid_acc 0.8575, Time 00:00:24,lr 0.0004\n",
      "epoch 111, loss 0.25539, train_acc 0.9117, valid_acc 0.8565, Time 00:00:25,lr 0.0004\n",
      "epoch 112, loss 0.25576, train_acc 0.9120, valid_acc 0.8555, Time 00:00:24,lr 0.0004\n",
      "epoch 113, loss 0.25426, train_acc 0.9129, valid_acc 0.8522, Time 00:00:24,lr 0.0004\n",
      "epoch 114, loss 0.25399, train_acc 0.9125, valid_acc 0.8544, Time 00:00:24,lr 0.0004\n",
      "epoch 115, loss 0.25090, train_acc 0.9127, valid_acc 0.8521, Time 00:00:24,lr 0.0004\n",
      "epoch 116, loss 0.24849, train_acc 0.9130, valid_acc 0.8567, Time 00:00:24,lr 0.0004\n",
      "epoch 117, loss 0.25278, train_acc 0.9111, valid_acc 0.8523, Time 00:00:24,lr 0.0004\n",
      "epoch 118, loss 0.25027, train_acc 0.9130, valid_acc 0.8512, Time 00:00:24,lr 0.0004\n",
      "epoch 119, loss 0.24702, train_acc 0.9142, valid_acc 0.8548, Time 00:00:24,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 gluon resnet18 vs my resnet18\n",
    "```\n",
    "there are three diff between them(I thought the diff may cause gluon resnet is design for imagenet and my gluon is design for CIFAR10):<br/>\n",
    "    a. first conv use diff kernel size and padding size:conv(k=7,p=3,s=1) vs conv(k=3,p=1,s=1)\n",
    "    b. after first block, gluon resnet18 use maxpool and my resnet not use\n",
    "    c. gluon resnet18 use 4 * 2 block with channel size [64, 128, 256, 512] and three downsample, and my resnet18 use 3 * 3 block with channle size [32, 64, 128] and two downsample, so last layer use global avg pooling.\n",
    "    it may said delay pooling is useful, pool to early is not good.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:40:01.865661Z",
     "start_time": "2018-03-04T08:40:01.754397Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (net): HybridSequential(\n",
      "    (0): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "    (2): Activation(relu)\n",
      "    (3): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (6): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (7): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (8): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (9): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv3): Conv2D(None -> 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (10): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (11): Residual(\n",
      "      (bn1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (bn2): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n",
      "      (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (12): AvgPool2D(size=(8, 8), stride=(8, 8), padding=(0, 0), ceil_mode=False)\n",
      "    (13): Flatten\n",
      "    (14): Dense(None -> 10, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net1 = ResNet(10)\n",
    "'''\n",
    "block1: conv(32, 3, 1, 1) + BN + relu\n",
    "block2: Residual_block(32) * 3\n",
    "block3: Resisual_block(64) * 3\n",
    "block4: Residual_block(128) * 3\n",
    "block5: AvgPool(8) + Flatten + Dense(10)\n",
    "\n",
    "actually it has 19 conv + 1 fc, some code name it as resnet20 (each Residual_block has 2 conv or 2 + 1 conv(two line))\n",
    "for cifar10 dataset, most code use this arch (https://github.com/yinglang/pytorch-cifar-models).\n",
    "'''\n",
    "net2 = get_resnet18_v1()\n",
    "'''\n",
    "block1: conv(64, 7, 3, 2) + BN + relu + MaxPool(3, 1, 2)\n",
    "block2: Residual_block(64) * 2\n",
    "block3: Resisual_block(128) * 2\n",
    "block4: Residual_block(256) * 2\n",
    "block5: Residual_block(256) * 2\n",
    "block6: GlobalAvgPool() + Dense(10) \n",
    "\n",
    "has 1 + (2 + 2 + 2 + 2) * 2 = 15 conv + 1 fc, name it as resnet18\n",
    "'''\n",
    "print net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:31:21.510639Z",
     "start_time": "2018-03-04T08:31:21.405189Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): HybridSequential(\n",
      "    (0): Conv2D(3 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n",
      "    (4): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(64 -> 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): HybridSequential(\n",
      "      (0): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlockV1(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  )\n",
      "  (1): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 gluon resnet18 vs my resnet 18: first conv is conv(k=7,p=3,s=2) vs conv(k=3,p=1,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T03:26:13.432236Z",
     "start_time": "2018-03-04T03:26:13.422231Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T04:08:38.921689Z",
     "start_time": "2018-03-04T03:26:16.924298Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.20239, train_acc 0.2669, valid_acc 0.3902, Time 00:00:32,lr 0.1\n",
      "epoch 1, loss 1.65517, train_acc 0.3912, valid_acc 0.4528, Time 00:00:33,lr 0.1\n",
      "epoch 2, loss 1.47576, train_acc 0.4642, valid_acc 0.4337, Time 00:00:33,lr 0.1\n",
      "epoch 3, loss 1.34809, train_acc 0.5176, valid_acc 0.5469, Time 00:00:30,lr 0.1\n",
      "epoch 4, loss 1.25687, train_acc 0.5574, valid_acc 0.5480, Time 00:00:30,lr 0.1\n",
      "epoch 5, loss 1.20031, train_acc 0.5783, valid_acc 0.5777, Time 00:00:30,lr 0.1\n",
      "epoch 6, loss 1.15576, train_acc 0.5958, valid_acc 0.6329, Time 00:00:30,lr 0.1\n",
      "epoch 7, loss 1.11209, train_acc 0.6100, valid_acc 0.6637, Time 00:00:30,lr 0.1\n",
      "epoch 8, loss 1.06140, train_acc 0.6298, valid_acc 0.5927, Time 00:00:30,lr 0.1\n",
      "epoch 9, loss 1.02432, train_acc 0.6445, valid_acc 0.6795, Time 00:00:30,lr 0.1\n",
      "epoch 10, loss 1.00051, train_acc 0.6551, valid_acc 0.6433, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 0.98188, train_acc 0.6637, valid_acc 0.6974, Time 00:00:30,lr 0.1\n",
      "epoch 12, loss 0.96989, train_acc 0.6664, valid_acc 0.6623, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 0.95365, train_acc 0.6740, valid_acc 0.6860, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 0.95321, train_acc 0.6733, valid_acc 0.6337, Time 00:00:30,lr 0.1\n",
      "epoch 15, loss 0.94458, train_acc 0.6777, valid_acc 0.6930, Time 00:00:30,lr 0.1\n",
      "epoch 16, loss 0.93070, train_acc 0.6824, valid_acc 0.7069, Time 00:00:30,lr 0.1\n",
      "epoch 17, loss 0.93221, train_acc 0.6827, valid_acc 0.6382, Time 00:00:30,lr 0.1\n",
      "epoch 18, loss 0.92776, train_acc 0.6840, valid_acc 0.7080, Time 00:00:30,lr 0.1\n",
      "epoch 19, loss 0.92143, train_acc 0.6869, valid_acc 0.6862, Time 00:00:30,lr 0.1\n",
      "epoch 20, loss 0.92236, train_acc 0.6877, valid_acc 0.6391, Time 00:00:30,lr 0.1\n",
      "epoch 21, loss 0.92049, train_acc 0.6863, valid_acc 0.6934, Time 00:00:30,lr 0.1\n",
      "epoch 22, loss 0.91650, train_acc 0.6887, valid_acc 0.6944, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 0.91073, train_acc 0.6888, valid_acc 0.6668, Time 00:00:30,lr 0.1\n",
      "epoch 24, loss 0.91163, train_acc 0.6908, valid_acc 0.6598, Time 00:00:30,lr 0.1\n",
      "epoch 25, loss 0.90582, train_acc 0.6902, valid_acc 0.6914, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 0.90684, train_acc 0.6901, valid_acc 0.6054, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 0.90768, train_acc 0.6901, valid_acc 0.6733, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 0.90371, train_acc 0.6923, valid_acc 0.6340, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 0.90607, train_acc 0.6906, valid_acc 0.6989, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 0.90220, train_acc 0.6922, valid_acc 0.5081, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 0.90443, train_acc 0.6917, valid_acc 0.6913, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 0.89974, train_acc 0.6936, valid_acc 0.6244, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 0.89704, train_acc 0.6960, valid_acc 0.7255, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 0.90115, train_acc 0.6939, valid_acc 0.6980, Time 00:00:32,lr 0.1\n",
      "epoch 35, loss 0.89469, train_acc 0.6961, valid_acc 0.6947, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 0.89557, train_acc 0.6974, valid_acc 0.6494, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 0.89949, train_acc 0.6951, valid_acc 0.7107, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 0.89622, train_acc 0.6963, valid_acc 0.5968, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 0.89347, train_acc 0.6955, valid_acc 0.6612, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 0.89192, train_acc 0.6984, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 0.89004, train_acc 0.6967, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 0.88865, train_acc 0.6992, valid_acc 0.6024, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 0.90129, train_acc 0.6922, valid_acc 0.6781, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 0.89063, train_acc 0.6980, valid_acc 0.6661, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 0.89031, train_acc 0.6983, valid_acc 0.6445, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 0.88648, train_acc 0.6996, valid_acc 0.7091, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 0.88954, train_acc 0.6960, valid_acc 0.7163, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 0.89009, train_acc 0.6982, valid_acc 0.7027, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 0.88359, train_acc 0.7001, valid_acc 0.6971, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 0.89414, train_acc 0.6958, valid_acc 0.6732, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 0.88717, train_acc 0.6997, valid_acc 0.7025, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 0.89172, train_acc 0.6956, valid_acc 0.6487, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.89213, train_acc 0.6993, valid_acc 0.6922, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 0.88663, train_acc 0.6969, valid_acc 0.7236, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 0.88679, train_acc 0.6988, valid_acc 0.6824, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 0.88335, train_acc 0.6991, valid_acc 0.6743, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 0.88368, train_acc 0.7007, valid_acc 0.6904, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 0.88596, train_acc 0.6983, valid_acc 0.7060, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 0.88443, train_acc 0.6993, valid_acc 0.6722, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 0.88799, train_acc 0.6990, valid_acc 0.6508, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 0.88132, train_acc 0.7016, valid_acc 0.6703, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 0.87931, train_acc 0.7012, valid_acc 0.6996, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 0.88957, train_acc 0.6987, valid_acc 0.7010, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 0.88078, train_acc 0.7020, valid_acc 0.6454, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 0.88306, train_acc 0.7007, valid_acc 0.6412, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 0.88245, train_acc 0.7001, valid_acc 0.6534, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 0.87989, train_acc 0.7020, valid_acc 0.6466, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 0.89072, train_acc 0.6977, valid_acc 0.7111, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 0.88862, train_acc 0.6984, valid_acc 0.6285, Time 00:00:35,lr 0.1\n",
      "epoch 70, loss 0.87356, train_acc 0.7045, valid_acc 0.5976, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 0.88543, train_acc 0.6981, valid_acc 0.7188, Time 00:00:38,lr 0.1\n",
      "epoch 72, loss 0.88926, train_acc 0.6963, valid_acc 0.7050, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 0.87890, train_acc 0.7006, valid_acc 0.6988, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 0.88213, train_acc 0.6988, valid_acc 0.6710, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 0.89143, train_acc 0.6980, valid_acc 0.6485, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 0.88201, train_acc 0.6990, valid_acc 0.6410, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.87847, train_acc 0.7010, valid_acc 0.6491, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.88841, train_acc 0.6974, valid_acc 0.6903, Time 00:00:35,lr 0.1\n",
      "epoch 79, loss 0.88179, train_acc 0.7004, valid_acc 0.6880, Time 00:00:34,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T05:14:50.043433Z",
     "start_time": "2018-03-04T04:10:53.460734Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.77403, train_acc 0.7361, valid_acc 0.6271, Time 00:00:28,lr 0.05\n",
      "epoch 1, loss 0.84400, train_acc 0.7124, valid_acc 0.6732, Time 00:00:30,lr 0.05\n",
      "epoch 2, loss 0.86126, train_acc 0.7060, valid_acc 0.7009, Time 00:00:30,lr 0.05\n",
      "epoch 3, loss 0.86573, train_acc 0.7063, valid_acc 0.6787, Time 00:00:30,lr 0.05\n",
      "epoch 4, loss 0.85972, train_acc 0.7096, valid_acc 0.6623, Time 00:00:30,lr 0.05\n",
      "epoch 5, loss 0.86412, train_acc 0.7051, valid_acc 0.6821, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.86789, train_acc 0.7018, valid_acc 0.6822, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.86329, train_acc 0.7054, valid_acc 0.6411, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.87379, train_acc 0.7028, valid_acc 0.7026, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.86685, train_acc 0.7050, valid_acc 0.6352, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.86982, train_acc 0.7020, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.86871, train_acc 0.7042, valid_acc 0.5869, Time 00:00:31,lr 0.05\n",
      "epoch 12, loss 0.86923, train_acc 0.7037, valid_acc 0.6458, Time 00:00:31,lr 0.05\n",
      "epoch 13, loss 0.86655, train_acc 0.7048, valid_acc 0.6600, Time 00:00:31,lr 0.05\n",
      "epoch 14, loss 0.86976, train_acc 0.7038, valid_acc 0.7020, Time 00:00:31,lr 0.05\n",
      "epoch 15, loss 0.85832, train_acc 0.7083, valid_acc 0.6998, Time 00:00:31,lr 0.05\n",
      "epoch 16, loss 0.86706, train_acc 0.7046, valid_acc 0.6969, Time 00:00:31,lr 0.05\n",
      "epoch 17, loss 0.86458, train_acc 0.7044, valid_acc 0.6914, Time 00:00:31,lr 0.05\n",
      "epoch 18, loss 0.86462, train_acc 0.7072, valid_acc 0.7017, Time 00:00:31,lr 0.05\n",
      "epoch 19, loss 0.86589, train_acc 0.7067, valid_acc 0.7007, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.87160, train_acc 0.7042, valid_acc 0.7153, Time 00:00:31,lr 0.05\n",
      "epoch 21, loss 0.86976, train_acc 0.7027, valid_acc 0.6829, Time 00:00:31,lr 0.05\n",
      "epoch 22, loss 0.85921, train_acc 0.7075, valid_acc 0.7036, Time 00:00:31,lr 0.05\n",
      "epoch 23, loss 0.86768, train_acc 0.7058, valid_acc 0.7057, Time 00:00:31,lr 0.05\n",
      "epoch 24, loss 0.86215, train_acc 0.7067, valid_acc 0.7118, Time 00:00:31,lr 0.05\n",
      "epoch 25, loss 0.86632, train_acc 0.7029, valid_acc 0.7084, Time 00:00:31,lr 0.05\n",
      "epoch 26, loss 0.87295, train_acc 0.7031, valid_acc 0.6990, Time 00:00:31,lr 0.05\n",
      "epoch 27, loss 0.87456, train_acc 0.7033, valid_acc 0.6865, Time 00:00:31,lr 0.05\n",
      "epoch 28, loss 0.86871, train_acc 0.7044, valid_acc 0.6970, Time 00:00:31,lr 0.05\n",
      "epoch 29, loss 0.86998, train_acc 0.7056, valid_acc 0.6560, Time 00:00:31,lr 0.05\n",
      "epoch 30, loss 0.59346, train_acc 0.7979, valid_acc 0.8265, Time 00:00:31,lr 0.01\n",
      "epoch 31, loss 0.54326, train_acc 0.8155, valid_acc 0.8227, Time 00:00:31,lr 0.01\n",
      "epoch 32, loss 0.53949, train_acc 0.8170, valid_acc 0.8028, Time 00:00:31,lr 0.01\n",
      "epoch 33, loss 0.54204, train_acc 0.8157, valid_acc 0.8154, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.53898, train_acc 0.8156, valid_acc 0.8274, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.54012, train_acc 0.8164, valid_acc 0.8242, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.54208, train_acc 0.8150, valid_acc 0.8189, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.53263, train_acc 0.8171, valid_acc 0.8306, Time 00:00:35,lr 0.01\n",
      "epoch 38, loss 0.53156, train_acc 0.8190, valid_acc 0.8076, Time 00:00:33,lr 0.01\n",
      "epoch 39, loss 0.52680, train_acc 0.8204, valid_acc 0.8144, Time 00:00:33,lr 0.01\n",
      "epoch 40, loss 0.52358, train_acc 0.8228, valid_acc 0.7965, Time 00:00:33,lr 0.01\n",
      "epoch 41, loss 0.52257, train_acc 0.8211, valid_acc 0.8152, Time 00:00:34,lr 0.01\n",
      "epoch 42, loss 0.52220, train_acc 0.8232, valid_acc 0.8375, Time 00:00:33,lr 0.01\n",
      "epoch 43, loss 0.51315, train_acc 0.8264, valid_acc 0.8182, Time 00:00:31,lr 0.01\n",
      "epoch 44, loss 0.51323, train_acc 0.8268, valid_acc 0.8144, Time 00:00:31,lr 0.01\n",
      "epoch 45, loss 0.51435, train_acc 0.8240, valid_acc 0.8080, Time 00:00:31,lr 0.01\n",
      "epoch 46, loss 0.51004, train_acc 0.8262, valid_acc 0.8189, Time 00:00:31,lr 0.01\n",
      "epoch 47, loss 0.50912, train_acc 0.8285, valid_acc 0.8285, Time 00:00:31,lr 0.01\n",
      "epoch 48, loss 0.51028, train_acc 0.8259, valid_acc 0.8171, Time 00:00:31,lr 0.01\n",
      "epoch 49, loss 0.50687, train_acc 0.8291, valid_acc 0.8301, Time 00:00:31,lr 0.01\n",
      "epoch 50, loss 0.50086, train_acc 0.8297, valid_acc 0.8241, Time 00:00:31,lr 0.01\n",
      "epoch 51, loss 0.49983, train_acc 0.8290, valid_acc 0.8161, Time 00:00:31,lr 0.01\n",
      "epoch 52, loss 0.50052, train_acc 0.8278, valid_acc 0.8267, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.49692, train_acc 0.8309, valid_acc 0.8120, Time 00:00:31,lr 0.01\n",
      "epoch 54, loss 0.50006, train_acc 0.8279, valid_acc 0.8300, Time 00:00:31,lr 0.01\n",
      "epoch 55, loss 0.49913, train_acc 0.8295, valid_acc 0.7949, Time 00:00:31,lr 0.01\n",
      "epoch 56, loss 0.49605, train_acc 0.8320, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.49692, train_acc 0.8300, valid_acc 0.8328, Time 00:00:31,lr 0.01\n",
      "epoch 58, loss 0.49206, train_acc 0.8337, valid_acc 0.8387, Time 00:00:31,lr 0.01\n",
      "epoch 59, loss 0.49343, train_acc 0.8311, valid_acc 0.8150, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.34705, train_acc 0.8820, valid_acc 0.8809, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.30127, train_acc 0.8983, valid_acc 0.8846, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.28316, train_acc 0.9037, valid_acc 0.8870, Time 00:00:31,lr 0.002\n",
      "epoch 63, loss 0.27436, train_acc 0.9062, valid_acc 0.8866, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.26887, train_acc 0.9072, valid_acc 0.8916, Time 00:00:31,lr 0.002\n",
      "epoch 65, loss 0.26500, train_acc 0.9096, valid_acc 0.8894, Time 00:00:31,lr 0.002\n",
      "epoch 66, loss 0.26247, train_acc 0.9101, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 67, loss 0.26025, train_acc 0.9112, valid_acc 0.8920, Time 00:00:32,lr 0.002\n",
      "epoch 68, loss 0.25301, train_acc 0.9135, valid_acc 0.8888, Time 00:00:32,lr 0.002\n",
      "epoch 69, loss 0.25118, train_acc 0.9139, valid_acc 0.8884, Time 00:00:31,lr 0.002\n",
      "epoch 70, loss 0.25089, train_acc 0.9137, valid_acc 0.8884, Time 00:00:32,lr 0.002\n",
      "epoch 71, loss 0.25031, train_acc 0.9147, valid_acc 0.8795, Time 00:00:31,lr 0.002\n",
      "epoch 72, loss 0.25265, train_acc 0.9130, valid_acc 0.8856, Time 00:00:33,lr 0.002\n",
      "epoch 73, loss 0.25271, train_acc 0.9129, valid_acc 0.8883, Time 00:00:31,lr 0.002\n",
      "epoch 74, loss 0.25271, train_acc 0.9128, valid_acc 0.8770, Time 00:00:32,lr 0.002\n",
      "epoch 75, loss 0.25109, train_acc 0.9137, valid_acc 0.8796, Time 00:00:32,lr 0.002\n",
      "epoch 76, loss 0.25106, train_acc 0.9132, valid_acc 0.8824, Time 00:00:32,lr 0.002\n",
      "epoch 77, loss 0.25249, train_acc 0.9136, valid_acc 0.8863, Time 00:00:32,lr 0.002\n",
      "epoch 78, loss 0.25310, train_acc 0.9126, valid_acc 0.8882, Time 00:00:31,lr 0.002\n",
      "epoch 79, loss 0.25188, train_acc 0.9132, valid_acc 0.8809, Time 00:00:31,lr 0.002\n",
      "epoch 80, loss 0.25087, train_acc 0.9143, valid_acc 0.8808, Time 00:00:31,lr 0.002\n",
      "epoch 81, loss 0.25043, train_acc 0.9133, valid_acc 0.8659, Time 00:00:31,lr 0.002\n",
      "epoch 82, loss 0.25028, train_acc 0.9126, valid_acc 0.8830, Time 00:00:31,lr 0.002\n",
      "epoch 83, loss 0.24958, train_acc 0.9140, valid_acc 0.8806, Time 00:00:31,lr 0.002\n",
      "epoch 84, loss 0.25462, train_acc 0.9128, valid_acc 0.8817, Time 00:00:31,lr 0.002\n",
      "epoch 85, loss 0.25416, train_acc 0.9109, valid_acc 0.8797, Time 00:00:31,lr 0.002\n",
      "epoch 86, loss 0.24963, train_acc 0.9134, valid_acc 0.8856, Time 00:00:31,lr 0.002\n",
      "epoch 87, loss 0.25088, train_acc 0.9143, valid_acc 0.8799, Time 00:00:32,lr 0.002\n",
      "epoch 88, loss 0.24692, train_acc 0.9147, valid_acc 0.8874, Time 00:00:31,lr 0.002\n",
      "epoch 89, loss 0.24720, train_acc 0.9146, valid_acc 0.8826, Time 00:00:31,lr 0.002\n",
      "epoch 90, loss 0.17336, train_acc 0.9407, valid_acc 0.9051, Time 00:00:31,lr 0.0004\n",
      "epoch 91, loss 0.14650, train_acc 0.9515, valid_acc 0.9075, Time 00:00:31,lr 0.0004\n",
      "epoch 92, loss 0.13297, train_acc 0.9564, valid_acc 0.9078, Time 00:00:31,lr 0.0004\n",
      "epoch 93, loss 0.12877, train_acc 0.9568, valid_acc 0.9069, Time 00:00:31,lr 0.0004\n",
      "epoch 94, loss 0.12118, train_acc 0.9599, valid_acc 0.9086, Time 00:00:32,lr 0.0004\n",
      "epoch 95, loss 0.11531, train_acc 0.9619, valid_acc 0.9090, Time 00:00:32,lr 0.0004\n",
      "epoch 96, loss 0.11811, train_acc 0.9601, valid_acc 0.9075, Time 00:00:35,lr 0.0004\n",
      "epoch 97, loss 0.11135, train_acc 0.9634, valid_acc 0.9066, Time 00:00:31,lr 0.0004\n",
      "epoch 98, loss 0.10657, train_acc 0.9653, valid_acc 0.9044, Time 00:00:31,lr 0.0004\n",
      "epoch 99, loss 0.10713, train_acc 0.9637, valid_acc 0.9063, Time 00:00:31,lr 0.0004\n",
      "epoch 100, loss 0.10466, train_acc 0.9656, valid_acc 0.9050, Time 00:00:31,lr 0.0004\n",
      "epoch 101, loss 0.09925, train_acc 0.9674, valid_acc 0.9043, Time 00:00:31,lr 0.0004\n",
      "epoch 102, loss 0.09841, train_acc 0.9671, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 103, loss 0.09499, train_acc 0.9692, valid_acc 0.9091, Time 00:00:31,lr 0.0004\n",
      "epoch 104, loss 0.09518, train_acc 0.9685, valid_acc 0.9089, Time 00:00:31,lr 0.0004\n",
      "epoch 105, loss 0.09390, train_acc 0.9696, valid_acc 0.9105, Time 00:00:31,lr 0.0004\n",
      "epoch 106, loss 0.09006, train_acc 0.9702, valid_acc 0.9070, Time 00:00:32,lr 0.0004\n",
      "epoch 107, loss 0.09035, train_acc 0.9709, valid_acc 0.9081, Time 00:00:31,lr 0.0004\n",
      "epoch 108, loss 0.08887, train_acc 0.9702, valid_acc 0.9073, Time 00:00:33,lr 0.0004\n",
      "epoch 109, loss 0.08896, train_acc 0.9715, valid_acc 0.9060, Time 00:00:31,lr 0.0004\n",
      "epoch 110, loss 0.08627, train_acc 0.9710, valid_acc 0.9103, Time 00:00:31,lr 0.0004\n",
      "epoch 111, loss 0.08818, train_acc 0.9706, valid_acc 0.9070, Time 00:00:31,lr 0.0004\n",
      "epoch 112, loss 0.08570, train_acc 0.9718, valid_acc 0.9063, Time 00:00:33,lr 0.0004\n",
      "epoch 113, loss 0.08473, train_acc 0.9719, valid_acc 0.9086, Time 00:00:31,lr 0.0004\n",
      "epoch 114, loss 0.08530, train_acc 0.9715, valid_acc 0.9090, Time 00:00:33,lr 0.0004\n",
      "epoch 115, loss 0.08070, train_acc 0.9733, valid_acc 0.9075, Time 00:00:33,lr 0.0004\n",
      "epoch 116, loss 0.07957, train_acc 0.9737, valid_acc 0.9041, Time 00:00:32,lr 0.0004\n",
      "epoch 117, loss 0.07966, train_acc 0.9733, valid_acc 0.9027, Time 00:00:32,lr 0.0004\n",
      "epoch 118, loss 0.08058, train_acc 0.9734, valid_acc 0.9058, Time 00:00:32,lr 0.0004\n",
      "epoch 119, loss 0.08208, train_acc 0.9728, valid_acc 0.9074, Time 00:00:35,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3\", ctx=ctx)\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 I wanto find out acc impove may from cancel downsample, so use k=7 but not downsmaple and k=3 but downsample to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T09:17:41.147882Z",
     "start_time": "2018-03-04T09:17:41.140250Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k7p3s1(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=7, strides=1, padding=3, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:17:41.976Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.30376, train_acc 0.2260, valid_acc 0.3361, Time 00:00:31,lr 0.1\n",
      "epoch 1, loss 1.68781, train_acc 0.3729, valid_acc 0.4189, Time 00:00:35,lr 0.1\n",
      "epoch 2, loss 1.52445, train_acc 0.4432, valid_acc 0.5236, Time 00:00:32,lr 0.1\n",
      "epoch 3, loss 1.37861, train_acc 0.5034, valid_acc 0.5116, Time 00:00:32,lr 0.1\n",
      "epoch 4, loss 1.29522, train_acc 0.5379, valid_acc 0.5644, Time 00:00:32,lr 0.1\n",
      "epoch 5, loss 1.23774, train_acc 0.5634, valid_acc 0.5271, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.18491, train_acc 0.5845, valid_acc 0.6008, Time 00:00:32,lr 0.1\n",
      "epoch 7, loss 1.13172, train_acc 0.6043, valid_acc 0.6084, Time 00:00:32,lr 0.1\n",
      "epoch 8, loss 1.10778, train_acc 0.6163, valid_acc 0.6573, Time 00:00:33,lr 0.1\n",
      "epoch 9, loss 1.06816, train_acc 0.6311, valid_acc 0.6118, Time 00:00:32,lr 0.1\n",
      "epoch 10, loss 1.03542, train_acc 0.6430, valid_acc 0.6466, Time 00:00:32,lr 0.1\n",
      "epoch 11, loss 1.02805, train_acc 0.6471, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 12, loss 1.01376, train_acc 0.6525, valid_acc 0.6565, Time 00:00:33,lr 0.1\n",
      "epoch 13, loss 1.00736, train_acc 0.6535, valid_acc 0.6435, Time 00:00:32,lr 0.1\n",
      "epoch 14, loss 0.99958, train_acc 0.6599, valid_acc 0.6482, Time 00:00:32,lr 0.1\n",
      "epoch 15, loss 0.98714, train_acc 0.6617, valid_acc 0.6799, Time 00:00:32,lr 0.1\n",
      "epoch 16, loss 0.97678, train_acc 0.6662, valid_acc 0.6960, Time 00:00:32,lr 0.1\n",
      "epoch 17, loss 0.97524, train_acc 0.6663, valid_acc 0.6830, Time 00:00:32,lr 0.1\n",
      "epoch 18, loss 0.97347, train_acc 0.6669, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 19, loss 0.97192, train_acc 0.6669, valid_acc 0.6242, Time 00:00:32,lr 0.1\n",
      "epoch 20, loss 0.97201, train_acc 0.6679, valid_acc 0.6656, Time 00:00:32,lr 0.1\n",
      "epoch 21, loss 0.96170, train_acc 0.6740, valid_acc 0.6367, Time 00:00:33,lr 0.1\n",
      "epoch 22, loss 0.96810, train_acc 0.6708, valid_acc 0.6185, Time 00:00:32,lr 0.1\n",
      "epoch 23, loss 0.95803, train_acc 0.6731, valid_acc 0.6805, Time 00:00:33,lr 0.1\n",
      "epoch 24, loss 0.95777, train_acc 0.6729, valid_acc 0.6490, Time 00:00:32,lr 0.1\n",
      "epoch 25, loss 0.96024, train_acc 0.6753, valid_acc 0.6834, Time 00:00:32,lr 0.1\n",
      "epoch 26, loss 0.95364, train_acc 0.6740, valid_acc 0.6864, Time 00:00:32,lr 0.1\n",
      "epoch 27, loss 0.95158, train_acc 0.6774, valid_acc 0.6646, Time 00:00:32,lr 0.1\n",
      "epoch 28, loss 0.94868, train_acc 0.6751, valid_acc 0.6987, Time 00:00:32,lr 0.1\n",
      "epoch 29, loss 0.95404, train_acc 0.6749, valid_acc 0.6318, Time 00:00:32,lr 0.1\n",
      "epoch 30, loss 0.95379, train_acc 0.6741, valid_acc 0.6917, Time 00:00:32,lr 0.1\n",
      "epoch 31, loss 0.94801, train_acc 0.6744, valid_acc 0.6283, Time 00:00:32,lr 0.1\n",
      "epoch 32, loss 0.95098, train_acc 0.6751, valid_acc 0.6732, Time 00:00:33,lr 0.1\n",
      "epoch 33, loss 0.94359, train_acc 0.6791, valid_acc 0.6295, Time 00:00:34,lr 0.1\n",
      "epoch 34, loss 0.94709, train_acc 0.6774, valid_acc 0.6200, Time 00:00:33,lr 0.1\n",
      "epoch 35, loss 0.94835, train_acc 0.6774, valid_acc 0.6354, Time 00:00:32,lr 0.1\n",
      "epoch 36, loss 0.94460, train_acc 0.6784, valid_acc 0.6478, Time 00:00:32,lr 0.1\n",
      "epoch 37, loss 0.94238, train_acc 0.6794, valid_acc 0.6423, Time 00:00:32,lr 0.1\n",
      "epoch 38, loss 0.94366, train_acc 0.6784, valid_acc 0.5573, Time 00:00:32,lr 0.1\n",
      "epoch 39, loss 0.93595, train_acc 0.6804, valid_acc 0.6669, Time 00:00:32,lr 0.1\n",
      "epoch 40, loss 0.94138, train_acc 0.6788, valid_acc 0.6633, Time 00:00:32,lr 0.1\n",
      "epoch 41, loss 0.94039, train_acc 0.6797, valid_acc 0.6339, Time 00:00:34,lr 0.1\n",
      "epoch 42, loss 0.93921, train_acc 0.6797, valid_acc 0.6167, Time 00:00:32,lr 0.1\n",
      "epoch 43, loss 0.93964, train_acc 0.6790, valid_acc 0.6928, Time 00:00:32,lr 0.1\n",
      "epoch 44, loss 0.93891, train_acc 0.6801, valid_acc 0.6485, Time 00:00:32,lr 0.1\n",
      "epoch 45, loss 0.93846, train_acc 0.6814, valid_acc 0.5983, Time 00:00:32,lr 0.1\n",
      "epoch 46, loss 0.93314, train_acc 0.6822, valid_acc 0.6216, Time 00:00:32,lr 0.1\n",
      "epoch 47, loss 0.93256, train_acc 0.6823, valid_acc 0.6778, Time 00:00:32,lr 0.1\n",
      "epoch 48, loss 0.94286, train_acc 0.6781, valid_acc 0.6123, Time 00:00:32,lr 0.1\n",
      "epoch 49, loss 0.93333, train_acc 0.6848, valid_acc 0.6390, Time 00:00:32,lr 0.1\n",
      "epoch 50, loss 0.93153, train_acc 0.6829, valid_acc 0.6682, Time 00:00:32,lr 0.1\n",
      "epoch 51, loss 0.93787, train_acc 0.6817, valid_acc 0.7057, Time 00:00:32,lr 0.1\n",
      "epoch 52, loss 0.93450, train_acc 0.6823, valid_acc 0.6080, Time 00:00:32,lr 0.1\n",
      "epoch 53, loss 0.93795, train_acc 0.6822, valid_acc 0.6733, Time 00:00:32,lr 0.1\n",
      "epoch 54, loss 0.92976, train_acc 0.6836, valid_acc 0.6498, Time 00:00:32,lr 0.1\n",
      "epoch 55, loss 0.93671, train_acc 0.6794, valid_acc 0.6334, Time 00:00:32,lr 0.1\n",
      "epoch 56, loss 0.93371, train_acc 0.6807, valid_acc 0.6849, Time 00:00:32,lr 0.1\n",
      "epoch 57, loss 0.93283, train_acc 0.6830, valid_acc 0.6737, Time 00:00:32,lr 0.1\n",
      "epoch 58, loss 0.94008, train_acc 0.6813, valid_acc 0.6681, Time 00:00:32,lr 0.1\n",
      "epoch 59, loss 0.92572, train_acc 0.6860, valid_acc 0.6837, Time 00:00:32,lr 0.1\n",
      "epoch 60, loss 0.93572, train_acc 0.6810, valid_acc 0.6855, Time 00:00:32,lr 0.1\n",
      "epoch 61, loss 0.92939, train_acc 0.6856, valid_acc 0.6800, Time 00:00:32,lr 0.1\n",
      "epoch 62, loss 0.92851, train_acc 0.6848, valid_acc 0.6933, Time 00:00:32,lr 0.1\n",
      "epoch 63, loss 0.93826, train_acc 0.6798, valid_acc 0.5912, Time 00:00:32,lr 0.1\n",
      "epoch 64, loss 0.92919, train_acc 0.6859, valid_acc 0.6371, Time 00:00:32,lr 0.1\n",
      "epoch 65, loss 0.92681, train_acc 0.6829, valid_acc 0.6720, Time 00:00:32,lr 0.1\n",
      "epoch 66, loss 0.92689, train_acc 0.6861, valid_acc 0.6765, Time 00:00:32,lr 0.1\n",
      "epoch 67, loss 0.92941, train_acc 0.6836, valid_acc 0.6244, Time 00:00:32,lr 0.1\n",
      "epoch 68, loss 0.92887, train_acc 0.6831, valid_acc 0.6602, Time 00:00:32,lr 0.1\n",
      "epoch 69, loss 0.93088, train_acc 0.6826, valid_acc 0.6989, Time 00:00:32,lr 0.1\n",
      "epoch 70, loss 0.92295, train_acc 0.6847, valid_acc 0.7020, Time 00:00:32,lr 0.1\n",
      "epoch 71, loss 0.92583, train_acc 0.6863, valid_acc 0.6810, Time 00:00:32,lr 0.1\n",
      "epoch 72, loss 0.92959, train_acc 0.6839, valid_acc 0.6843, Time 00:00:32,lr 0.1\n",
      "epoch 73, loss 0.93152, train_acc 0.6827, valid_acc 0.6619, Time 00:00:32,lr 0.1\n",
      "epoch 74, loss 0.92909, train_acc 0.6839, valid_acc 0.6328, Time 00:00:32,lr 0.1\n",
      "epoch 75, loss 0.92891, train_acc 0.6839, valid_acc 0.6598, Time 00:00:32,lr 0.1\n",
      "epoch 76, loss 0.93157, train_acc 0.6843, valid_acc 0.6716, Time 00:00:32,lr 0.1\n",
      "epoch 77, loss 0.93235, train_acc 0.6823, valid_acc 0.6870, Time 00:00:32,lr 0.1\n",
      "epoch 78, loss 0.92986, train_acc 0.6865, valid_acc 0.6388, Time 00:00:32,lr 0.1\n",
      "epoch 79, loss 0.92761, train_acc 0.6849, valid_acc 0.6662, Time 00:00:32,lr 0.1\n",
      "epoch 0, loss 0.80896, train_acc 0.7271, valid_acc 0.6203, Time 00:00:30,lr 0.05\n",
      "epoch 1, loss 0.88513, train_acc 0.6974, valid_acc 0.6184, Time 00:00:32,lr 0.05\n",
      "epoch 2, loss 0.90256, train_acc 0.6931, valid_acc 0.7099, Time 00:00:32,lr 0.05\n",
      "epoch 3, loss 0.90922, train_acc 0.6893, valid_acc 0.6401, Time 00:00:32,lr 0.05\n",
      "epoch 4, loss 0.91353, train_acc 0.6878, valid_acc 0.6681, Time 00:00:32,lr 0.05\n",
      "epoch 5, loss 0.90272, train_acc 0.6925, valid_acc 0.6439, Time 00:00:32,lr 0.05\n",
      "epoch 6, loss 0.91182, train_acc 0.6895, valid_acc 0.7033, Time 00:00:32,lr 0.05\n",
      "epoch 7, loss 0.91077, train_acc 0.6885, valid_acc 0.6915, Time 00:00:32,lr 0.05\n",
      "epoch 8, loss 0.91324, train_acc 0.6878, valid_acc 0.6665, Time 00:00:32,lr 0.05\n",
      "epoch 9, loss 0.91369, train_acc 0.6868, valid_acc 0.6593, Time 00:00:32,lr 0.05\n",
      "epoch 10, loss 0.91246, train_acc 0.6879, valid_acc 0.6637, Time 00:00:32,lr 0.05\n",
      "epoch 11, loss 0.91111, train_acc 0.6916, valid_acc 0.7026, Time 00:00:32,lr 0.05\n",
      "epoch 12, loss 0.91954, train_acc 0.6874, valid_acc 0.6634, Time 00:00:32,lr 0.05\n",
      "epoch 13, loss 0.91728, train_acc 0.6863, valid_acc 0.6654, Time 00:00:32,lr 0.05\n",
      "epoch 14, loss 0.91430, train_acc 0.6878, valid_acc 0.6514, Time 00:00:32,lr 0.05\n",
      "epoch 15, loss 0.91699, train_acc 0.6876, valid_acc 0.6709, Time 00:00:32,lr 0.05\n",
      "epoch 16, loss 0.91586, train_acc 0.6865, valid_acc 0.6684, Time 00:00:32,lr 0.05\n",
      "epoch 17, loss 0.90909, train_acc 0.6914, valid_acc 0.6667, Time 00:00:32,lr 0.05\n",
      "epoch 18, loss 0.90777, train_acc 0.6873, valid_acc 0.6822, Time 00:00:32,lr 0.05\n",
      "epoch 19, loss 0.91364, train_acc 0.6880, valid_acc 0.6613, Time 00:00:32,lr 0.05\n",
      "epoch 20, loss 0.91116, train_acc 0.6876, valid_acc 0.6495, Time 00:00:32,lr 0.05\n",
      "epoch 21, loss 0.90711, train_acc 0.6921, valid_acc 0.6621, Time 00:00:32,lr 0.05\n",
      "epoch 22, loss 0.91377, train_acc 0.6874, valid_acc 0.7030, Time 00:00:32,lr 0.05\n",
      "epoch 23, loss 0.91145, train_acc 0.6884, valid_acc 0.6387, Time 00:00:32,lr 0.05\n",
      "epoch 24, loss 0.91180, train_acc 0.6891, valid_acc 0.7214, Time 00:00:32,lr 0.05\n",
      "epoch 25, loss 0.91215, train_acc 0.6880, valid_acc 0.6815, Time 00:00:32,lr 0.05\n",
      "epoch 26, loss 0.90945, train_acc 0.6892, valid_acc 0.6991, Time 00:00:32,lr 0.05\n",
      "epoch 27, loss 0.91134, train_acc 0.6903, valid_acc 0.6897, Time 00:00:32,lr 0.05\n",
      "epoch 28, loss 0.91023, train_acc 0.6889, valid_acc 0.6460, Time 00:00:32,lr 0.05\n",
      "epoch 29, loss 0.90890, train_acc 0.6892, valid_acc 0.6105, Time 00:00:32,lr 0.05\n",
      "epoch 30, loss 0.62314, train_acc 0.7879, valid_acc 0.8119, Time 00:00:32,lr 0.01\n",
      "epoch 31, loss 0.57048, train_acc 0.8056, valid_acc 0.8057, Time 00:00:32,lr 0.01\n",
      "epoch 32, loss 0.56539, train_acc 0.8086, valid_acc 0.8094, Time 00:00:32,lr 0.01\n",
      "epoch 33, loss 0.56050, train_acc 0.8097, valid_acc 0.8174, Time 00:00:32,lr 0.01\n",
      "epoch 34, loss 0.56481, train_acc 0.8090, valid_acc 0.7986, Time 00:00:32,lr 0.01\n",
      "epoch 35, loss 0.56655, train_acc 0.8061, valid_acc 0.8173, Time 00:00:32,lr 0.01\n",
      "epoch 36, loss 0.55780, train_acc 0.8100, valid_acc 0.7797, Time 00:00:32,lr 0.01\n",
      "epoch 37, loss 0.55701, train_acc 0.8100, valid_acc 0.8271, Time 00:00:32,lr 0.01\n",
      "epoch 38, loss 0.55273, train_acc 0.8132, valid_acc 0.8075, Time 00:00:32,lr 0.01\n",
      "epoch 39, loss 0.55390, train_acc 0.8106, valid_acc 0.8073, Time 00:00:32,lr 0.01\n",
      "epoch 40, loss 0.54441, train_acc 0.8154, valid_acc 0.8010, Time 00:00:32,lr 0.01\n",
      "epoch 41, loss 0.54516, train_acc 0.8139, valid_acc 0.8111, Time 00:00:32,lr 0.01\n",
      "epoch 42, loss 0.53969, train_acc 0.8148, valid_acc 0.8155, Time 00:00:32,lr 0.01\n",
      "epoch 43, loss 0.53975, train_acc 0.8157, valid_acc 0.8236, Time 00:00:32,lr 0.01\n",
      "epoch 44, loss 0.53627, train_acc 0.8176, valid_acc 0.8247, Time 00:00:32,lr 0.01\n",
      "epoch 45, loss 0.53035, train_acc 0.8196, valid_acc 0.8118, Time 00:00:32,lr 0.01\n",
      "epoch 46, loss 0.53219, train_acc 0.8190, valid_acc 0.8203, Time 00:00:32,lr 0.01\n",
      "epoch 47, loss 0.53024, train_acc 0.8185, valid_acc 0.8109, Time 00:00:32,lr 0.01\n",
      "epoch 48, loss 0.52706, train_acc 0.8210, valid_acc 0.8323, Time 00:00:32,lr 0.01\n",
      "epoch 49, loss 0.52530, train_acc 0.8220, valid_acc 0.7924, Time 00:00:32,lr 0.01\n",
      "epoch 50, loss 0.52793, train_acc 0.8215, valid_acc 0.8195, Time 00:00:32,lr 0.01\n",
      "epoch 51, loss 0.52560, train_acc 0.8216, valid_acc 0.8287, Time 00:00:32,lr 0.01\n",
      "epoch 52, loss 0.51781, train_acc 0.8240, valid_acc 0.8058, Time 00:00:32,lr 0.01\n",
      "epoch 53, loss 0.51830, train_acc 0.8228, valid_acc 0.8306, Time 00:00:32,lr 0.01\n",
      "epoch 54, loss 0.51420, train_acc 0.8254, valid_acc 0.8258, Time 00:00:32,lr 0.01\n",
      "epoch 55, loss 0.51919, train_acc 0.8242, valid_acc 0.8067, Time 00:00:32,lr 0.01\n",
      "epoch 56, loss 0.51772, train_acc 0.8231, valid_acc 0.8133, Time 00:00:32,lr 0.01\n",
      "epoch 57, loss 0.50818, train_acc 0.8279, valid_acc 0.8285, Time 00:00:32,lr 0.01\n",
      "epoch 58, loss 0.51014, train_acc 0.8252, valid_acc 0.8101, Time 00:00:32,lr 0.01\n",
      "epoch 59, loss 0.51187, train_acc 0.8270, valid_acc 0.8182, Time 00:00:32,lr 0.01\n",
      "epoch 60, loss 0.35304, train_acc 0.8818, valid_acc 0.8777, Time 00:00:32,lr 0.002\n",
      "epoch 61, loss 0.31041, train_acc 0.8950, valid_acc 0.8848, Time 00:00:32,lr 0.002\n",
      "epoch 62, loss 0.29029, train_acc 0.9032, valid_acc 0.8857, Time 00:00:32,lr 0.002\n",
      "epoch 63, loss 0.28051, train_acc 0.9045, valid_acc 0.8852, Time 00:00:32,lr 0.002\n",
      "epoch 64, loss 0.27056, train_acc 0.9079, valid_acc 0.8854, Time 00:00:32,lr 0.002\n",
      "epoch 65, loss 0.26793, train_acc 0.9099, valid_acc 0.8821, Time 00:00:33,lr 0.002\n",
      "epoch 66, loss 0.26292, train_acc 0.9110, valid_acc 0.8788, Time 00:00:35,lr 0.002\n",
      "epoch 67, loss 0.25597, train_acc 0.9123, valid_acc 0.8886, Time 00:00:33,lr 0.002\n",
      "epoch 68, loss 0.25621, train_acc 0.9120, valid_acc 0.8809, Time 00:00:34,lr 0.002\n",
      "epoch 69, loss 0.25495, train_acc 0.9139, valid_acc 0.8817, Time 00:00:34,lr 0.002\n",
      "epoch 70, loss 0.25461, train_acc 0.9128, valid_acc 0.8820, Time 00:00:35,lr 0.002\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k7p3s1()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k7p3s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:17:43.020Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_k3p1s2(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=2, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:17:44.325Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_k3p1s2()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_k3p1s2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 gluon resnet18 vs my resnet 18: use max pooling vs no max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:33:26.079468Z",
     "start_time": "2018-03-04T06:33:26.069002Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v1_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v1(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[1:3])\n",
    "        net.add(*resnet.features[4:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T06:31:03.467325Z",
     "start_time": "2018-03-04T05:14:50.124830Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.38056, train_acc 0.1657, valid_acc 0.2737, Time 00:00:56,lr 0.1\n",
      "epoch 1, loss 1.72326, train_acc 0.3529, valid_acc 0.3568, Time 00:00:57,lr 0.1\n",
      "epoch 2, loss 1.41782, train_acc 0.4814, valid_acc 0.5539, Time 00:00:56,lr 0.1\n",
      "epoch 3, loss 1.18895, train_acc 0.5769, valid_acc 0.4499, Time 00:00:56,lr 0.1\n",
      "epoch 4, loss 1.06284, train_acc 0.6269, valid_acc 0.6079, Time 00:00:56,lr 0.1\n",
      "epoch 5, loss 0.94214, train_acc 0.6724, valid_acc 0.6066, Time 00:00:56,lr 0.1\n",
      "epoch 6, loss 0.88145, train_acc 0.6946, valid_acc 0.5705, Time 00:00:57,lr 0.1\n",
      "epoch 7, loss 0.84403, train_acc 0.7099, valid_acc 0.5912, Time 00:00:56,lr 0.1\n",
      "epoch 8, loss 0.81196, train_acc 0.7204, valid_acc 0.7287, Time 00:00:56,lr 0.1\n",
      "epoch 9, loss 0.79473, train_acc 0.7280, valid_acc 0.6951, Time 00:00:56,lr 0.1\n",
      "epoch 10, loss 0.78191, train_acc 0.7316, valid_acc 0.6971, Time 00:00:56,lr 0.1\n",
      "epoch 11, loss 0.77675, train_acc 0.7329, valid_acc 0.7301, Time 00:00:56,lr 0.1\n",
      "epoch 12, loss 0.75665, train_acc 0.7406, valid_acc 0.6933, Time 00:00:56,lr 0.1\n",
      "epoch 13, loss 0.75395, train_acc 0.7421, valid_acc 0.6948, Time 00:00:56,lr 0.1\n",
      "epoch 14, loss 0.74696, train_acc 0.7435, valid_acc 0.7115, Time 00:00:56,lr 0.1\n",
      "epoch 15, loss 0.73717, train_acc 0.7483, valid_acc 0.6977, Time 00:00:56,lr 0.1\n",
      "epoch 16, loss 0.73225, train_acc 0.7511, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 17, loss 0.73271, train_acc 0.7495, valid_acc 0.7008, Time 00:00:56,lr 0.1\n",
      "epoch 18, loss 0.72847, train_acc 0.7509, valid_acc 0.6897, Time 00:00:56,lr 0.1\n",
      "epoch 19, loss 0.72694, train_acc 0.7517, valid_acc 0.5876, Time 00:00:56,lr 0.1\n",
      "epoch 20, loss 0.72430, train_acc 0.7523, valid_acc 0.7296, Time 00:00:56,lr 0.1\n",
      "epoch 21, loss 0.71836, train_acc 0.7536, valid_acc 0.6899, Time 00:00:56,lr 0.1\n",
      "epoch 22, loss 0.71967, train_acc 0.7531, valid_acc 0.6885, Time 00:00:56,lr 0.1\n",
      "epoch 23, loss 0.71953, train_acc 0.7555, valid_acc 0.7130, Time 00:00:56,lr 0.1\n",
      "epoch 24, loss 0.70939, train_acc 0.7559, valid_acc 0.6943, Time 00:00:56,lr 0.1\n",
      "epoch 25, loss 0.71292, train_acc 0.7573, valid_acc 0.7349, Time 00:00:56,lr 0.1\n",
      "epoch 26, loss 0.71025, train_acc 0.7568, valid_acc 0.7351, Time 00:00:56,lr 0.1\n",
      "epoch 27, loss 0.71876, train_acc 0.7551, valid_acc 0.7187, Time 00:00:56,lr 0.1\n",
      "epoch 28, loss 0.70902, train_acc 0.7583, valid_acc 0.7208, Time 00:00:56,lr 0.1\n",
      "epoch 29, loss 0.70804, train_acc 0.7554, valid_acc 0.7308, Time 00:00:56,lr 0.1\n",
      "epoch 30, loss 0.69986, train_acc 0.7599, valid_acc 0.7021, Time 00:00:56,lr 0.1\n",
      "epoch 31, loss 0.70725, train_acc 0.7590, valid_acc 0.7294, Time 00:00:56,lr 0.1\n",
      "epoch 32, loss 0.70756, train_acc 0.7607, valid_acc 0.7474, Time 00:00:56,lr 0.1\n",
      "epoch 33, loss 0.70504, train_acc 0.7586, valid_acc 0.7125, Time 00:00:56,lr 0.1\n",
      "epoch 34, loss 0.70649, train_acc 0.7594, valid_acc 0.6955, Time 00:00:56,lr 0.1\n",
      "epoch 35, loss 0.70067, train_acc 0.7614, valid_acc 0.7556, Time 00:00:56,lr 0.1\n",
      "epoch 36, loss 0.70107, train_acc 0.7616, valid_acc 0.5762, Time 00:00:56,lr 0.1\n",
      "epoch 37, loss 0.69916, train_acc 0.7612, valid_acc 0.7656, Time 00:00:56,lr 0.1\n",
      "epoch 38, loss 0.70580, train_acc 0.7588, valid_acc 0.6723, Time 00:00:56,lr 0.1\n",
      "epoch 39, loss 0.69736, train_acc 0.7607, valid_acc 0.7138, Time 00:00:56,lr 0.1\n",
      "epoch 40, loss 0.70295, train_acc 0.7603, valid_acc 0.7250, Time 00:00:56,lr 0.1\n",
      "epoch 41, loss 0.69599, train_acc 0.7622, valid_acc 0.6537, Time 00:00:57,lr 0.1\n",
      "epoch 42, loss 0.70267, train_acc 0.7622, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 43, loss 0.70145, train_acc 0.7609, valid_acc 0.6835, Time 00:00:56,lr 0.1\n",
      "epoch 44, loss 0.69758, train_acc 0.7642, valid_acc 0.6921, Time 00:00:56,lr 0.1\n",
      "epoch 45, loss 0.69759, train_acc 0.7621, valid_acc 0.7180, Time 00:00:56,lr 0.1\n",
      "epoch 46, loss 0.69861, train_acc 0.7599, valid_acc 0.6791, Time 00:00:56,lr 0.1\n",
      "epoch 47, loss 0.69245, train_acc 0.7646, valid_acc 0.7376, Time 00:00:58,lr 0.1\n",
      "epoch 48, loss 0.70034, train_acc 0.7613, valid_acc 0.6330, Time 00:01:02,lr 0.1\n",
      "epoch 49, loss 0.69684, train_acc 0.7638, valid_acc 0.7106, Time 00:00:59,lr 0.1\n",
      "epoch 50, loss 0.69907, train_acc 0.7642, valid_acc 0.7437, Time 00:01:02,lr 0.1\n",
      "epoch 51, loss 0.69313, train_acc 0.7632, valid_acc 0.7042, Time 00:01:03,lr 0.1\n",
      "epoch 52, loss 0.69796, train_acc 0.7624, valid_acc 0.6378, Time 00:00:56,lr 0.1\n",
      "epoch 53, loss 0.69387, train_acc 0.7634, valid_acc 0.7631, Time 00:00:56,lr 0.1\n",
      "epoch 54, loss 0.69179, train_acc 0.7639, valid_acc 0.6815, Time 00:00:56,lr 0.1\n",
      "epoch 55, loss 0.70082, train_acc 0.7620, valid_acc 0.7479, Time 00:00:56,lr 0.1\n",
      "epoch 56, loss 0.69588, train_acc 0.7631, valid_acc 0.7191, Time 00:00:56,lr 0.1\n",
      "epoch 57, loss 0.69733, train_acc 0.7639, valid_acc 0.7269, Time 00:00:57,lr 0.1\n",
      "epoch 58, loss 0.69333, train_acc 0.7659, valid_acc 0.6627, Time 00:00:56,lr 0.1\n",
      "epoch 59, loss 0.69576, train_acc 0.7629, valid_acc 0.7450, Time 00:00:56,lr 0.1\n",
      "epoch 60, loss 0.69494, train_acc 0.7634, valid_acc 0.7199, Time 00:00:56,lr 0.1\n",
      "epoch 61, loss 0.69445, train_acc 0.7622, valid_acc 0.7363, Time 00:00:56,lr 0.1\n",
      "epoch 62, loss 0.69379, train_acc 0.7620, valid_acc 0.7078, Time 00:00:56,lr 0.1\n",
      "epoch 63, loss 0.69461, train_acc 0.7638, valid_acc 0.7163, Time 00:00:56,lr 0.1\n",
      "epoch 64, loss 0.69229, train_acc 0.7644, valid_acc 0.7119, Time 00:00:56,lr 0.1\n",
      "epoch 65, loss 0.69231, train_acc 0.7630, valid_acc 0.7010, Time 00:00:56,lr 0.1\n",
      "epoch 66, loss 0.69897, train_acc 0.7628, valid_acc 0.6015, Time 00:01:00,lr 0.1\n",
      "epoch 67, loss 0.69971, train_acc 0.7618, valid_acc 0.7291, Time 00:01:00,lr 0.1\n",
      "epoch 68, loss 0.69992, train_acc 0.7627, valid_acc 0.6869, Time 00:00:59,lr 0.1\n",
      "epoch 69, loss 0.69482, train_acc 0.7612, valid_acc 0.7649, Time 00:01:03,lr 0.1\n",
      "epoch 70, loss 0.69304, train_acc 0.7629, valid_acc 0.6619, Time 00:00:57,lr 0.1\n",
      "epoch 71, loss 0.69240, train_acc 0.7654, valid_acc 0.7104, Time 00:00:56,lr 0.1\n",
      "epoch 72, loss 0.69880, train_acc 0.7608, valid_acc 0.7013, Time 00:00:56,lr 0.1\n",
      "epoch 73, loss 0.69195, train_acc 0.7630, valid_acc 0.6779, Time 00:00:56,lr 0.1\n",
      "epoch 74, loss 0.69141, train_acc 0.7646, valid_acc 0.6973, Time 00:00:59,lr 0.1\n",
      "epoch 75, loss 0.68878, train_acc 0.7666, valid_acc 0.6534, Time 00:00:59,lr 0.1\n",
      "epoch 76, loss 0.69063, train_acc 0.7644, valid_acc 0.6998, Time 00:01:00,lr 0.1\n",
      "epoch 77, loss 0.69243, train_acc 0.7637, valid_acc 0.7247, Time 00:00:56,lr 0.1\n",
      "epoch 78, loss 0.70374, train_acc 0.7610, valid_acc 0.6934, Time 00:00:57,lr 0.1\n",
      "epoch 79, loss 0.69154, train_acc 0.7635, valid_acc 0.7347, Time 00:01:01,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = [80]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T08:28:52.313297Z",
     "start_time": "2018-03-04T06:33:29.684980Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.58195, train_acc 0.8029, valid_acc 0.7700, Time 00:00:52,lr 0.05\n",
      "epoch 1, loss 0.65821, train_acc 0.7764, valid_acc 0.7590, Time 00:00:55,lr 0.05\n",
      "epoch 2, loss 0.68045, train_acc 0.7693, valid_acc 0.4827, Time 00:00:56,lr 0.05\n",
      "epoch 3, loss 0.68091, train_acc 0.7667, valid_acc 0.7464, Time 00:00:56,lr 0.05\n",
      "epoch 4, loss 0.68455, train_acc 0.7671, valid_acc 0.7237, Time 00:00:56,lr 0.05\n",
      "epoch 5, loss 0.68415, train_acc 0.7694, valid_acc 0.7485, Time 00:00:57,lr 0.05\n",
      "epoch 6, loss 0.68346, train_acc 0.7678, valid_acc 0.5743, Time 00:00:58,lr 0.05\n",
      "epoch 7, loss 0.68119, train_acc 0.7682, valid_acc 0.6967, Time 00:00:56,lr 0.05\n",
      "epoch 8, loss 0.68399, train_acc 0.7670, valid_acc 0.6504, Time 00:00:58,lr 0.05\n",
      "epoch 9, loss 0.68451, train_acc 0.7676, valid_acc 0.6430, Time 00:00:57,lr 0.05\n",
      "epoch 10, loss 0.68556, train_acc 0.7673, valid_acc 0.6783, Time 00:00:59,lr 0.05\n",
      "epoch 11, loss 0.68058, train_acc 0.7669, valid_acc 0.6660, Time 00:00:57,lr 0.05\n",
      "epoch 12, loss 0.69183, train_acc 0.7643, valid_acc 0.6486, Time 00:00:57,lr 0.05\n",
      "epoch 13, loss 0.68374, train_acc 0.7672, valid_acc 0.7040, Time 00:00:57,lr 0.05\n",
      "epoch 14, loss 0.68587, train_acc 0.7647, valid_acc 0.7557, Time 00:00:58,lr 0.05\n",
      "epoch 15, loss 0.68559, train_acc 0.7704, valid_acc 0.6823, Time 00:00:57,lr 0.05\n",
      "epoch 16, loss 0.68928, train_acc 0.7640, valid_acc 0.6444, Time 00:00:59,lr 0.05\n",
      "epoch 17, loss 0.68680, train_acc 0.7662, valid_acc 0.5039, Time 00:00:58,lr 0.05\n",
      "epoch 18, loss 0.68851, train_acc 0.7660, valid_acc 0.6354, Time 00:01:01,lr 0.05\n",
      "epoch 19, loss 0.68628, train_acc 0.7667, valid_acc 0.6202, Time 00:00:59,lr 0.05\n",
      "epoch 20, loss 0.69103, train_acc 0.7665, valid_acc 0.7290, Time 00:01:01,lr 0.05\n",
      "epoch 21, loss 0.68566, train_acc 0.7666, valid_acc 0.6471, Time 00:00:59,lr 0.05\n",
      "epoch 22, loss 0.69282, train_acc 0.7654, valid_acc 0.7272, Time 00:01:01,lr 0.05\n",
      "epoch 23, loss 0.68336, train_acc 0.7706, valid_acc 0.6115, Time 00:01:02,lr 0.05\n",
      "epoch 24, loss 0.68217, train_acc 0.7695, valid_acc 0.7322, Time 00:01:00,lr 0.05\n",
      "epoch 25, loss 0.68338, train_acc 0.7686, valid_acc 0.7349, Time 00:01:01,lr 0.05\n",
      "epoch 26, loss 0.68748, train_acc 0.7667, valid_acc 0.7034, Time 00:01:04,lr 0.05\n",
      "epoch 27, loss 0.69171, train_acc 0.7651, valid_acc 0.7070, Time 00:00:59,lr 0.05\n",
      "epoch 28, loss 0.68284, train_acc 0.7680, valid_acc 0.7419, Time 00:00:57,lr 0.05\n",
      "epoch 29, loss 0.68182, train_acc 0.7672, valid_acc 0.6750, Time 00:00:57,lr 0.05\n",
      "epoch 30, loss 0.43710, train_acc 0.8514, valid_acc 0.8458, Time 00:00:56,lr 0.01\n",
      "epoch 31, loss 0.38423, train_acc 0.8686, valid_acc 0.8636, Time 00:00:56,lr 0.01\n",
      "epoch 32, loss 0.38351, train_acc 0.8686, valid_acc 0.8521, Time 00:01:01,lr 0.01\n",
      "epoch 33, loss 0.38735, train_acc 0.8679, valid_acc 0.8384, Time 00:01:00,lr 0.01\n",
      "epoch 34, loss 0.38082, train_acc 0.8711, valid_acc 0.8641, Time 00:00:56,lr 0.01\n",
      "epoch 35, loss 0.38738, train_acc 0.8672, valid_acc 0.8248, Time 00:00:55,lr 0.01\n",
      "epoch 36, loss 0.38062, train_acc 0.8709, valid_acc 0.8618, Time 00:00:55,lr 0.01\n",
      "epoch 37, loss 0.38049, train_acc 0.8718, valid_acc 0.8291, Time 00:00:57,lr 0.01\n",
      "epoch 38, loss 0.37551, train_acc 0.8724, valid_acc 0.8629, Time 00:00:58,lr 0.01\n",
      "epoch 39, loss 0.37463, train_acc 0.8717, valid_acc 0.8566, Time 00:01:00,lr 0.01\n",
      "epoch 40, loss 0.37176, train_acc 0.8730, valid_acc 0.8559, Time 00:01:00,lr 0.01\n",
      "epoch 41, loss 0.36386, train_acc 0.8775, valid_acc 0.8362, Time 00:00:56,lr 0.01\n",
      "epoch 42, loss 0.36582, train_acc 0.8751, valid_acc 0.8740, Time 00:00:56,lr 0.01\n",
      "epoch 43, loss 0.36425, train_acc 0.8759, valid_acc 0.8561, Time 00:00:56,lr 0.01\n",
      "epoch 44, loss 0.35750, train_acc 0.8785, valid_acc 0.8442, Time 00:00:56,lr 0.01\n",
      "epoch 45, loss 0.35708, train_acc 0.8781, valid_acc 0.8600, Time 00:00:57,lr 0.01\n",
      "epoch 46, loss 0.35498, train_acc 0.8784, valid_acc 0.8639, Time 00:00:56,lr 0.01\n",
      "epoch 47, loss 0.35540, train_acc 0.8789, valid_acc 0.8602, Time 00:00:56,lr 0.01\n",
      "epoch 48, loss 0.35173, train_acc 0.8803, valid_acc 0.8466, Time 00:00:56,lr 0.01\n",
      "epoch 49, loss 0.35126, train_acc 0.8813, valid_acc 0.8660, Time 00:00:56,lr 0.01\n",
      "epoch 50, loss 0.34843, train_acc 0.8821, valid_acc 0.8473, Time 00:00:56,lr 0.01\n",
      "epoch 51, loss 0.34383, train_acc 0.8816, valid_acc 0.8726, Time 00:00:57,lr 0.01\n",
      "epoch 52, loss 0.34999, train_acc 0.8809, valid_acc 0.8602, Time 00:01:01,lr 0.01\n",
      "epoch 53, loss 0.34417, train_acc 0.8835, valid_acc 0.8419, Time 00:00:56,lr 0.01\n",
      "epoch 54, loss 0.34483, train_acc 0.8828, valid_acc 0.8654, Time 00:00:56,lr 0.01\n",
      "epoch 55, loss 0.34106, train_acc 0.8856, valid_acc 0.8673, Time 00:00:56,lr 0.01\n",
      "epoch 56, loss 0.34146, train_acc 0.8839, valid_acc 0.8758, Time 00:01:01,lr 0.01\n",
      "epoch 57, loss 0.34200, train_acc 0.8854, valid_acc 0.8456, Time 00:00:58,lr 0.01\n",
      "epoch 58, loss 0.34399, train_acc 0.8828, valid_acc 0.8639, Time 00:00:58,lr 0.01\n",
      "epoch 59, loss 0.34053, train_acc 0.8847, valid_acc 0.8597, Time 00:00:58,lr 0.01\n",
      "epoch 60, loss 0.20173, train_acc 0.9333, valid_acc 0.9198, Time 00:00:56,lr 0.002\n",
      "epoch 61, loss 0.15869, train_acc 0.9455, valid_acc 0.9215, Time 00:00:56,lr 0.002\n",
      "epoch 62, loss 0.14265, train_acc 0.9519, valid_acc 0.9222, Time 00:00:56,lr 0.002\n",
      "epoch 63, loss 0.13336, train_acc 0.9559, valid_acc 0.9245, Time 00:00:58,lr 0.002\n",
      "epoch 64, loss 0.12715, train_acc 0.9568, valid_acc 0.9178, Time 00:00:56,lr 0.002\n",
      "epoch 65, loss 0.12213, train_acc 0.9591, valid_acc 0.9210, Time 00:00:58,lr 0.002\n",
      "epoch 66, loss 0.11907, train_acc 0.9592, valid_acc 0.9224, Time 00:00:57,lr 0.002\n",
      "epoch 67, loss 0.11596, train_acc 0.9613, valid_acc 0.9201, Time 00:00:56,lr 0.002\n",
      "epoch 68, loss 0.11954, train_acc 0.9596, valid_acc 0.9202, Time 00:00:56,lr 0.002\n",
      "epoch 69, loss 0.11487, train_acc 0.9614, valid_acc 0.9212, Time 00:00:56,lr 0.002\n",
      "epoch 70, loss 0.11803, train_acc 0.9598, valid_acc 0.9105, Time 00:00:56,lr 0.002\n",
      "epoch 71, loss 0.11419, train_acc 0.9616, valid_acc 0.9124, Time 00:00:56,lr 0.002\n",
      "epoch 72, loss 0.11577, train_acc 0.9608, valid_acc 0.9108, Time 00:00:56,lr 0.002\n",
      "epoch 73, loss 0.11624, train_acc 0.9604, valid_acc 0.9160, Time 00:00:56,lr 0.002\n",
      "epoch 74, loss 0.11776, train_acc 0.9603, valid_acc 0.9185, Time 00:00:56,lr 0.002\n",
      "epoch 75, loss 0.11450, train_acc 0.9622, valid_acc 0.9117, Time 00:00:56,lr 0.002\n",
      "epoch 76, loss 0.11900, train_acc 0.9613, valid_acc 0.9097, Time 00:01:00,lr 0.002\n",
      "epoch 77, loss 0.12027, train_acc 0.9592, valid_acc 0.9142, Time 00:00:57,lr 0.002\n",
      "epoch 78, loss 0.12142, train_acc 0.9593, valid_acc 0.9121, Time 00:00:56,lr 0.002\n",
      "epoch 79, loss 0.12121, train_acc 0.9592, valid_acc 0.9147, Time 00:00:57,lr 0.002\n",
      "epoch 80, loss 0.12328, train_acc 0.9580, valid_acc 0.9057, Time 00:00:57,lr 0.002\n",
      "epoch 81, loss 0.12675, train_acc 0.9566, valid_acc 0.9083, Time 00:00:56,lr 0.002\n",
      "epoch 82, loss 0.11973, train_acc 0.9583, valid_acc 0.9099, Time 00:00:58,lr 0.002\n",
      "epoch 83, loss 0.12241, train_acc 0.9586, valid_acc 0.9161, Time 00:00:55,lr 0.002\n",
      "epoch 84, loss 0.11883, train_acc 0.9593, valid_acc 0.9097, Time 00:00:56,lr 0.002\n",
      "epoch 85, loss 0.12814, train_acc 0.9568, valid_acc 0.9161, Time 00:00:56,lr 0.002\n",
      "epoch 86, loss 0.11971, train_acc 0.9590, valid_acc 0.9075, Time 00:00:55,lr 0.002\n",
      "epoch 87, loss 0.12493, train_acc 0.9573, valid_acc 0.9143, Time 00:00:56,lr 0.002\n",
      "epoch 88, loss 0.11748, train_acc 0.9599, valid_acc 0.9079, Time 00:01:00,lr 0.002\n",
      "epoch 89, loss 0.12178, train_acc 0.9585, valid_acc 0.9043, Time 00:00:55,lr 0.002\n",
      "epoch 90, loss 0.06359, train_acc 0.9809, valid_acc 0.9297, Time 00:00:56,lr 0.0004\n",
      "epoch 91, loss 0.04226, train_acc 0.9878, valid_acc 0.9347, Time 00:00:55,lr 0.0004\n",
      "epoch 92, loss 0.03643, train_acc 0.9901, valid_acc 0.9339, Time 00:00:56,lr 0.0004\n",
      "epoch 93, loss 0.03193, train_acc 0.9917, valid_acc 0.9344, Time 00:00:55,lr 0.0004\n",
      "epoch 94, loss 0.03020, train_acc 0.9919, valid_acc 0.9354, Time 00:00:56,lr 0.0004\n",
      "epoch 95, loss 0.02928, train_acc 0.9922, valid_acc 0.9342, Time 00:00:55,lr 0.0004\n",
      "epoch 96, loss 0.02602, train_acc 0.9928, valid_acc 0.9355, Time 00:00:55,lr 0.0004\n",
      "epoch 97, loss 0.02409, train_acc 0.9935, valid_acc 0.9350, Time 00:00:55,lr 0.0004\n",
      "epoch 98, loss 0.02386, train_acc 0.9939, valid_acc 0.9329, Time 00:00:56,lr 0.0004\n",
      "epoch 99, loss 0.01987, train_acc 0.9951, valid_acc 0.9358, Time 00:00:56,lr 0.0004\n",
      "epoch 100, loss 0.02084, train_acc 0.9946, valid_acc 0.9337, Time 00:00:57,lr 0.0004\n",
      "epoch 101, loss 0.01851, train_acc 0.9954, valid_acc 0.9360, Time 00:00:58,lr 0.0004\n",
      "epoch 102, loss 0.01870, train_acc 0.9954, valid_acc 0.9357, Time 00:00:56,lr 0.0004\n",
      "epoch 103, loss 0.01807, train_acc 0.9956, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 104, loss 0.01849, train_acc 0.9954, valid_acc 0.9336, Time 00:00:56,lr 0.0004\n",
      "epoch 105, loss 0.01738, train_acc 0.9961, valid_acc 0.9343, Time 00:00:57,lr 0.0004\n",
      "epoch 106, loss 0.01575, train_acc 0.9961, valid_acc 0.9359, Time 00:01:01,lr 0.0004\n",
      "epoch 107, loss 0.01618, train_acc 0.9963, valid_acc 0.9350, Time 00:00:56,lr 0.0004\n",
      "epoch 108, loss 0.01708, train_acc 0.9954, valid_acc 0.9334, Time 00:00:56,lr 0.0004\n",
      "epoch 109, loss 0.01544, train_acc 0.9964, valid_acc 0.9364, Time 00:00:56,lr 0.0004\n",
      "epoch 110, loss 0.01496, train_acc 0.9965, valid_acc 0.9346, Time 00:00:57,lr 0.0004\n",
      "epoch 111, loss 0.01420, train_acc 0.9966, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 112, loss 0.01472, train_acc 0.9962, valid_acc 0.9342, Time 00:00:56,lr 0.0004\n",
      "epoch 113, loss 0.01399, train_acc 0.9968, valid_acc 0.9355, Time 00:00:57,lr 0.0004\n",
      "epoch 114, loss 0.01392, train_acc 0.9969, valid_acc 0.9344, Time 00:00:57,lr 0.0004\n",
      "epoch 115, loss 0.01281, train_acc 0.9975, valid_acc 0.9342, Time 00:00:58,lr 0.0004\n",
      "epoch 116, loss 0.01356, train_acc 0.9968, valid_acc 0.9359, Time 00:00:56,lr 0.0004\n",
      "epoch 117, loss 0.01356, train_acc 0.9971, valid_acc 0.9362, Time 00:00:58,lr 0.0004\n",
      "epoch 118, loss 0.01320, train_acc 0.9970, valid_acc 0.9353, Time 00:00:59,lr 0.0004\n",
      "epoch 119, loss 0.01289, train_acc 0.9971, valid_acc 0.9349, Time 00:01:00,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 120\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = [30, 60, 90]\n",
    "lr_decay=0.2\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.load_params(\"../../models/resnet18_v1_80e_aug_3x3_no_max_pool\", ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "net.save_params('../../models/resnet18_v1_9_3x3_no_max_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 general lr policy train resnet18_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:18:01.823Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v1_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v1_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 general lr policy train resnet18_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:18:05.213Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resnet18_v2_3x3_no_maxpool(pretrained=True):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        resnet = model.resnet18_v2(pretrained=pretrained, ctx=ctx)\n",
    "        conv1 = nn.Conv2D(64, kernel_size=3, strides=1, padding=1, use_bias=False)\n",
    "        output = nn.Dense(10)\n",
    "        net.add(resnet.features[0])\n",
    "        net.add(conv1)\n",
    "        net.add(*resnet.features[2:4])\n",
    "        net.add(*resnet.features[5:])\n",
    "        net.add(output)\n",
    "\n",
    "    net.hybridize()\n",
    "    if pretrained:\n",
    "        conv1.initialize(ctx=ctx)\n",
    "        output.initialize(ctx=ctx)\n",
    "    else:\n",
    "        net.initialize(ctx=ctx)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:18:07.512Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = get_resnet18_v2_3x3_no_maxpool()\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_v2_200e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 try delay pooling resnet arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:24:34.961Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyResNet18_delay_downsample(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1),\n",
    "                   nn.BatchNorm(),\n",
    "                   nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(5):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(1):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print 'Block %d output %s' % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-04T09:24:35.841Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [80, 150]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA1, num_workers=4)\n",
    "net = MyResNet18_delay_downsample(10)\n",
    "net.initialize(ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, \n",
    "      [], log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_522_e200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. use rewrite train function to train function to make sure not train code bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T17:35:08.794769Z",
     "start_time": "2018-03-03T17:35:08.750443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def get_lr(e, i, step):\n",
    "    if e >= step[i]:\n",
    "        return i + 1\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, lr_decay, wd, ctx, output_file=None, step={}):\n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "        print \" # valid_acc\", valid_acc\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    step_i = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if epoch > 0:\n",
    "            if len(step.keys()) == 0:\n",
    "                if epoch % lr_period == 0:\n",
    "                    trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            else:\n",
    "                if epoch >= step.keys()[step_i]:\n",
    "                    step_i = step_i + 1\n",
    "                    trainer.set_learning_rate(step[step.keys()[step_i]])\n",
    "        \n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            \n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += utils.accuracy(output, label)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "        \n",
    "        nd.waitall()\n",
    "        \n",
    "    if output_file != sys.stdout:\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:17:26.291786Z",
     "start_time": "2018-03-03T17:35:08.796140Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.100838658147\n",
      "epoch 0, loss 2.17650, train_acc 0.1815, valid_acc 0.3233, Time 00:00:29,lr 0.1\n",
      "epoch 1, loss 1.74745, train_acc 0.3639, valid_acc 0.2111, Time 00:00:31,lr 0.1\n",
      "epoch 2, loss 1.48485, train_acc 0.4729, valid_acc 0.5586, Time 00:00:31,lr 0.1\n",
      "epoch 3, loss 1.32675, train_acc 0.5326, valid_acc 0.5165, Time 00:00:31,lr 0.1\n",
      "epoch 4, loss 1.23933, train_acc 0.5676, valid_acc 0.4877, Time 00:00:31,lr 0.1\n",
      "epoch 5, loss 1.18796, train_acc 0.5838, valid_acc 0.5885, Time 00:00:31,lr 0.1\n",
      "epoch 6, loss 1.15784, train_acc 0.5958, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 7, loss 1.12028, train_acc 0.6103, valid_acc 0.6090, Time 00:00:31,lr 0.1\n",
      "epoch 8, loss 1.11800, train_acc 0.6141, valid_acc 0.6106, Time 00:00:31,lr 0.1\n",
      "epoch 9, loss 1.09796, train_acc 0.6208, valid_acc 0.6072, Time 00:00:31,lr 0.1\n",
      "epoch 10, loss 1.08225, train_acc 0.6251, valid_acc 0.5205, Time 00:00:31,lr 0.1\n",
      "epoch 11, loss 1.07237, train_acc 0.6293, valid_acc 0.5745, Time 00:00:31,lr 0.1\n",
      "epoch 12, loss 1.06935, train_acc 0.6309, valid_acc 0.6407, Time 00:00:31,lr 0.1\n",
      "epoch 13, loss 1.05872, train_acc 0.6334, valid_acc 0.5826, Time 00:00:31,lr 0.1\n",
      "epoch 14, loss 1.04889, train_acc 0.6376, valid_acc 0.6672, Time 00:00:31,lr 0.1\n",
      "epoch 15, loss 1.05632, train_acc 0.6361, valid_acc 0.6499, Time 00:00:31,lr 0.1\n",
      "epoch 16, loss 1.05060, train_acc 0.6386, valid_acc 0.6274, Time 00:00:31,lr 0.1\n",
      "epoch 17, loss 1.04430, train_acc 0.6395, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 18, loss 1.04184, train_acc 0.6421, valid_acc 0.6441, Time 00:00:31,lr 0.1\n",
      "epoch 19, loss 1.04526, train_acc 0.6398, valid_acc 0.5606, Time 00:00:31,lr 0.1\n",
      "epoch 20, loss 1.04432, train_acc 0.6404, valid_acc 0.6367, Time 00:00:31,lr 0.1\n",
      "epoch 21, loss 1.03036, train_acc 0.6456, valid_acc 0.7069, Time 00:00:31,lr 0.1\n",
      "epoch 22, loss 1.03184, train_acc 0.6459, valid_acc 0.6150, Time 00:00:31,lr 0.1\n",
      "epoch 23, loss 1.03162, train_acc 0.6419, valid_acc 0.6809, Time 00:00:31,lr 0.1\n",
      "epoch 24, loss 1.03152, train_acc 0.6440, valid_acc 0.6096, Time 00:00:31,lr 0.1\n",
      "epoch 25, loss 1.02944, train_acc 0.6447, valid_acc 0.6779, Time 00:00:31,lr 0.1\n",
      "epoch 26, loss 1.02975, train_acc 0.6446, valid_acc 0.5822, Time 00:00:31,lr 0.1\n",
      "epoch 27, loss 1.02655, train_acc 0.6464, valid_acc 0.6957, Time 00:00:31,lr 0.1\n",
      "epoch 28, loss 1.02281, train_acc 0.6490, valid_acc 0.6738, Time 00:00:31,lr 0.1\n",
      "epoch 29, loss 1.02572, train_acc 0.6455, valid_acc 0.6794, Time 00:00:31,lr 0.1\n",
      "epoch 30, loss 1.01937, train_acc 0.6497, valid_acc 0.5929, Time 00:00:31,lr 0.1\n",
      "epoch 31, loss 1.01610, train_acc 0.6499, valid_acc 0.5739, Time 00:00:31,lr 0.1\n",
      "epoch 32, loss 1.02582, train_acc 0.6454, valid_acc 0.6808, Time 00:00:31,lr 0.1\n",
      "epoch 33, loss 1.02411, train_acc 0.6468, valid_acc 0.7068, Time 00:00:31,lr 0.1\n",
      "epoch 34, loss 1.01952, train_acc 0.6492, valid_acc 0.6673, Time 00:00:31,lr 0.1\n",
      "epoch 35, loss 1.02947, train_acc 0.6447, valid_acc 0.6387, Time 00:00:31,lr 0.1\n",
      "epoch 36, loss 1.01511, train_acc 0.6521, valid_acc 0.5688, Time 00:00:31,lr 0.1\n",
      "epoch 37, loss 1.02304, train_acc 0.6474, valid_acc 0.6549, Time 00:00:31,lr 0.1\n",
      "epoch 38, loss 1.01642, train_acc 0.6496, valid_acc 0.6774, Time 00:00:31,lr 0.1\n",
      "epoch 39, loss 1.01767, train_acc 0.6498, valid_acc 0.5900, Time 00:00:31,lr 0.1\n",
      "epoch 40, loss 1.01177, train_acc 0.6501, valid_acc 0.6799, Time 00:00:31,lr 0.1\n",
      "epoch 41, loss 1.01933, train_acc 0.6482, valid_acc 0.6359, Time 00:00:31,lr 0.1\n",
      "epoch 42, loss 1.01835, train_acc 0.6475, valid_acc 0.6245, Time 00:00:31,lr 0.1\n",
      "epoch 43, loss 1.02311, train_acc 0.6466, valid_acc 0.6745, Time 00:00:31,lr 0.1\n",
      "epoch 44, loss 1.01084, train_acc 0.6527, valid_acc 0.6729, Time 00:00:31,lr 0.1\n",
      "epoch 45, loss 1.01739, train_acc 0.6508, valid_acc 0.6601, Time 00:00:31,lr 0.1\n",
      "epoch 46, loss 1.02528, train_acc 0.6453, valid_acc 0.6812, Time 00:00:31,lr 0.1\n",
      "epoch 47, loss 1.01437, train_acc 0.6516, valid_acc 0.6579, Time 00:00:31,lr 0.1\n",
      "epoch 48, loss 1.02278, train_acc 0.6477, valid_acc 0.6893, Time 00:00:31,lr 0.1\n",
      "epoch 49, loss 1.01775, train_acc 0.6479, valid_acc 0.7160, Time 00:00:31,lr 0.1\n",
      "epoch 50, loss 1.01944, train_acc 0.6485, valid_acc 0.6020, Time 00:00:31,lr 0.1\n",
      "epoch 51, loss 1.02941, train_acc 0.6463, valid_acc 0.6422, Time 00:00:31,lr 0.1\n",
      "epoch 52, loss 1.01799, train_acc 0.6497, valid_acc 0.7113, Time 00:00:31,lr 0.1\n",
      "epoch 53, loss 1.01612, train_acc 0.6477, valid_acc 0.6327, Time 00:00:31,lr 0.1\n",
      "epoch 54, loss 1.02105, train_acc 0.6492, valid_acc 0.6325, Time 00:00:31,lr 0.1\n",
      "epoch 55, loss 1.02410, train_acc 0.6450, valid_acc 0.6888, Time 00:00:31,lr 0.1\n",
      "epoch 56, loss 1.01827, train_acc 0.6495, valid_acc 0.6665, Time 00:00:31,lr 0.1\n",
      "epoch 57, loss 1.01920, train_acc 0.6502, valid_acc 0.6775, Time 00:00:31,lr 0.1\n",
      "epoch 58, loss 1.02319, train_acc 0.6464, valid_acc 0.6765, Time 00:00:31,lr 0.1\n",
      "epoch 59, loss 1.01978, train_acc 0.6507, valid_acc 0.5037, Time 00:00:31,lr 0.1\n",
      "epoch 60, loss 1.02047, train_acc 0.6486, valid_acc 0.6431, Time 00:00:31,lr 0.1\n",
      "epoch 61, loss 1.01323, train_acc 0.6509, valid_acc 0.6778, Time 00:00:31,lr 0.1\n",
      "epoch 62, loss 1.01677, train_acc 0.6475, valid_acc 0.6488, Time 00:00:31,lr 0.1\n",
      "epoch 63, loss 1.02140, train_acc 0.6477, valid_acc 0.6320, Time 00:00:31,lr 0.1\n",
      "epoch 64, loss 1.01649, train_acc 0.6486, valid_acc 0.6985, Time 00:00:31,lr 0.1\n",
      "epoch 65, loss 1.01495, train_acc 0.6494, valid_acc 0.6110, Time 00:00:31,lr 0.1\n",
      "epoch 66, loss 1.02517, train_acc 0.6482, valid_acc 0.6281, Time 00:00:31,lr 0.1\n",
      "epoch 67, loss 1.01754, train_acc 0.6505, valid_acc 0.6933, Time 00:00:31,lr 0.1\n",
      "epoch 68, loss 1.01817, train_acc 0.6487, valid_acc 0.5997, Time 00:00:31,lr 0.1\n",
      "epoch 69, loss 1.02060, train_acc 0.6480, valid_acc 0.6438, Time 00:00:31,lr 0.1\n",
      "epoch 70, loss 1.01663, train_acc 0.6501, valid_acc 0.6557, Time 00:00:31,lr 0.1\n",
      "epoch 71, loss 1.01802, train_acc 0.6506, valid_acc 0.6037, Time 00:00:31,lr 0.1\n",
      "epoch 72, loss 1.02473, train_acc 0.6465, valid_acc 0.6869, Time 00:00:31,lr 0.1\n",
      "epoch 73, loss 1.02214, train_acc 0.6481, valid_acc 0.6365, Time 00:00:31,lr 0.1\n",
      "epoch 74, loss 1.02075, train_acc 0.6494, valid_acc 0.6727, Time 00:00:31,lr 0.1\n",
      "epoch 75, loss 1.02465, train_acc 0.6455, valid_acc 0.6005, Time 00:00:31,lr 0.1\n",
      "epoch 76, loss 1.01342, train_acc 0.6509, valid_acc 0.6316, Time 00:00:31,lr 0.1\n",
      "epoch 77, loss 1.02116, train_acc 0.6466, valid_acc 0.6909, Time 00:00:31,lr 0.1\n",
      "epoch 78, loss 1.02252, train_acc 0.6469, valid_acc 0.7035, Time 00:00:31,lr 0.1\n",
      "epoch 79, loss 1.01862, train_acc 0.6496, valid_acc 0.7132, Time 00:00:31,lr 0.1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = 80\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)\n",
    "\n",
    "net.save_params(\"../../models/resnet18_me_80e_aug2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:33.999408Z",
     "start_time": "2018-03-03T18:17:26.293466Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # valid_acc 0.713158945687\n",
      "epoch 0, loss 0.89016, train_acc 0.6943, valid_acc 0.6973, Time 00:00:29,lr 0.05\n",
      "epoch 1, loss 0.97071, train_acc 0.6649, valid_acc 0.6228, Time 00:00:31,lr 0.05\n",
      "epoch 2, loss 0.97855, train_acc 0.6619, valid_acc 0.5962, Time 00:00:31,lr 0.05\n",
      "epoch 3, loss 0.98917, train_acc 0.6604, valid_acc 0.6651, Time 00:00:31,lr 0.05\n",
      "epoch 4, loss 0.98467, train_acc 0.6614, valid_acc 0.6855, Time 00:00:31,lr 0.05\n",
      "epoch 5, loss 0.98934, train_acc 0.6582, valid_acc 0.6544, Time 00:00:31,lr 0.05\n",
      "epoch 6, loss 0.99133, train_acc 0.6577, valid_acc 0.6867, Time 00:00:31,lr 0.05\n",
      "epoch 7, loss 0.99419, train_acc 0.6565, valid_acc 0.6996, Time 00:00:31,lr 0.05\n",
      "epoch 8, loss 0.99295, train_acc 0.6574, valid_acc 0.6707, Time 00:00:31,lr 0.05\n",
      "epoch 9, loss 0.98785, train_acc 0.6583, valid_acc 0.6599, Time 00:00:31,lr 0.05\n",
      "epoch 10, loss 0.72795, train_acc 0.7497, valid_acc 0.7739, Time 00:00:31,lr 0.01\n",
      "epoch 11, loss 0.67062, train_acc 0.7695, valid_acc 0.7807, Time 00:00:31,lr 0.01\n",
      "epoch 12, loss 0.66014, train_acc 0.7741, valid_acc 0.8119, Time 00:00:31,lr 0.01\n",
      "epoch 13, loss 0.66186, train_acc 0.7753, valid_acc 0.8148, Time 00:00:31,lr 0.01\n",
      "epoch 14, loss 0.66272, train_acc 0.7717, valid_acc 0.7799, Time 00:00:31,lr 0.01\n",
      "epoch 15, loss 0.64879, train_acc 0.7773, valid_acc 0.8168, Time 00:00:31,lr 0.01\n",
      "epoch 16, loss 0.65313, train_acc 0.7758, valid_acc 0.8103, Time 00:00:31,lr 0.01\n",
      "epoch 17, loss 0.64368, train_acc 0.7803, valid_acc 0.8027, Time 00:00:31,lr 0.01\n",
      "epoch 18, loss 0.64573, train_acc 0.7790, valid_acc 0.7865, Time 00:00:31,lr 0.01\n",
      "epoch 19, loss 0.63965, train_acc 0.7808, valid_acc 0.8113, Time 00:00:31,lr 0.01\n",
      "epoch 20, loss 0.49768, train_acc 0.8307, valid_acc 0.8597, Time 00:00:31,lr 0.002\n",
      "epoch 21, loss 0.45085, train_acc 0.8470, valid_acc 0.8730, Time 00:00:31,lr 0.002\n",
      "epoch 22, loss 0.43494, train_acc 0.8515, valid_acc 0.8760, Time 00:00:31,lr 0.002\n",
      "epoch 23, loss 0.42184, train_acc 0.8574, valid_acc 0.8801, Time 00:00:31,lr 0.002\n",
      "epoch 24, loss 0.41728, train_acc 0.8585, valid_acc 0.8719, Time 00:00:31,lr 0.002\n",
      "epoch 25, loss 0.41803, train_acc 0.8566, valid_acc 0.8713, Time 00:00:31,lr 0.002\n",
      "epoch 26, loss 0.41157, train_acc 0.8586, valid_acc 0.8734, Time 00:00:31,lr 0.002\n",
      "epoch 27, loss 0.40844, train_acc 0.8605, valid_acc 0.8761, Time 00:00:31,lr 0.002\n",
      "epoch 28, loss 0.40713, train_acc 0.8602, valid_acc 0.8781, Time 00:00:31,lr 0.002\n",
      "epoch 29, loss 0.40274, train_acc 0.8636, valid_acc 0.8810, Time 00:00:31,lr 0.002\n",
      "epoch 30, loss 0.34908, train_acc 0.8807, valid_acc 0.8912, Time 00:00:31,lr 0.0004\n",
      "epoch 31, loss 0.32949, train_acc 0.8883, valid_acc 0.8973, Time 00:00:31,lr 0.0004\n",
      "epoch 32, loss 0.31813, train_acc 0.8929, valid_acc 0.8957, Time 00:00:31,lr 0.0004\n",
      "epoch 33, loss 0.31530, train_acc 0.8933, valid_acc 0.8949, Time 00:00:31,lr 0.0004\n",
      "epoch 34, loss 0.30938, train_acc 0.8967, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 35, loss 0.30692, train_acc 0.8977, valid_acc 0.8967, Time 00:00:31,lr 0.0004\n",
      "epoch 36, loss 0.30172, train_acc 0.8962, valid_acc 0.8979, Time 00:00:31,lr 0.0004\n",
      "epoch 37, loss 0.29754, train_acc 0.9001, valid_acc 0.8978, Time 00:00:31,lr 0.0004\n",
      "epoch 38, loss 0.29589, train_acc 0.8995, valid_acc 0.8993, Time 00:00:31,lr 0.0004\n",
      "epoch 39, loss 0.29325, train_acc 0.9014, valid_acc 0.9010, Time 00:00:31,lr 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-3\n",
    "lr_period = 10\n",
    "lr_decay=0.2\n",
    "log_file=None\n",
    "\n",
    "train_data, valid_data = data_loader(batch_size, transform_train_DA2, num_workers=4)\n",
    "net = get_net(ctx)\n",
    "net.load_params(\"../../models/resnet18_me_80e_aug2\", ctx=ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T18:38:34.023597Z",
     "start_time": "2018-03-03T18:38:34.000689Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.save_params('../../models/resnet18_me_9_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
