{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:12.021724Z",
     "start_time": "2017-11-05T03:46:11.585655Z"
    },
    "collapsed": true
   },
   "source": [
    "# 1. import needed package and set global env\n",
    "\n",
    "you need to download CIFAR10 dataset from kaggle and then extract to \\${data_dir}, (such as \"/root/Workspace/data/CIFAR10_kaggle/\")<br/>\n",
    "\\${data_dir} should like this(dir \"train_valid_test\" will generate in data prepared model):</br>\n",
    "![](../../_image/data_dir.png)<br/>\n",
    "and then set variable\n",
    "```\n",
    "set var data_dir=${data_dir}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.insert(0, '../../utils')\n",
    "from netlib import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ctx = mx.gpu(1)\n",
    "\n",
    "data_dir = \"/root/Workspace/data/CIFAR10_kaggle/\"\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "label_file = \"trainLabels.csv\"\n",
    "input_dir = 'train_valid_test/'\n",
    "valid_ratio = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "force_recreate_dir = False\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data prepared(need not run this block repeatly)\n",
    "if you have do dataset prepared and generate 'train_valid_test' dir to orgnize data, just ignore this block(that mean you need not run this model and just skip it)<br/>\n",
    "\n",
    "the step may cost about 40~60min.<br/>\n",
    "\n",
    "so to avoid run this model repeatly, we check the created dir, and if exist will not create again, if you want to recreate them, set force_recreate_dir=True.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:12.087726Z",
     "start_time": "2017-11-05T03:46:12.036394Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_valid_test dir has been created and if you want to recreate them set force_recreate=True(normally you needn't to do it.)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data prepare, reference to http://zh.gluon.ai/chapter_computer-vision/kaggle-gluon-cifar10.html\n",
    "\"\"\"\n",
    "def reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio):\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict((int(idx), label)for idx, label in tokens)\n",
    "    labels = set(idx_label.values())\n",
    "    print labels\n",
    "    \n",
    "    num_train = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    num_train_tuning = int(num_train * (1 - valid_ratio))\n",
    "    assert 0 < num_train_tuning < num_train\n",
    "    num_train_tuning_per_label = num_train_tuning // len(labels)\n",
    "    label_count = dict()\n",
    "    \n",
    "    def mkdir_if_not_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "    \n",
    "    # copy to create train set and valid set and train_valid set (train/label, valid/label, train_valid/label)\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = int(train_file.split('.')[0])\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                  os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label_count.get(label, 0) < num_train_tuning_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                       os.path.join(data_dir, input_dir, 'train', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                       os.path.join(data_dir, input_dir, 'valid', label))\n",
    "    \n",
    "    # copy to create test set (test/unkown)\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unkown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file), \n",
    "                   os.path.join(data_dir, input_dir, 'test', 'unkown'))\n",
    "\n",
    "\n",
    "if force_recreate_dir==False and os.path.exists(data_dir + input_dir) and os.path.exists(data_dir + input_dir + train_dir) and os.path.exists(data_dir + input_dir + test_dir):\n",
    "    print \"train_valid_test dir has been created and if you want to recreate them\",\n",
    "    print \"set force_recreate=True(normally you needn't to do it.)\"\n",
    "else:\n",
    "    reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data loader, data argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:12.804734Z",
     "start_time": "2017-11-05T03:46:12.785031Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data loader\n",
    "\"\"\"\n",
    "def _transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def data_loader(batch_size, transform_train, transform_test=None):\n",
    "    if transform_train is None:\n",
    "        transform_train = _transform_train\n",
    "    if transform_test is None:\n",
    "        transform_test = _transform_test\n",
    "        \n",
    "    # flag=1 mean 3 channel image\n",
    "    train_ds = vision.ImageFolderDataset(data_dir + \"/\" + input_dir + '/train', flag=1, transform=transform_train)\n",
    "    valid_ds = vision.ImageFolderDataset(data_dir + \"/\" + input_dir + '/valid', flag=1, transform=transform_test)\n",
    "    train_valid_ds = vision.ImageFolderDataset(data_dir + \"/\" + input_dir + '/train_valid', flag=1, transform=transform_train)\n",
    "    test_ds = vision.ImageFolderDataset(data_dir + \"/\" + input_dir + \"/test\", flag=1, transform=transform_test)\n",
    "\n",
    "    loader = gluon.data.DataLoader\n",
    "    train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "    valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "    train_valid_data = loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "    test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep')\n",
    "    return train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:14.408716Z",
     "start_time": "2017-11-05T03:46:14.361070Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data argument\n",
    "\"\"\"\n",
    "def transform_train_DA1(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def transform_train_DA2(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "    \n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "    \n",
    "\n",
    "random_clip_rate = 0.3\n",
    "def transform_train_DA3(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "#                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "#                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "        \n",
    "    if random.random() > random_clip_rate:\n",
    "        im = im.clip(0, 1)\n",
    "    _aug = image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                   std=np.array([0.2023, 0.1994, 0.2010]),)\n",
    "    im = _aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 mixup\n",
    "1. mixup define\n",
    "2. mixup test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mixup(x1, y1, x2, y2, alpha):\n",
    "    lam = np.random.beta(alpha)\n",
    "    x = lam * x1 + (1 - lam) * x2\n",
    "    y = lam * y1 + (1 - lam) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[[[ 1.13775897  1.38976288  1.3122232  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 0.96329474  1.21529865  1.37037802 ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 0.51744169  0.98267967  1.02144945 ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   ..., \n",
      "   [-0.33549476 -0.66503835 -0.62626851 ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[ 1.1217773   1.47578037  1.43644679 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 0.94477564  1.23977828  1.45611358 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 0.649773    1.04310989  1.06277668 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   ..., \n",
      "   [-0.17623456 -0.56957144 -0.53023773 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[ 0.90025371  1.34899032  1.30996978 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 0.76368165  1.09535658  1.34899032 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 0.51004791  0.93927431  0.93927431 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   ..., \n",
      "   [ 0.00278022 -0.38742557 -0.42644617 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]]]\n",
      "\n",
      "\n",
      " [[[-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   ..., \n",
      "   [-0.70380813 -0.56811374 -0.82011765 ..., -0.80073273 -0.74257797\n",
      "    -0.41303441]\n",
      "   [-0.68442321 -0.93642712 -1.05273664 ..., -0.31610984 -0.72319305\n",
      "    -0.66503835]\n",
      "   [-0.66503835 -1.22720098 -1.13027632 ..., -0.37426457 -0.47118917\n",
      "    -0.95581204]]\n",
      "\n",
      "  [[-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   ..., \n",
      "   [-0.82524037 -0.74657303 -1.02190876 ..., -0.80557352 -1.00224197\n",
      "    -0.64823878]\n",
      "   [-0.76623988 -1.04157567 -1.12024307 ..., -0.21556824 -0.80557352\n",
      "    -0.76623988]\n",
      "   [-0.68757248 -1.23824418 -1.10057616 ..., -0.09756719 -0.33356932\n",
      "    -0.88424093]]\n",
      "\n",
      "  [[-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   ..., \n",
      "   [-0.87518281 -0.75812107 -1.03126526 ..., -0.85567254 -0.91420341\n",
      "    -0.5239976 ]\n",
      "   [-0.85567254 -1.0702858  -1.12881672 ..., -0.38742557 -0.81665194\n",
      "    -0.64105934]\n",
      "   [-0.81665194 -1.28489912 -1.08979607 ..., -0.348405   -0.40693587\n",
      "    -0.79714167]]]\n",
      "\n",
      "\n",
      " [[[-2.4290657  -2.4290657  -2.4290657  ...,  1.23468351  1.06021929\n",
      "     0.98267967]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -0.33549476 -0.12226067\n",
      "    -0.41303441]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ...,  0.98267967  1.00206459\n",
      "     0.80821544]\n",
      "   ..., \n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[-2.41825485 -2.41825485 -2.41825485 ...,  1.00377619  1.10211039\n",
      "     1.37744617]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -0.2352351   0.15810193\n",
      "     0.05976756]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ...,  1.53478098  1.53478098\n",
      "     1.29877877]\n",
      "   ..., \n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[-2.22139311 -2.22139311 -2.22139311 ..., -0.05575065 -0.05575065\n",
      "     0.10033166]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -1.48000193 -1.18734753\n",
      "    -1.32391965]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ...,  0.10033166  0.06131108\n",
      "    -0.11428153]\n",
      "   ..., \n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 1.9519254   1.89377069  1.91315567 ...,  1.89377069 -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 2.0294652   1.97131038  1.99069524 ...,  1.93254054 -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 2.06823492  2.0294652   2.06823492 ...,  1.97131038 -2.4290657\n",
      "    -2.4290657 ]\n",
      "   ..., \n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[ 2.02645206  1.96745145  1.98711836 ...,  1.96745145 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 2.10511947  2.04611897  2.06578565 ...,  2.00678515 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 2.14445305  2.10511947  2.14445305 ...,  2.04611897 -2.41825485\n",
      "    -2.41825485]\n",
      "   ..., \n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[ 2.18793273  2.12940192  2.14891219 ...,  2.12940192 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 2.26597404  2.20744324  2.22695351 ...,  2.16842246 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 2.30499458  2.26597404  2.30499458 ...,  2.20744324 -2.22139311\n",
      "    -2.22139311]\n",
      "   ..., \n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]]]\n",
      "\n",
      "\n",
      " [[[ 0.71129084  1.11837411  1.69992149 ...,  0.69190592  0.65313607\n",
      "    -2.4290657 ]\n",
      "   [ 0.86637014  1.38976288  1.64176679 ...,  0.65313607  0.80821544\n",
      "    -2.4290657 ]\n",
      "   [ 0.80821544  1.4479177   1.40914786 ...,  1.11837411  1.48668742\n",
      "    -2.4290657 ]\n",
      "   ..., \n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[ 0.29576984  0.66943985  1.41677988 ...,  1.25944519  1.41677988\n",
      "    -2.41825485]\n",
      "   [ 0.17776877  0.63010615  0.98410934 ...,  1.10211039  1.49544728\n",
      "    -2.41825485]\n",
      "   [ 0.15810193  0.649773    0.66943985 ...,  1.47578037  2.02645206\n",
      "    -2.41825485]\n",
      "   ..., \n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[-0.75812107 -0.28987414  0.74417138 ...,  0.95878458  1.19290805\n",
      "    -2.22139311]\n",
      "   [-1.08979607 -0.68007994 -0.09477124 ...,  0.82221252  1.29045951\n",
      "    -2.22139311]\n",
      "   [-1.28489912 -0.81665194 -0.5239976  ...,  1.23192859  1.79772699\n",
      "    -2.22139311]\n",
      "   ..., \n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]]]\n",
      "\n",
      "\n",
      " [[[ 1.75807631  1.73869133  1.69992149 ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 1.46730256  1.38976288  1.66115177 ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 1.27345335  1.52545726  1.89377069 ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   ..., \n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[ 1.71178257  1.67244887  1.63311517 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 1.41677988  1.33811247  1.61344826 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 1.22011149  1.45611358  1.82978356 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   ..., \n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[ 1.64164472  1.52458298  1.42703152 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 1.29045951  1.15388751  1.38801098 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 1.13437724  1.40752125  1.81723738 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   ..., \n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]]]]\n",
      "<NDArray 128x3x32x32 @cpu(0)> \n",
      "[[ 2.]\n",
      " [ 6.]\n",
      " [ 6.]\n",
      " [ 0.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [ 0.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 9.]\n",
      " [ 4.]\n",
      " [ 6.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 4.]\n",
      " [ 1.]\n",
      " [ 8.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 6.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 0.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 9.]\n",
      " [ 6.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 5.]\n",
      " [ 0.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 0.]\n",
      " [ 6.]\n",
      " [ 0.]\n",
      " [ 7.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 1.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 7.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [ 5.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 2.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 9.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 8.]\n",
      " [ 0.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 7.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 9.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 7.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 0.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 9.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 1.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 4.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 8.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 0.]\n",
      " [ 7.]\n",
      " [ 8.]]\n",
      "<NDArray 128x1 @cpu(0)>\n",
      "\n",
      "[[[[-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   ..., \n",
      "   [ 0.82760036  1.02144945 -0.10287576 ..., -0.12226067 -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 0.38174728  0.7694456  -0.74257797 ..., -1.42105019 -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-0.12226067 -0.21918526 -1.03335166 ..., -2.21583176 -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   ..., \n",
      "   [ 0.39410403  0.49243826 -0.53023773 ..., -0.60890514 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-0.11723403  0.33510351 -1.02190876 ..., -1.59224725 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-0.45157036 -0.47123721 -1.21857727 ..., -2.14291906 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   ..., \n",
      "   [ 0.51004791  0.49053761 -0.40693587 ..., -0.3679153  -2.22139311\n",
      "    -2.22139311]\n",
      "   [-0.17281239  0.37347588 -0.79714167 ..., -0.93371373 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-0.348405   -0.17281239 -0.7776314  ..., -1.16783726 -2.22139311\n",
      "    -2.22139311]]]\n",
      "\n",
      "\n",
      " [[[-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   ..., \n",
      "   [ 1.38976288  0.49805677  0.07158863 ...,  2.28146911 -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 0.86637014  0.24605286  0.20728303 ...,  2.28146911 -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 0.59498137  0.16851321  1.33160818 ...,  2.28146911 -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   ..., \n",
      "   [ 1.43644679  0.59077245  0.27610299 ...,  2.34112144 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 0.90544194  0.33510351  0.33510351 ...,  2.34112144 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 0.63010615  0.19743562  1.35777938 ...,  2.34112144 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   ..., \n",
      "   [ 1.71968591  0.80270225  0.43200675 ...,  2.61715913 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 1.19290805  0.58808905  0.54906851 ...,  2.61715913 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 0.91976398  0.49053761  1.64164472 ...,  2.61715913 -2.22139311\n",
      "    -2.22139311]]]\n",
      "\n",
      "\n",
      " [[[-2.4290657   1.81623101  1.93254054 ...,  1.64176679  1.66115177\n",
      "     1.69992149]\n",
      "   [-2.4290657   1.81623101  1.93254054 ...,  1.60299695  1.60299695\n",
      "     1.64176679]\n",
      "   [-2.4290657   1.79684615  1.91315567 ...,  1.54484224  1.5642271\n",
      "     1.60299695]\n",
      "   ..., \n",
      "   [-2.4290657   1.73869133  1.71930647 ...,  1.62238193  1.48668742\n",
      "     1.38976288]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[-2.41825485  1.77078307  1.88878417 ...,  1.73144937  1.73144937\n",
      "     1.77078307]\n",
      "   [-2.41825485  1.77078307  1.88878417 ...,  1.69211566  1.69211566\n",
      "     1.73144937]\n",
      "   [-2.41825485  1.77078307  1.88878417 ...,  1.65278196  1.69211566\n",
      "     1.71178257]\n",
      "   ..., \n",
      "   [-2.41825485  1.82978356  1.81011677 ...,  1.73144937  1.63311517\n",
      "     1.55444777]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[-2.22139311  1.23192859  1.32948005 ...,  1.09535658  1.11486685\n",
      "     1.15388751]\n",
      "   [-2.22139311  1.23192859  1.34899032 ...,  0.99780518  0.99780518\n",
      "     1.03682578]\n",
      "   [-2.22139311  1.23192859  1.32948005 ...,  0.93927431  0.97829485\n",
      "     0.99780518]\n",
      "   ..., \n",
      "   [-2.22139311  1.32948005  1.29045951 ...,  1.15388751  1.07584631\n",
      "     1.01731539]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.9245249   1.13775897  1.11837411 ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 0.86637014  1.06021929  1.13775897 ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [ 0.9245249   1.21529865  1.19591379 ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   ..., \n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[ 1.18077779  1.39711308  1.37744617 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 1.1217773   1.31844568  1.39711308 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [ 1.16111088  1.47578037  1.45611358 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   ..., \n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[ 1.19290805  1.40752125  1.38801098 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 1.15388751  1.32948005  1.40752125 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [ 1.17339778  1.48556244  1.46605217 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   ..., \n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]]]\n",
      "\n",
      "\n",
      " [[[-2.4290657  -2.4290657  -2.4290657  ...,  2.39777851  2.1457746\n",
      "     1.5060724 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ...,  2.49470305  1.38976288\n",
      "    -0.56811374]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ...,  2.51408792  1.4479177\n",
      "    -0.27733999]\n",
      "   ..., \n",
      "   [-2.4290657  -2.4290657  -2.4290657  ...,  0.88575506  0.78883052\n",
      "     0.88575506]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ...,  0.82760036  0.65313607\n",
      "     0.63375115]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[-2.41825485 -2.41825485 -2.41825485 ...,  2.45912266  2.20345354\n",
      "     1.59378147]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ...,  2.45912266  1.41677988\n",
      "    -0.33356932]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ...,  2.41978884  1.51511407\n",
      "    -0.15656772]\n",
      "   ..., \n",
      "   [-2.41825485 -2.41825485 -2.41825485 ...,  1.20044458  1.18077779\n",
      "     1.23977828]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ...,  1.16111088  1.00377619\n",
      "     1.02344298]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[-2.22139311 -2.22139311 -2.22139311 ..., -0.79714167  0.13935225\n",
      "     0.43200675]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -0.87518281 -0.17281239\n",
      "    -0.50448734]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -0.83616227 -0.05575065\n",
      "    -0.6020388 ]\n",
      "   ..., \n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -0.25085357 -0.40693587\n",
      "    -0.21183297]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -0.42644617 -0.62154907\n",
      "    -0.46546674]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]]]\n",
      "\n",
      "\n",
      " [[[ 1.00206459  1.11837411  1.21529865 ...,  0.9245249   0.9245249\n",
      "     0.98267967]\n",
      "   [ 1.21529865  1.33160818  1.27345335 ...,  0.94390982  0.94390982\n",
      "     1.00206459]\n",
      "   [ 1.21529865  1.19591379  1.21529865 ...,  1.19591379  1.21529865\n",
      "     1.27345335]\n",
      "   ..., \n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]\n",
      "   [-2.4290657  -2.4290657  -2.4290657  ..., -2.4290657  -2.4290657\n",
      "    -2.4290657 ]]\n",
      "\n",
      "  [[ 1.41677988  1.53478098  1.63311517 ...,  1.41677988  1.39711308\n",
      "     1.45611358]\n",
      "   [ 1.63311517  1.75111628  1.69211566 ...,  1.47578037  1.45611358\n",
      "     1.53478098]\n",
      "   [ 1.65278196  1.63311517  1.65278196 ...,  1.71178257  1.71178257\n",
      "     1.81011677]\n",
      "   ..., \n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]\n",
      "   [-2.41825485 -2.41825485 -2.41825485 ..., -2.41825485 -2.41825485\n",
      "    -2.41825485]]\n",
      "\n",
      "  [[ 1.81723738  1.93429911  2.03185058 ...,  1.73919618  1.71968591\n",
      "     1.79772699]\n",
      "   [ 2.01234031  2.12940192  2.07087111 ...,  1.73919618  1.73919618\n",
      "     1.81723738]\n",
      "   [ 1.95380938  1.95380938  1.97331965 ...,  1.99282992  2.01234031\n",
      "     2.09038138]\n",
      "   ..., \n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]\n",
      "   [-2.22139311 -2.22139311 -2.22139311 ..., -2.22139311 -2.22139311\n",
      "    -2.22139311]]]]\n",
      "<NDArray 128x3x32x32 @cpu(0)> \n",
      "[[ 3.]\n",
      " [ 0.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 9.]\n",
      " [ 6.]\n",
      " [ 5.]\n",
      " [ 1.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [ 4.]\n",
      " [ 8.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 0.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 0.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 8.]\n",
      " [ 4.]\n",
      " [ 8.]\n",
      " [ 3.]\n",
      " [ 0.]\n",
      " [ 2.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 2.]\n",
      " [ 5.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 6.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 9.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 2.]\n",
      " [ 6.]\n",
      " [ 2.]\n",
      " [ 9.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 7.]\n",
      " [ 2.]\n",
      " [ 5.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [ 1.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [ 0.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 8.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 4.]\n",
      " [ 9.]\n",
      " [ 2.]\n",
      " [ 7.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 9.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 6.]\n",
      " [ 2.]\n",
      " [ 7.]\n",
      " [ 3.]\n",
      " [ 9.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 0.]\n",
      " [ 9.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 6.]\n",
      " [ 2.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [ 3.]\n",
      " [ 6.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 9.]\n",
      " [ 5.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 4.]]\n",
      "<NDArray 128x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as model\n",
    "batch_size = 128\n",
    "transform_train = transform_train_DA1\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "mixup_train_data, _, _, _, _, _ = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 1\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "\n",
    "for x, y in train_data:\n",
    "    print x, y\n",
    "    for x, y in mixup_train_data:\n",
    "        print x, y\n",
    "        break\n",
    "    break\n",
    "# for (x1, y1), (x2, y2) in zip(train_data, mixup_train_data):\n",
    "#     print x1.shape, y1.shape\n",
    "    #data, label = mixup(x1, y1, x2, y2, mixup_alpha)\n",
    "#     label = label.as_in_context(ctx)\n",
    "#     with autograd.record():\n",
    "#         output = net(data.as_in_context(ctx))\n",
    "#         loss = loss_f(output, label)\n",
    "#     loss.backward()\n",
    "#     trainer.step(data.shape[0])\n",
    "#     print \"loss\", nd.mean(loss).asscalar()\n",
    "#     print \"acc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:15.590464Z",
     "start_time": "2017-11-05T03:46:15.513162Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def abs_mean(W):\n",
    "    return nd.mean(nd.abs(W)).asscalar()\n",
    "\n",
    "def in_list(e, l):\n",
    "    for i in l:\n",
    "        if i == e:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, \n",
    "          lr_decay, wd, ctx, w_key, output_file=None, verbose=False, loss_f=gluon.loss.SoftmaxCrossEntropyLoss(), \n",
    "          mixup_train_data=None, mixup_alpha=0.2):\n",
    "    def train_batch(data, label):\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        _loss = nd.mean(loss).asscalar()\n",
    "        _acc = utils.accuracy(output, label)\n",
    "        train_loss += _loss\n",
    "        train_acc += _acc\n",
    "\n",
    "        if verbose:\n",
    "            print \" # iter\", i,\n",
    "            print \"loss %.5f\" % _loss, \"acc %.5f\" % _acc,\n",
    "            print \"w (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.data()),\n",
    "            print \") g (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.grad()),\n",
    "            print \")\"\n",
    "            i += 1\n",
    "            \n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        stdout = sys.stdout\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = output_file\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    if verbose:\n",
    "        print \" #\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "    \n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if in_list(epoch, lr_period):\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            \n",
    "        if mixup_train_data is None:\n",
    "            for data, label in train_data:\n",
    "                train_batch(data, label)\n",
    "        else:\n",
    "            for (x1, y1), (x2, y2) in zip(train_data, mixup_train_data):\n",
    "                data, label = mixup(x1, y1, x2, y2, mixup_alpha)\n",
    "                train_batch(data, label)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "    if output_file != stdout:\n",
    "        sys.stdout = stdout\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. get net and do EXP\n",
    "the model define in netlib.py, just invoke to create it, and not use any pretrain model<br/>\n",
    "\n",
    "#### note: if you just want to got a submission result, just ignore Exp1~5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp1: res164_v2 + DA1: 0.9529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA1\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [90, 140]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"models/shelock_resnet_orign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp2:res164_v2 + DA2: 0.9527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA2\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train2)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 300\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 225]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/resnet164_e300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp3: res164_v2 + focal loss + DA3: 0.9540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "tranform_train = transform_train_DA3\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = FocalLoss()\n",
    "\n",
    "num_epochs = 255\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 225]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_valid_data, None, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/res164__2_e255_focal_clip_all_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp4: res164_v2 + focal loss + DA3 + only train_data: 0.9506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA3\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = FocalLoss()\n",
    "\n",
    "num_epochs = 255\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 225]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/resnet164_e0-255_focal_clip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp5: sherlock_densenet: 0.9539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA1\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = DenseNet(growthRate=12, depth=100, reduction=0.5, bottleneck=True, nClasses=10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [90, 140]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.hybridize()\n",
    "net.initialize(ctx=ctx)\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/shelock_densenet_orign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. merge result\n",
    "to run this two block, you need at least five model params file, name as follow in model dir.\n",
    "```\n",
    "model_list = ['res164__2_e255_focal_clip_all_data', 'resnet164_e300', 'resnet164_e0-255_focal_clip',\n",
    "              'shelock_densenet_orign', 'shelock_resnet_orign']\n",
    "```\n",
    "you can train from **5. get net and do EXP**, or you can download them from\n",
    "```\n",
    "link: https://pan.baidu.com/s/1pLjzQWj key: f6p3\n",
    "```\n",
    "you can only download this five model param file in model dir, and then run first block to generate middle result in result dir.<br/>\n",
    "or you can download all 8 model param file in models and the middle result file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:21.596302Z",
     "start_time": "2017-11-05T03:46:21.241490Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "generate CIFAR10 output result\n",
    "\"\"\"\n",
    "mkdir_if_not_exist('result/')\n",
    "\n",
    "def save_net_result(net, filename, test_data, ctx):\n",
    "    output = nd.zeros(shape=(300000, 10), ctx=ctx)\n",
    "    for i, (data, label) in enumerate(test_data):\n",
    "        output[i*batch_size:i*batch_size+data.shape[0],:] = net(data.as_in_context(ctx))\n",
    "    nd.save(filename, output)\n",
    "\n",
    "def test_net(data):\n",
    "    return data.reshape((data.shape[0], -1))[:, :10]\n",
    "\n",
    "def save_model_result(model_name, ctx):\n",
    "    net.load_params(\"models/\" + model_name, ctx=ctx)\n",
    "    save_net_result(net, \"result/\" + model_name, test_data, ctx)\n",
    "\n",
    "model_list = ['resnet164_e255_focal_clip', 'res164__2_e255_focal_clip_all_data', 'resnet164_e300','resnet164_e0-255_focal_clip',\n",
    "              'res18_9', \n",
    "              'log_shelock_densenet', 'shelock_densenet_orign',\n",
    "              'shelock_resnet_orign']\n",
    "weight_list = [0.9535, 0.9540, 0.95270, 0.95, 0.93230, 0.9346, 0.9539, 0.95]\n",
    "\n",
    "net = ResNet164_v2(10)\n",
    "for model_name in model_list[:4]:\n",
    "    if not os.path.exists(\"models/\" + model_name): continue\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)\n",
    "        \n",
    "net = ResNet(10)\n",
    "for model_name in model_list[4:5]:\n",
    "    if not os.path.exists(\"models/\" + model_name): continue\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)\n",
    "        \n",
    "net = DenseNet(growthRate=12, depth=100, reduction=0.5, bottleneck=True, nClasses=10)\n",
    "for model_name in model_list[5:7]:\n",
    "    if not os.path.exists(\"models/\" + model_name): continue\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)\n",
    "        \n",
    "net = ResNet164_v2(10)\n",
    "for model_name in model_list[7:]:\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-05T03:47:12.464Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "classfiy test set from generated result\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(128, transform_train_DA1)\n",
    "\n",
    "def mesuare_sum(preds, weight_list=None):\n",
    "    if weight_list is None:\n",
    "        weight_list = [1] * len(preds)\n",
    "    output = preds[0] * weight_list[0]\n",
    "    for i in range(1, len(preds)):\n",
    "        output = output + preds[i] * weight_list[i]\n",
    "    preds = output.argmax(axis=1).astype(int).asnumpy() % 10\n",
    "    return preds\n",
    "\n",
    "def mesuare_softmax_sum(preds, weight_list=None):\n",
    "    if weight_list is None:\n",
    "        weight_list = [1] * len(preds)\n",
    "    output = nd.softmax(preds[0], axis=1) * weight_list[0]\n",
    "    for i in range(1, len(preds)):\n",
    "        output = output + nd.softmax(preds[i], axis=1) * weight_list[i]\n",
    "    preds = output.argmax(axis=1).astype(int).asnumpy() % 10\n",
    "    return preds\n",
    "\n",
    "def mesuare_biggest(preds, weight_list=None):\n",
    "    if weight_list is not None:\n",
    "        for i in range(len(preds)):\n",
    "            preds[i] = preds[i] * weight_list[i]\n",
    "    output = nd.concat(*preds, dim=1)\n",
    "    preds = output.argmax(axis=1).astype(int).asnumpy() % 10\n",
    "    return preds\n",
    "\n",
    "model_list = ['res164__2_e255_focal_clip_all_data', 'resnet164_e300', 'resnet164_e0-255_focal_clip',\n",
    "              'shelock_densenet_orign', 'shelock_resnet_orign']\n",
    "weight_list = [0.9540, 0.95270, 0.95, 0.9539, 0.95]\n",
    "#weight_list=None\n",
    "\n",
    "preds = []\n",
    "for result_name in model_list:\n",
    "    preds.append(nd.load(\"result/\"+result_name)[0].as_in_context(ctx))\n",
    "\n",
    "#preds = mesuare_biggest(preds, weight_list)\n",
    "preds = mesuare_sum(preds, weight_list)\n",
    "#preds = mesuare_softmax_sum(preds, weight_list)\n",
    "\n",
    "sorted_ids = list(range(1, 300000 + 1))\n",
    "sorted_ids.sort(key=lambda x: str(x))\n",
    "\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\n",
    "df.to_csv('submission/concat_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
